{
  "experiment_name": "lr1e-05_rank2",
  "model_config": {
    "hidden_size": 896,
    "num_attention_heads": 14,
    "num_hidden_layers": 24,
    "intermediate_size": 4864,
    "head_dim": 64,
    "vocab_size": 151936,
    "lora_r": 2,
    "lora_target_modules": [
      "q_proj",
      "v_proj"
    ]
  },
  "max_budget": 1e+17,
  "start_time": "2025-03-23 14:00:19",
  "operations": [
    {
      "type": "training",
      "description": "Training step 0",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:31",
      "total_flops_so_far": 1430567036928.0,
      "budget_used_percent": 0.0014305670369280001
    },
    {
      "type": "training",
      "description": "Training step 1",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:33",
      "total_flops_so_far": 2861134073856.0,
      "budget_used_percent": 0.0028611340738560003
    },
    {
      "type": "training",
      "description": "Training step 2",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:33",
      "total_flops_so_far": 4291701110784.0,
      "budget_used_percent": 0.004291701110784
    },
    {
      "type": "training",
      "description": "Training step 3",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:33",
      "total_flops_so_far": 5722268147712.0,
      "budget_used_percent": 0.005722268147712001
    },
    {
      "type": "training",
      "description": "Training step 4",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:33",
      "total_flops_so_far": 7152835184640.0,
      "budget_used_percent": 0.007152835184639999
    },
    {
      "type": "training",
      "description": "Training step 5",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:33",
      "total_flops_so_far": 8583402221568.0,
      "budget_used_percent": 0.008583402221568
    },
    {
      "type": "training",
      "description": "Training step 6",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:33",
      "total_flops_so_far": 10013969258496.0,
      "budget_used_percent": 0.010013969258496
    },
    {
      "type": "training",
      "description": "Training step 7",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:33",
      "total_flops_so_far": 11444536295424.0,
      "budget_used_percent": 0.011444536295424001
    },
    {
      "type": "training",
      "description": "Training step 8",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:33",
      "total_flops_so_far": 12875103332352.0,
      "budget_used_percent": 0.012875103332351999
    },
    {
      "type": "training",
      "description": "Training step 9",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:34",
      "total_flops_so_far": 14305670369280.0,
      "budget_used_percent": 0.014305670369279998
    },
    {
      "type": "training",
      "description": "Training step 10",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:34",
      "total_flops_so_far": 15736237406208.0,
      "budget_used_percent": 0.015736237406208
    },
    {
      "type": "training",
      "description": "Training step 11",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:34",
      "total_flops_so_far": 17166804443136.0,
      "budget_used_percent": 0.017166804443136
    },
    {
      "type": "training",
      "description": "Training step 12",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:34",
      "total_flops_so_far": 18597371480064.0,
      "budget_used_percent": 0.018597371480063997
    },
    {
      "type": "training",
      "description": "Training step 13",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:34",
      "total_flops_so_far": 20027938516992.0,
      "budget_used_percent": 0.020027938516992
    },
    {
      "type": "training",
      "description": "Training step 14",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:34",
      "total_flops_so_far": 21458505553920.0,
      "budget_used_percent": 0.02145850555392
    },
    {
      "type": "training",
      "description": "Training step 15",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:34",
      "total_flops_so_far": 22889072590848.0,
      "budget_used_percent": 0.022889072590848002
    },
    {
      "type": "training",
      "description": "Training step 16",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:34",
      "total_flops_so_far": 24319639627776.0,
      "budget_used_percent": 0.024319639627776002
    },
    {
      "type": "training",
      "description": "Training step 17",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:34",
      "total_flops_so_far": 25750206664704.0,
      "budget_used_percent": 0.025750206664703998
    },
    {
      "type": "training",
      "description": "Training step 18",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:35",
      "total_flops_so_far": 27180773701632.0,
      "budget_used_percent": 0.027180773701631997
    },
    {
      "type": "training",
      "description": "Training step 19",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:35",
      "total_flops_so_far": 28611340738560.0,
      "budget_used_percent": 0.028611340738559997
    },
    {
      "type": "training",
      "description": "Training step 20",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:35",
      "total_flops_so_far": 30041907775488.0,
      "budget_used_percent": 0.030041907775488003
    },
    {
      "type": "training",
      "description": "Training step 21",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:35",
      "total_flops_so_far": 31472474812416.0,
      "budget_used_percent": 0.031472474812416
    },
    {
      "type": "training",
      "description": "Training step 22",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:35",
      "total_flops_so_far": 32903041849344.0,
      "budget_used_percent": 0.032903041849344
    },
    {
      "type": "training",
      "description": "Training step 23",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:35",
      "total_flops_so_far": 34333608886272.0,
      "budget_used_percent": 0.034333608886272
    },
    {
      "type": "training",
      "description": "Training step 24",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:35",
      "total_flops_so_far": 35764175923200.0,
      "budget_used_percent": 0.0357641759232
    },
    {
      "type": "training",
      "description": "Training step 25",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:35",
      "total_flops_so_far": 37194742960128.0,
      "budget_used_percent": 0.037194742960127994
    },
    {
      "type": "training",
      "description": "Training step 26",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:36",
      "total_flops_so_far": 38625309997056.0,
      "budget_used_percent": 0.038625309997056004
    },
    {
      "type": "training",
      "description": "Training step 27",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:36",
      "total_flops_so_far": 40055877033984.0,
      "budget_used_percent": 0.040055877033984
    },
    {
      "type": "training",
      "description": "Training step 28",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:36",
      "total_flops_so_far": 41486444070912.0,
      "budget_used_percent": 0.041486444070912
    },
    {
      "type": "training",
      "description": "Training step 29",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:36",
      "total_flops_so_far": 42917011107840.0,
      "budget_used_percent": 0.04291701110784
    },
    {
      "type": "training",
      "description": "Training step 30",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:36",
      "total_flops_so_far": 44347578144768.0,
      "budget_used_percent": 0.044347578144767995
    },
    {
      "type": "training",
      "description": "Training step 31",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:36",
      "total_flops_so_far": 45778145181696.0,
      "budget_used_percent": 0.045778145181696005
    },
    {
      "type": "training",
      "description": "Training step 32",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:36",
      "total_flops_so_far": 47208712218624.0,
      "budget_used_percent": 0.047208712218624
    },
    {
      "type": "training",
      "description": "Training step 33",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:36",
      "total_flops_so_far": 48639279255552.0,
      "budget_used_percent": 0.048639279255552004
    },
    {
      "type": "training",
      "description": "Training step 34",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:36",
      "total_flops_so_far": 50069846292480.0,
      "budget_used_percent": 0.05006984629248
    },
    {
      "type": "training",
      "description": "Training step 35",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:37",
      "total_flops_so_far": 51500413329408.0,
      "budget_used_percent": 0.051500413329407996
    },
    {
      "type": "training",
      "description": "Training step 36",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:37",
      "total_flops_so_far": 52930980366336.0,
      "budget_used_percent": 0.052930980366336
    },
    {
      "type": "training",
      "description": "Training step 37",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:37",
      "total_flops_so_far": 54361547403264.0,
      "budget_used_percent": 0.054361547403263995
    },
    {
      "type": "training",
      "description": "Training step 38",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:37",
      "total_flops_so_far": 55792114440192.0,
      "budget_used_percent": 0.055792114440192
    },
    {
      "type": "training",
      "description": "Training step 39",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:37",
      "total_flops_so_far": 57222681477120.0,
      "budget_used_percent": 0.057222681477119994
    },
    {
      "type": "training",
      "description": "Training step 40",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:37",
      "total_flops_so_far": 58653248514048.0,
      "budget_used_percent": 0.058653248514048004
    },
    {
      "type": "training",
      "description": "Training step 41",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:37",
      "total_flops_so_far": 60083815550976.0,
      "budget_used_percent": 0.06008381555097601
    },
    {
      "type": "training",
      "description": "Training step 42",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:37",
      "total_flops_so_far": 61514382587904.0,
      "budget_used_percent": 0.061514382587904
    },
    {
      "type": "training",
      "description": "Training step 43",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:38",
      "total_flops_so_far": 62944949624832.0,
      "budget_used_percent": 0.062944949624832
    },
    {
      "type": "training",
      "description": "Training step 44",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:38",
      "total_flops_so_far": 64375516661760.0,
      "budget_used_percent": 0.06437551666176
    },
    {
      "type": "training",
      "description": "Training step 45",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:38",
      "total_flops_so_far": 65806083698688.0,
      "budget_used_percent": 0.065806083698688
    },
    {
      "type": "training",
      "description": "Training step 46",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:38",
      "total_flops_so_far": 67236650735616.0,
      "budget_used_percent": 0.067236650735616
    },
    {
      "type": "training",
      "description": "Training step 47",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:38",
      "total_flops_so_far": 68667217772544.0,
      "budget_used_percent": 0.068667217772544
    },
    {
      "type": "training",
      "description": "Training step 48",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:38",
      "total_flops_so_far": 70097784809472.0,
      "budget_used_percent": 0.070097784809472
    },
    {
      "type": "training",
      "description": "Training step 49",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:38",
      "total_flops_so_far": 71528351846400.0,
      "budget_used_percent": 0.0715283518464
    },
    {
      "type": "training",
      "description": "Training step 50",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:38",
      "total_flops_so_far": 72958918883328.0,
      "budget_used_percent": 0.07295891888332799
    },
    {
      "type": "training",
      "description": "Training step 51",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:38",
      "total_flops_so_far": 74389485920256.0,
      "budget_used_percent": 0.07438948592025599
    },
    {
      "type": "training",
      "description": "Training step 52",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:39",
      "total_flops_so_far": 75820052957184.0,
      "budget_used_percent": 0.075820052957184
    },
    {
      "type": "training",
      "description": "Training step 53",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:39",
      "total_flops_so_far": 77250619994112.0,
      "budget_used_percent": 0.07725061999411201
    },
    {
      "type": "training",
      "description": "Training step 54",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:39",
      "total_flops_so_far": 78681187031040.0,
      "budget_used_percent": 0.07868118703104
    },
    {
      "type": "training",
      "description": "Training step 55",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:39",
      "total_flops_so_far": 80111754067968.0,
      "budget_used_percent": 0.080111754067968
    },
    {
      "type": "training",
      "description": "Training step 56",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:39",
      "total_flops_so_far": 81542321104896.0,
      "budget_used_percent": 0.081542321104896
    },
    {
      "type": "training",
      "description": "Training step 57",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:39",
      "total_flops_so_far": 82972888141824.0,
      "budget_used_percent": 0.082972888141824
    },
    {
      "type": "training",
      "description": "Training step 58",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:39",
      "total_flops_so_far": 84403455178752.0,
      "budget_used_percent": 0.084403455178752
    },
    {
      "type": "training",
      "description": "Training step 59",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:39",
      "total_flops_so_far": 85834022215680.0,
      "budget_used_percent": 0.08583402221568
    },
    {
      "type": "training",
      "description": "Training step 60",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:40",
      "total_flops_so_far": 87264589252608.0,
      "budget_used_percent": 0.087264589252608
    },
    {
      "type": "training",
      "description": "Training step 61",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:40",
      "total_flops_so_far": 88695156289536.0,
      "budget_used_percent": 0.08869515628953599
    },
    {
      "type": "training",
      "description": "Training step 62",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:40",
      "total_flops_so_far": 90125723326464.0,
      "budget_used_percent": 0.090125723326464
    },
    {
      "type": "training",
      "description": "Training step 63",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:40",
      "total_flops_so_far": 91556290363392.0,
      "budget_used_percent": 0.09155629036339201
    },
    {
      "type": "training",
      "description": "Training step 64",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:40",
      "total_flops_so_far": 92986857400320.0,
      "budget_used_percent": 0.09298685740032
    },
    {
      "type": "training",
      "description": "Training step 65",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:40",
      "total_flops_so_far": 94417424437248.0,
      "budget_used_percent": 0.094417424437248
    },
    {
      "type": "training",
      "description": "Training step 66",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:40",
      "total_flops_so_far": 95847991474176.0,
      "budget_used_percent": 0.095847991474176
    },
    {
      "type": "training",
      "description": "Training step 67",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:40",
      "total_flops_so_far": 97278558511104.0,
      "budget_used_percent": 0.09727855851110401
    },
    {
      "type": "training",
      "description": "Training step 68",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:41",
      "total_flops_so_far": 98709125548032.0,
      "budget_used_percent": 0.09870912554803199
    },
    {
      "type": "training",
      "description": "Training step 69",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:41",
      "total_flops_so_far": 100139692584960.0,
      "budget_used_percent": 0.10013969258496
    },
    {
      "type": "training",
      "description": "Training step 70",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:41",
      "total_flops_so_far": 101570259621888.0,
      "budget_used_percent": 0.10157025962188801
    },
    {
      "type": "training",
      "description": "Training step 71",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:41",
      "total_flops_so_far": 103000826658816.0,
      "budget_used_percent": 0.10300082665881599
    },
    {
      "type": "training",
      "description": "Training step 72",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:41",
      "total_flops_so_far": 104431393695744.0,
      "budget_used_percent": 0.104431393695744
    },
    {
      "type": "training",
      "description": "Training step 73",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:41",
      "total_flops_so_far": 105861960732672.0,
      "budget_used_percent": 0.105861960732672
    },
    {
      "type": "training",
      "description": "Training step 74",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:41",
      "total_flops_so_far": 107292527769600.0,
      "budget_used_percent": 0.10729252776960001
    },
    {
      "type": "training",
      "description": "Training step 75",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:41",
      "total_flops_so_far": 108723094806528.0,
      "budget_used_percent": 0.10872309480652799
    },
    {
      "type": "training",
      "description": "Training step 76",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:41",
      "total_flops_so_far": 110153661843456.0,
      "budget_used_percent": 0.110153661843456
    },
    {
      "type": "training",
      "description": "Training step 77",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:42",
      "total_flops_so_far": 111584228880384.0,
      "budget_used_percent": 0.111584228880384
    },
    {
      "type": "training",
      "description": "Training step 78",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:42",
      "total_flops_so_far": 113014795917312.0,
      "budget_used_percent": 0.113014795917312
    },
    {
      "type": "training",
      "description": "Training step 79",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:42",
      "total_flops_so_far": 114445362954240.0,
      "budget_used_percent": 0.11444536295423999
    },
    {
      "type": "training",
      "description": "Training step 80",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:42",
      "total_flops_so_far": 115875929991168.0,
      "budget_used_percent": 0.115875929991168
    },
    {
      "type": "training",
      "description": "Training step 81",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:42",
      "total_flops_so_far": 117306497028096.0,
      "budget_used_percent": 0.11730649702809601
    },
    {
      "type": "training",
      "description": "Training step 82",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:42",
      "total_flops_so_far": 118737064065024.0,
      "budget_used_percent": 0.118737064065024
    },
    {
      "type": "training",
      "description": "Training step 83",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:42",
      "total_flops_so_far": 120167631101952.0,
      "budget_used_percent": 0.12016763110195201
    },
    {
      "type": "training",
      "description": "Training step 84",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:42",
      "total_flops_so_far": 121598198138880.0,
      "budget_used_percent": 0.12159819813888
    },
    {
      "type": "training",
      "description": "Training step 85",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:43",
      "total_flops_so_far": 123028765175808.0,
      "budget_used_percent": 0.123028765175808
    },
    {
      "type": "training",
      "description": "Training step 86",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:43",
      "total_flops_so_far": 124459332212736.0,
      "budget_used_percent": 0.124459332212736
    },
    {
      "type": "training",
      "description": "Training step 87",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:43",
      "total_flops_so_far": 125889899249664.0,
      "budget_used_percent": 0.125889899249664
    },
    {
      "type": "training",
      "description": "Training step 88",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:43",
      "total_flops_so_far": 127320466286592.0,
      "budget_used_percent": 0.127320466286592
    },
    {
      "type": "training",
      "description": "Training step 89",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:43",
      "total_flops_so_far": 128751033323520.0,
      "budget_used_percent": 0.12875103332352
    },
    {
      "type": "training",
      "description": "Training step 90",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:43",
      "total_flops_so_far": 130181600360448.0,
      "budget_used_percent": 0.13018160036044799
    },
    {
      "type": "training",
      "description": "Training step 91",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:43",
      "total_flops_so_far": 131612167397376.0,
      "budget_used_percent": 0.131612167397376
    },
    {
      "type": "training",
      "description": "Training step 92",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:43",
      "total_flops_so_far": 133042734434304.0,
      "budget_used_percent": 0.133042734434304
    },
    {
      "type": "training",
      "description": "Training step 93",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:43",
      "total_flops_so_far": 134473301471232.0,
      "budget_used_percent": 0.134473301471232
    },
    {
      "type": "training",
      "description": "Training step 94",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:44",
      "total_flops_so_far": 135903868508160.0,
      "budget_used_percent": 0.13590386850816
    },
    {
      "type": "training",
      "description": "Training step 95",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:44",
      "total_flops_so_far": 137334435545088.0,
      "budget_used_percent": 0.137334435545088
    },
    {
      "type": "training",
      "description": "Training step 96",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:44",
      "total_flops_so_far": 138765002582016.0,
      "budget_used_percent": 0.13876500258201602
    },
    {
      "type": "training",
      "description": "Training step 97",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:44",
      "total_flops_so_far": 140195569618944.0,
      "budget_used_percent": 0.140195569618944
    },
    {
      "type": "training",
      "description": "Training step 98",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:44",
      "total_flops_so_far": 141626136655872.0,
      "budget_used_percent": 0.141626136655872
    },
    {
      "type": "training",
      "description": "Training step 99",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:44",
      "total_flops_so_far": 143056703692800.0,
      "budget_used_percent": 0.1430567036928
    },
    {
      "type": "training",
      "description": "Training step 100",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:44",
      "total_flops_so_far": 144487270729728.0,
      "budget_used_percent": 0.144487270729728
    },
    {
      "type": "training",
      "description": "Training step 101",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:44",
      "total_flops_so_far": 145917837766656.0,
      "budget_used_percent": 0.14591783776665598
    },
    {
      "type": "training",
      "description": "Training step 102",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:45",
      "total_flops_so_far": 147348404803584.0,
      "budget_used_percent": 0.147348404803584
    },
    {
      "type": "training",
      "description": "Training step 103",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:45",
      "total_flops_so_far": 148778971840512.0,
      "budget_used_percent": 0.14877897184051198
    },
    {
      "type": "training",
      "description": "Training step 104",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:45",
      "total_flops_so_far": 150209538877440.0,
      "budget_used_percent": 0.15020953887743999
    },
    {
      "type": "training",
      "description": "Training step 105",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:45",
      "total_flops_so_far": 151640105914368.0,
      "budget_used_percent": 0.151640105914368
    },
    {
      "type": "training",
      "description": "Training step 106",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:45",
      "total_flops_so_far": 153070672951296.0,
      "budget_used_percent": 0.153070672951296
    },
    {
      "type": "training",
      "description": "Training step 107",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:45",
      "total_flops_so_far": 154501239988224.0,
      "budget_used_percent": 0.15450123998822402
    },
    {
      "type": "training",
      "description": "Training step 108",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:45",
      "total_flops_so_far": 155931807025152.0,
      "budget_used_percent": 0.155931807025152
    },
    {
      "type": "training",
      "description": "Training step 109",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:45",
      "total_flops_so_far": 157362374062080.0,
      "budget_used_percent": 0.15736237406208
    },
    {
      "type": "training",
      "description": "Training step 110",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:46",
      "total_flops_so_far": 158792941099008.0,
      "budget_used_percent": 0.158792941099008
    },
    {
      "type": "training",
      "description": "Training step 111",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:46",
      "total_flops_so_far": 160223508135936.0,
      "budget_used_percent": 0.160223508135936
    },
    {
      "type": "training",
      "description": "Training step 112",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:46",
      "total_flops_so_far": 161654075172864.0,
      "budget_used_percent": 0.16165407517286398
    },
    {
      "type": "training",
      "description": "Training step 113",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:46",
      "total_flops_so_far": 163084642209792.0,
      "budget_used_percent": 0.163084642209792
    },
    {
      "type": "training",
      "description": "Training step 114",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:46",
      "total_flops_so_far": 164515209246720.0,
      "budget_used_percent": 0.16451520924672
    },
    {
      "type": "training",
      "description": "Training step 115",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:46",
      "total_flops_so_far": 165945776283648.0,
      "budget_used_percent": 0.165945776283648
    },
    {
      "type": "training",
      "description": "Training step 116",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:46",
      "total_flops_so_far": 167376343320576.0,
      "budget_used_percent": 0.16737634332057602
    },
    {
      "type": "training",
      "description": "Training step 117",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:46",
      "total_flops_so_far": 168806910357504.0,
      "budget_used_percent": 0.168806910357504
    },
    {
      "type": "training",
      "description": "Training step 118",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:46",
      "total_flops_so_far": 170237477394432.0,
      "budget_used_percent": 0.170237477394432
    },
    {
      "type": "training",
      "description": "Training step 119",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:47",
      "total_flops_so_far": 171668044431360.0,
      "budget_used_percent": 0.17166804443136
    },
    {
      "type": "training",
      "description": "Training step 120",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:47",
      "total_flops_so_far": 173098611468288.0,
      "budget_used_percent": 0.173098611468288
    },
    {
      "type": "training",
      "description": "Training step 121",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:47",
      "total_flops_so_far": 174529178505216.0,
      "budget_used_percent": 0.174529178505216
    },
    {
      "type": "training",
      "description": "Training step 122",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:47",
      "total_flops_so_far": 175959745542144.0,
      "budget_used_percent": 0.175959745542144
    },
    {
      "type": "training",
      "description": "Training step 123",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:47",
      "total_flops_so_far": 177390312579072.0,
      "budget_used_percent": 0.17739031257907198
    },
    {
      "type": "training",
      "description": "Training step 124",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:47",
      "total_flops_so_far": 178820879616000.0,
      "budget_used_percent": 0.178820879616
    },
    {
      "type": "training",
      "description": "Training step 125",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:47",
      "total_flops_so_far": 180251446652928.0,
      "budget_used_percent": 0.180251446652928
    },
    {
      "type": "training",
      "description": "Training step 126",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:47",
      "total_flops_so_far": 181682013689856.0,
      "budget_used_percent": 0.181682013689856
    },
    {
      "type": "training",
      "description": "Training step 127",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:48",
      "total_flops_so_far": 183112580726784.0,
      "budget_used_percent": 0.18311258072678402
    },
    {
      "type": "training",
      "description": "Training step 128",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:48",
      "total_flops_so_far": 184543147763712.0,
      "budget_used_percent": 0.184543147763712
    },
    {
      "type": "training",
      "description": "Training step 129",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:48",
      "total_flops_so_far": 185973714800640.0,
      "budget_used_percent": 0.18597371480064
    },
    {
      "type": "training",
      "description": "Training step 130",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:48",
      "total_flops_so_far": 187404281837568.0,
      "budget_used_percent": 0.187404281837568
    },
    {
      "type": "training",
      "description": "Training step 131",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:48",
      "total_flops_so_far": 188834848874496.0,
      "budget_used_percent": 0.188834848874496
    },
    {
      "type": "training",
      "description": "Training step 132",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:48",
      "total_flops_so_far": 190265415911424.0,
      "budget_used_percent": 0.19026541591142399
    },
    {
      "type": "training",
      "description": "Training step 133",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:48",
      "total_flops_so_far": 191695982948352.0,
      "budget_used_percent": 0.191695982948352
    },
    {
      "type": "training",
      "description": "Training step 134",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:48",
      "total_flops_so_far": 193126549985280.0,
      "budget_used_percent": 0.19312654998528
    },
    {
      "type": "training",
      "description": "Training step 135",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:49",
      "total_flops_so_far": 194557117022208.0,
      "budget_used_percent": 0.19455711702220801
    },
    {
      "type": "training",
      "description": "Training step 136",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:49",
      "total_flops_so_far": 195987684059136.0,
      "budget_used_percent": 0.19598768405913602
    },
    {
      "type": "training",
      "description": "Training step 137",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:49",
      "total_flops_so_far": 197418251096064.0,
      "budget_used_percent": 0.19741825109606398
    },
    {
      "type": "training",
      "description": "Training step 138",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:49",
      "total_flops_so_far": 198848818132992.0,
      "budget_used_percent": 0.198848818132992
    },
    {
      "type": "training",
      "description": "Training step 139",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:49",
      "total_flops_so_far": 200279385169920.0,
      "budget_used_percent": 0.20027938516992
    },
    {
      "type": "training",
      "description": "Training step 140",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:49",
      "total_flops_so_far": 201709952206848.0,
      "budget_used_percent": 0.201709952206848
    },
    {
      "type": "training",
      "description": "Training step 141",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:49",
      "total_flops_so_far": 203140519243776.0,
      "budget_used_percent": 0.20314051924377602
    },
    {
      "type": "training",
      "description": "Training step 142",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:49",
      "total_flops_so_far": 204571086280704.0,
      "budget_used_percent": 0.20457108628070397
    },
    {
      "type": "training",
      "description": "Training step 143",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:50",
      "total_flops_so_far": 206001653317632.0,
      "budget_used_percent": 0.20600165331763198
    },
    {
      "type": "training",
      "description": "Training step 144",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:50",
      "total_flops_so_far": 207432220354560.0,
      "budget_used_percent": 0.20743222035456
    },
    {
      "type": "training",
      "description": "Training step 145",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:50",
      "total_flops_so_far": 208862787391488.0,
      "budget_used_percent": 0.208862787391488
    },
    {
      "type": "training",
      "description": "Training step 146",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:50",
      "total_flops_so_far": 210293354428416.0,
      "budget_used_percent": 0.21029335442841599
    },
    {
      "type": "training",
      "description": "Training step 147",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:50",
      "total_flops_so_far": 211723921465344.0,
      "budget_used_percent": 0.211723921465344
    },
    {
      "type": "training",
      "description": "Training step 148",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:50",
      "total_flops_so_far": 213154488502272.0,
      "budget_used_percent": 0.213154488502272
    },
    {
      "type": "training",
      "description": "Training step 149",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:50",
      "total_flops_so_far": 214585055539200.0,
      "budget_used_percent": 0.21458505553920001
    },
    {
      "type": "training",
      "description": "Training step 150",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:50",
      "total_flops_so_far": 216015622576128.0,
      "budget_used_percent": 0.21601562257612802
    },
    {
      "type": "training",
      "description": "Training step 151",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:50",
      "total_flops_so_far": 217446189613056.0,
      "budget_used_percent": 0.21744618961305598
    },
    {
      "type": "training",
      "description": "Training step 152",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:51",
      "total_flops_so_far": 218876756649984.0,
      "budget_used_percent": 0.218876756649984
    },
    {
      "type": "training",
      "description": "Training step 153",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:51",
      "total_flops_so_far": 220307323686912.0,
      "budget_used_percent": 0.220307323686912
    },
    {
      "type": "training",
      "description": "Training step 154",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:51",
      "total_flops_so_far": 221737890723840.0,
      "budget_used_percent": 0.22173789072384
    },
    {
      "type": "training",
      "description": "Training step 155",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:51",
      "total_flops_so_far": 223168457760768.0,
      "budget_used_percent": 0.223168457760768
    },
    {
      "type": "training",
      "description": "Training step 156",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:51",
      "total_flops_so_far": 224599024797696.0,
      "budget_used_percent": 0.224599024797696
    },
    {
      "type": "training",
      "description": "Training step 157",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:51",
      "total_flops_so_far": 226029591834624.0,
      "budget_used_percent": 0.226029591834624
    },
    {
      "type": "training",
      "description": "Training step 158",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:51",
      "total_flops_so_far": 227460158871552.0,
      "budget_used_percent": 0.22746015887155202
    },
    {
      "type": "training",
      "description": "Training step 159",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:51",
      "total_flops_so_far": 228890725908480.0,
      "budget_used_percent": 0.22889072590847998
    },
    {
      "type": "training",
      "description": "Training step 160",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:52",
      "total_flops_so_far": 230321292945408.0,
      "budget_used_percent": 0.23032129294540798
    },
    {
      "type": "training",
      "description": "Training step 161",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:52",
      "total_flops_so_far": 231751859982336.0,
      "budget_used_percent": 0.231751859982336
    },
    {
      "type": "training",
      "description": "Training step 162",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:52",
      "total_flops_so_far": 233182427019264.0,
      "budget_used_percent": 0.233182427019264
    },
    {
      "type": "training",
      "description": "Training step 163",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:52",
      "total_flops_so_far": 234612994056192.0,
      "budget_used_percent": 0.23461299405619201
    },
    {
      "type": "training",
      "description": "Training step 164",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:52",
      "total_flops_so_far": 236043561093120.0,
      "budget_used_percent": 0.23604356109312
    },
    {
      "type": "training",
      "description": "Training step 165",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:52",
      "total_flops_so_far": 237474128130048.0,
      "budget_used_percent": 0.237474128130048
    },
    {
      "type": "training",
      "description": "Training step 166",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:52",
      "total_flops_so_far": 238904695166976.0,
      "budget_used_percent": 0.23890469516697602
    },
    {
      "type": "training",
      "description": "Training step 167",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:52",
      "total_flops_so_far": 240335262203904.0,
      "budget_used_percent": 0.24033526220390403
    },
    {
      "type": "training",
      "description": "Training step 168",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:53",
      "total_flops_so_far": 241765829240832.0,
      "budget_used_percent": 0.24176582924083198
    },
    {
      "type": "training",
      "description": "Training step 169",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:53",
      "total_flops_so_far": 243196396277760.0,
      "budget_used_percent": 0.24319639627776
    },
    {
      "type": "training",
      "description": "Training step 170",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:53",
      "total_flops_so_far": 244626963314688.0,
      "budget_used_percent": 0.244626963314688
    },
    {
      "type": "training",
      "description": "Training step 171",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:53",
      "total_flops_so_far": 246057530351616.0,
      "budget_used_percent": 0.246057530351616
    },
    {
      "type": "training",
      "description": "Training step 172",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:53",
      "total_flops_so_far": 247488097388544.0,
      "budget_used_percent": 0.247488097388544
    },
    {
      "type": "training",
      "description": "Training step 173",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:53",
      "total_flops_so_far": 248918664425472.0,
      "budget_used_percent": 0.248918664425472
    },
    {
      "type": "training",
      "description": "Training step 174",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:53",
      "total_flops_so_far": 250349231462400.0,
      "budget_used_percent": 0.2503492314624
    },
    {
      "type": "training",
      "description": "Training step 175",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:53",
      "total_flops_so_far": 251779798499328.0,
      "budget_used_percent": 0.251779798499328
    },
    {
      "type": "training",
      "description": "Training step 176",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:54",
      "total_flops_so_far": 253210365536256.0,
      "budget_used_percent": 0.25321036553625603
    },
    {
      "type": "training",
      "description": "Training step 177",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:54",
      "total_flops_so_far": 254640932573184.0,
      "budget_used_percent": 0.254640932573184
    },
    {
      "type": "training",
      "description": "Training step 178",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:54",
      "total_flops_so_far": 256071499610112.0,
      "budget_used_percent": 0.256071499610112
    },
    {
      "type": "training",
      "description": "Training step 179",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:54",
      "total_flops_so_far": 257502066647040.0,
      "budget_used_percent": 0.25750206664704
    },
    {
      "type": "training",
      "description": "Training step 180",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:54",
      "total_flops_so_far": 258932633683968.0,
      "budget_used_percent": 0.258932633683968
    },
    {
      "type": "training",
      "description": "Training step 181",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:54",
      "total_flops_so_far": 260363200720896.0,
      "budget_used_percent": 0.26036320072089597
    },
    {
      "type": "training",
      "description": "Training step 182",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:54",
      "total_flops_so_far": 261793767757824.0,
      "budget_used_percent": 0.261793767757824
    },
    {
      "type": "training",
      "description": "Training step 183",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:54",
      "total_flops_so_far": 263224334794752.0,
      "budget_used_percent": 0.263224334794752
    },
    {
      "type": "training",
      "description": "Training step 184",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:54",
      "total_flops_so_far": 264654901831680.0,
      "budget_used_percent": 0.26465490183168
    },
    {
      "type": "training",
      "description": "Training step 185",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:55",
      "total_flops_so_far": 266085468868608.0,
      "budget_used_percent": 0.266085468868608
    },
    {
      "type": "training",
      "description": "Training step 186",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:55",
      "total_flops_so_far": 267516035905536.0,
      "budget_used_percent": 0.26751603590553596
    },
    {
      "type": "training",
      "description": "Training step 187",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:55",
      "total_flops_so_far": 268946602942464.0,
      "budget_used_percent": 0.268946602942464
    },
    {
      "type": "training",
      "description": "Training step 188",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:55",
      "total_flops_so_far": 270377169979392.0,
      "budget_used_percent": 0.270377169979392
    },
    {
      "type": "training",
      "description": "Training step 189",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:55",
      "total_flops_so_far": 271807737016320.0,
      "budget_used_percent": 0.27180773701632
    },
    {
      "type": "training",
      "description": "Training step 190",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:55",
      "total_flops_so_far": 273238304053248.0,
      "budget_used_percent": 0.273238304053248
    },
    {
      "type": "training",
      "description": "Training step 191",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:55",
      "total_flops_so_far": 274668871090176.0,
      "budget_used_percent": 0.274668871090176
    },
    {
      "type": "training",
      "description": "Training step 192",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:55",
      "total_flops_so_far": 276099438127104.0,
      "budget_used_percent": 0.276099438127104
    },
    {
      "type": "training",
      "description": "Training step 193",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:56",
      "total_flops_so_far": 277530005164032.0,
      "budget_used_percent": 0.27753000516403203
    },
    {
      "type": "training",
      "description": "Training step 194",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:56",
      "total_flops_so_far": 278960572200960.0,
      "budget_used_percent": 0.27896057220096
    },
    {
      "type": "training",
      "description": "Training step 195",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:56",
      "total_flops_so_far": 280391139237888.0,
      "budget_used_percent": 0.280391139237888
    },
    {
      "type": "training",
      "description": "Training step 196",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:56",
      "total_flops_so_far": 281821706274816.0,
      "budget_used_percent": 0.281821706274816
    },
    {
      "type": "training",
      "description": "Training step 197",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:56",
      "total_flops_so_far": 283252273311744.0,
      "budget_used_percent": 0.283252273311744
    },
    {
      "type": "training",
      "description": "Training step 198",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:56",
      "total_flops_so_far": 284682840348672.0,
      "budget_used_percent": 0.28468284034867203
    },
    {
      "type": "training",
      "description": "Training step 199",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:56",
      "total_flops_so_far": 286113407385600.0,
      "budget_used_percent": 0.2861134073856
    },
    {
      "type": "training",
      "description": "Training step 200",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:56",
      "total_flops_so_far": 287543974422528.0,
      "budget_used_percent": 0.287543974422528
    },
    {
      "type": "training",
      "description": "Training step 201",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:57",
      "total_flops_so_far": 288974541459456.0,
      "budget_used_percent": 0.288974541459456
    },
    {
      "type": "training",
      "description": "Training step 202",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:57",
      "total_flops_so_far": 290405108496384.0,
      "budget_used_percent": 0.290405108496384
    },
    {
      "type": "training",
      "description": "Training step 203",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:57",
      "total_flops_so_far": 291835675533312.0,
      "budget_used_percent": 0.29183567553331197
    },
    {
      "type": "training",
      "description": "Training step 204",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:57",
      "total_flops_so_far": 293266242570240.0,
      "budget_used_percent": 0.29326624257024
    },
    {
      "type": "training",
      "description": "Training step 205",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:57",
      "total_flops_so_far": 294696809607168.0,
      "budget_used_percent": 0.294696809607168
    },
    {
      "type": "training",
      "description": "Training step 206",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:57",
      "total_flops_so_far": 296127376644096.0,
      "budget_used_percent": 0.296127376644096
    },
    {
      "type": "training",
      "description": "Training step 207",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:57",
      "total_flops_so_far": 297557943681024.0,
      "budget_used_percent": 0.29755794368102395
    },
    {
      "type": "training",
      "description": "Training step 208",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:57",
      "total_flops_so_far": 298988510717952.0,
      "budget_used_percent": 0.29898851071795196
    },
    {
      "type": "training",
      "description": "Training step 209",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:58",
      "total_flops_so_far": 300419077754880.0,
      "budget_used_percent": 0.30041907775487997
    },
    {
      "type": "training",
      "description": "Training step 210",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:58",
      "total_flops_so_far": 301849644791808.0,
      "budget_used_percent": 0.301849644791808
    },
    {
      "type": "training",
      "description": "Training step 211",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:58",
      "total_flops_so_far": 303280211828736.0,
      "budget_used_percent": 0.303280211828736
    },
    {
      "type": "training",
      "description": "Training step 212",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:58",
      "total_flops_so_far": 304710778865664.0,
      "budget_used_percent": 0.304710778865664
    },
    {
      "type": "training",
      "description": "Training step 213",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:58",
      "total_flops_so_far": 306141345902592.0,
      "budget_used_percent": 0.306141345902592
    },
    {
      "type": "training",
      "description": "Training step 214",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:58",
      "total_flops_so_far": 307571912939520.0,
      "budget_used_percent": 0.30757191293952
    },
    {
      "type": "training",
      "description": "Training step 215",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:58",
      "total_flops_so_far": 309002479976448.0,
      "budget_used_percent": 0.30900247997644803
    },
    {
      "type": "training",
      "description": "Training step 216",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:58",
      "total_flops_so_far": 310433047013376.0,
      "budget_used_percent": 0.310433047013376
    },
    {
      "type": "training",
      "description": "Training step 217",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:59",
      "total_flops_so_far": 311863614050304.0,
      "budget_used_percent": 0.311863614050304
    },
    {
      "type": "training",
      "description": "Training step 218",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:59",
      "total_flops_so_far": 313294181087232.0,
      "budget_used_percent": 0.313294181087232
    },
    {
      "type": "training",
      "description": "Training step 219",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:59",
      "total_flops_so_far": 314724748124160.0,
      "budget_used_percent": 0.31472474812416
    },
    {
      "type": "training",
      "description": "Training step 220",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:59",
      "total_flops_so_far": 316155315161088.0,
      "budget_used_percent": 0.316155315161088
    },
    {
      "type": "training",
      "description": "Training step 221",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:59",
      "total_flops_so_far": 317585882198016.0,
      "budget_used_percent": 0.317585882198016
    },
    {
      "type": "training",
      "description": "Training step 222",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:59",
      "total_flops_so_far": 319016449234944.0,
      "budget_used_percent": 0.319016449234944
    },
    {
      "type": "training",
      "description": "Training step 223",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:59",
      "total_flops_so_far": 320447016271872.0,
      "budget_used_percent": 0.320447016271872
    },
    {
      "type": "training",
      "description": "Training step 224",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:00:59",
      "total_flops_so_far": 321877583308800.0,
      "budget_used_percent": 0.3218775833088
    },
    {
      "type": "training",
      "description": "Training step 225",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:00",
      "total_flops_so_far": 323308150345728.0,
      "budget_used_percent": 0.32330815034572796
    },
    {
      "type": "training",
      "description": "Training step 226",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:00",
      "total_flops_so_far": 324738717382656.0,
      "budget_used_percent": 0.324738717382656
    },
    {
      "type": "training",
      "description": "Training step 227",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:00",
      "total_flops_so_far": 326169284419584.0,
      "budget_used_percent": 0.326169284419584
    },
    {
      "type": "training",
      "description": "Training step 228",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:00",
      "total_flops_so_far": 327599851456512.0,
      "budget_used_percent": 0.327599851456512
    },
    {
      "type": "training",
      "description": "Training step 229",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:00",
      "total_flops_so_far": 329030418493440.0,
      "budget_used_percent": 0.32903041849344
    },
    {
      "type": "training",
      "description": "Training step 230",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:00",
      "total_flops_so_far": 330460985530368.0,
      "budget_used_percent": 0.330460985530368
    },
    {
      "type": "training",
      "description": "Training step 231",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:00",
      "total_flops_so_far": 331891552567296.0,
      "budget_used_percent": 0.331891552567296
    },
    {
      "type": "training",
      "description": "Training step 232",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:00",
      "total_flops_so_far": 333322119604224.0,
      "budget_used_percent": 0.33332211960422403
    },
    {
      "type": "training",
      "description": "Training step 233",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:00",
      "total_flops_so_far": 334752686641152.0,
      "budget_used_percent": 0.33475268664115204
    },
    {
      "type": "training",
      "description": "Training step 234",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:01",
      "total_flops_so_far": 336183253678080.0,
      "budget_used_percent": 0.33618325367808
    },
    {
      "type": "training",
      "description": "Training step 235",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:01",
      "total_flops_so_far": 337613820715008.0,
      "budget_used_percent": 0.337613820715008
    },
    {
      "type": "training",
      "description": "Training step 236",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:01",
      "total_flops_so_far": 339044387751936.0,
      "budget_used_percent": 0.339044387751936
    },
    {
      "type": "training",
      "description": "Training step 237",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:01",
      "total_flops_so_far": 340474954788864.0,
      "budget_used_percent": 0.340474954788864
    },
    {
      "type": "training",
      "description": "Training step 238",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:01",
      "total_flops_so_far": 341905521825792.0,
      "budget_used_percent": 0.341905521825792
    },
    {
      "type": "training",
      "description": "Training step 239",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:01",
      "total_flops_so_far": 343336088862720.0,
      "budget_used_percent": 0.34333608886272
    },
    {
      "type": "training",
      "description": "Training step 240",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:01",
      "total_flops_so_far": 344766655899648.0,
      "budget_used_percent": 0.344766655899648
    },
    {
      "type": "training",
      "description": "Training step 241",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:01",
      "total_flops_so_far": 346197222936576.0,
      "budget_used_percent": 0.346197222936576
    },
    {
      "type": "training",
      "description": "Training step 242",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:02",
      "total_flops_so_far": 347627789973504.0,
      "budget_used_percent": 0.34762778997350396
    },
    {
      "type": "training",
      "description": "Training step 243",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:02",
      "total_flops_so_far": 349058357010432.0,
      "budget_used_percent": 0.349058357010432
    },
    {
      "type": "training",
      "description": "Training step 244",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:02",
      "total_flops_so_far": 350488924047360.0,
      "budget_used_percent": 0.35048892404736
    },
    {
      "type": "training",
      "description": "Training step 245",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:02",
      "total_flops_so_far": 351919491084288.0,
      "budget_used_percent": 0.351919491084288
    },
    {
      "type": "training",
      "description": "Training step 246",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:02",
      "total_flops_so_far": 353350058121216.0,
      "budget_used_percent": 0.353350058121216
    },
    {
      "type": "training",
      "description": "Training step 247",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:02",
      "total_flops_so_far": 354780625158144.0,
      "budget_used_percent": 0.35478062515814396
    },
    {
      "type": "training",
      "description": "Training step 248",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:02",
      "total_flops_so_far": 356211192195072.0,
      "budget_used_percent": 0.35621119219507197
    },
    {
      "type": "training",
      "description": "Training step 249",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:02",
      "total_flops_so_far": 357641759232000.0,
      "budget_used_percent": 0.357641759232
    },
    {
      "type": "training",
      "description": "Training step 250",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:03",
      "total_flops_so_far": 359072326268928.0,
      "budget_used_percent": 0.359072326268928
    },
    {
      "type": "training",
      "description": "Training step 251",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:03",
      "total_flops_so_far": 360502893305856.0,
      "budget_used_percent": 0.360502893305856
    },
    {
      "type": "training",
      "description": "Training step 252",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:03",
      "total_flops_so_far": 361933460342784.0,
      "budget_used_percent": 0.361933460342784
    },
    {
      "type": "training",
      "description": "Training step 253",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:03",
      "total_flops_so_far": 363364027379712.0,
      "budget_used_percent": 0.363364027379712
    },
    {
      "type": "training",
      "description": "Training step 254",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:03",
      "total_flops_so_far": 364794594416640.0,
      "budget_used_percent": 0.36479459441664003
    },
    {
      "type": "training",
      "description": "Training step 255",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:03",
      "total_flops_so_far": 366225161453568.0,
      "budget_used_percent": 0.36622516145356804
    },
    {
      "type": "training",
      "description": "Training step 256",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:03",
      "total_flops_so_far": 367655728490496.0,
      "budget_used_percent": 0.367655728490496
    },
    {
      "type": "training",
      "description": "Training step 257",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:03",
      "total_flops_so_far": 369086295527424.0,
      "budget_used_percent": 0.369086295527424
    },
    {
      "type": "training",
      "description": "Training step 258",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:04",
      "total_flops_so_far": 370516862564352.0,
      "budget_used_percent": 0.370516862564352
    },
    {
      "type": "training",
      "description": "Training step 259",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:04",
      "total_flops_so_far": 371947429601280.0,
      "budget_used_percent": 0.37194742960128
    },
    {
      "type": "training",
      "description": "Training step 260",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:04",
      "total_flops_so_far": 373377996638208.0,
      "budget_used_percent": 0.373377996638208
    },
    {
      "type": "training",
      "description": "Training step 261",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:04",
      "total_flops_so_far": 374808563675136.0,
      "budget_used_percent": 0.374808563675136
    },
    {
      "type": "training",
      "description": "Training step 262",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:04",
      "total_flops_so_far": 376239130712064.0,
      "budget_used_percent": 0.376239130712064
    },
    {
      "type": "training",
      "description": "Training step 263",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:04",
      "total_flops_so_far": 377669697748992.0,
      "budget_used_percent": 0.377669697748992
    },
    {
      "type": "training",
      "description": "Training step 264",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:04",
      "total_flops_so_far": 379100264785920.0,
      "budget_used_percent": 0.37910026478591996
    },
    {
      "type": "training",
      "description": "Training step 265",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:04",
      "total_flops_so_far": 380530831822848.0,
      "budget_used_percent": 0.38053083182284797
    },
    {
      "type": "training",
      "description": "Training step 266",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:05",
      "total_flops_so_far": 381961398859776.0,
      "budget_used_percent": 0.381961398859776
    },
    {
      "type": "training",
      "description": "Training step 267",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:05",
      "total_flops_so_far": 383391965896704.0,
      "budget_used_percent": 0.383391965896704
    },
    {
      "type": "training",
      "description": "Training step 268",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:05",
      "total_flops_so_far": 384822532933632.0,
      "budget_used_percent": 0.384822532933632
    },
    {
      "type": "training",
      "description": "Training step 269",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:05",
      "total_flops_so_far": 386253099970560.0,
      "budget_used_percent": 0.38625309997056
    },
    {
      "type": "training",
      "description": "Training step 270",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:05",
      "total_flops_so_far": 387683667007488.0,
      "budget_used_percent": 0.387683667007488
    },
    {
      "type": "training",
      "description": "Training step 271",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:05",
      "total_flops_so_far": 389114234044416.0,
      "budget_used_percent": 0.38911423404441603
    },
    {
      "type": "training",
      "description": "Training step 272",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:05",
      "total_flops_so_far": 390544801081344.0,
      "budget_used_percent": 0.39054480108134404
    },
    {
      "type": "training",
      "description": "Training step 273",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:05",
      "total_flops_so_far": 391975368118272.0,
      "budget_used_percent": 0.39197536811827205
    },
    {
      "type": "training",
      "description": "Training step 274",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:06",
      "total_flops_so_far": 393405935155200.0,
      "budget_used_percent": 0.39340593515520006
    },
    {
      "type": "training",
      "description": "Training step 275",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:06",
      "total_flops_so_far": 394836502192128.0,
      "budget_used_percent": 0.39483650219212796
    },
    {
      "type": "training",
      "description": "Training step 276",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:06",
      "total_flops_so_far": 396267069229056.0,
      "budget_used_percent": 0.39626706922905597
    },
    {
      "type": "training",
      "description": "Training step 277",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:06",
      "total_flops_so_far": 397697636265984.0,
      "budget_used_percent": 0.397697636265984
    },
    {
      "type": "training",
      "description": "Training step 278",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:06",
      "total_flops_so_far": 399128203302912.0,
      "budget_used_percent": 0.399128203302912
    },
    {
      "type": "training",
      "description": "Training step 279",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:06",
      "total_flops_so_far": 400558770339840.0,
      "budget_used_percent": 0.40055877033984
    },
    {
      "type": "training",
      "description": "Training step 280",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:06",
      "total_flops_so_far": 401989337376768.0,
      "budget_used_percent": 0.401989337376768
    },
    {
      "type": "training",
      "description": "Training step 281",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:06",
      "total_flops_so_far": 403419904413696.0,
      "budget_used_percent": 0.403419904413696
    },
    {
      "type": "training",
      "description": "Training step 282",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:07",
      "total_flops_so_far": 404850471450624.0,
      "budget_used_percent": 0.404850471450624
    },
    {
      "type": "training",
      "description": "Training step 283",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:07",
      "total_flops_so_far": 406281038487552.0,
      "budget_used_percent": 0.40628103848755204
    },
    {
      "type": "training",
      "description": "Training step 284",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:07",
      "total_flops_so_far": 407711605524480.0,
      "budget_used_percent": 0.40771160552447994
    },
    {
      "type": "training",
      "description": "Training step 285",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:07",
      "total_flops_so_far": 409142172561408.0,
      "budget_used_percent": 0.40914217256140795
    },
    {
      "type": "training",
      "description": "Training step 286",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:07",
      "total_flops_so_far": 410572739598336.0,
      "budget_used_percent": 0.41057273959833596
    },
    {
      "type": "training",
      "description": "Training step 287",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:07",
      "total_flops_so_far": 412003306635264.0,
      "budget_used_percent": 0.41200330663526397
    },
    {
      "type": "training",
      "description": "Training step 288",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:07",
      "total_flops_so_far": 413433873672192.0,
      "budget_used_percent": 0.413433873672192
    },
    {
      "type": "training",
      "description": "Training step 289",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:07",
      "total_flops_so_far": 414864440709120.0,
      "budget_used_percent": 0.41486444070912
    },
    {
      "type": "training",
      "description": "Training step 290",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:08",
      "total_flops_so_far": 416295007746048.0,
      "budget_used_percent": 0.416295007746048
    },
    {
      "type": "training",
      "description": "Training step 291",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:08",
      "total_flops_so_far": 417725574782976.0,
      "budget_used_percent": 0.417725574782976
    },
    {
      "type": "training",
      "description": "Training step 292",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:08",
      "total_flops_so_far": 419156141819904.0,
      "budget_used_percent": 0.419156141819904
    },
    {
      "type": "training",
      "description": "Training step 293",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:08",
      "total_flops_so_far": 420586708856832.0,
      "budget_used_percent": 0.42058670885683197
    },
    {
      "type": "training",
      "description": "Training step 294",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:08",
      "total_flops_so_far": 422017275893760.0,
      "budget_used_percent": 0.42201727589376
    },
    {
      "type": "training",
      "description": "Training step 295",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:08",
      "total_flops_so_far": 423447842930688.0,
      "budget_used_percent": 0.423447842930688
    },
    {
      "type": "training",
      "description": "Training step 296",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:08",
      "total_flops_so_far": 424878409967616.0,
      "budget_used_percent": 0.424878409967616
    },
    {
      "type": "training",
      "description": "Training step 297",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:08",
      "total_flops_so_far": 426308977004544.0,
      "budget_used_percent": 0.426308977004544
    },
    {
      "type": "training",
      "description": "Training step 298",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:09",
      "total_flops_so_far": 427739544041472.0,
      "budget_used_percent": 0.427739544041472
    },
    {
      "type": "training",
      "description": "Training step 299",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:09",
      "total_flops_so_far": 429170111078400.0,
      "budget_used_percent": 0.42917011107840003
    },
    {
      "type": "training",
      "description": "Training step 300",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:09",
      "total_flops_so_far": 430600678115328.0,
      "budget_used_percent": 0.43060067811532804
    },
    {
      "type": "training",
      "description": "Training step 301",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:09",
      "total_flops_so_far": 432031245152256.0,
      "budget_used_percent": 0.43203124515225605
    },
    {
      "type": "training",
      "description": "Training step 302",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:09",
      "total_flops_so_far": 433461812189184.0,
      "budget_used_percent": 0.43346181218918395
    },
    {
      "type": "training",
      "description": "Training step 303",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:09",
      "total_flops_so_far": 434892379226112.0,
      "budget_used_percent": 0.43489237922611196
    },
    {
      "type": "training",
      "description": "Training step 304",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:09",
      "total_flops_so_far": 436322946263040.0,
      "budget_used_percent": 0.43632294626303997
    },
    {
      "type": "training",
      "description": "Training step 305",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:09",
      "total_flops_so_far": 437753513299968.0,
      "budget_used_percent": 0.437753513299968
    },
    {
      "type": "training",
      "description": "Training step 306",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:10",
      "total_flops_so_far": 439184080336896.0,
      "budget_used_percent": 0.439184080336896
    },
    {
      "type": "training",
      "description": "Training step 307",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:10",
      "total_flops_so_far": 440614647373824.0,
      "budget_used_percent": 0.440614647373824
    },
    {
      "type": "training",
      "description": "Training step 308",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:10",
      "total_flops_so_far": 442045214410752.0,
      "budget_used_percent": 0.442045214410752
    },
    {
      "type": "training",
      "description": "Training step 309",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:10",
      "total_flops_so_far": 443475781447680.0,
      "budget_used_percent": 0.44347578144768
    },
    {
      "type": "training",
      "description": "Training step 310",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:10",
      "total_flops_so_far": 444906348484608.0,
      "budget_used_percent": 0.44490634848460797
    },
    {
      "type": "training",
      "description": "Training step 311",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:10",
      "total_flops_so_far": 446336915521536.0,
      "budget_used_percent": 0.446336915521536
    },
    {
      "type": "training",
      "description": "Training step 312",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:10",
      "total_flops_so_far": 447767482558464.0,
      "budget_used_percent": 0.447767482558464
    },
    {
      "type": "training",
      "description": "Training step 313",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:10",
      "total_flops_so_far": 449198049595392.0,
      "budget_used_percent": 0.449198049595392
    },
    {
      "type": "training",
      "description": "Training step 314",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:11",
      "total_flops_so_far": 450628616632320.0,
      "budget_used_percent": 0.45062861663232
    },
    {
      "type": "training",
      "description": "Training step 315",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:11",
      "total_flops_so_far": 452059183669248.0,
      "budget_used_percent": 0.452059183669248
    },
    {
      "type": "training",
      "description": "Training step 316",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:11",
      "total_flops_so_far": 453489750706176.0,
      "budget_used_percent": 0.45348975070617603
    },
    {
      "type": "training",
      "description": "Training step 317",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:11",
      "total_flops_so_far": 454920317743104.0,
      "budget_used_percent": 0.45492031774310404
    },
    {
      "type": "training",
      "description": "Training step 318",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:11",
      "total_flops_so_far": 456350884780032.0,
      "budget_used_percent": 0.45635088478003205
    },
    {
      "type": "training",
      "description": "Training step 319",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:11",
      "total_flops_so_far": 457781451816960.0,
      "budget_used_percent": 0.45778145181695995
    },
    {
      "type": "training",
      "description": "Training step 320",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:11",
      "total_flops_so_far": 459212018853888.0,
      "budget_used_percent": 0.45921201885388796
    },
    {
      "type": "training",
      "description": "Training step 321",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:11",
      "total_flops_so_far": 460642585890816.0,
      "budget_used_percent": 0.46064258589081597
    },
    {
      "type": "training",
      "description": "Training step 322",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:12",
      "total_flops_so_far": 462073152927744.0,
      "budget_used_percent": 0.462073152927744
    },
    {
      "type": "training",
      "description": "Training step 323",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:12",
      "total_flops_so_far": 463503719964672.0,
      "budget_used_percent": 0.463503719964672
    },
    {
      "type": "training",
      "description": "Training step 324",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:12",
      "total_flops_so_far": 464934287001600.0,
      "budget_used_percent": 0.4649342870016
    },
    {
      "type": "training",
      "description": "Training step 325",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:12",
      "total_flops_so_far": 466364854038528.0,
      "budget_used_percent": 0.466364854038528
    },
    {
      "type": "training",
      "description": "Training step 326",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:12",
      "total_flops_so_far": 467795421075456.0,
      "budget_used_percent": 0.467795421075456
    },
    {
      "type": "training",
      "description": "Training step 327",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:12",
      "total_flops_so_far": 469225988112384.0,
      "budget_used_percent": 0.46922598811238403
    },
    {
      "type": "training",
      "description": "Training step 328",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:12",
      "total_flops_so_far": 470656555149312.0,
      "budget_used_percent": 0.470656555149312
    },
    {
      "type": "training",
      "description": "Training step 329",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:12",
      "total_flops_so_far": 472087122186240.0,
      "budget_used_percent": 0.47208712218624
    },
    {
      "type": "training",
      "description": "Training step 330",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:13",
      "total_flops_so_far": 473517689223168.0,
      "budget_used_percent": 0.473517689223168
    },
    {
      "type": "training",
      "description": "Training step 331",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:13",
      "total_flops_so_far": 474948256260096.0,
      "budget_used_percent": 0.474948256260096
    },
    {
      "type": "training",
      "description": "Training step 332",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:13",
      "total_flops_so_far": 476378823297024.0,
      "budget_used_percent": 0.476378823297024
    },
    {
      "type": "training",
      "description": "Training step 333",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:13",
      "total_flops_so_far": 477809390333952.0,
      "budget_used_percent": 0.47780939033395203
    },
    {
      "type": "training",
      "description": "Training step 334",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:13",
      "total_flops_so_far": 479239957370880.0,
      "budget_used_percent": 0.47923995737088004
    },
    {
      "type": "training",
      "description": "Training step 335",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:13",
      "total_flops_so_far": 480670524407808.0,
      "budget_used_percent": 0.48067052440780805
    },
    {
      "type": "training",
      "description": "Training step 336",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:13",
      "total_flops_so_far": 482101091444736.0,
      "budget_used_percent": 0.48210109144473606
    },
    {
      "type": "training",
      "description": "Training step 337",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:14",
      "total_flops_so_far": 483531658481664.0,
      "budget_used_percent": 0.48353165848166396
    },
    {
      "type": "training",
      "description": "Training step 338",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:14",
      "total_flops_so_far": 484962225518592.0,
      "budget_used_percent": 0.48496222551859197
    },
    {
      "type": "training",
      "description": "Training step 339",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:14",
      "total_flops_so_far": 486392792555520.0,
      "budget_used_percent": 0.48639279255552
    },
    {
      "type": "training",
      "description": "Training step 340",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:14",
      "total_flops_so_far": 487823359592448.0,
      "budget_used_percent": 0.487823359592448
    },
    {
      "type": "training",
      "description": "Training step 341",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:14",
      "total_flops_so_far": 489253926629376.0,
      "budget_used_percent": 0.489253926629376
    },
    {
      "type": "training",
      "description": "Training step 342",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:14",
      "total_flops_so_far": 490684493666304.0,
      "budget_used_percent": 0.490684493666304
    },
    {
      "type": "training",
      "description": "Training step 343",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:14",
      "total_flops_so_far": 492115060703232.0,
      "budget_used_percent": 0.492115060703232
    },
    {
      "type": "training",
      "description": "Training step 344",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:14",
      "total_flops_so_far": 493545627740160.0,
      "budget_used_percent": 0.49354562774016003
    },
    {
      "type": "training",
      "description": "Training step 345",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:15",
      "total_flops_so_far": 494976194777088.0,
      "budget_used_percent": 0.494976194777088
    },
    {
      "type": "training",
      "description": "Training step 346",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:15",
      "total_flops_so_far": 496406761814016.0,
      "budget_used_percent": 0.496406761814016
    },
    {
      "type": "training",
      "description": "Training step 347",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:15",
      "total_flops_so_far": 497837328850944.0,
      "budget_used_percent": 0.497837328850944
    },
    {
      "type": "training",
      "description": "Training step 348",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:15",
      "total_flops_so_far": 499267895887872.0,
      "budget_used_percent": 0.499267895887872
    },
    {
      "type": "training",
      "description": "Training step 349",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:15",
      "total_flops_so_far": 500698462924800.0,
      "budget_used_percent": 0.5006984629248
    },
    {
      "type": "training",
      "description": "Training step 350",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:15",
      "total_flops_so_far": 502129029961728.0,
      "budget_used_percent": 0.502129029961728
    },
    {
      "type": "training",
      "description": "Training step 351",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:15",
      "total_flops_so_far": 503559596998656.0,
      "budget_used_percent": 0.503559596998656
    },
    {
      "type": "training",
      "description": "Training step 352",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:15",
      "total_flops_so_far": 504990164035584.0,
      "budget_used_percent": 0.504990164035584
    },
    {
      "type": "training",
      "description": "Training step 353",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:16",
      "total_flops_so_far": 506420731072512.0,
      "budget_used_percent": 0.5064207310725121
    },
    {
      "type": "training",
      "description": "Training step 354",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:16",
      "total_flops_so_far": 507851298109440.0,
      "budget_used_percent": 0.50785129810944
    },
    {
      "type": "training",
      "description": "Training step 355",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:16",
      "total_flops_so_far": 509281865146368.0,
      "budget_used_percent": 0.509281865146368
    },
    {
      "type": "training",
      "description": "Training step 356",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:16",
      "total_flops_so_far": 510712432183296.0,
      "budget_used_percent": 0.510712432183296
    },
    {
      "type": "training",
      "description": "Training step 357",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:16",
      "total_flops_so_far": 512142999220224.0,
      "budget_used_percent": 0.512142999220224
    },
    {
      "type": "training",
      "description": "Training step 358",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:16",
      "total_flops_so_far": 513573566257152.0,
      "budget_used_percent": 0.513573566257152
    },
    {
      "type": "training",
      "description": "Training step 359",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:16",
      "total_flops_so_far": 515004133294080.0,
      "budget_used_percent": 0.51500413329408
    },
    {
      "type": "training",
      "description": "Training step 360",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:16",
      "total_flops_so_far": 516434700331008.0,
      "budget_used_percent": 0.516434700331008
    },
    {
      "type": "training",
      "description": "Training step 361",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:17",
      "total_flops_so_far": 517865267367936.0,
      "budget_used_percent": 0.517865267367936
    },
    {
      "type": "training",
      "description": "Training step 362",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:17",
      "total_flops_so_far": 519295834404864.0,
      "budget_used_percent": 0.519295834404864
    },
    {
      "type": "training",
      "description": "Training step 363",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:17",
      "total_flops_so_far": 520726401441792.0,
      "budget_used_percent": 0.5207264014417919
    },
    {
      "type": "training",
      "description": "Training step 364",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:17",
      "total_flops_so_far": 522156968478720.0,
      "budget_used_percent": 0.52215696847872
    },
    {
      "type": "training",
      "description": "Training step 365",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:17",
      "total_flops_so_far": 523587535515648.0,
      "budget_used_percent": 0.523587535515648
    },
    {
      "type": "training",
      "description": "Training step 366",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:17",
      "total_flops_so_far": 525018102552576.0,
      "budget_used_percent": 0.525018102552576
    },
    {
      "type": "training",
      "description": "Training step 367",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:17",
      "total_flops_so_far": 526448669589504.0,
      "budget_used_percent": 0.526448669589504
    },
    {
      "type": "training",
      "description": "Training step 368",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:17",
      "total_flops_so_far": 527879236626432.0,
      "budget_used_percent": 0.527879236626432
    },
    {
      "type": "training",
      "description": "Training step 369",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:18",
      "total_flops_so_far": 529309803663360.0,
      "budget_used_percent": 0.52930980366336
    },
    {
      "type": "training",
      "description": "Training step 370",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:18",
      "total_flops_so_far": 530740370700288.0,
      "budget_used_percent": 0.530740370700288
    },
    {
      "type": "training",
      "description": "Training step 371",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:18",
      "total_flops_so_far": 532170937737216.0,
      "budget_used_percent": 0.532170937737216
    },
    {
      "type": "training",
      "description": "Training step 372",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:18",
      "total_flops_so_far": 533601504774144.0,
      "budget_used_percent": 0.5336015047741439
    },
    {
      "type": "training",
      "description": "Training step 373",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:18",
      "total_flops_so_far": 535032071811072.0,
      "budget_used_percent": 0.5350320718110719
    },
    {
      "type": "training",
      "description": "Training step 374",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:18",
      "total_flops_so_far": 536462638848000.0,
      "budget_used_percent": 0.5364626388479999
    },
    {
      "type": "training",
      "description": "Training step 375",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:18",
      "total_flops_so_far": 537893205884928.0,
      "budget_used_percent": 0.537893205884928
    },
    {
      "type": "training",
      "description": "Training step 376",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:18",
      "total_flops_so_far": 539323772921856.0,
      "budget_used_percent": 0.539323772921856
    },
    {
      "type": "training",
      "description": "Training step 377",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:19",
      "total_flops_so_far": 540754339958784.0,
      "budget_used_percent": 0.540754339958784
    },
    {
      "type": "training",
      "description": "Training step 378",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:19",
      "total_flops_so_far": 542184906995712.0,
      "budget_used_percent": 0.542184906995712
    },
    {
      "type": "training",
      "description": "Training step 379",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:19",
      "total_flops_so_far": 543615474032640.0,
      "budget_used_percent": 0.54361547403264
    },
    {
      "type": "training",
      "description": "Training step 380",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:19",
      "total_flops_so_far": 545046041069568.0,
      "budget_used_percent": 0.545046041069568
    },
    {
      "type": "training",
      "description": "Training step 381",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:19",
      "total_flops_so_far": 546476608106496.0,
      "budget_used_percent": 0.546476608106496
    },
    {
      "type": "training",
      "description": "Training step 382",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:19",
      "total_flops_so_far": 547907175143424.0,
      "budget_used_percent": 0.547907175143424
    },
    {
      "type": "training",
      "description": "Training step 383",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:19",
      "total_flops_so_far": 549337742180352.0,
      "budget_used_percent": 0.549337742180352
    },
    {
      "type": "training",
      "description": "Training step 384",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:19",
      "total_flops_so_far": 550768309217280.0,
      "budget_used_percent": 0.55076830921728
    },
    {
      "type": "training",
      "description": "Training step 385",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:20",
      "total_flops_so_far": 552198876254208.0,
      "budget_used_percent": 0.552198876254208
    },
    {
      "type": "training",
      "description": "Training step 386",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:20",
      "total_flops_so_far": 553629443291136.0,
      "budget_used_percent": 0.5536294432911361
    },
    {
      "type": "training",
      "description": "Training step 387",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:20",
      "total_flops_so_far": 555060010328064.0,
      "budget_used_percent": 0.5550600103280641
    },
    {
      "type": "training",
      "description": "Training step 388",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:20",
      "total_flops_so_far": 556490577364992.0,
      "budget_used_percent": 0.5564905773649921
    },
    {
      "type": "training",
      "description": "Training step 389",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:20",
      "total_flops_so_far": 557921144401920.0,
      "budget_used_percent": 0.55792114440192
    },
    {
      "type": "training",
      "description": "Training step 390",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:20",
      "total_flops_so_far": 559351711438848.0,
      "budget_used_percent": 0.559351711438848
    },
    {
      "type": "training",
      "description": "Training step 391",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:20",
      "total_flops_so_far": 560782278475776.0,
      "budget_used_percent": 0.560782278475776
    },
    {
      "type": "training",
      "description": "Training step 392",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:20",
      "total_flops_so_far": 562212845512704.0,
      "budget_used_percent": 0.562212845512704
    },
    {
      "type": "training",
      "description": "Training step 393",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:21",
      "total_flops_so_far": 563643412549632.0,
      "budget_used_percent": 0.563643412549632
    },
    {
      "type": "training",
      "description": "Training step 394",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:21",
      "total_flops_so_far": 565073979586560.0,
      "budget_used_percent": 0.56507397958656
    },
    {
      "type": "training",
      "description": "Training step 395",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:21",
      "total_flops_so_far": 566504546623488.0,
      "budget_used_percent": 0.566504546623488
    },
    {
      "type": "training",
      "description": "Training step 396",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:21",
      "total_flops_so_far": 567935113660416.0,
      "budget_used_percent": 0.567935113660416
    },
    {
      "type": "training",
      "description": "Training step 397",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:21",
      "total_flops_so_far": 569365680697344.0,
      "budget_used_percent": 0.5693656806973441
    },
    {
      "type": "training",
      "description": "Training step 398",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:21",
      "total_flops_so_far": 570796247734272.0,
      "budget_used_percent": 0.570796247734272
    },
    {
      "type": "training",
      "description": "Training step 399",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:21",
      "total_flops_so_far": 572226814771200.0,
      "budget_used_percent": 0.5722268147712
    },
    {
      "type": "training",
      "description": "Training step 400",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:21",
      "total_flops_so_far": 573657381808128.0,
      "budget_used_percent": 0.573657381808128
    },
    {
      "type": "training",
      "description": "Training step 401",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:22",
      "total_flops_so_far": 575087948845056.0,
      "budget_used_percent": 0.575087948845056
    },
    {
      "type": "training",
      "description": "Training step 402",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:22",
      "total_flops_so_far": 576518515881984.0,
      "budget_used_percent": 0.576518515881984
    },
    {
      "type": "training",
      "description": "Training step 403",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:22",
      "total_flops_so_far": 577949082918912.0,
      "budget_used_percent": 0.577949082918912
    },
    {
      "type": "training",
      "description": "Training step 404",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:22",
      "total_flops_so_far": 579379649955840.0,
      "budget_used_percent": 0.57937964995584
    },
    {
      "type": "training",
      "description": "Training step 405",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:22",
      "total_flops_so_far": 580810216992768.0,
      "budget_used_percent": 0.580810216992768
    },
    {
      "type": "training",
      "description": "Training step 406",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:22",
      "total_flops_so_far": 582240784029696.0,
      "budget_used_percent": 0.582240784029696
    },
    {
      "type": "training",
      "description": "Training step 407",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:22",
      "total_flops_so_far": 583671351066624.0,
      "budget_used_percent": 0.5836713510666239
    },
    {
      "type": "training",
      "description": "Training step 408",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:23",
      "total_flops_so_far": 585101918103552.0,
      "budget_used_percent": 0.5851019181035519
    },
    {
      "type": "training",
      "description": "Training step 409",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:23",
      "total_flops_so_far": 586532485140480.0,
      "budget_used_percent": 0.58653248514048
    },
    {
      "type": "training",
      "description": "Training step 410",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:23",
      "total_flops_so_far": 587963052177408.0,
      "budget_used_percent": 0.587963052177408
    },
    {
      "type": "training",
      "description": "Training step 411",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:23",
      "total_flops_so_far": 589393619214336.0,
      "budget_used_percent": 0.589393619214336
    },
    {
      "type": "training",
      "description": "Training step 412",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:23",
      "total_flops_so_far": 590824186251264.0,
      "budget_used_percent": 0.590824186251264
    },
    {
      "type": "training",
      "description": "Training step 413",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:23",
      "total_flops_so_far": 592254753288192.0,
      "budget_used_percent": 0.592254753288192
    },
    {
      "type": "training",
      "description": "Training step 414",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:23",
      "total_flops_so_far": 593685320325120.0,
      "budget_used_percent": 0.59368532032512
    },
    {
      "type": "training",
      "description": "Training step 415",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:23",
      "total_flops_so_far": 595115887362048.0,
      "budget_used_percent": 0.5951158873620479
    },
    {
      "type": "training",
      "description": "Training step 416",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:24",
      "total_flops_so_far": 596546454398976.0,
      "budget_used_percent": 0.596546454398976
    },
    {
      "type": "training",
      "description": "Training step 417",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:24",
      "total_flops_so_far": 597977021435904.0,
      "budget_used_percent": 0.5979770214359039
    },
    {
      "type": "training",
      "description": "Training step 418",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:24",
      "total_flops_so_far": 599407588472832.0,
      "budget_used_percent": 0.599407588472832
    },
    {
      "type": "training",
      "description": "Training step 419",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:24",
      "total_flops_so_far": 600838155509760.0,
      "budget_used_percent": 0.6008381555097599
    },
    {
      "type": "training",
      "description": "Training step 420",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:24",
      "total_flops_so_far": 602268722546688.0,
      "budget_used_percent": 0.6022687225466881
    },
    {
      "type": "training",
      "description": "Training step 421",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:24",
      "total_flops_so_far": 603699289583616.0,
      "budget_used_percent": 0.603699289583616
    },
    {
      "type": "training",
      "description": "Training step 422",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:24",
      "total_flops_so_far": 605129856620544.0,
      "budget_used_percent": 0.6051298566205441
    },
    {
      "type": "training",
      "description": "Training step 423",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:24",
      "total_flops_so_far": 606560423657472.0,
      "budget_used_percent": 0.606560423657472
    },
    {
      "type": "training",
      "description": "Training step 424",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:25",
      "total_flops_so_far": 607990990694400.0,
      "budget_used_percent": 0.6079909906944
    },
    {
      "type": "training",
      "description": "Training step 425",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:25",
      "total_flops_so_far": 609421557731328.0,
      "budget_used_percent": 0.609421557731328
    },
    {
      "type": "training",
      "description": "Training step 426",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:25",
      "total_flops_so_far": 610852124768256.0,
      "budget_used_percent": 0.610852124768256
    },
    {
      "type": "training",
      "description": "Training step 427",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:25",
      "total_flops_so_far": 612282691805184.0,
      "budget_used_percent": 0.612282691805184
    },
    {
      "type": "training",
      "description": "Training step 428",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:25",
      "total_flops_so_far": 613713258842112.0,
      "budget_used_percent": 0.613713258842112
    },
    {
      "type": "training",
      "description": "Training step 429",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:25",
      "total_flops_so_far": 615143825879040.0,
      "budget_used_percent": 0.61514382587904
    },
    {
      "type": "training",
      "description": "Training step 430",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:25",
      "total_flops_so_far": 616574392915968.0,
      "budget_used_percent": 0.616574392915968
    },
    {
      "type": "training",
      "description": "Training step 431",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:25",
      "total_flops_so_far": 618004959952896.0,
      "budget_used_percent": 0.6180049599528961
    },
    {
      "type": "training",
      "description": "Training step 432",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:26",
      "total_flops_so_far": 619435526989824.0,
      "budget_used_percent": 0.6194355269898241
    },
    {
      "type": "training",
      "description": "Training step 433",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:26",
      "total_flops_so_far": 620866094026752.0,
      "budget_used_percent": 0.620866094026752
    },
    {
      "type": "training",
      "description": "Training step 434",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:26",
      "total_flops_so_far": 622296661063680.0,
      "budget_used_percent": 0.62229666106368
    },
    {
      "type": "training",
      "description": "Training step 435",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:26",
      "total_flops_so_far": 623727228100608.0,
      "budget_used_percent": 0.623727228100608
    },
    {
      "type": "training",
      "description": "Training step 436",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:26",
      "total_flops_so_far": 625157795137536.0,
      "budget_used_percent": 0.625157795137536
    },
    {
      "type": "training",
      "description": "Training step 437",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:26",
      "total_flops_so_far": 626588362174464.0,
      "budget_used_percent": 0.626588362174464
    },
    {
      "type": "training",
      "description": "Training step 438",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:26",
      "total_flops_so_far": 628018929211392.0,
      "budget_used_percent": 0.628018929211392
    },
    {
      "type": "training",
      "description": "Training step 439",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:26",
      "total_flops_so_far": 629449496248320.0,
      "budget_used_percent": 0.62944949624832
    },
    {
      "type": "training",
      "description": "Training step 440",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:27",
      "total_flops_so_far": 630880063285248.0,
      "budget_used_percent": 0.630880063285248
    },
    {
      "type": "training",
      "description": "Training step 441",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:27",
      "total_flops_so_far": 632310630322176.0,
      "budget_used_percent": 0.632310630322176
    },
    {
      "type": "training",
      "description": "Training step 442",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:27",
      "total_flops_so_far": 633741197359104.0,
      "budget_used_percent": 0.633741197359104
    },
    {
      "type": "training",
      "description": "Training step 443",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:27",
      "total_flops_so_far": 635171764396032.0,
      "budget_used_percent": 0.635171764396032
    },
    {
      "type": "training",
      "description": "Training step 444",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:27",
      "total_flops_so_far": 636602331432960.0,
      "budget_used_percent": 0.63660233143296
    },
    {
      "type": "training",
      "description": "Training step 445",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:27",
      "total_flops_so_far": 638032898469888.0,
      "budget_used_percent": 0.638032898469888
    },
    {
      "type": "training",
      "description": "Training step 446",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:27",
      "total_flops_so_far": 639463465506816.0,
      "budget_used_percent": 0.639463465506816
    },
    {
      "type": "training",
      "description": "Training step 447",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:28",
      "total_flops_so_far": 640894032543744.0,
      "budget_used_percent": 0.640894032543744
    },
    {
      "type": "training",
      "description": "Training step 448",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:28",
      "total_flops_so_far": 642324599580672.0,
      "budget_used_percent": 0.642324599580672
    },
    {
      "type": "training",
      "description": "Training step 449",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:28",
      "total_flops_so_far": 643755166617600.0,
      "budget_used_percent": 0.6437551666176
    },
    {
      "type": "training",
      "description": "Training step 450",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:28",
      "total_flops_so_far": 645185733654528.0,
      "budget_used_percent": 0.6451857336545279
    },
    {
      "type": "training",
      "description": "Training step 451",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:28",
      "total_flops_so_far": 646616300691456.0,
      "budget_used_percent": 0.6466163006914559
    },
    {
      "type": "training",
      "description": "Training step 452",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:28",
      "total_flops_so_far": 648046867728384.0,
      "budget_used_percent": 0.6480468677283839
    },
    {
      "type": "training",
      "description": "Training step 453",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:28",
      "total_flops_so_far": 649477434765312.0,
      "budget_used_percent": 0.649477434765312
    },
    {
      "type": "training",
      "description": "Training step 454",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:28",
      "total_flops_so_far": 650908001802240.0,
      "budget_used_percent": 0.65090800180224
    },
    {
      "type": "training",
      "description": "Training step 455",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:29",
      "total_flops_so_far": 652338568839168.0,
      "budget_used_percent": 0.652338568839168
    },
    {
      "type": "training",
      "description": "Training step 456",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:29",
      "total_flops_so_far": 653769135876096.0,
      "budget_used_percent": 0.653769135876096
    },
    {
      "type": "training",
      "description": "Training step 457",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:29",
      "total_flops_so_far": 655199702913024.0,
      "budget_used_percent": 0.655199702913024
    },
    {
      "type": "training",
      "description": "Training step 458",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:29",
      "total_flops_so_far": 656630269949952.0,
      "budget_used_percent": 0.656630269949952
    },
    {
      "type": "training",
      "description": "Training step 459",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:29",
      "total_flops_so_far": 658060836986880.0,
      "budget_used_percent": 0.65806083698688
    },
    {
      "type": "training",
      "description": "Training step 460",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:29",
      "total_flops_so_far": 659491404023808.0,
      "budget_used_percent": 0.659491404023808
    },
    {
      "type": "training",
      "description": "Training step 461",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:29",
      "total_flops_so_far": 660921971060736.0,
      "budget_used_percent": 0.660921971060736
    },
    {
      "type": "training",
      "description": "Training step 462",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:29",
      "total_flops_so_far": 662352538097664.0,
      "budget_used_percent": 0.662352538097664
    },
    {
      "type": "training",
      "description": "Training step 463",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:30",
      "total_flops_so_far": 663783105134592.0,
      "budget_used_percent": 0.663783105134592
    },
    {
      "type": "training",
      "description": "Training step 464",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:30",
      "total_flops_so_far": 665213672171520.0,
      "budget_used_percent": 0.66521367217152
    },
    {
      "type": "training",
      "description": "Training step 465",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:30",
      "total_flops_so_far": 666644239208448.0,
      "budget_used_percent": 0.6666442392084481
    },
    {
      "type": "training",
      "description": "Training step 466",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:30",
      "total_flops_so_far": 668074806245376.0,
      "budget_used_percent": 0.6680748062453761
    },
    {
      "type": "training",
      "description": "Training step 467",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:30",
      "total_flops_so_far": 669505373282304.0,
      "budget_used_percent": 0.6695053732823041
    },
    {
      "type": "training",
      "description": "Training step 468",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:30",
      "total_flops_so_far": 670935940319232.0,
      "budget_used_percent": 0.670935940319232
    },
    {
      "type": "training",
      "description": "Training step 469",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:30",
      "total_flops_so_far": 672366507356160.0,
      "budget_used_percent": 0.67236650735616
    },
    {
      "type": "training",
      "description": "Training step 470",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:30",
      "total_flops_so_far": 673797074393088.0,
      "budget_used_percent": 0.673797074393088
    },
    {
      "type": "training",
      "description": "Training step 471",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:31",
      "total_flops_so_far": 675227641430016.0,
      "budget_used_percent": 0.675227641430016
    },
    {
      "type": "training",
      "description": "Training step 472",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:31",
      "total_flops_so_far": 676658208466944.0,
      "budget_used_percent": 0.676658208466944
    },
    {
      "type": "training",
      "description": "Training step 473",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:31",
      "total_flops_so_far": 678088775503872.0,
      "budget_used_percent": 0.678088775503872
    },
    {
      "type": "training",
      "description": "Training step 474",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:31",
      "total_flops_so_far": 679519342540800.0,
      "budget_used_percent": 0.6795193425408
    },
    {
      "type": "training",
      "description": "Training step 475",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:31",
      "total_flops_so_far": 680949909577728.0,
      "budget_used_percent": 0.680949909577728
    },
    {
      "type": "training",
      "description": "Training step 476",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:31",
      "total_flops_so_far": 682380476614656.0,
      "budget_used_percent": 0.6823804766146561
    },
    {
      "type": "training",
      "description": "Training step 477",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:31",
      "total_flops_so_far": 683811043651584.0,
      "budget_used_percent": 0.683811043651584
    },
    {
      "type": "training",
      "description": "Training step 478",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:32",
      "total_flops_so_far": 685241610688512.0,
      "budget_used_percent": 0.685241610688512
    },
    {
      "type": "training",
      "description": "Training step 479",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:32",
      "total_flops_so_far": 686672177725440.0,
      "budget_used_percent": 0.68667217772544
    },
    {
      "type": "training",
      "description": "Training step 480",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:32",
      "total_flops_so_far": 688102744762368.0,
      "budget_used_percent": 0.688102744762368
    },
    {
      "type": "training",
      "description": "Training step 481",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:32",
      "total_flops_so_far": 689533311799296.0,
      "budget_used_percent": 0.689533311799296
    },
    {
      "type": "training",
      "description": "Training step 482",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:32",
      "total_flops_so_far": 690963878836224.0,
      "budget_used_percent": 0.690963878836224
    },
    {
      "type": "training",
      "description": "Training step 483",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:32",
      "total_flops_so_far": 692394445873152.0,
      "budget_used_percent": 0.692394445873152
    },
    {
      "type": "training",
      "description": "Training step 484",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:32",
      "total_flops_so_far": 693825012910080.0,
      "budget_used_percent": 0.69382501291008
    },
    {
      "type": "training",
      "description": "Training step 485",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:32",
      "total_flops_so_far": 695255579947008.0,
      "budget_used_percent": 0.6952555799470079
    },
    {
      "type": "training",
      "description": "Training step 486",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:33",
      "total_flops_so_far": 696686146983936.0,
      "budget_used_percent": 0.6966861469839359
    },
    {
      "type": "training",
      "description": "Training step 487",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:33",
      "total_flops_so_far": 698116714020864.0,
      "budget_used_percent": 0.698116714020864
    },
    {
      "type": "training",
      "description": "Training step 488",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:33",
      "total_flops_so_far": 699547281057792.0,
      "budget_used_percent": 0.699547281057792
    },
    {
      "type": "training",
      "description": "Training step 489",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:33",
      "total_flops_so_far": 700977848094720.0,
      "budget_used_percent": 0.70097784809472
    },
    {
      "type": "training",
      "description": "Training step 490",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:33",
      "total_flops_so_far": 702408415131648.0,
      "budget_used_percent": 0.702408415131648
    },
    {
      "type": "training",
      "description": "Training step 491",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:33",
      "total_flops_so_far": 703838982168576.0,
      "budget_used_percent": 0.703838982168576
    },
    {
      "type": "training",
      "description": "Training step 492",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:33",
      "total_flops_so_far": 705269549205504.0,
      "budget_used_percent": 0.705269549205504
    },
    {
      "type": "training",
      "description": "Training step 493",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:33",
      "total_flops_so_far": 706700116242432.0,
      "budget_used_percent": 0.706700116242432
    },
    {
      "type": "training",
      "description": "Training step 494",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:34",
      "total_flops_so_far": 708130683279360.0,
      "budget_used_percent": 0.7081306832793599
    },
    {
      "type": "training",
      "description": "Training step 495",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:34",
      "total_flops_so_far": 709561250316288.0,
      "budget_used_percent": 0.7095612503162879
    },
    {
      "type": "training",
      "description": "Training step 496",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:34",
      "total_flops_so_far": 710991817353216.0,
      "budget_used_percent": 0.7109918173532159
    },
    {
      "type": "training",
      "description": "Training step 497",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:34",
      "total_flops_so_far": 712422384390144.0,
      "budget_used_percent": 0.7124223843901439
    },
    {
      "type": "training",
      "description": "Training step 498",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:34",
      "total_flops_so_far": 713852951427072.0,
      "budget_used_percent": 0.713852951427072
    },
    {
      "type": "training",
      "description": "Training step 499",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:34",
      "total_flops_so_far": 715283518464000.0,
      "budget_used_percent": 0.715283518464
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 551857894496.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:36",
      "total_flops_so_far": 715835376358496.0,
      "budget_used_percent": 0.715835376358496
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 555458032848.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:37",
      "total_flops_so_far": 716390834391344.0,
      "budget_used_percent": 0.716390834391344
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 553657603480.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:39",
      "total_flops_so_far": 716944491994824.0,
      "budget_used_percent": 0.716944491994824
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 551857894496.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:40",
      "total_flops_so_far": 717496349889320.0,
      "budget_used_percent": 0.71749634988932
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 554557728116.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:41",
      "total_flops_so_far": 718050907617436.0,
      "budget_used_percent": 0.718050907617436
    },
    {
      "type": "training",
      "description": "Training step 500",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:42",
      "total_flops_so_far": 719481474654364.0,
      "budget_used_percent": 0.7194814746543641
    },
    {
      "type": "training",
      "description": "Training step 501",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:42",
      "total_flops_so_far": 720912041691292.0,
      "budget_used_percent": 0.720912041691292
    },
    {
      "type": "training",
      "description": "Training step 502",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:42",
      "total_flops_so_far": 722342608728220.0,
      "budget_used_percent": 0.7223426087282201
    },
    {
      "type": "training",
      "description": "Training step 503",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:42",
      "total_flops_so_far": 723773175765148.0,
      "budget_used_percent": 0.723773175765148
    },
    {
      "type": "training",
      "description": "Training step 504",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:42",
      "total_flops_so_far": 725203742802076.0,
      "budget_used_percent": 0.725203742802076
    },
    {
      "type": "training",
      "description": "Training step 505",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:42",
      "total_flops_so_far": 726634309839004.0,
      "budget_used_percent": 0.726634309839004
    },
    {
      "type": "training",
      "description": "Training step 506",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:42",
      "total_flops_so_far": 728064876875932.0,
      "budget_used_percent": 0.728064876875932
    },
    {
      "type": "training",
      "description": "Training step 507",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:42",
      "total_flops_so_far": 729495443912860.0,
      "budget_used_percent": 0.72949544391286
    },
    {
      "type": "training",
      "description": "Training step 508",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:43",
      "total_flops_so_far": 730926010949788.0,
      "budget_used_percent": 0.730926010949788
    },
    {
      "type": "training",
      "description": "Training step 509",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:43",
      "total_flops_so_far": 732356577986716.0,
      "budget_used_percent": 0.732356577986716
    },
    {
      "type": "training",
      "description": "Training step 510",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:43",
      "total_flops_so_far": 733787145023644.0,
      "budget_used_percent": 0.7337871450236441
    },
    {
      "type": "training",
      "description": "Training step 511",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:43",
      "total_flops_so_far": 735217712060572.0,
      "budget_used_percent": 0.7352177120605721
    },
    {
      "type": "training",
      "description": "Training step 512",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:43",
      "total_flops_so_far": 736648279097500.0,
      "budget_used_percent": 0.7366482790975
    },
    {
      "type": "training",
      "description": "Training step 513",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:43",
      "total_flops_so_far": 738078846134428.0,
      "budget_used_percent": 0.738078846134428
    },
    {
      "type": "training",
      "description": "Training step 514",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:43",
      "total_flops_so_far": 739509413171356.0,
      "budget_used_percent": 0.739509413171356
    },
    {
      "type": "training",
      "description": "Training step 515",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:43",
      "total_flops_so_far": 740939980208284.0,
      "budget_used_percent": 0.740939980208284
    },
    {
      "type": "training",
      "description": "Training step 516",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:44",
      "total_flops_so_far": 742370547245212.0,
      "budget_used_percent": 0.742370547245212
    },
    {
      "type": "training",
      "description": "Training step 517",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:44",
      "total_flops_so_far": 743801114282140.0,
      "budget_used_percent": 0.74380111428214
    },
    {
      "type": "training",
      "description": "Training step 518",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:44",
      "total_flops_so_far": 745231681319068.0,
      "budget_used_percent": 0.745231681319068
    },
    {
      "type": "training",
      "description": "Training step 519",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:44",
      "total_flops_so_far": 746662248355996.0,
      "budget_used_percent": 0.746662248355996
    },
    {
      "type": "training",
      "description": "Training step 520",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:44",
      "total_flops_so_far": 748092815392924.0,
      "budget_used_percent": 0.748092815392924
    },
    {
      "type": "training",
      "description": "Training step 521",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:44",
      "total_flops_so_far": 749523382429852.0,
      "budget_used_percent": 0.749523382429852
    },
    {
      "type": "training",
      "description": "Training step 522",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:44",
      "total_flops_so_far": 750953949466780.0,
      "budget_used_percent": 0.75095394946678
    },
    {
      "type": "training",
      "description": "Training step 523",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:45",
      "total_flops_so_far": 752384516503708.0,
      "budget_used_percent": 0.752384516503708
    },
    {
      "type": "training",
      "description": "Training step 524",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:45",
      "total_flops_so_far": 753815083540636.0,
      "budget_used_percent": 0.753815083540636
    },
    {
      "type": "training",
      "description": "Training step 525",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:45",
      "total_flops_so_far": 755245650577564.0,
      "budget_used_percent": 0.755245650577564
    },
    {
      "type": "training",
      "description": "Training step 526",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:45",
      "total_flops_so_far": 756676217614492.0,
      "budget_used_percent": 0.756676217614492
    },
    {
      "type": "training",
      "description": "Training step 527",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:45",
      "total_flops_so_far": 758106784651420.0,
      "budget_used_percent": 0.75810678465142
    },
    {
      "type": "training",
      "description": "Training step 528",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:45",
      "total_flops_so_far": 759537351688348.0,
      "budget_used_percent": 0.759537351688348
    },
    {
      "type": "training",
      "description": "Training step 529",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:45",
      "total_flops_so_far": 760967918725276.0,
      "budget_used_percent": 0.7609679187252759
    },
    {
      "type": "training",
      "description": "Training step 530",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:45",
      "total_flops_so_far": 762398485762204.0,
      "budget_used_percent": 0.7623984857622039
    },
    {
      "type": "training",
      "description": "Training step 531",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:46",
      "total_flops_so_far": 763829052799132.0,
      "budget_used_percent": 0.7638290527991319
    },
    {
      "type": "training",
      "description": "Training step 532",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:46",
      "total_flops_so_far": 765259619836060.0,
      "budget_used_percent": 0.76525961983606
    },
    {
      "type": "training",
      "description": "Training step 533",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:46",
      "total_flops_so_far": 766690186872988.0,
      "budget_used_percent": 0.766690186872988
    },
    {
      "type": "training",
      "description": "Training step 534",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:46",
      "total_flops_so_far": 768120753909916.0,
      "budget_used_percent": 0.768120753909916
    },
    {
      "type": "training",
      "description": "Training step 535",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:46",
      "total_flops_so_far": 769551320946844.0,
      "budget_used_percent": 0.769551320946844
    },
    {
      "type": "training",
      "description": "Training step 536",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:46",
      "total_flops_so_far": 770981887983772.0,
      "budget_used_percent": 0.770981887983772
    },
    {
      "type": "training",
      "description": "Training step 537",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:46",
      "total_flops_so_far": 772412455020700.0,
      "budget_used_percent": 0.7724124550207
    },
    {
      "type": "training",
      "description": "Training step 538",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:46",
      "total_flops_so_far": 773843022057628.0,
      "budget_used_percent": 0.773843022057628
    },
    {
      "type": "training",
      "description": "Training step 539",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:47",
      "total_flops_so_far": 775273589094556.0,
      "budget_used_percent": 0.775273589094556
    },
    {
      "type": "training",
      "description": "Training step 540",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:47",
      "total_flops_so_far": 776704156131484.0,
      "budget_used_percent": 0.776704156131484
    },
    {
      "type": "training",
      "description": "Training step 541",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:47",
      "total_flops_so_far": 778134723168412.0,
      "budget_used_percent": 0.778134723168412
    },
    {
      "type": "training",
      "description": "Training step 542",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:47",
      "total_flops_so_far": 779565290205340.0,
      "budget_used_percent": 0.77956529020534
    },
    {
      "type": "training",
      "description": "Training step 543",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:47",
      "total_flops_so_far": 780995857242268.0,
      "budget_used_percent": 0.780995857242268
    },
    {
      "type": "training",
      "description": "Training step 544",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:47",
      "total_flops_so_far": 782426424279196.0,
      "budget_used_percent": 0.782426424279196
    },
    {
      "type": "training",
      "description": "Training step 545",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:47",
      "total_flops_so_far": 783856991316124.0,
      "budget_used_percent": 0.783856991316124
    },
    {
      "type": "training",
      "description": "Training step 546",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:48",
      "total_flops_so_far": 785287558353052.0,
      "budget_used_percent": 0.785287558353052
    },
    {
      "type": "training",
      "description": "Training step 547",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:48",
      "total_flops_so_far": 786718125389980.0,
      "budget_used_percent": 0.78671812538998
    },
    {
      "type": "training",
      "description": "Training step 548",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:48",
      "total_flops_so_far": 788148692426908.0,
      "budget_used_percent": 0.788148692426908
    },
    {
      "type": "training",
      "description": "Training step 549",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:48",
      "total_flops_so_far": 789579259463836.0,
      "budget_used_percent": 0.789579259463836
    },
    {
      "type": "training",
      "description": "Training step 550",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:48",
      "total_flops_so_far": 791009826500764.0,
      "budget_used_percent": 0.791009826500764
    },
    {
      "type": "training",
      "description": "Training step 551",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:48",
      "total_flops_so_far": 792440393537692.0,
      "budget_used_percent": 0.792440393537692
    },
    {
      "type": "training",
      "description": "Training step 552",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:48",
      "total_flops_so_far": 793870960574620.0,
      "budget_used_percent": 0.79387096057462
    },
    {
      "type": "training",
      "description": "Training step 553",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:48",
      "total_flops_so_far": 795301527611548.0,
      "budget_used_percent": 0.795301527611548
    },
    {
      "type": "training",
      "description": "Training step 554",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:49",
      "total_flops_so_far": 796732094648476.0,
      "budget_used_percent": 0.796732094648476
    },
    {
      "type": "training",
      "description": "Training step 555",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:49",
      "total_flops_so_far": 798162661685404.0,
      "budget_used_percent": 0.7981626616854041
    },
    {
      "type": "training",
      "description": "Training step 556",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:49",
      "total_flops_so_far": 799593228722332.0,
      "budget_used_percent": 0.7995932287223321
    },
    {
      "type": "training",
      "description": "Training step 557",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:49",
      "total_flops_so_far": 801023795759260.0,
      "budget_used_percent": 0.8010237957592601
    },
    {
      "type": "training",
      "description": "Training step 558",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:49",
      "total_flops_so_far": 802454362796188.0,
      "budget_used_percent": 0.8024543627961881
    },
    {
      "type": "training",
      "description": "Training step 559",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:49",
      "total_flops_so_far": 803884929833116.0,
      "budget_used_percent": 0.8038849298331161
    },
    {
      "type": "training",
      "description": "Training step 560",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:49",
      "total_flops_so_far": 805315496870044.0,
      "budget_used_percent": 0.8053154968700439
    },
    {
      "type": "training",
      "description": "Training step 561",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:49",
      "total_flops_so_far": 806746063906972.0,
      "budget_used_percent": 0.8067460639069719
    },
    {
      "type": "training",
      "description": "Training step 562",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:50",
      "total_flops_so_far": 808176630943900.0,
      "budget_used_percent": 0.8081766309438999
    },
    {
      "type": "training",
      "description": "Training step 563",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:50",
      "total_flops_so_far": 809607197980828.0,
      "budget_used_percent": 0.8096071979808279
    },
    {
      "type": "training",
      "description": "Training step 564",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:50",
      "total_flops_so_far": 811037765017756.0,
      "budget_used_percent": 0.8110377650177559
    },
    {
      "type": "training",
      "description": "Training step 565",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:50",
      "total_flops_so_far": 812468332054684.0,
      "budget_used_percent": 0.8124683320546839
    },
    {
      "type": "training",
      "description": "Training step 566",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:50",
      "total_flops_so_far": 813898899091612.0,
      "budget_used_percent": 0.813898899091612
    },
    {
      "type": "training",
      "description": "Training step 567",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:50",
      "total_flops_so_far": 815329466128540.0,
      "budget_used_percent": 0.81532946612854
    },
    {
      "type": "training",
      "description": "Training step 568",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:50",
      "total_flops_so_far": 816760033165468.0,
      "budget_used_percent": 0.816760033165468
    },
    {
      "type": "training",
      "description": "Training step 569",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:51",
      "total_flops_so_far": 818190600202396.0,
      "budget_used_percent": 0.818190600202396
    },
    {
      "type": "training",
      "description": "Training step 570",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:51",
      "total_flops_so_far": 819621167239324.0,
      "budget_used_percent": 0.819621167239324
    },
    {
      "type": "training",
      "description": "Training step 571",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:51",
      "total_flops_so_far": 821051734276252.0,
      "budget_used_percent": 0.821051734276252
    },
    {
      "type": "training",
      "description": "Training step 572",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:51",
      "total_flops_so_far": 822482301313180.0,
      "budget_used_percent": 0.82248230131318
    },
    {
      "type": "training",
      "description": "Training step 573",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:51",
      "total_flops_so_far": 823912868350108.0,
      "budget_used_percent": 0.823912868350108
    },
    {
      "type": "training",
      "description": "Training step 574",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:51",
      "total_flops_so_far": 825343435387036.0,
      "budget_used_percent": 0.825343435387036
    },
    {
      "type": "training",
      "description": "Training step 575",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:51",
      "total_flops_so_far": 826774002423964.0,
      "budget_used_percent": 0.826774002423964
    },
    {
      "type": "training",
      "description": "Training step 576",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:51",
      "total_flops_so_far": 828204569460892.0,
      "budget_used_percent": 0.828204569460892
    },
    {
      "type": "training",
      "description": "Training step 577",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:52",
      "total_flops_so_far": 829635136497820.0,
      "budget_used_percent": 0.8296351364978201
    },
    {
      "type": "training",
      "description": "Training step 578",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:52",
      "total_flops_so_far": 831065703534748.0,
      "budget_used_percent": 0.831065703534748
    },
    {
      "type": "training",
      "description": "Training step 579",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:52",
      "total_flops_so_far": 832496270571676.0,
      "budget_used_percent": 0.832496270571676
    },
    {
      "type": "training",
      "description": "Training step 580",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:52",
      "total_flops_so_far": 833926837608604.0,
      "budget_used_percent": 0.833926837608604
    },
    {
      "type": "training",
      "description": "Training step 581",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:52",
      "total_flops_so_far": 835357404645532.0,
      "budget_used_percent": 0.835357404645532
    },
    {
      "type": "training",
      "description": "Training step 582",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:52",
      "total_flops_so_far": 836787971682460.0,
      "budget_used_percent": 0.83678797168246
    },
    {
      "type": "training",
      "description": "Training step 583",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:52",
      "total_flops_so_far": 838218538719388.0,
      "budget_used_percent": 0.838218538719388
    },
    {
      "type": "training",
      "description": "Training step 584",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:52",
      "total_flops_so_far": 839649105756316.0,
      "budget_used_percent": 0.839649105756316
    },
    {
      "type": "training",
      "description": "Training step 585",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:53",
      "total_flops_so_far": 841079672793244.0,
      "budget_used_percent": 0.841079672793244
    },
    {
      "type": "training",
      "description": "Training step 586",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:53",
      "total_flops_so_far": 842510239830172.0,
      "budget_used_percent": 0.842510239830172
    },
    {
      "type": "training",
      "description": "Training step 587",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:53",
      "total_flops_so_far": 843940806867100.0,
      "budget_used_percent": 0.8439408068671
    },
    {
      "type": "training",
      "description": "Training step 588",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:53",
      "total_flops_so_far": 845371373904028.0,
      "budget_used_percent": 0.8453713739040281
    },
    {
      "type": "training",
      "description": "Training step 589",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:53",
      "total_flops_so_far": 846801940940956.0,
      "budget_used_percent": 0.8468019409409561
    },
    {
      "type": "training",
      "description": "Training step 590",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:53",
      "total_flops_so_far": 848232507977884.0,
      "budget_used_percent": 0.8482325079778841
    },
    {
      "type": "training",
      "description": "Training step 591",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:53",
      "total_flops_so_far": 849663075014812.0,
      "budget_used_percent": 0.8496630750148121
    },
    {
      "type": "training",
      "description": "Training step 592",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:54",
      "total_flops_so_far": 851093642051740.0,
      "budget_used_percent": 0.8510936420517401
    },
    {
      "type": "training",
      "description": "Training step 593",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:54",
      "total_flops_so_far": 852524209088668.0,
      "budget_used_percent": 0.8525242090886681
    },
    {
      "type": "training",
      "description": "Training step 594",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:54",
      "total_flops_so_far": 853954776125596.0,
      "budget_used_percent": 0.8539547761255961
    },
    {
      "type": "training",
      "description": "Training step 595",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:54",
      "total_flops_so_far": 855385343162524.0,
      "budget_used_percent": 0.8553853431625239
    },
    {
      "type": "training",
      "description": "Training step 596",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:54",
      "total_flops_so_far": 856815910199452.0,
      "budget_used_percent": 0.8568159101994519
    },
    {
      "type": "training",
      "description": "Training step 597",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:54",
      "total_flops_so_far": 858246477236380.0,
      "budget_used_percent": 0.8582464772363799
    },
    {
      "type": "training",
      "description": "Training step 598",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:54",
      "total_flops_so_far": 859677044273308.0,
      "budget_used_percent": 0.8596770442733079
    },
    {
      "type": "training",
      "description": "Training step 599",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:54",
      "total_flops_so_far": 861107611310236.0,
      "budget_used_percent": 0.8611076113102359
    },
    {
      "type": "training",
      "description": "Training step 600",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:55",
      "total_flops_so_far": 862538178347164.0,
      "budget_used_percent": 0.862538178347164
    },
    {
      "type": "training",
      "description": "Training step 601",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:55",
      "total_flops_so_far": 863968745384092.0,
      "budget_used_percent": 0.863968745384092
    },
    {
      "type": "training",
      "description": "Training step 602",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:55",
      "total_flops_so_far": 865399312421020.0,
      "budget_used_percent": 0.86539931242102
    },
    {
      "type": "training",
      "description": "Training step 603",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:55",
      "total_flops_so_far": 866829879457948.0,
      "budget_used_percent": 0.866829879457948
    },
    {
      "type": "training",
      "description": "Training step 604",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:55",
      "total_flops_so_far": 868260446494876.0,
      "budget_used_percent": 0.868260446494876
    },
    {
      "type": "training",
      "description": "Training step 605",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:55",
      "total_flops_so_far": 869691013531804.0,
      "budget_used_percent": 0.869691013531804
    },
    {
      "type": "training",
      "description": "Training step 606",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:55",
      "total_flops_so_far": 871121580568732.0,
      "budget_used_percent": 0.871121580568732
    },
    {
      "type": "training",
      "description": "Training step 607",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:56",
      "total_flops_so_far": 872552147605660.0,
      "budget_used_percent": 0.87255214760566
    },
    {
      "type": "training",
      "description": "Training step 608",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:56",
      "total_flops_so_far": 873982714642588.0,
      "budget_used_percent": 0.873982714642588
    },
    {
      "type": "training",
      "description": "Training step 609",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:56",
      "total_flops_so_far": 875413281679516.0,
      "budget_used_percent": 0.875413281679516
    },
    {
      "type": "training",
      "description": "Training step 610",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:56",
      "total_flops_so_far": 876843848716444.0,
      "budget_used_percent": 0.876843848716444
    },
    {
      "type": "training",
      "description": "Training step 611",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:56",
      "total_flops_so_far": 878274415753372.0,
      "budget_used_percent": 0.8782744157533721
    },
    {
      "type": "training",
      "description": "Training step 612",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:56",
      "total_flops_so_far": 879704982790300.0,
      "budget_used_percent": 0.8797049827903001
    },
    {
      "type": "training",
      "description": "Training step 613",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:56",
      "total_flops_so_far": 881135549827228.0,
      "budget_used_percent": 0.881135549827228
    },
    {
      "type": "training",
      "description": "Training step 614",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:56",
      "total_flops_so_far": 882566116864156.0,
      "budget_used_percent": 0.882566116864156
    },
    {
      "type": "training",
      "description": "Training step 615",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:57",
      "total_flops_so_far": 883996683901084.0,
      "budget_used_percent": 0.883996683901084
    },
    {
      "type": "training",
      "description": "Training step 616",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:57",
      "total_flops_so_far": 885427250938012.0,
      "budget_used_percent": 0.885427250938012
    },
    {
      "type": "training",
      "description": "Training step 617",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:57",
      "total_flops_so_far": 886857817974940.0,
      "budget_used_percent": 0.88685781797494
    },
    {
      "type": "training",
      "description": "Training step 618",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:57",
      "total_flops_so_far": 888288385011868.0,
      "budget_used_percent": 0.888288385011868
    },
    {
      "type": "training",
      "description": "Training step 619",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:57",
      "total_flops_so_far": 889718952048796.0,
      "budget_used_percent": 0.889718952048796
    },
    {
      "type": "training",
      "description": "Training step 620",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:57",
      "total_flops_so_far": 891149519085724.0,
      "budget_used_percent": 0.891149519085724
    },
    {
      "type": "training",
      "description": "Training step 621",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:57",
      "total_flops_so_far": 892580086122652.0,
      "budget_used_percent": 0.892580086122652
    },
    {
      "type": "training",
      "description": "Training step 622",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:58",
      "total_flops_so_far": 894010653159580.0,
      "budget_used_percent": 0.8940106531595801
    },
    {
      "type": "training",
      "description": "Training step 623",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:58",
      "total_flops_so_far": 895441220196508.0,
      "budget_used_percent": 0.8954412201965081
    },
    {
      "type": "training",
      "description": "Training step 624",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:58",
      "total_flops_so_far": 896871787233436.0,
      "budget_used_percent": 0.8968717872334361
    },
    {
      "type": "training",
      "description": "Training step 625",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:58",
      "total_flops_so_far": 898302354270364.0,
      "budget_used_percent": 0.8983023542703641
    },
    {
      "type": "training",
      "description": "Training step 626",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:58",
      "total_flops_so_far": 899732921307292.0,
      "budget_used_percent": 0.8997329213072921
    },
    {
      "type": "training",
      "description": "Training step 627",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:58",
      "total_flops_so_far": 901163488344220.0,
      "budget_used_percent": 0.9011634883442201
    },
    {
      "type": "training",
      "description": "Training step 628",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:58",
      "total_flops_so_far": 902594055381148.0,
      "budget_used_percent": 0.9025940553811481
    },
    {
      "type": "training",
      "description": "Training step 629",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:58",
      "total_flops_so_far": 904024622418076.0,
      "budget_used_percent": 0.9040246224180761
    },
    {
      "type": "training",
      "description": "Training step 630",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:59",
      "total_flops_so_far": 905455189455004.0,
      "budget_used_percent": 0.9054551894550039
    },
    {
      "type": "training",
      "description": "Training step 631",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:59",
      "total_flops_so_far": 906885756491932.0,
      "budget_used_percent": 0.9068857564919319
    },
    {
      "type": "training",
      "description": "Training step 632",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:59",
      "total_flops_so_far": 908316323528860.0,
      "budget_used_percent": 0.9083163235288599
    },
    {
      "type": "training",
      "description": "Training step 633",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:59",
      "total_flops_so_far": 909746890565788.0,
      "budget_used_percent": 0.909746890565788
    },
    {
      "type": "training",
      "description": "Training step 634",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:59",
      "total_flops_so_far": 911177457602716.0,
      "budget_used_percent": 0.911177457602716
    },
    {
      "type": "training",
      "description": "Training step 635",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:59",
      "total_flops_so_far": 912608024639644.0,
      "budget_used_percent": 0.912608024639644
    },
    {
      "type": "training",
      "description": "Training step 636",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:01:59",
      "total_flops_so_far": 914038591676572.0,
      "budget_used_percent": 0.914038591676572
    },
    {
      "type": "training",
      "description": "Training step 637",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:00",
      "total_flops_so_far": 915469158713500.0,
      "budget_used_percent": 0.9154691587135
    },
    {
      "type": "training",
      "description": "Training step 638",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:00",
      "total_flops_so_far": 916899725750428.0,
      "budget_used_percent": 0.916899725750428
    },
    {
      "type": "training",
      "description": "Training step 639",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:00",
      "total_flops_so_far": 918330292787356.0,
      "budget_used_percent": 0.918330292787356
    },
    {
      "type": "training",
      "description": "Training step 640",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:00",
      "total_flops_so_far": 919760859824284.0,
      "budget_used_percent": 0.919760859824284
    },
    {
      "type": "training",
      "description": "Training step 641",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:00",
      "total_flops_so_far": 921191426861212.0,
      "budget_used_percent": 0.921191426861212
    },
    {
      "type": "training",
      "description": "Training step 642",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:00",
      "total_flops_so_far": 922621993898140.0,
      "budget_used_percent": 0.92262199389814
    },
    {
      "type": "training",
      "description": "Training step 643",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:00",
      "total_flops_so_far": 924052560935068.0,
      "budget_used_percent": 0.924052560935068
    },
    {
      "type": "training",
      "description": "Training step 644",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:00",
      "total_flops_so_far": 925483127971996.0,
      "budget_used_percent": 0.9254831279719961
    },
    {
      "type": "training",
      "description": "Training step 645",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:01",
      "total_flops_so_far": 926913695008924.0,
      "budget_used_percent": 0.9269136950089241
    },
    {
      "type": "training",
      "description": "Training step 646",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:01",
      "total_flops_so_far": 928344262045852.0,
      "budget_used_percent": 0.9283442620458521
    },
    {
      "type": "training",
      "description": "Training step 647",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:01",
      "total_flops_so_far": 929774829082780.0,
      "budget_used_percent": 0.9297748290827801
    },
    {
      "type": "training",
      "description": "Training step 648",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:01",
      "total_flops_so_far": 931205396119708.0,
      "budget_used_percent": 0.9312053961197079
    },
    {
      "type": "training",
      "description": "Training step 649",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:01",
      "total_flops_so_far": 932635963156636.0,
      "budget_used_percent": 0.9326359631566359
    },
    {
      "type": "training",
      "description": "Training step 650",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:01",
      "total_flops_so_far": 934066530193564.0,
      "budget_used_percent": 0.9340665301935639
    },
    {
      "type": "training",
      "description": "Training step 651",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:01",
      "total_flops_so_far": 935497097230492.0,
      "budget_used_percent": 0.9354970972304919
    },
    {
      "type": "training",
      "description": "Training step 652",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:02",
      "total_flops_so_far": 936927664267420.0,
      "budget_used_percent": 0.9369276642674199
    },
    {
      "type": "training",
      "description": "Training step 653",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:02",
      "total_flops_so_far": 938358231304348.0,
      "budget_used_percent": 0.9383582313043479
    },
    {
      "type": "training",
      "description": "Training step 654",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:02",
      "total_flops_so_far": 939788798341276.0,
      "budget_used_percent": 0.9397887983412759
    },
    {
      "type": "training",
      "description": "Training step 655",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:02",
      "total_flops_so_far": 941219365378204.0,
      "budget_used_percent": 0.9412193653782039
    },
    {
      "type": "training",
      "description": "Training step 656",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:02",
      "total_flops_so_far": 942649932415132.0,
      "budget_used_percent": 0.942649932415132
    },
    {
      "type": "training",
      "description": "Training step 657",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:02",
      "total_flops_so_far": 944080499452060.0,
      "budget_used_percent": 0.94408049945206
    },
    {
      "type": "training",
      "description": "Training step 658",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:02",
      "total_flops_so_far": 945511066488988.0,
      "budget_used_percent": 0.945511066488988
    },
    {
      "type": "training",
      "description": "Training step 659",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:02",
      "total_flops_so_far": 946941633525916.0,
      "budget_used_percent": 0.946941633525916
    },
    {
      "type": "training",
      "description": "Training step 660",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:03",
      "total_flops_so_far": 948372200562844.0,
      "budget_used_percent": 0.948372200562844
    },
    {
      "type": "training",
      "description": "Training step 661",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:03",
      "total_flops_so_far": 949802767599772.0,
      "budget_used_percent": 0.949802767599772
    },
    {
      "type": "training",
      "description": "Training step 662",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:03",
      "total_flops_so_far": 951233334636700.0,
      "budget_used_percent": 0.9512333346367
    },
    {
      "type": "training",
      "description": "Training step 663",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:03",
      "total_flops_so_far": 952663901673628.0,
      "budget_used_percent": 0.952663901673628
    },
    {
      "type": "training",
      "description": "Training step 664",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:03",
      "total_flops_so_far": 954094468710556.0,
      "budget_used_percent": 0.954094468710556
    },
    {
      "type": "training",
      "description": "Training step 665",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:03",
      "total_flops_so_far": 955525035747484.0,
      "budget_used_percent": 0.9555250357474839
    },
    {
      "type": "training",
      "description": "Training step 666",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:03",
      "total_flops_so_far": 956955602784412.0,
      "budget_used_percent": 0.9569556027844119
    },
    {
      "type": "training",
      "description": "Training step 667",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:04",
      "total_flops_so_far": 958386169821340.0,
      "budget_used_percent": 0.95838616982134
    },
    {
      "type": "training",
      "description": "Training step 668",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:04",
      "total_flops_so_far": 959816736858268.0,
      "budget_used_percent": 0.959816736858268
    },
    {
      "type": "training",
      "description": "Training step 669",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:04",
      "total_flops_so_far": 961247303895196.0,
      "budget_used_percent": 0.961247303895196
    },
    {
      "type": "training",
      "description": "Training step 670",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:04",
      "total_flops_so_far": 962677870932124.0,
      "budget_used_percent": 0.962677870932124
    },
    {
      "type": "training",
      "description": "Training step 671",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:04",
      "total_flops_so_far": 964108437969052.0,
      "budget_used_percent": 0.964108437969052
    },
    {
      "type": "training",
      "description": "Training step 672",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:04",
      "total_flops_so_far": 965539005005980.0,
      "budget_used_percent": 0.96553900500598
    },
    {
      "type": "training",
      "description": "Training step 673",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:04",
      "total_flops_so_far": 966969572042908.0,
      "budget_used_percent": 0.966969572042908
    },
    {
      "type": "training",
      "description": "Training step 674",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:04",
      "total_flops_so_far": 968400139079836.0,
      "budget_used_percent": 0.968400139079836
    },
    {
      "type": "training",
      "description": "Training step 675",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:05",
      "total_flops_so_far": 969830706116764.0,
      "budget_used_percent": 0.969830706116764
    },
    {
      "type": "training",
      "description": "Training step 676",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:05",
      "total_flops_so_far": 971261273153692.0,
      "budget_used_percent": 0.971261273153692
    },
    {
      "type": "training",
      "description": "Training step 677",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:05",
      "total_flops_so_far": 972691840190620.0,
      "budget_used_percent": 0.97269184019062
    },
    {
      "type": "training",
      "description": "Training step 678",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:05",
      "total_flops_so_far": 974122407227548.0,
      "budget_used_percent": 0.9741224072275481
    },
    {
      "type": "training",
      "description": "Training step 679",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:05",
      "total_flops_so_far": 975552974264476.0,
      "budget_used_percent": 0.9755529742644761
    },
    {
      "type": "training",
      "description": "Training step 680",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:05",
      "total_flops_so_far": 976983541301404.0,
      "budget_used_percent": 0.9769835413014041
    },
    {
      "type": "training",
      "description": "Training step 681",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:05",
      "total_flops_so_far": 978414108338332.0,
      "budget_used_percent": 0.9784141083383321
    },
    {
      "type": "training",
      "description": "Training step 682",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:06",
      "total_flops_so_far": 979844675375260.0,
      "budget_used_percent": 0.9798446753752601
    },
    {
      "type": "training",
      "description": "Training step 683",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:06",
      "total_flops_so_far": 981275242412188.0,
      "budget_used_percent": 0.9812752424121879
    },
    {
      "type": "training",
      "description": "Training step 684",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:06",
      "total_flops_so_far": 982705809449116.0,
      "budget_used_percent": 0.9827058094491159
    },
    {
      "type": "training",
      "description": "Training step 685",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:06",
      "total_flops_so_far": 984136376486044.0,
      "budget_used_percent": 0.9841363764860439
    },
    {
      "type": "training",
      "description": "Training step 686",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:06",
      "total_flops_so_far": 985566943522972.0,
      "budget_used_percent": 0.9855669435229719
    },
    {
      "type": "training",
      "description": "Training step 687",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:06",
      "total_flops_so_far": 986997510559900.0,
      "budget_used_percent": 0.9869975105598999
    },
    {
      "type": "training",
      "description": "Training step 688",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:06",
      "total_flops_so_far": 988428077596828.0,
      "budget_used_percent": 0.9884280775968279
    },
    {
      "type": "training",
      "description": "Training step 689",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:06",
      "total_flops_so_far": 989858644633756.0,
      "budget_used_percent": 0.989858644633756
    },
    {
      "type": "training",
      "description": "Training step 690",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:07",
      "total_flops_so_far": 991289211670684.0,
      "budget_used_percent": 0.991289211670684
    },
    {
      "type": "training",
      "description": "Training step 691",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:07",
      "total_flops_so_far": 992719778707612.0,
      "budget_used_percent": 0.992719778707612
    },
    {
      "type": "training",
      "description": "Training step 692",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:07",
      "total_flops_so_far": 994150345744540.0,
      "budget_used_percent": 0.99415034574454
    },
    {
      "type": "training",
      "description": "Training step 693",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:07",
      "total_flops_so_far": 995580912781468.0,
      "budget_used_percent": 0.995580912781468
    },
    {
      "type": "training",
      "description": "Training step 694",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:07",
      "total_flops_so_far": 997011479818396.0,
      "budget_used_percent": 0.997011479818396
    },
    {
      "type": "training",
      "description": "Training step 695",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:07",
      "total_flops_so_far": 998442046855324.0,
      "budget_used_percent": 0.998442046855324
    },
    {
      "type": "training",
      "description": "Training step 696",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:07",
      "total_flops_so_far": 999872613892252.0,
      "budget_used_percent": 0.999872613892252
    },
    {
      "type": "training",
      "description": "Training step 697",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:08",
      "total_flops_so_far": 1001303180929180.0,
      "budget_used_percent": 1.0013031809291801
    },
    {
      "type": "training",
      "description": "Training step 698",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:08",
      "total_flops_so_far": 1002733747966108.0,
      "budget_used_percent": 1.002733747966108
    },
    {
      "type": "training",
      "description": "Training step 699",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:08",
      "total_flops_so_far": 1004164315003036.0,
      "budget_used_percent": 1.0041643150030362
    },
    {
      "type": "training",
      "description": "Training step 700",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:08",
      "total_flops_so_far": 1005594882039964.0,
      "budget_used_percent": 1.0055948820399638
    },
    {
      "type": "training",
      "description": "Training step 701",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:08",
      "total_flops_so_far": 1007025449076892.0,
      "budget_used_percent": 1.007025449076892
    },
    {
      "type": "training",
      "description": "Training step 702",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:08",
      "total_flops_so_far": 1008456016113820.0,
      "budget_used_percent": 1.0084560161138199
    },
    {
      "type": "training",
      "description": "Training step 703",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:08",
      "total_flops_so_far": 1009886583150748.0,
      "budget_used_percent": 1.009886583150748
    },
    {
      "type": "training",
      "description": "Training step 704",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:08",
      "total_flops_so_far": 1011317150187676.0,
      "budget_used_percent": 1.0113171501876759
    },
    {
      "type": "training",
      "description": "Training step 705",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:09",
      "total_flops_so_far": 1012747717224604.0,
      "budget_used_percent": 1.012747717224604
    },
    {
      "type": "training",
      "description": "Training step 706",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:09",
      "total_flops_so_far": 1014178284261532.0,
      "budget_used_percent": 1.014178284261532
    },
    {
      "type": "training",
      "description": "Training step 707",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:09",
      "total_flops_so_far": 1015608851298460.0,
      "budget_used_percent": 1.01560885129846
    },
    {
      "type": "training",
      "description": "Training step 708",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:09",
      "total_flops_so_far": 1017039418335388.0,
      "budget_used_percent": 1.017039418335388
    },
    {
      "type": "training",
      "description": "Training step 709",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:09",
      "total_flops_so_far": 1018469985372316.0,
      "budget_used_percent": 1.018469985372316
    },
    {
      "type": "training",
      "description": "Training step 710",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:09",
      "total_flops_so_far": 1019900552409244.0,
      "budget_used_percent": 1.019900552409244
    },
    {
      "type": "training",
      "description": "Training step 711",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:10",
      "total_flops_so_far": 1021331119446172.0,
      "budget_used_percent": 1.021331119446172
    },
    {
      "type": "training",
      "description": "Training step 712",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:10",
      "total_flops_so_far": 1022761686483100.0,
      "budget_used_percent": 1.0227616864831
    },
    {
      "type": "training",
      "description": "Training step 713",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:10",
      "total_flops_so_far": 1024192253520028.0,
      "budget_used_percent": 1.024192253520028
    },
    {
      "type": "training",
      "description": "Training step 714",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:10",
      "total_flops_so_far": 1025622820556956.0,
      "budget_used_percent": 1.025622820556956
    },
    {
      "type": "training",
      "description": "Training step 715",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:10",
      "total_flops_so_far": 1027053387593884.0,
      "budget_used_percent": 1.027053387593884
    },
    {
      "type": "training",
      "description": "Training step 716",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:10",
      "total_flops_so_far": 1028483954630812.0,
      "budget_used_percent": 1.028483954630812
    },
    {
      "type": "training",
      "description": "Training step 717",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:10",
      "total_flops_so_far": 1029914521667740.0,
      "budget_used_percent": 1.0299145216677401
    },
    {
      "type": "training",
      "description": "Training step 718",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:10",
      "total_flops_so_far": 1031345088704668.0,
      "budget_used_percent": 1.031345088704668
    },
    {
      "type": "training",
      "description": "Training step 719",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:11",
      "total_flops_so_far": 1032775655741596.0,
      "budget_used_percent": 1.032775655741596
    },
    {
      "type": "training",
      "description": "Training step 720",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:11",
      "total_flops_so_far": 1034206222778524.0,
      "budget_used_percent": 1.034206222778524
    },
    {
      "type": "training",
      "description": "Training step 721",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:11",
      "total_flops_so_far": 1035636789815452.0,
      "budget_used_percent": 1.035636789815452
    },
    {
      "type": "training",
      "description": "Training step 722",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:11",
      "total_flops_so_far": 1037067356852380.0,
      "budget_used_percent": 1.03706735685238
    },
    {
      "type": "training",
      "description": "Training step 723",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:11",
      "total_flops_so_far": 1038497923889308.0,
      "budget_used_percent": 1.038497923889308
    },
    {
      "type": "training",
      "description": "Training step 724",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:11",
      "total_flops_so_far": 1039928490926236.0,
      "budget_used_percent": 1.039928490926236
    },
    {
      "type": "training",
      "description": "Training step 725",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:11",
      "total_flops_so_far": 1041359057963164.0,
      "budget_used_percent": 1.041359057963164
    },
    {
      "type": "training",
      "description": "Training step 726",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:12",
      "total_flops_so_far": 1042789625000092.0,
      "budget_used_percent": 1.042789625000092
    },
    {
      "type": "training",
      "description": "Training step 727",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:12",
      "total_flops_so_far": 1044220192037020.0,
      "budget_used_percent": 1.04422019203702
    },
    {
      "type": "training",
      "description": "Training step 728",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:12",
      "total_flops_so_far": 1045650759073948.0,
      "budget_used_percent": 1.045650759073948
    },
    {
      "type": "training",
      "description": "Training step 729",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:12",
      "total_flops_so_far": 1047081326110876.0,
      "budget_used_percent": 1.047081326110876
    },
    {
      "type": "training",
      "description": "Training step 730",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:12",
      "total_flops_so_far": 1048511893147804.0,
      "budget_used_percent": 1.0485118931478041
    },
    {
      "type": "training",
      "description": "Training step 731",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:12",
      "total_flops_so_far": 1049942460184732.0,
      "budget_used_percent": 1.049942460184732
    },
    {
      "type": "training",
      "description": "Training step 732",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:12",
      "total_flops_so_far": 1051373027221660.0,
      "budget_used_percent": 1.0513730272216602
    },
    {
      "type": "training",
      "description": "Training step 733",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:13",
      "total_flops_so_far": 1052803594258588.0,
      "budget_used_percent": 1.052803594258588
    },
    {
      "type": "training",
      "description": "Training step 734",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:13",
      "total_flops_so_far": 1054234161295516.0,
      "budget_used_percent": 1.0542341612955162
    },
    {
      "type": "training",
      "description": "Training step 735",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:13",
      "total_flops_so_far": 1055664728332444.0,
      "budget_used_percent": 1.0556647283324438
    },
    {
      "type": "training",
      "description": "Training step 736",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:13",
      "total_flops_so_far": 1057095295369372.0,
      "budget_used_percent": 1.057095295369372
    },
    {
      "type": "training",
      "description": "Training step 737",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:13",
      "total_flops_so_far": 1058525862406300.0,
      "budget_used_percent": 1.0585258624062999
    },
    {
      "type": "training",
      "description": "Training step 738",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:13",
      "total_flops_so_far": 1059956429443228.0,
      "budget_used_percent": 1.059956429443228
    },
    {
      "type": "training",
      "description": "Training step 739",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:13",
      "total_flops_so_far": 1061386996480156.0,
      "budget_used_percent": 1.0613869964801559
    },
    {
      "type": "training",
      "description": "Training step 740",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:13",
      "total_flops_so_far": 1062817563517084.0,
      "budget_used_percent": 1.062817563517084
    },
    {
      "type": "training",
      "description": "Training step 741",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:14",
      "total_flops_so_far": 1064248130554012.0,
      "budget_used_percent": 1.064248130554012
    },
    {
      "type": "training",
      "description": "Training step 742",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:14",
      "total_flops_so_far": 1065678697590940.0,
      "budget_used_percent": 1.06567869759094
    },
    {
      "type": "training",
      "description": "Training step 743",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:14",
      "total_flops_so_far": 1067109264627868.0,
      "budget_used_percent": 1.067109264627868
    },
    {
      "type": "training",
      "description": "Training step 744",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:14",
      "total_flops_so_far": 1068539831664796.0,
      "budget_used_percent": 1.068539831664796
    },
    {
      "type": "training",
      "description": "Training step 745",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:14",
      "total_flops_so_far": 1069970398701724.0,
      "budget_used_percent": 1.069970398701724
    },
    {
      "type": "training",
      "description": "Training step 746",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:14",
      "total_flops_so_far": 1071400965738652.0,
      "budget_used_percent": 1.071400965738652
    },
    {
      "type": "training",
      "description": "Training step 747",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:14",
      "total_flops_so_far": 1072831532775580.0,
      "budget_used_percent": 1.07283153277558
    },
    {
      "type": "training",
      "description": "Training step 748",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:15",
      "total_flops_so_far": 1074262099812508.0,
      "budget_used_percent": 1.074262099812508
    },
    {
      "type": "training",
      "description": "Training step 749",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:15",
      "total_flops_so_far": 1075692666849436.0,
      "budget_used_percent": 1.075692666849436
    },
    {
      "type": "training",
      "description": "Training step 750",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:15",
      "total_flops_so_far": 1077123233886364.0,
      "budget_used_percent": 1.077123233886364
    },
    {
      "type": "training",
      "description": "Training step 751",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:15",
      "total_flops_so_far": 1078553800923292.0,
      "budget_used_percent": 1.078553800923292
    },
    {
      "type": "training",
      "description": "Training step 752",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:15",
      "total_flops_so_far": 1079984367960220.0,
      "budget_used_percent": 1.0799843679602201
    },
    {
      "type": "training",
      "description": "Training step 753",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:15",
      "total_flops_so_far": 1081414934997148.0,
      "budget_used_percent": 1.081414934997148
    },
    {
      "type": "training",
      "description": "Training step 754",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:15",
      "total_flops_so_far": 1082845502034076.0,
      "budget_used_percent": 1.082845502034076
    },
    {
      "type": "training",
      "description": "Training step 755",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:15",
      "total_flops_so_far": 1084276069071004.0,
      "budget_used_percent": 1.084276069071004
    },
    {
      "type": "training",
      "description": "Training step 756",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:16",
      "total_flops_so_far": 1085706636107932.0,
      "budget_used_percent": 1.085706636107932
    },
    {
      "type": "training",
      "description": "Training step 757",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:16",
      "total_flops_so_far": 1087137203144860.0,
      "budget_used_percent": 1.08713720314486
    },
    {
      "type": "training",
      "description": "Training step 758",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:16",
      "total_flops_so_far": 1088567770181788.0,
      "budget_used_percent": 1.088567770181788
    },
    {
      "type": "training",
      "description": "Training step 759",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:16",
      "total_flops_so_far": 1089998337218716.0,
      "budget_used_percent": 1.089998337218716
    },
    {
      "type": "training",
      "description": "Training step 760",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:16",
      "total_flops_so_far": 1091428904255644.0,
      "budget_used_percent": 1.091428904255644
    },
    {
      "type": "training",
      "description": "Training step 761",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:16",
      "total_flops_so_far": 1092859471292572.0,
      "budget_used_percent": 1.092859471292572
    },
    {
      "type": "training",
      "description": "Training step 762",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:16",
      "total_flops_so_far": 1094290038329500.0,
      "budget_used_percent": 1.0942900383295
    },
    {
      "type": "training",
      "description": "Training step 763",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:17",
      "total_flops_so_far": 1095720605366428.0,
      "budget_used_percent": 1.0957206053664281
    },
    {
      "type": "training",
      "description": "Training step 764",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:17",
      "total_flops_so_far": 1097151172403356.0,
      "budget_used_percent": 1.097151172403356
    },
    {
      "type": "training",
      "description": "Training step 765",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:17",
      "total_flops_so_far": 1098581739440284.0,
      "budget_used_percent": 1.0985817394402841
    },
    {
      "type": "training",
      "description": "Training step 766",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:17",
      "total_flops_so_far": 1100012306477212.0,
      "budget_used_percent": 1.100012306477212
    },
    {
      "type": "training",
      "description": "Training step 767",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:17",
      "total_flops_so_far": 1101442873514140.0,
      "budget_used_percent": 1.1014428735141402
    },
    {
      "type": "training",
      "description": "Training step 768",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:17",
      "total_flops_so_far": 1102873440551068.0,
      "budget_used_percent": 1.102873440551068
    },
    {
      "type": "training",
      "description": "Training step 769",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:17",
      "total_flops_so_far": 1104304007587996.0,
      "budget_used_percent": 1.1043040075879962
    },
    {
      "type": "training",
      "description": "Training step 770",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:18",
      "total_flops_so_far": 1105734574624924.0,
      "budget_used_percent": 1.1057345746249239
    },
    {
      "type": "training",
      "description": "Training step 771",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:18",
      "total_flops_so_far": 1107165141661852.0,
      "budget_used_percent": 1.107165141661852
    },
    {
      "type": "training",
      "description": "Training step 772",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:18",
      "total_flops_so_far": 1108595708698780.0,
      "budget_used_percent": 1.1085957086987799
    },
    {
      "type": "training",
      "description": "Training step 773",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:18",
      "total_flops_so_far": 1110026275735708.0,
      "budget_used_percent": 1.110026275735708
    },
    {
      "type": "training",
      "description": "Training step 774",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:18",
      "total_flops_so_far": 1111456842772636.0,
      "budget_used_percent": 1.111456842772636
    },
    {
      "type": "training",
      "description": "Training step 775",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:18",
      "total_flops_so_far": 1112887409809564.0,
      "budget_used_percent": 1.112887409809564
    },
    {
      "type": "training",
      "description": "Training step 776",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:18",
      "total_flops_so_far": 1114317976846492.0,
      "budget_used_percent": 1.114317976846492
    },
    {
      "type": "training",
      "description": "Training step 777",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:18",
      "total_flops_so_far": 1115748543883420.0,
      "budget_used_percent": 1.11574854388342
    },
    {
      "type": "training",
      "description": "Training step 778",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:19",
      "total_flops_so_far": 1117179110920348.0,
      "budget_used_percent": 1.117179110920348
    },
    {
      "type": "training",
      "description": "Training step 779",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:19",
      "total_flops_so_far": 1118609677957276.0,
      "budget_used_percent": 1.118609677957276
    },
    {
      "type": "training",
      "description": "Training step 780",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:19",
      "total_flops_so_far": 1120040244994204.0,
      "budget_used_percent": 1.120040244994204
    },
    {
      "type": "training",
      "description": "Training step 781",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:19",
      "total_flops_so_far": 1121470812031132.0,
      "budget_used_percent": 1.121470812031132
    },
    {
      "type": "training",
      "description": "Training step 782",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:19",
      "total_flops_so_far": 1122901379068060.0,
      "budget_used_percent": 1.12290137906806
    },
    {
      "type": "training",
      "description": "Training step 783",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:19",
      "total_flops_so_far": 1124331946104988.0,
      "budget_used_percent": 1.124331946104988
    },
    {
      "type": "training",
      "description": "Training step 784",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:19",
      "total_flops_so_far": 1125762513141916.0,
      "budget_used_percent": 1.125762513141916
    },
    {
      "type": "training",
      "description": "Training step 785",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:20",
      "total_flops_so_far": 1127193080178844.0,
      "budget_used_percent": 1.1271930801788441
    },
    {
      "type": "training",
      "description": "Training step 786",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:20",
      "total_flops_so_far": 1128623647215772.0,
      "budget_used_percent": 1.128623647215772
    },
    {
      "type": "training",
      "description": "Training step 787",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:20",
      "total_flops_so_far": 1130054214252700.0,
      "budget_used_percent": 1.1300542142527001
    },
    {
      "type": "training",
      "description": "Training step 788",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:20",
      "total_flops_so_far": 1131484781289628.0,
      "budget_used_percent": 1.1314847812896278
    },
    {
      "type": "training",
      "description": "Training step 789",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:20",
      "total_flops_so_far": 1132915348326556.0,
      "budget_used_percent": 1.132915348326556
    },
    {
      "type": "training",
      "description": "Training step 790",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:20",
      "total_flops_so_far": 1134345915363484.0,
      "budget_used_percent": 1.1343459153634838
    },
    {
      "type": "training",
      "description": "Training step 791",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:20",
      "total_flops_so_far": 1135776482400412.0,
      "budget_used_percent": 1.135776482400412
    },
    {
      "type": "training",
      "description": "Training step 792",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:21",
      "total_flops_so_far": 1137207049437340.0,
      "budget_used_percent": 1.1372070494373399
    },
    {
      "type": "training",
      "description": "Training step 793",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:21",
      "total_flops_so_far": 1138637616474268.0,
      "budget_used_percent": 1.138637616474268
    },
    {
      "type": "training",
      "description": "Training step 794",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:21",
      "total_flops_so_far": 1140068183511196.0,
      "budget_used_percent": 1.1400681835111959
    },
    {
      "type": "training",
      "description": "Training step 795",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:21",
      "total_flops_so_far": 1141498750548124.0,
      "budget_used_percent": 1.141498750548124
    },
    {
      "type": "training",
      "description": "Training step 796",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:21",
      "total_flops_so_far": 1142929317585052.0,
      "budget_used_percent": 1.142929317585052
    },
    {
      "type": "training",
      "description": "Training step 797",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:21",
      "total_flops_so_far": 1144359884621980.0,
      "budget_used_percent": 1.14435988462198
    },
    {
      "type": "training",
      "description": "Training step 798",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:21",
      "total_flops_so_far": 1145790451658908.0,
      "budget_used_percent": 1.145790451658908
    },
    {
      "type": "training",
      "description": "Training step 799",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:21",
      "total_flops_so_far": 1147221018695836.0,
      "budget_used_percent": 1.147221018695836
    },
    {
      "type": "training",
      "description": "Training step 800",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:22",
      "total_flops_so_far": 1148651585732764.0,
      "budget_used_percent": 1.148651585732764
    },
    {
      "type": "training",
      "description": "Training step 801",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:22",
      "total_flops_so_far": 1150082152769692.0,
      "budget_used_percent": 1.150082152769692
    },
    {
      "type": "training",
      "description": "Training step 802",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:22",
      "total_flops_so_far": 1151512719806620.0,
      "budget_used_percent": 1.15151271980662
    },
    {
      "type": "training",
      "description": "Training step 803",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:22",
      "total_flops_so_far": 1152943286843548.0,
      "budget_used_percent": 1.152943286843548
    },
    {
      "type": "training",
      "description": "Training step 804",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:22",
      "total_flops_so_far": 1154373853880476.0,
      "budget_used_percent": 1.154373853880476
    },
    {
      "type": "training",
      "description": "Training step 805",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:22",
      "total_flops_so_far": 1155804420917404.0,
      "budget_used_percent": 1.1558044209174039
    },
    {
      "type": "training",
      "description": "Training step 806",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:22",
      "total_flops_so_far": 1157234987954332.0,
      "budget_used_percent": 1.157234987954332
    },
    {
      "type": "training",
      "description": "Training step 807",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:23",
      "total_flops_so_far": 1158665554991260.0,
      "budget_used_percent": 1.15866555499126
    },
    {
      "type": "training",
      "description": "Training step 808",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:23",
      "total_flops_so_far": 1160096122028188.0,
      "budget_used_percent": 1.160096122028188
    },
    {
      "type": "training",
      "description": "Training step 809",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:23",
      "total_flops_so_far": 1161526689065116.0,
      "budget_used_percent": 1.161526689065116
    },
    {
      "type": "training",
      "description": "Training step 810",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:23",
      "total_flops_so_far": 1162957256102044.0,
      "budget_used_percent": 1.162957256102044
    },
    {
      "type": "training",
      "description": "Training step 811",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:23",
      "total_flops_so_far": 1164387823138972.0,
      "budget_used_percent": 1.164387823138972
    },
    {
      "type": "training",
      "description": "Training step 812",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:23",
      "total_flops_so_far": 1165818390175900.0,
      "budget_used_percent": 1.1658183901759
    },
    {
      "type": "training",
      "description": "Training step 813",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:23",
      "total_flops_so_far": 1167248957212828.0,
      "budget_used_percent": 1.167248957212828
    },
    {
      "type": "training",
      "description": "Training step 814",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:24",
      "total_flops_so_far": 1168679524249756.0,
      "budget_used_percent": 1.168679524249756
    },
    {
      "type": "training",
      "description": "Training step 815",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:24",
      "total_flops_so_far": 1170110091286684.0,
      "budget_used_percent": 1.170110091286684
    },
    {
      "type": "training",
      "description": "Training step 816",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:24",
      "total_flops_so_far": 1171540658323612.0,
      "budget_used_percent": 1.171540658323612
    },
    {
      "type": "training",
      "description": "Training step 817",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:24",
      "total_flops_so_far": 1172971225360540.0,
      "budget_used_percent": 1.17297122536054
    },
    {
      "type": "training",
      "description": "Training step 818",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:24",
      "total_flops_so_far": 1174401792397468.0,
      "budget_used_percent": 1.1744017923974681
    },
    {
      "type": "training",
      "description": "Training step 819",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:24",
      "total_flops_so_far": 1175832359434396.0,
      "budget_used_percent": 1.175832359434396
    },
    {
      "type": "training",
      "description": "Training step 820",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:24",
      "total_flops_so_far": 1177262926471324.0,
      "budget_used_percent": 1.1772629264713241
    },
    {
      "type": "training",
      "description": "Training step 821",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:25",
      "total_flops_so_far": 1178693493508252.0,
      "budget_used_percent": 1.178693493508252
    },
    {
      "type": "training",
      "description": "Training step 822",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:25",
      "total_flops_so_far": 1180124060545180.0,
      "budget_used_percent": 1.1801240605451802
    },
    {
      "type": "training",
      "description": "Training step 823",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:25",
      "total_flops_so_far": 1181554627582108.0,
      "budget_used_percent": 1.1815546275821078
    },
    {
      "type": "training",
      "description": "Training step 824",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:25",
      "total_flops_so_far": 1182985194619036.0,
      "budget_used_percent": 1.182985194619036
    },
    {
      "type": "training",
      "description": "Training step 825",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:25",
      "total_flops_so_far": 1184415761655964.0,
      "budget_used_percent": 1.1844157616559639
    },
    {
      "type": "training",
      "description": "Training step 826",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:25",
      "total_flops_so_far": 1185846328692892.0,
      "budget_used_percent": 1.185846328692892
    },
    {
      "type": "training",
      "description": "Training step 827",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:25",
      "total_flops_so_far": 1187276895729820.0,
      "budget_used_percent": 1.1872768957298199
    },
    {
      "type": "training",
      "description": "Training step 828",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:25",
      "total_flops_so_far": 1188707462766748.0,
      "budget_used_percent": 1.188707462766748
    },
    {
      "type": "training",
      "description": "Training step 829",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:26",
      "total_flops_so_far": 1190138029803676.0,
      "budget_used_percent": 1.190138029803676
    },
    {
      "type": "training",
      "description": "Training step 830",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:26",
      "total_flops_so_far": 1191568596840604.0,
      "budget_used_percent": 1.191568596840604
    },
    {
      "type": "training",
      "description": "Training step 831",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:26",
      "total_flops_so_far": 1192999163877532.0,
      "budget_used_percent": 1.192999163877532
    },
    {
      "type": "training",
      "description": "Training step 832",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:26",
      "total_flops_so_far": 1194429730914460.0,
      "budget_used_percent": 1.19442973091446
    },
    {
      "type": "training",
      "description": "Training step 833",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:26",
      "total_flops_so_far": 1195860297951388.0,
      "budget_used_percent": 1.195860297951388
    },
    {
      "type": "training",
      "description": "Training step 834",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:26",
      "total_flops_so_far": 1197290864988316.0,
      "budget_used_percent": 1.197290864988316
    },
    {
      "type": "training",
      "description": "Training step 835",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:26",
      "total_flops_so_far": 1198721432025244.0,
      "budget_used_percent": 1.198721432025244
    },
    {
      "type": "training",
      "description": "Training step 836",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:27",
      "total_flops_so_far": 1200151999062172.0,
      "budget_used_percent": 1.200151999062172
    },
    {
      "type": "training",
      "description": "Training step 837",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:27",
      "total_flops_so_far": 1201582566099100.0,
      "budget_used_percent": 1.2015825660991
    },
    {
      "type": "training",
      "description": "Training step 838",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:27",
      "total_flops_so_far": 1203013133136028.0,
      "budget_used_percent": 1.203013133136028
    },
    {
      "type": "training",
      "description": "Training step 839",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:27",
      "total_flops_so_far": 1204443700172956.0,
      "budget_used_percent": 1.204443700172956
    },
    {
      "type": "training",
      "description": "Training step 840",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:27",
      "total_flops_so_far": 1205874267209884.0,
      "budget_used_percent": 1.2058742672098839
    },
    {
      "type": "training",
      "description": "Training step 841",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:27",
      "total_flops_so_far": 1207304834246812.0,
      "budget_used_percent": 1.207304834246812
    },
    {
      "type": "training",
      "description": "Training step 842",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:27",
      "total_flops_so_far": 1208735401283740.0,
      "budget_used_percent": 1.20873540128374
    },
    {
      "type": "training",
      "description": "Training step 843",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:28",
      "total_flops_so_far": 1210165968320668.0,
      "budget_used_percent": 1.210165968320668
    },
    {
      "type": "training",
      "description": "Training step 844",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:28",
      "total_flops_so_far": 1211596535357596.0,
      "budget_used_percent": 1.211596535357596
    },
    {
      "type": "training",
      "description": "Training step 845",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:28",
      "total_flops_so_far": 1213027102394524.0,
      "budget_used_percent": 1.213027102394524
    },
    {
      "type": "training",
      "description": "Training step 846",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:28",
      "total_flops_so_far": 1214457669431452.0,
      "budget_used_percent": 1.214457669431452
    },
    {
      "type": "training",
      "description": "Training step 847",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:28",
      "total_flops_so_far": 1215888236468380.0,
      "budget_used_percent": 1.21588823646838
    },
    {
      "type": "training",
      "description": "Training step 848",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:28",
      "total_flops_so_far": 1217318803505308.0,
      "budget_used_percent": 1.217318803505308
    },
    {
      "type": "training",
      "description": "Training step 849",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:28",
      "total_flops_so_far": 1218749370542236.0,
      "budget_used_percent": 1.218749370542236
    },
    {
      "type": "training",
      "description": "Training step 850",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:29",
      "total_flops_so_far": 1220179937579164.0,
      "budget_used_percent": 1.220179937579164
    },
    {
      "type": "training",
      "description": "Training step 851",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:29",
      "total_flops_so_far": 1221610504616092.0,
      "budget_used_percent": 1.221610504616092
    },
    {
      "type": "training",
      "description": "Training step 852",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:29",
      "total_flops_so_far": 1223041071653020.0,
      "budget_used_percent": 1.22304107165302
    },
    {
      "type": "training",
      "description": "Training step 853",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:29",
      "total_flops_so_far": 1224471638689948.0,
      "budget_used_percent": 1.2244716386899481
    },
    {
      "type": "training",
      "description": "Training step 854",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:29",
      "total_flops_so_far": 1225902205726876.0,
      "budget_used_percent": 1.225902205726876
    },
    {
      "type": "training",
      "description": "Training step 855",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:29",
      "total_flops_so_far": 1227332772763804.0,
      "budget_used_percent": 1.2273327727638041
    },
    {
      "type": "training",
      "description": "Training step 856",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:29",
      "total_flops_so_far": 1228763339800732.0,
      "budget_used_percent": 1.228763339800732
    },
    {
      "type": "training",
      "description": "Training step 857",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:29",
      "total_flops_so_far": 1230193906837660.0,
      "budget_used_percent": 1.2301939068376602
    },
    {
      "type": "training",
      "description": "Training step 858",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:30",
      "total_flops_so_far": 1231624473874588.0,
      "budget_used_percent": 1.2316244738745878
    },
    {
      "type": "training",
      "description": "Training step 859",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:30",
      "total_flops_so_far": 1233055040911516.0,
      "budget_used_percent": 1.233055040911516
    },
    {
      "type": "training",
      "description": "Training step 860",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:30",
      "total_flops_so_far": 1234485607948444.0,
      "budget_used_percent": 1.2344856079484439
    },
    {
      "type": "training",
      "description": "Training step 861",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:30",
      "total_flops_so_far": 1235916174985372.0,
      "budget_used_percent": 1.235916174985372
    },
    {
      "type": "training",
      "description": "Training step 862",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:30",
      "total_flops_so_far": 1237346742022300.0,
      "budget_used_percent": 1.2373467420222999
    },
    {
      "type": "training",
      "description": "Training step 863",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:30",
      "total_flops_so_far": 1238777309059228.0,
      "budget_used_percent": 1.238777309059228
    },
    {
      "type": "training",
      "description": "Training step 864",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:30",
      "total_flops_so_far": 1240207876096156.0,
      "budget_used_percent": 1.240207876096156
    },
    {
      "type": "training",
      "description": "Training step 865",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:31",
      "total_flops_so_far": 1241638443133084.0,
      "budget_used_percent": 1.241638443133084
    },
    {
      "type": "training",
      "description": "Training step 866",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:31",
      "total_flops_so_far": 1243069010170012.0,
      "budget_used_percent": 1.243069010170012
    },
    {
      "type": "training",
      "description": "Training step 867",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:31",
      "total_flops_so_far": 1244499577206940.0,
      "budget_used_percent": 1.24449957720694
    },
    {
      "type": "training",
      "description": "Training step 868",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:31",
      "total_flops_so_far": 1245930144243868.0,
      "budget_used_percent": 1.245930144243868
    },
    {
      "type": "training",
      "description": "Training step 869",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:31",
      "total_flops_so_far": 1247360711280796.0,
      "budget_used_percent": 1.247360711280796
    },
    {
      "type": "training",
      "description": "Training step 870",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:31",
      "total_flops_so_far": 1248791278317724.0,
      "budget_used_percent": 1.248791278317724
    },
    {
      "type": "training",
      "description": "Training step 871",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:31",
      "total_flops_so_far": 1250221845354652.0,
      "budget_used_percent": 1.250221845354652
    },
    {
      "type": "training",
      "description": "Training step 872",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:32",
      "total_flops_so_far": 1251652412391580.0,
      "budget_used_percent": 1.25165241239158
    },
    {
      "type": "training",
      "description": "Training step 873",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:32",
      "total_flops_so_far": 1253082979428508.0,
      "budget_used_percent": 1.253082979428508
    },
    {
      "type": "training",
      "description": "Training step 874",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:32",
      "total_flops_so_far": 1254513546465436.0,
      "budget_used_percent": 1.254513546465436
    },
    {
      "type": "training",
      "description": "Training step 875",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:32",
      "total_flops_so_far": 1255944113502364.0,
      "budget_used_percent": 1.255944113502364
    },
    {
      "type": "training",
      "description": "Training step 876",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:32",
      "total_flops_so_far": 1257374680539292.0,
      "budget_used_percent": 1.257374680539292
    },
    {
      "type": "training",
      "description": "Training step 877",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:32",
      "total_flops_so_far": 1258805247576220.0,
      "budget_used_percent": 1.25880524757622
    },
    {
      "type": "training",
      "description": "Training step 878",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:32",
      "total_flops_so_far": 1260235814613148.0,
      "budget_used_percent": 1.260235814613148
    },
    {
      "type": "training",
      "description": "Training step 879",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:33",
      "total_flops_so_far": 1261666381650076.0,
      "budget_used_percent": 1.261666381650076
    },
    {
      "type": "training",
      "description": "Training step 880",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:33",
      "total_flops_so_far": 1263096948687004.0,
      "budget_used_percent": 1.263096948687004
    },
    {
      "type": "training",
      "description": "Training step 881",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:33",
      "total_flops_so_far": 1264527515723932.0,
      "budget_used_percent": 1.264527515723932
    },
    {
      "type": "training",
      "description": "Training step 882",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:33",
      "total_flops_so_far": 1265958082760860.0,
      "budget_used_percent": 1.26595808276086
    },
    {
      "type": "training",
      "description": "Training step 883",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:33",
      "total_flops_so_far": 1267388649797788.0,
      "budget_used_percent": 1.267388649797788
    },
    {
      "type": "training",
      "description": "Training step 884",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:33",
      "total_flops_so_far": 1268819216834716.0,
      "budget_used_percent": 1.268819216834716
    },
    {
      "type": "training",
      "description": "Training step 885",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:33",
      "total_flops_so_far": 1270249783871644.0,
      "budget_used_percent": 1.270249783871644
    },
    {
      "type": "training",
      "description": "Training step 886",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:34",
      "total_flops_so_far": 1271680350908572.0,
      "budget_used_percent": 1.2716803509085721
    },
    {
      "type": "training",
      "description": "Training step 887",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:34",
      "total_flops_so_far": 1273110917945500.0,
      "budget_used_percent": 1.2731109179455
    },
    {
      "type": "training",
      "description": "Training step 888",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:34",
      "total_flops_so_far": 1274541484982428.0,
      "budget_used_percent": 1.2745414849824281
    },
    {
      "type": "training",
      "description": "Training step 889",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:34",
      "total_flops_so_far": 1275972052019356.0,
      "budget_used_percent": 1.275972052019356
    },
    {
      "type": "training",
      "description": "Training step 890",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:34",
      "total_flops_so_far": 1277402619056284.0,
      "budget_used_percent": 1.2774026190562842
    },
    {
      "type": "training",
      "description": "Training step 891",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:34",
      "total_flops_so_far": 1278833186093212.0,
      "budget_used_percent": 1.278833186093212
    },
    {
      "type": "training",
      "description": "Training step 892",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:34",
      "total_flops_so_far": 1280263753130140.0,
      "budget_used_percent": 1.2802637531301402
    },
    {
      "type": "training",
      "description": "Training step 893",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:35",
      "total_flops_so_far": 1281694320167068.0,
      "budget_used_percent": 1.2816943201670679
    },
    {
      "type": "training",
      "description": "Training step 894",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:35",
      "total_flops_so_far": 1283124887203996.0,
      "budget_used_percent": 1.283124887203996
    },
    {
      "type": "training",
      "description": "Training step 895",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:35",
      "total_flops_so_far": 1284555454240924.0,
      "budget_used_percent": 1.2845554542409239
    },
    {
      "type": "training",
      "description": "Training step 896",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:35",
      "total_flops_so_far": 1285986021277852.0,
      "budget_used_percent": 1.285986021277852
    },
    {
      "type": "training",
      "description": "Training step 897",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:35",
      "total_flops_so_far": 1287416588314780.0,
      "budget_used_percent": 1.28741658831478
    },
    {
      "type": "training",
      "description": "Training step 898",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:35",
      "total_flops_so_far": 1288847155351708.0,
      "budget_used_percent": 1.288847155351708
    },
    {
      "type": "training",
      "description": "Training step 899",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:35",
      "total_flops_so_far": 1290277722388636.0,
      "budget_used_percent": 1.290277722388636
    },
    {
      "type": "training",
      "description": "Training step 900",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:36",
      "total_flops_so_far": 1291708289425564.0,
      "budget_used_percent": 1.291708289425564
    },
    {
      "type": "training",
      "description": "Training step 901",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:36",
      "total_flops_so_far": 1293138856462492.0,
      "budget_used_percent": 1.293138856462492
    },
    {
      "type": "training",
      "description": "Training step 902",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:36",
      "total_flops_so_far": 1294569423499420.0,
      "budget_used_percent": 1.29456942349942
    },
    {
      "type": "training",
      "description": "Training step 903",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:36",
      "total_flops_so_far": 1295999990536348.0,
      "budget_used_percent": 1.295999990536348
    },
    {
      "type": "training",
      "description": "Training step 904",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:36",
      "total_flops_so_far": 1297430557573276.0,
      "budget_used_percent": 1.297430557573276
    },
    {
      "type": "training",
      "description": "Training step 905",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:36",
      "total_flops_so_far": 1298861124610204.0,
      "budget_used_percent": 1.298861124610204
    },
    {
      "type": "training",
      "description": "Training step 906",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:36",
      "total_flops_so_far": 1300291691647132.0,
      "budget_used_percent": 1.300291691647132
    },
    {
      "type": "training",
      "description": "Training step 907",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:37",
      "total_flops_so_far": 1301722258684060.0,
      "budget_used_percent": 1.30172225868406
    },
    {
      "type": "training",
      "description": "Training step 908",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:37",
      "total_flops_so_far": 1303152825720988.0,
      "budget_used_percent": 1.3031528257209881
    },
    {
      "type": "training",
      "description": "Training step 909",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:37",
      "total_flops_so_far": 1304583392757916.0,
      "budget_used_percent": 1.304583392757916
    },
    {
      "type": "training",
      "description": "Training step 910",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:37",
      "total_flops_so_far": 1306013959794844.0,
      "budget_used_percent": 1.306013959794844
    },
    {
      "type": "training",
      "description": "Training step 911",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:37",
      "total_flops_so_far": 1307444526831772.0,
      "budget_used_percent": 1.3074445268317718
    },
    {
      "type": "training",
      "description": "Training step 912",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:37",
      "total_flops_so_far": 1308875093868700.0,
      "budget_used_percent": 1.3088750938687
    },
    {
      "type": "training",
      "description": "Training step 913",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:37",
      "total_flops_so_far": 1310305660905628.0,
      "budget_used_percent": 1.310305660905628
    },
    {
      "type": "training",
      "description": "Training step 914",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:38",
      "total_flops_so_far": 1311736227942556.0,
      "budget_used_percent": 1.311736227942556
    },
    {
      "type": "training",
      "description": "Training step 915",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:38",
      "total_flops_so_far": 1313166794979484.0,
      "budget_used_percent": 1.3131667949794839
    },
    {
      "type": "training",
      "description": "Training step 916",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:38",
      "total_flops_so_far": 1314597362016412.0,
      "budget_used_percent": 1.314597362016412
    },
    {
      "type": "training",
      "description": "Training step 917",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:38",
      "total_flops_so_far": 1316027929053340.0,
      "budget_used_percent": 1.31602792905334
    },
    {
      "type": "training",
      "description": "Training step 918",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:38",
      "total_flops_so_far": 1317458496090268.0,
      "budget_used_percent": 1.317458496090268
    },
    {
      "type": "training",
      "description": "Training step 919",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:38",
      "total_flops_so_far": 1318889063127196.0,
      "budget_used_percent": 1.318889063127196
    },
    {
      "type": "training",
      "description": "Training step 920",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:38",
      "total_flops_so_far": 1320319630164124.0,
      "budget_used_percent": 1.320319630164124
    },
    {
      "type": "training",
      "description": "Training step 921",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:39",
      "total_flops_so_far": 1321750197201052.0,
      "budget_used_percent": 1.3217501972010521
    },
    {
      "type": "training",
      "description": "Training step 922",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:39",
      "total_flops_so_far": 1323180764237980.0,
      "budget_used_percent": 1.32318076423798
    },
    {
      "type": "training",
      "description": "Training step 923",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:39",
      "total_flops_so_far": 1324611331274908.0,
      "budget_used_percent": 1.324611331274908
    },
    {
      "type": "training",
      "description": "Training step 924",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:39",
      "total_flops_so_far": 1326041898311836.0,
      "budget_used_percent": 1.326041898311836
    },
    {
      "type": "training",
      "description": "Training step 925",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:39",
      "total_flops_so_far": 1327472465348764.0,
      "budget_used_percent": 1.3274724653487642
    },
    {
      "type": "training",
      "description": "Training step 926",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:39",
      "total_flops_so_far": 1328903032385692.0,
      "budget_used_percent": 1.328903032385692
    },
    {
      "type": "training",
      "description": "Training step 927",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:39",
      "total_flops_so_far": 1330333599422620.0,
      "budget_used_percent": 1.33033359942262
    },
    {
      "type": "training",
      "description": "Training step 928",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:40",
      "total_flops_so_far": 1331764166459548.0,
      "budget_used_percent": 1.3317641664595479
    },
    {
      "type": "training",
      "description": "Training step 929",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:40",
      "total_flops_so_far": 1333194733496476.0,
      "budget_used_percent": 1.333194733496476
    },
    {
      "type": "training",
      "description": "Training step 930",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:40",
      "total_flops_so_far": 1334625300533404.0,
      "budget_used_percent": 1.334625300533404
    },
    {
      "type": "training",
      "description": "Training step 931",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:40",
      "total_flops_so_far": 1336055867570332.0,
      "budget_used_percent": 1.336055867570332
    },
    {
      "type": "training",
      "description": "Training step 932",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:40",
      "total_flops_so_far": 1337486434607260.0,
      "budget_used_percent": 1.33748643460726
    },
    {
      "type": "training",
      "description": "Training step 933",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:40",
      "total_flops_so_far": 1338917001644188.0,
      "budget_used_percent": 1.338917001644188
    },
    {
      "type": "training",
      "description": "Training step 934",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:40",
      "total_flops_so_far": 1340347568681116.0,
      "budget_used_percent": 1.340347568681116
    },
    {
      "type": "training",
      "description": "Training step 935",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:41",
      "total_flops_so_far": 1341778135718044.0,
      "budget_used_percent": 1.341778135718044
    },
    {
      "type": "training",
      "description": "Training step 936",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:41",
      "total_flops_so_far": 1343208702754972.0,
      "budget_used_percent": 1.343208702754972
    },
    {
      "type": "training",
      "description": "Training step 937",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:41",
      "total_flops_so_far": 1344639269791900.0,
      "budget_used_percent": 1.3446392697919
    },
    {
      "type": "training",
      "description": "Training step 938",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:41",
      "total_flops_so_far": 1346069836828828.0,
      "budget_used_percent": 1.346069836828828
    },
    {
      "type": "training",
      "description": "Training step 939",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:41",
      "total_flops_so_far": 1347500403865756.0,
      "budget_used_percent": 1.347500403865756
    },
    {
      "type": "training",
      "description": "Training step 940",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:41",
      "total_flops_so_far": 1348930970902684.0,
      "budget_used_percent": 1.348930970902684
    },
    {
      "type": "training",
      "description": "Training step 941",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:41",
      "total_flops_so_far": 1350361537939612.0,
      "budget_used_percent": 1.350361537939612
    },
    {
      "type": "training",
      "description": "Training step 942",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:42",
      "total_flops_so_far": 1351792104976540.0,
      "budget_used_percent": 1.35179210497654
    },
    {
      "type": "training",
      "description": "Training step 943",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:42",
      "total_flops_so_far": 1353222672013468.0,
      "budget_used_percent": 1.3532226720134681
    },
    {
      "type": "training",
      "description": "Training step 944",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:42",
      "total_flops_so_far": 1354653239050396.0,
      "budget_used_percent": 1.354653239050396
    },
    {
      "type": "training",
      "description": "Training step 945",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:42",
      "total_flops_so_far": 1356083806087324.0,
      "budget_used_percent": 1.356083806087324
    },
    {
      "type": "training",
      "description": "Training step 946",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:42",
      "total_flops_so_far": 1357514373124252.0,
      "budget_used_percent": 1.3575143731242518
    },
    {
      "type": "training",
      "description": "Training step 947",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:42",
      "total_flops_so_far": 1358944940161180.0,
      "budget_used_percent": 1.35894494016118
    },
    {
      "type": "training",
      "description": "Training step 948",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:42",
      "total_flops_so_far": 1360375507198108.0,
      "budget_used_percent": 1.3603755071981078
    },
    {
      "type": "training",
      "description": "Training step 949",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:42",
      "total_flops_so_far": 1361806074235036.0,
      "budget_used_percent": 1.361806074235036
    },
    {
      "type": "training",
      "description": "Training step 950",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:43",
      "total_flops_so_far": 1363236641271964.0,
      "budget_used_percent": 1.3632366412719639
    },
    {
      "type": "training",
      "description": "Training step 951",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:43",
      "total_flops_so_far": 1364667208308892.0,
      "budget_used_percent": 1.364667208308892
    },
    {
      "type": "training",
      "description": "Training step 952",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:43",
      "total_flops_so_far": 1366097775345820.0,
      "budget_used_percent": 1.3660977753458199
    },
    {
      "type": "training",
      "description": "Training step 953",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:43",
      "total_flops_so_far": 1367528342382748.0,
      "budget_used_percent": 1.367528342382748
    },
    {
      "type": "training",
      "description": "Training step 954",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:43",
      "total_flops_so_far": 1368958909419676.0,
      "budget_used_percent": 1.368958909419676
    },
    {
      "type": "training",
      "description": "Training step 955",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:43",
      "total_flops_so_far": 1370389476456604.0,
      "budget_used_percent": 1.370389476456604
    },
    {
      "type": "training",
      "description": "Training step 956",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:43",
      "total_flops_so_far": 1371820043493532.0,
      "budget_used_percent": 1.371820043493532
    },
    {
      "type": "training",
      "description": "Training step 957",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:44",
      "total_flops_so_far": 1373250610530460.0,
      "budget_used_percent": 1.37325061053046
    },
    {
      "type": "training",
      "description": "Training step 958",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:44",
      "total_flops_so_far": 1374681177567388.0,
      "budget_used_percent": 1.374681177567388
    },
    {
      "type": "training",
      "description": "Training step 959",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:44",
      "total_flops_so_far": 1376111744604316.0,
      "budget_used_percent": 1.376111744604316
    },
    {
      "type": "training",
      "description": "Training step 960",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:44",
      "total_flops_so_far": 1377542311641244.0,
      "budget_used_percent": 1.377542311641244
    },
    {
      "type": "training",
      "description": "Training step 961",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:44",
      "total_flops_so_far": 1378972878678172.0,
      "budget_used_percent": 1.378972878678172
    },
    {
      "type": "training",
      "description": "Training step 962",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:44",
      "total_flops_so_far": 1380403445715100.0,
      "budget_used_percent": 1.3804034457151
    },
    {
      "type": "training",
      "description": "Training step 963",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:44",
      "total_flops_so_far": 1381834012752028.0,
      "budget_used_percent": 1.3818340127520279
    },
    {
      "type": "training",
      "description": "Training step 964",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:45",
      "total_flops_so_far": 1383264579788956.0,
      "budget_used_percent": 1.383264579788956
    },
    {
      "type": "training",
      "description": "Training step 965",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:45",
      "total_flops_so_far": 1384695146825884.0,
      "budget_used_percent": 1.384695146825884
    },
    {
      "type": "training",
      "description": "Training step 966",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:45",
      "total_flops_so_far": 1386125713862812.0,
      "budget_used_percent": 1.386125713862812
    },
    {
      "type": "training",
      "description": "Training step 967",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:45",
      "total_flops_so_far": 1387556280899740.0,
      "budget_used_percent": 1.38755628089974
    },
    {
      "type": "training",
      "description": "Training step 968",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:45",
      "total_flops_so_far": 1388986847936668.0,
      "budget_used_percent": 1.388986847936668
    },
    {
      "type": "training",
      "description": "Training step 969",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:45",
      "total_flops_so_far": 1390417414973596.0,
      "budget_used_percent": 1.390417414973596
    },
    {
      "type": "training",
      "description": "Training step 970",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:45",
      "total_flops_so_far": 1391847982010524.0,
      "budget_used_percent": 1.391847982010524
    },
    {
      "type": "training",
      "description": "Training step 971",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:46",
      "total_flops_so_far": 1393278549047452.0,
      "budget_used_percent": 1.393278549047452
    },
    {
      "type": "training",
      "description": "Training step 972",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:46",
      "total_flops_so_far": 1394709116084380.0,
      "budget_used_percent": 1.39470911608438
    },
    {
      "type": "training",
      "description": "Training step 973",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:46",
      "total_flops_so_far": 1396139683121308.0,
      "budget_used_percent": 1.396139683121308
    },
    {
      "type": "training",
      "description": "Training step 974",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:46",
      "total_flops_so_far": 1397570250158236.0,
      "budget_used_percent": 1.397570250158236
    },
    {
      "type": "training",
      "description": "Training step 975",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:46",
      "total_flops_so_far": 1399000817195164.0,
      "budget_used_percent": 1.399000817195164
    },
    {
      "type": "training",
      "description": "Training step 976",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:46",
      "total_flops_so_far": 1400431384232092.0,
      "budget_used_percent": 1.4004313842320921
    },
    {
      "type": "training",
      "description": "Training step 977",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:46",
      "total_flops_so_far": 1401861951269020.0,
      "budget_used_percent": 1.40186195126902
    },
    {
      "type": "training",
      "description": "Training step 978",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:47",
      "total_flops_so_far": 1403292518305948.0,
      "budget_used_percent": 1.4032925183059481
    },
    {
      "type": "training",
      "description": "Training step 979",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:47",
      "total_flops_so_far": 1404723085342876.0,
      "budget_used_percent": 1.404723085342876
    },
    {
      "type": "training",
      "description": "Training step 980",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:47",
      "total_flops_so_far": 1406153652379804.0,
      "budget_used_percent": 1.406153652379804
    },
    {
      "type": "training",
      "description": "Training step 981",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:47",
      "total_flops_so_far": 1407584219416732.0,
      "budget_used_percent": 1.4075842194167318
    },
    {
      "type": "training",
      "description": "Training step 982",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:47",
      "total_flops_so_far": 1409014786453660.0,
      "budget_used_percent": 1.40901478645366
    },
    {
      "type": "training",
      "description": "Training step 983",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:47",
      "total_flops_so_far": 1410445353490588.0,
      "budget_used_percent": 1.4104453534905879
    },
    {
      "type": "training",
      "description": "Training step 984",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:47",
      "total_flops_so_far": 1411875920527516.0,
      "budget_used_percent": 1.411875920527516
    },
    {
      "type": "training",
      "description": "Training step 985",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:48",
      "total_flops_so_far": 1413306487564444.0,
      "budget_used_percent": 1.4133064875644439
    },
    {
      "type": "training",
      "description": "Training step 986",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:48",
      "total_flops_so_far": 1414737054601372.0,
      "budget_used_percent": 1.414737054601372
    },
    {
      "type": "training",
      "description": "Training step 987",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:48",
      "total_flops_so_far": 1416167621638300.0,
      "budget_used_percent": 1.4161676216383
    },
    {
      "type": "training",
      "description": "Training step 988",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:48",
      "total_flops_so_far": 1417598188675228.0,
      "budget_used_percent": 1.417598188675228
    },
    {
      "type": "training",
      "description": "Training step 989",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:48",
      "total_flops_so_far": 1419028755712156.0,
      "budget_used_percent": 1.419028755712156
    },
    {
      "type": "training",
      "description": "Training step 990",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:48",
      "total_flops_so_far": 1420459322749084.0,
      "budget_used_percent": 1.420459322749084
    },
    {
      "type": "training",
      "description": "Training step 991",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:48",
      "total_flops_so_far": 1421889889786012.0,
      "budget_used_percent": 1.421889889786012
    },
    {
      "type": "training",
      "description": "Training step 992",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:49",
      "total_flops_so_far": 1423320456822940.0,
      "budget_used_percent": 1.42332045682294
    },
    {
      "type": "training",
      "description": "Training step 993",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:49",
      "total_flops_so_far": 1424751023859868.0,
      "budget_used_percent": 1.424751023859868
    },
    {
      "type": "training",
      "description": "Training step 994",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:49",
      "total_flops_so_far": 1426181590896796.0,
      "budget_used_percent": 1.426181590896796
    },
    {
      "type": "training",
      "description": "Training step 995",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:49",
      "total_flops_so_far": 1427612157933724.0,
      "budget_used_percent": 1.427612157933724
    },
    {
      "type": "training",
      "description": "Training step 996",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:49",
      "total_flops_so_far": 1429042724970652.0,
      "budget_used_percent": 1.429042724970652
    },
    {
      "type": "training",
      "description": "Training step 997",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:49",
      "total_flops_so_far": 1430473292007580.0,
      "budget_used_percent": 1.43047329200758
    },
    {
      "type": "training",
      "description": "Training step 998",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:49",
      "total_flops_so_far": 1431903859044508.0,
      "budget_used_percent": 1.431903859044508
    },
    {
      "type": "training",
      "description": "Training step 999",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:50",
      "total_flops_so_far": 1433334426081436.0,
      "budget_used_percent": 1.433334426081436
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 551857894496.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:51",
      "total_flops_so_far": 1433886283975932.0,
      "budget_used_percent": 1.433886283975932
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 555458032848.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:52",
      "total_flops_so_far": 1434441742008780.0,
      "budget_used_percent": 1.43444174200878
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 553657603480.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:54",
      "total_flops_so_far": 1434995399612260.0,
      "budget_used_percent": 1.43499539961226
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 551857894496.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:55",
      "total_flops_so_far": 1435547257506756.0,
      "budget_used_percent": 1.435547257506756
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 554557728116.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:56",
      "total_flops_so_far": 1436101815234872.0,
      "budget_used_percent": 1.436101815234872
    },
    {
      "type": "training",
      "description": "Training step 1000",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:56",
      "total_flops_so_far": 1437532382271800.0,
      "budget_used_percent": 1.4375323822718
    },
    {
      "type": "training",
      "description": "Training step 1001",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:56",
      "total_flops_so_far": 1438962949308728.0,
      "budget_used_percent": 1.4389629493087281
    },
    {
      "type": "training",
      "description": "Training step 1002",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:57",
      "total_flops_so_far": 1440393516345656.0,
      "budget_used_percent": 1.440393516345656
    },
    {
      "type": "training",
      "description": "Training step 1003",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:57",
      "total_flops_so_far": 1441824083382584.0,
      "budget_used_percent": 1.441824083382584
    },
    {
      "type": "training",
      "description": "Training step 1004",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:57",
      "total_flops_so_far": 1443254650419512.0,
      "budget_used_percent": 1.443254650419512
    },
    {
      "type": "training",
      "description": "Training step 1005",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:57",
      "total_flops_so_far": 1444685217456440.0,
      "budget_used_percent": 1.4446852174564402
    },
    {
      "type": "training",
      "description": "Training step 1006",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:57",
      "total_flops_so_far": 1446115784493368.0,
      "budget_used_percent": 1.446115784493368
    },
    {
      "type": "training",
      "description": "Training step 1007",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:57",
      "total_flops_so_far": 1447546351530296.0,
      "budget_used_percent": 1.447546351530296
    },
    {
      "type": "training",
      "description": "Training step 1008",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:57",
      "total_flops_so_far": 1448976918567224.0,
      "budget_used_percent": 1.4489769185672239
    },
    {
      "type": "training",
      "description": "Training step 1009",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:58",
      "total_flops_so_far": 1450407485604152.0,
      "budget_used_percent": 1.450407485604152
    },
    {
      "type": "training",
      "description": "Training step 1010",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:58",
      "total_flops_so_far": 1451838052641080.0,
      "budget_used_percent": 1.45183805264108
    },
    {
      "type": "training",
      "description": "Training step 1011",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:58",
      "total_flops_so_far": 1453268619678008.0,
      "budget_used_percent": 1.453268619678008
    },
    {
      "type": "training",
      "description": "Training step 1012",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:58",
      "total_flops_so_far": 1454699186714936.0,
      "budget_used_percent": 1.454699186714936
    },
    {
      "type": "training",
      "description": "Training step 1013",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:58",
      "total_flops_so_far": 1456129753751864.0,
      "budget_used_percent": 1.456129753751864
    },
    {
      "type": "training",
      "description": "Training step 1014",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:58",
      "total_flops_so_far": 1457560320788792.0,
      "budget_used_percent": 1.457560320788792
    },
    {
      "type": "training",
      "description": "Training step 1015",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:58",
      "total_flops_so_far": 1458990887825720.0,
      "budget_used_percent": 1.45899088782572
    },
    {
      "type": "training",
      "description": "Training step 1016",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:59",
      "total_flops_so_far": 1460421454862648.0,
      "budget_used_percent": 1.460421454862648
    },
    {
      "type": "training",
      "description": "Training step 1017",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:59",
      "total_flops_so_far": 1461852021899576.0,
      "budget_used_percent": 1.461852021899576
    },
    {
      "type": "training",
      "description": "Training step 1018",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:59",
      "total_flops_so_far": 1463282588936504.0,
      "budget_used_percent": 1.463282588936504
    },
    {
      "type": "training",
      "description": "Training step 1019",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:59",
      "total_flops_so_far": 1464713155973432.0,
      "budget_used_percent": 1.464713155973432
    },
    {
      "type": "training",
      "description": "Training step 1020",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:59",
      "total_flops_so_far": 1466143723010360.0,
      "budget_used_percent": 1.46614372301036
    },
    {
      "type": "training",
      "description": "Training step 1021",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:59",
      "total_flops_so_far": 1467574290047288.0,
      "budget_used_percent": 1.4675742900472881
    },
    {
      "type": "training",
      "description": "Training step 1022",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:02:59",
      "total_flops_so_far": 1469004857084216.0,
      "budget_used_percent": 1.469004857084216
    },
    {
      "type": "training",
      "description": "Training step 1023",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:00",
      "total_flops_so_far": 1470435424121144.0,
      "budget_used_percent": 1.4704354241211441
    },
    {
      "type": "training",
      "description": "Training step 1024",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:00",
      "total_flops_so_far": 1471865991158072.0,
      "budget_used_percent": 1.4718659911580718
    },
    {
      "type": "training",
      "description": "Training step 1025",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:00",
      "total_flops_so_far": 1473296558195000.0,
      "budget_used_percent": 1.473296558195
    },
    {
      "type": "training",
      "description": "Training step 1026",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:00",
      "total_flops_so_far": 1474727125231928.0,
      "budget_used_percent": 1.4747271252319278
    },
    {
      "type": "training",
      "description": "Training step 1027",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:00",
      "total_flops_so_far": 1476157692268856.0,
      "budget_used_percent": 1.476157692268856
    },
    {
      "type": "training",
      "description": "Training step 1028",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:00",
      "total_flops_so_far": 1477588259305784.0,
      "budget_used_percent": 1.4775882593057839
    },
    {
      "type": "training",
      "description": "Training step 1029",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:00",
      "total_flops_so_far": 1479018826342712.0,
      "budget_used_percent": 1.479018826342712
    },
    {
      "type": "training",
      "description": "Training step 1030",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:01",
      "total_flops_so_far": 1480449393379640.0,
      "budget_used_percent": 1.4804493933796399
    },
    {
      "type": "training",
      "description": "Training step 1031",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:01",
      "total_flops_so_far": 1481879960416568.0,
      "budget_used_percent": 1.481879960416568
    },
    {
      "type": "training",
      "description": "Training step 1032",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:01",
      "total_flops_so_far": 1483310527453496.0,
      "budget_used_percent": 1.483310527453496
    },
    {
      "type": "training",
      "description": "Training step 1033",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:01",
      "total_flops_so_far": 1484741094490424.0,
      "budget_used_percent": 1.484741094490424
    },
    {
      "type": "training",
      "description": "Training step 1034",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:01",
      "total_flops_so_far": 1486171661527352.0,
      "budget_used_percent": 1.486171661527352
    },
    {
      "type": "training",
      "description": "Training step 1035",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:01",
      "total_flops_so_far": 1487602228564280.0,
      "budget_used_percent": 1.48760222856428
    },
    {
      "type": "training",
      "description": "Training step 1036",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:01",
      "total_flops_so_far": 1489032795601208.0,
      "budget_used_percent": 1.489032795601208
    },
    {
      "type": "training",
      "description": "Training step 1037",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:02",
      "total_flops_so_far": 1490463362638136.0,
      "budget_used_percent": 1.490463362638136
    },
    {
      "type": "training",
      "description": "Training step 1038",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:02",
      "total_flops_so_far": 1491893929675064.0,
      "budget_used_percent": 1.491893929675064
    },
    {
      "type": "training",
      "description": "Training step 1039",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:02",
      "total_flops_so_far": 1493324496711992.0,
      "budget_used_percent": 1.493324496711992
    },
    {
      "type": "training",
      "description": "Training step 1040",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:02",
      "total_flops_so_far": 1494755063748920.0,
      "budget_used_percent": 1.49475506374892
    },
    {
      "type": "training",
      "description": "Training step 1041",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:02",
      "total_flops_so_far": 1496185630785848.0,
      "budget_used_percent": 1.496185630785848
    },
    {
      "type": "training",
      "description": "Training step 1042",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:02",
      "total_flops_so_far": 1497616197822776.0,
      "budget_used_percent": 1.497616197822776
    },
    {
      "type": "training",
      "description": "Training step 1043",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:02",
      "total_flops_so_far": 1499046764859704.0,
      "budget_used_percent": 1.499046764859704
    },
    {
      "type": "training",
      "description": "Training step 1044",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:03",
      "total_flops_so_far": 1500477331896632.0,
      "budget_used_percent": 1.500477331896632
    },
    {
      "type": "training",
      "description": "Training step 1045",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:03",
      "total_flops_so_far": 1501907898933560.0,
      "budget_used_percent": 1.50190789893356
    },
    {
      "type": "training",
      "description": "Training step 1046",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:03",
      "total_flops_so_far": 1503338465970488.0,
      "budget_used_percent": 1.503338465970488
    },
    {
      "type": "training",
      "description": "Training step 1047",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:03",
      "total_flops_so_far": 1504769033007416.0,
      "budget_used_percent": 1.504769033007416
    },
    {
      "type": "training",
      "description": "Training step 1048",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:03",
      "total_flops_so_far": 1506199600044344.0,
      "budget_used_percent": 1.506199600044344
    },
    {
      "type": "training",
      "description": "Training step 1049",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:03",
      "total_flops_so_far": 1507630167081272.0,
      "budget_used_percent": 1.507630167081272
    },
    {
      "type": "training",
      "description": "Training step 1050",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:03",
      "total_flops_so_far": 1509060734118200.0,
      "budget_used_percent": 1.5090607341182
    },
    {
      "type": "training",
      "description": "Training step 1051",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:04",
      "total_flops_so_far": 1510491301155128.0,
      "budget_used_percent": 1.510491301155128
    },
    {
      "type": "training",
      "description": "Training step 1052",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:04",
      "total_flops_so_far": 1511921868192056.0,
      "budget_used_percent": 1.511921868192056
    },
    {
      "type": "training",
      "description": "Training step 1053",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:04",
      "total_flops_so_far": 1513352435228984.0,
      "budget_used_percent": 1.513352435228984
    },
    {
      "type": "training",
      "description": "Training step 1054",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:04",
      "total_flops_so_far": 1514783002265912.0,
      "budget_used_percent": 1.5147830022659121
    },
    {
      "type": "training",
      "description": "Training step 1055",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:04",
      "total_flops_so_far": 1516213569302840.0,
      "budget_used_percent": 1.51621356930284
    },
    {
      "type": "training",
      "description": "Training step 1056",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:04",
      "total_flops_so_far": 1517644136339768.0,
      "budget_used_percent": 1.5176441363397681
    },
    {
      "type": "training",
      "description": "Training step 1057",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:04",
      "total_flops_so_far": 1519074703376696.0,
      "budget_used_percent": 1.519074703376696
    },
    {
      "type": "training",
      "description": "Training step 1058",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:05",
      "total_flops_so_far": 1520505270413624.0,
      "budget_used_percent": 1.5205052704136242
    },
    {
      "type": "training",
      "description": "Training step 1059",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:05",
      "total_flops_so_far": 1521935837450552.0,
      "budget_used_percent": 1.5219358374505518
    },
    {
      "type": "training",
      "description": "Training step 1060",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:05",
      "total_flops_so_far": 1523366404487480.0,
      "budget_used_percent": 1.52336640448748
    },
    {
      "type": "training",
      "description": "Training step 1061",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:05",
      "total_flops_so_far": 1524796971524408.0,
      "budget_used_percent": 1.5247969715244079
    },
    {
      "type": "training",
      "description": "Training step 1062",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:05",
      "total_flops_so_far": 1526227538561336.0,
      "budget_used_percent": 1.526227538561336
    },
    {
      "type": "training",
      "description": "Training step 1063",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:05",
      "total_flops_so_far": 1527658105598264.0,
      "budget_used_percent": 1.5276581055982639
    },
    {
      "type": "training",
      "description": "Training step 1064",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:05",
      "total_flops_so_far": 1529088672635192.0,
      "budget_used_percent": 1.529088672635192
    },
    {
      "type": "training",
      "description": "Training step 1065",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:06",
      "total_flops_so_far": 1530519239672120.0,
      "budget_used_percent": 1.53051923967212
    },
    {
      "type": "training",
      "description": "Training step 1066",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:06",
      "total_flops_so_far": 1531949806709048.0,
      "budget_used_percent": 1.531949806709048
    },
    {
      "type": "training",
      "description": "Training step 1067",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:06",
      "total_flops_so_far": 1533380373745976.0,
      "budget_used_percent": 1.533380373745976
    },
    {
      "type": "training",
      "description": "Training step 1068",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:06",
      "total_flops_so_far": 1534810940782904.0,
      "budget_used_percent": 1.534810940782904
    },
    {
      "type": "training",
      "description": "Training step 1069",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:06",
      "total_flops_so_far": 1536241507819832.0,
      "budget_used_percent": 1.536241507819832
    },
    {
      "type": "training",
      "description": "Training step 1070",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:06",
      "total_flops_so_far": 1537672074856760.0,
      "budget_used_percent": 1.53767207485676
    },
    {
      "type": "training",
      "description": "Training step 1071",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:06",
      "total_flops_so_far": 1539102641893688.0,
      "budget_used_percent": 1.539102641893688
    },
    {
      "type": "training",
      "description": "Training step 1072",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:07",
      "total_flops_so_far": 1540533208930616.0,
      "budget_used_percent": 1.540533208930616
    },
    {
      "type": "training",
      "description": "Training step 1073",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:07",
      "total_flops_so_far": 1541963775967544.0,
      "budget_used_percent": 1.541963775967544
    },
    {
      "type": "training",
      "description": "Training step 1074",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:07",
      "total_flops_so_far": 1543394343004472.0,
      "budget_used_percent": 1.543394343004472
    },
    {
      "type": "training",
      "description": "Training step 1075",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:07",
      "total_flops_so_far": 1544824910041400.0,
      "budget_used_percent": 1.5448249100414
    },
    {
      "type": "training",
      "description": "Training step 1076",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:07",
      "total_flops_so_far": 1546255477078328.0,
      "budget_used_percent": 1.5462554770783281
    },
    {
      "type": "training",
      "description": "Training step 1077",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:07",
      "total_flops_so_far": 1547686044115256.0,
      "budget_used_percent": 1.547686044115256
    },
    {
      "type": "training",
      "description": "Training step 1078",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:08",
      "total_flops_so_far": 1549116611152184.0,
      "budget_used_percent": 1.549116611152184
    },
    {
      "type": "training",
      "description": "Training step 1079",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:08",
      "total_flops_so_far": 1550547178189112.0,
      "budget_used_percent": 1.550547178189112
    },
    {
      "type": "training",
      "description": "Training step 1080",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:08",
      "total_flops_so_far": 1551977745226040.0,
      "budget_used_percent": 1.55197774522604
    },
    {
      "type": "training",
      "description": "Training step 1081",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:08",
      "total_flops_so_far": 1553408312262968.0,
      "budget_used_percent": 1.553408312262968
    },
    {
      "type": "training",
      "description": "Training step 1082",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:08",
      "total_flops_so_far": 1554838879299896.0,
      "budget_used_percent": 1.554838879299896
    },
    {
      "type": "training",
      "description": "Training step 1083",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:08",
      "total_flops_so_far": 1556269446336824.0,
      "budget_used_percent": 1.556269446336824
    },
    {
      "type": "training",
      "description": "Training step 1084",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:08",
      "total_flops_so_far": 1557700013373752.0,
      "budget_used_percent": 1.557700013373752
    },
    {
      "type": "training",
      "description": "Training step 1085",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:09",
      "total_flops_so_far": 1559130580410680.0,
      "budget_used_percent": 1.55913058041068
    },
    {
      "type": "training",
      "description": "Training step 1086",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:09",
      "total_flops_so_far": 1560561147447608.0,
      "budget_used_percent": 1.560561147447608
    },
    {
      "type": "training",
      "description": "Training step 1087",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:09",
      "total_flops_so_far": 1561991714484536.0,
      "budget_used_percent": 1.561991714484536
    },
    {
      "type": "training",
      "description": "Training step 1088",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:09",
      "total_flops_so_far": 1563422281521464.0,
      "budget_used_percent": 1.5634222815214638
    },
    {
      "type": "training",
      "description": "Training step 1089",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:09",
      "total_flops_so_far": 1564852848558392.0,
      "budget_used_percent": 1.564852848558392
    },
    {
      "type": "training",
      "description": "Training step 1090",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:09",
      "total_flops_so_far": 1566283415595320.0,
      "budget_used_percent": 1.5662834155953198
    },
    {
      "type": "training",
      "description": "Training step 1091",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:09",
      "total_flops_so_far": 1567713982632248.0,
      "budget_used_percent": 1.567713982632248
    },
    {
      "type": "training",
      "description": "Training step 1092",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:10",
      "total_flops_so_far": 1569144549669176.0,
      "budget_used_percent": 1.5691445496691758
    },
    {
      "type": "training",
      "description": "Training step 1093",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:10",
      "total_flops_so_far": 1570575116706104.0,
      "budget_used_percent": 1.570575116706104
    },
    {
      "type": "training",
      "description": "Training step 1094",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:10",
      "total_flops_so_far": 1572005683743032.0,
      "budget_used_percent": 1.5720056837430318
    },
    {
      "type": "training",
      "description": "Training step 1095",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:10",
      "total_flops_so_far": 1573436250779960.0,
      "budget_used_percent": 1.57343625077996
    },
    {
      "type": "training",
      "description": "Training step 1096",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:10",
      "total_flops_so_far": 1574866817816888.0,
      "budget_used_percent": 1.5748668178168879
    },
    {
      "type": "training",
      "description": "Training step 1097",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:10",
      "total_flops_so_far": 1576297384853816.0,
      "budget_used_percent": 1.576297384853816
    },
    {
      "type": "training",
      "description": "Training step 1098",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:10",
      "total_flops_so_far": 1577727951890744.0,
      "budget_used_percent": 1.5777279518907439
    },
    {
      "type": "training",
      "description": "Training step 1099",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:11",
      "total_flops_so_far": 1579158518927672.0,
      "budget_used_percent": 1.579158518927672
    },
    {
      "type": "training",
      "description": "Training step 1100",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:11",
      "total_flops_so_far": 1580589085964600.0,
      "budget_used_percent": 1.5805890859646
    },
    {
      "type": "training",
      "description": "Training step 1101",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:11",
      "total_flops_so_far": 1582019653001528.0,
      "budget_used_percent": 1.582019653001528
    },
    {
      "type": "training",
      "description": "Training step 1102",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:11",
      "total_flops_so_far": 1583450220038456.0,
      "budget_used_percent": 1.583450220038456
    },
    {
      "type": "training",
      "description": "Training step 1103",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:11",
      "total_flops_so_far": 1584880787075384.0,
      "budget_used_percent": 1.584880787075384
    },
    {
      "type": "training",
      "description": "Training step 1104",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:11",
      "total_flops_so_far": 1586311354112312.0,
      "budget_used_percent": 1.586311354112312
    },
    {
      "type": "training",
      "description": "Training step 1105",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:11",
      "total_flops_so_far": 1587741921149240.0,
      "budget_used_percent": 1.58774192114924
    },
    {
      "type": "training",
      "description": "Training step 1106",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:12",
      "total_flops_so_far": 1589172488186168.0,
      "budget_used_percent": 1.589172488186168
    },
    {
      "type": "training",
      "description": "Training step 1107",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:12",
      "total_flops_so_far": 1590603055223096.0,
      "budget_used_percent": 1.590603055223096
    },
    {
      "type": "training",
      "description": "Training step 1108",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:12",
      "total_flops_so_far": 1592033622260024.0,
      "budget_used_percent": 1.592033622260024
    },
    {
      "type": "training",
      "description": "Training step 1109",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:12",
      "total_flops_so_far": 1593464189296952.0,
      "budget_used_percent": 1.593464189296952
    },
    {
      "type": "training",
      "description": "Training step 1110",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:12",
      "total_flops_so_far": 1594894756333880.0,
      "budget_used_percent": 1.59489475633388
    },
    {
      "type": "training",
      "description": "Training step 1111",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:12",
      "total_flops_so_far": 1596325323370808.0,
      "budget_used_percent": 1.5963253233708081
    },
    {
      "type": "training",
      "description": "Training step 1112",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:12",
      "total_flops_so_far": 1597755890407736.0,
      "budget_used_percent": 1.597755890407736
    },
    {
      "type": "training",
      "description": "Training step 1113",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:13",
      "total_flops_so_far": 1599186457444664.0,
      "budget_used_percent": 1.5991864574446641
    },
    {
      "type": "training",
      "description": "Training step 1114",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:13",
      "total_flops_so_far": 1600617024481592.0,
      "budget_used_percent": 1.600617024481592
    },
    {
      "type": "training",
      "description": "Training step 1115",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:13",
      "total_flops_so_far": 1602047591518520.0,
      "budget_used_percent": 1.6020475915185202
    },
    {
      "type": "training",
      "description": "Training step 1116",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:13",
      "total_flops_so_far": 1603478158555448.0,
      "budget_used_percent": 1.603478158555448
    },
    {
      "type": "training",
      "description": "Training step 1117",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:13",
      "total_flops_so_far": 1604908725592376.0,
      "budget_used_percent": 1.6049087255923762
    },
    {
      "type": "training",
      "description": "Training step 1118",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:13",
      "total_flops_so_far": 1606339292629304.0,
      "budget_used_percent": 1.606339292629304
    },
    {
      "type": "training",
      "description": "Training step 1119",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:13",
      "total_flops_so_far": 1607769859666232.0,
      "budget_used_percent": 1.6077698596662322
    },
    {
      "type": "training",
      "description": "Training step 1120",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:14",
      "total_flops_so_far": 1609200426703160.0,
      "budget_used_percent": 1.60920042670316
    },
    {
      "type": "training",
      "description": "Training step 1121",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:14",
      "total_flops_so_far": 1610630993740088.0,
      "budget_used_percent": 1.6106309937400878
    },
    {
      "type": "training",
      "description": "Training step 1122",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:14",
      "total_flops_so_far": 1612061560777016.0,
      "budget_used_percent": 1.612061560777016
    },
    {
      "type": "training",
      "description": "Training step 1123",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:14",
      "total_flops_so_far": 1613492127813944.0,
      "budget_used_percent": 1.6134921278139438
    },
    {
      "type": "training",
      "description": "Training step 1124",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:14",
      "total_flops_so_far": 1614922694850872.0,
      "budget_used_percent": 1.614922694850872
    },
    {
      "type": "training",
      "description": "Training step 1125",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:14",
      "total_flops_so_far": 1616353261887800.0,
      "budget_used_percent": 1.6163532618877998
    },
    {
      "type": "training",
      "description": "Training step 1126",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:14",
      "total_flops_so_far": 1617783828924728.0,
      "budget_used_percent": 1.617783828924728
    },
    {
      "type": "training",
      "description": "Training step 1127",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:15",
      "total_flops_so_far": 1619214395961656.0,
      "budget_used_percent": 1.6192143959616558
    },
    {
      "type": "training",
      "description": "Training step 1128",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:15",
      "total_flops_so_far": 1620644962998584.0,
      "budget_used_percent": 1.620644962998584
    },
    {
      "type": "training",
      "description": "Training step 1129",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:15",
      "total_flops_so_far": 1622075530035512.0,
      "budget_used_percent": 1.6220755300355119
    },
    {
      "type": "training",
      "description": "Training step 1130",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:15",
      "total_flops_so_far": 1623506097072440.0,
      "budget_used_percent": 1.62350609707244
    },
    {
      "type": "training",
      "description": "Training step 1131",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:15",
      "total_flops_so_far": 1624936664109368.0,
      "budget_used_percent": 1.6249366641093679
    },
    {
      "type": "training",
      "description": "Training step 1132",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:15",
      "total_flops_so_far": 1626367231146296.0,
      "budget_used_percent": 1.626367231146296
    },
    {
      "type": "training",
      "description": "Training step 1133",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:15",
      "total_flops_so_far": 1627797798183224.0,
      "budget_used_percent": 1.627797798183224
    },
    {
      "type": "training",
      "description": "Training step 1134",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:16",
      "total_flops_so_far": 1629228365220152.0,
      "budget_used_percent": 1.629228365220152
    },
    {
      "type": "training",
      "description": "Training step 1135",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:16",
      "total_flops_so_far": 1630658932257080.0,
      "budget_used_percent": 1.63065893225708
    },
    {
      "type": "training",
      "description": "Training step 1136",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:16",
      "total_flops_so_far": 1632089499294008.0,
      "budget_used_percent": 1.632089499294008
    },
    {
      "type": "training",
      "description": "Training step 1137",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:16",
      "total_flops_so_far": 1633520066330936.0,
      "budget_used_percent": 1.633520066330936
    },
    {
      "type": "training",
      "description": "Training step 1138",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:16",
      "total_flops_so_far": 1634950633367864.0,
      "budget_used_percent": 1.634950633367864
    },
    {
      "type": "training",
      "description": "Training step 1139",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:16",
      "total_flops_so_far": 1636381200404792.0,
      "budget_used_percent": 1.636381200404792
    },
    {
      "type": "training",
      "description": "Training step 1140",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:16",
      "total_flops_so_far": 1637811767441720.0,
      "budget_used_percent": 1.63781176744172
    },
    {
      "type": "training",
      "description": "Training step 1141",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:17",
      "total_flops_so_far": 1639242334478648.0,
      "budget_used_percent": 1.639242334478648
    },
    {
      "type": "training",
      "description": "Training step 1142",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:17",
      "total_flops_so_far": 1640672901515576.0,
      "budget_used_percent": 1.640672901515576
    },
    {
      "type": "training",
      "description": "Training step 1143",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:17",
      "total_flops_so_far": 1642103468552504.0,
      "budget_used_percent": 1.642103468552504
    },
    {
      "type": "training",
      "description": "Training step 1144",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:17",
      "total_flops_so_far": 1643534035589432.0,
      "budget_used_percent": 1.6435340355894321
    },
    {
      "type": "training",
      "description": "Training step 1145",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:17",
      "total_flops_so_far": 1644964602626360.0,
      "budget_used_percent": 1.64496460262636
    },
    {
      "type": "training",
      "description": "Training step 1146",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:17",
      "total_flops_so_far": 1646395169663288.0,
      "budget_used_percent": 1.6463951696632881
    },
    {
      "type": "training",
      "description": "Training step 1147",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:17",
      "total_flops_so_far": 1647825736700216.0,
      "budget_used_percent": 1.647825736700216
    },
    {
      "type": "training",
      "description": "Training step 1148",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:18",
      "total_flops_so_far": 1649256303737144.0,
      "budget_used_percent": 1.6492563037371442
    },
    {
      "type": "training",
      "description": "Training step 1149",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:18",
      "total_flops_so_far": 1650686870774072.0,
      "budget_used_percent": 1.650686870774072
    },
    {
      "type": "training",
      "description": "Training step 1150",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:18",
      "total_flops_so_far": 1652117437811000.0,
      "budget_used_percent": 1.6521174378110002
    },
    {
      "type": "training",
      "description": "Training step 1151",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:18",
      "total_flops_so_far": 1653548004847928.0,
      "budget_used_percent": 1.653548004847928
    },
    {
      "type": "training",
      "description": "Training step 1152",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:18",
      "total_flops_so_far": 1654978571884856.0,
      "budget_used_percent": 1.6549785718848562
    },
    {
      "type": "training",
      "description": "Training step 1153",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:18",
      "total_flops_so_far": 1656409138921784.0,
      "budget_used_percent": 1.656409138921784
    },
    {
      "type": "training",
      "description": "Training step 1154",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:18",
      "total_flops_so_far": 1657839705958712.0,
      "budget_used_percent": 1.6578397059587122
    },
    {
      "type": "training",
      "description": "Training step 1155",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:19",
      "total_flops_so_far": 1659270272995640.0,
      "budget_used_percent": 1.6592702729956401
    },
    {
      "type": "training",
      "description": "Training step 1156",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:19",
      "total_flops_so_far": 1660700840032568.0,
      "budget_used_percent": 1.6607008400325678
    },
    {
      "type": "training",
      "description": "Training step 1157",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:19",
      "total_flops_so_far": 1662131407069496.0,
      "budget_used_percent": 1.662131407069496
    },
    {
      "type": "training",
      "description": "Training step 1158",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:19",
      "total_flops_so_far": 1663561974106424.0,
      "budget_used_percent": 1.6635619741064238
    },
    {
      "type": "training",
      "description": "Training step 1159",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:19",
      "total_flops_so_far": 1664992541143352.0,
      "budget_used_percent": 1.664992541143352
    },
    {
      "type": "training",
      "description": "Training step 1160",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:19",
      "total_flops_so_far": 1666423108180280.0,
      "budget_used_percent": 1.6664231081802798
    },
    {
      "type": "training",
      "description": "Training step 1161",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:19",
      "total_flops_so_far": 1667853675217208.0,
      "budget_used_percent": 1.667853675217208
    },
    {
      "type": "training",
      "description": "Training step 1162",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:20",
      "total_flops_so_far": 1669284242254136.0,
      "budget_used_percent": 1.6692842422541359
    },
    {
      "type": "training",
      "description": "Training step 1163",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:20",
      "total_flops_so_far": 1670714809291064.0,
      "budget_used_percent": 1.670714809291064
    },
    {
      "type": "training",
      "description": "Training step 1164",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:20",
      "total_flops_so_far": 1672145376327992.0,
      "budget_used_percent": 1.6721453763279919
    },
    {
      "type": "training",
      "description": "Training step 1165",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:20",
      "total_flops_so_far": 1673575943364920.0,
      "budget_used_percent": 1.67357594336492
    },
    {
      "type": "training",
      "description": "Training step 1166",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:20",
      "total_flops_so_far": 1675006510401848.0,
      "budget_used_percent": 1.675006510401848
    },
    {
      "type": "training",
      "description": "Training step 1167",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:20",
      "total_flops_so_far": 1676437077438776.0,
      "budget_used_percent": 1.676437077438776
    },
    {
      "type": "training",
      "description": "Training step 1168",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:20",
      "total_flops_so_far": 1677867644475704.0,
      "budget_used_percent": 1.677867644475704
    },
    {
      "type": "training",
      "description": "Training step 1169",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:21",
      "total_flops_so_far": 1679298211512632.0,
      "budget_used_percent": 1.679298211512632
    },
    {
      "type": "training",
      "description": "Training step 1170",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:21",
      "total_flops_so_far": 1680728778549560.0,
      "budget_used_percent": 1.68072877854956
    },
    {
      "type": "training",
      "description": "Training step 1171",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:21",
      "total_flops_so_far": 1682159345586488.0,
      "budget_used_percent": 1.682159345586488
    },
    {
      "type": "training",
      "description": "Training step 1172",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:21",
      "total_flops_so_far": 1683589912623416.0,
      "budget_used_percent": 1.683589912623416
    },
    {
      "type": "training",
      "description": "Training step 1173",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:21",
      "total_flops_so_far": 1685020479660344.0,
      "budget_used_percent": 1.685020479660344
    },
    {
      "type": "training",
      "description": "Training step 1174",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:21",
      "total_flops_so_far": 1686451046697272.0,
      "budget_used_percent": 1.686451046697272
    },
    {
      "type": "training",
      "description": "Training step 1175",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:21",
      "total_flops_so_far": 1687881613734200.0,
      "budget_used_percent": 1.6878816137342
    },
    {
      "type": "training",
      "description": "Training step 1176",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:22",
      "total_flops_so_far": 1689312180771128.0,
      "budget_used_percent": 1.689312180771128
    },
    {
      "type": "training",
      "description": "Training step 1177",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:22",
      "total_flops_so_far": 1690742747808056.0,
      "budget_used_percent": 1.6907427478080561
    },
    {
      "type": "training",
      "description": "Training step 1178",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:22",
      "total_flops_so_far": 1692173314844984.0,
      "budget_used_percent": 1.692173314844984
    },
    {
      "type": "training",
      "description": "Training step 1179",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:22",
      "total_flops_so_far": 1693603881881912.0,
      "budget_used_percent": 1.6936038818819121
    },
    {
      "type": "training",
      "description": "Training step 1180",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:22",
      "total_flops_so_far": 1695034448918840.0,
      "budget_used_percent": 1.69503444891884
    },
    {
      "type": "training",
      "description": "Training step 1181",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:22",
      "total_flops_so_far": 1696465015955768.0,
      "budget_used_percent": 1.6964650159557682
    },
    {
      "type": "training",
      "description": "Training step 1182",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:22",
      "total_flops_so_far": 1697895582992696.0,
      "budget_used_percent": 1.697895582992696
    },
    {
      "type": "training",
      "description": "Training step 1183",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:23",
      "total_flops_so_far": 1699326150029624.0,
      "budget_used_percent": 1.6993261500296242
    },
    {
      "type": "training",
      "description": "Training step 1184",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:23",
      "total_flops_so_far": 1700756717066552.0,
      "budget_used_percent": 1.700756717066552
    },
    {
      "type": "training",
      "description": "Training step 1185",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:23",
      "total_flops_so_far": 1702187284103480.0,
      "budget_used_percent": 1.7021872841034802
    },
    {
      "type": "training",
      "description": "Training step 1186",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:23",
      "total_flops_so_far": 1703617851140408.0,
      "budget_used_percent": 1.703617851140408
    },
    {
      "type": "training",
      "description": "Training step 1187",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:23",
      "total_flops_so_far": 1705048418177336.0,
      "budget_used_percent": 1.7050484181773362
    },
    {
      "type": "training",
      "description": "Training step 1188",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:23",
      "total_flops_so_far": 1706478985214264.0,
      "budget_used_percent": 1.706478985214264
    },
    {
      "type": "training",
      "description": "Training step 1189",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:23",
      "total_flops_so_far": 1707909552251192.0,
      "budget_used_percent": 1.7079095522511922
    },
    {
      "type": "training",
      "description": "Training step 1190",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:24",
      "total_flops_so_far": 1709340119288120.0,
      "budget_used_percent": 1.7093401192881201
    },
    {
      "type": "training",
      "description": "Training step 1191",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:24",
      "total_flops_so_far": 1710770686325048.0,
      "budget_used_percent": 1.7107706863250478
    },
    {
      "type": "training",
      "description": "Training step 1192",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:24",
      "total_flops_so_far": 1712201253361976.0,
      "budget_used_percent": 1.712201253361976
    },
    {
      "type": "training",
      "description": "Training step 1193",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:24",
      "total_flops_so_far": 1713631820398904.0,
      "budget_used_percent": 1.7136318203989038
    },
    {
      "type": "training",
      "description": "Training step 1194",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:24",
      "total_flops_so_far": 1715062387435832.0,
      "budget_used_percent": 1.715062387435832
    },
    {
      "type": "training",
      "description": "Training step 1195",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:24",
      "total_flops_so_far": 1716492954472760.0,
      "budget_used_percent": 1.7164929544727598
    },
    {
      "type": "training",
      "description": "Training step 1196",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:24",
      "total_flops_so_far": 1717923521509688.0,
      "budget_used_percent": 1.717923521509688
    },
    {
      "type": "training",
      "description": "Training step 1197",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:25",
      "total_flops_so_far": 1719354088546616.0,
      "budget_used_percent": 1.7193540885466159
    },
    {
      "type": "training",
      "description": "Training step 1198",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:25",
      "total_flops_so_far": 1720784655583544.0,
      "budget_used_percent": 1.720784655583544
    },
    {
      "type": "training",
      "description": "Training step 1199",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:25",
      "total_flops_so_far": 1722215222620472.0,
      "budget_used_percent": 1.7222152226204719
    },
    {
      "type": "training",
      "description": "Training step 1200",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:25",
      "total_flops_so_far": 1723645789657400.0,
      "budget_used_percent": 1.7236457896574
    },
    {
      "type": "training",
      "description": "Training step 1201",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:25",
      "total_flops_so_far": 1725076356694328.0,
      "budget_used_percent": 1.725076356694328
    },
    {
      "type": "training",
      "description": "Training step 1202",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:25",
      "total_flops_so_far": 1726506923731256.0,
      "budget_used_percent": 1.726506923731256
    },
    {
      "type": "training",
      "description": "Training step 1203",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:25",
      "total_flops_so_far": 1727937490768184.0,
      "budget_used_percent": 1.727937490768184
    },
    {
      "type": "training",
      "description": "Training step 1204",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:26",
      "total_flops_so_far": 1729368057805112.0,
      "budget_used_percent": 1.729368057805112
    },
    {
      "type": "training",
      "description": "Training step 1205",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:26",
      "total_flops_so_far": 1730798624842040.0,
      "budget_used_percent": 1.73079862484204
    },
    {
      "type": "training",
      "description": "Training step 1206",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:26",
      "total_flops_so_far": 1732229191878968.0,
      "budget_used_percent": 1.732229191878968
    },
    {
      "type": "training",
      "description": "Training step 1207",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:26",
      "total_flops_so_far": 1733659758915896.0,
      "budget_used_percent": 1.733659758915896
    },
    {
      "type": "training",
      "description": "Training step 1208",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:26",
      "total_flops_so_far": 1735090325952824.0,
      "budget_used_percent": 1.735090325952824
    },
    {
      "type": "training",
      "description": "Training step 1209",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:26",
      "total_flops_so_far": 1736520892989752.0,
      "budget_used_percent": 1.736520892989752
    },
    {
      "type": "training",
      "description": "Training step 1210",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:27",
      "total_flops_so_far": 1737951460026680.0,
      "budget_used_percent": 1.73795146002668
    },
    {
      "type": "training",
      "description": "Training step 1211",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:27",
      "total_flops_so_far": 1739382027063608.0,
      "budget_used_percent": 1.739382027063608
    },
    {
      "type": "training",
      "description": "Training step 1212",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:27",
      "total_flops_so_far": 1740812594100536.0,
      "budget_used_percent": 1.7408125941005361
    },
    {
      "type": "training",
      "description": "Training step 1213",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:27",
      "total_flops_so_far": 1742243161137464.0,
      "budget_used_percent": 1.742243161137464
    },
    {
      "type": "training",
      "description": "Training step 1214",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:27",
      "total_flops_so_far": 1743673728174392.0,
      "budget_used_percent": 1.7436737281743921
    },
    {
      "type": "training",
      "description": "Training step 1215",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:27",
      "total_flops_so_far": 1745104295211320.0,
      "budget_used_percent": 1.74510429521132
    },
    {
      "type": "training",
      "description": "Training step 1216",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:27",
      "total_flops_so_far": 1746534862248248.0,
      "budget_used_percent": 1.7465348622482482
    },
    {
      "type": "training",
      "description": "Training step 1217",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:28",
      "total_flops_so_far": 1747965429285176.0,
      "budget_used_percent": 1.747965429285176
    },
    {
      "type": "training",
      "description": "Training step 1218",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:28",
      "total_flops_so_far": 1749395996322104.0,
      "budget_used_percent": 1.7493959963221042
    },
    {
      "type": "training",
      "description": "Training step 1219",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:28",
      "total_flops_so_far": 1750826563359032.0,
      "budget_used_percent": 1.750826563359032
    },
    {
      "type": "training",
      "description": "Training step 1220",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:28",
      "total_flops_so_far": 1752257130395960.0,
      "budget_used_percent": 1.7522571303959602
    },
    {
      "type": "training",
      "description": "Training step 1221",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:28",
      "total_flops_so_far": 1753687697432888.0,
      "budget_used_percent": 1.753687697432888
    },
    {
      "type": "training",
      "description": "Training step 1222",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:28",
      "total_flops_so_far": 1755118264469816.0,
      "budget_used_percent": 1.7551182644698162
    },
    {
      "type": "training",
      "description": "Training step 1223",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:29",
      "total_flops_so_far": 1756548831506744.0,
      "budget_used_percent": 1.7565488315067441
    },
    {
      "type": "training",
      "description": "Training step 1224",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:29",
      "total_flops_so_far": 1757979398543672.0,
      "budget_used_percent": 1.7579793985436722
    },
    {
      "type": "training",
      "description": "Training step 1225",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:29",
      "total_flops_so_far": 1759409965580600.0,
      "budget_used_percent": 1.7594099655806001
    },
    {
      "type": "training",
      "description": "Training step 1226",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:29",
      "total_flops_so_far": 1760840532617528.0,
      "budget_used_percent": 1.7608405326175278
    },
    {
      "type": "training",
      "description": "Training step 1227",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:29",
      "total_flops_so_far": 1762271099654456.0,
      "budget_used_percent": 1.762271099654456
    },
    {
      "type": "training",
      "description": "Training step 1228",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:29",
      "total_flops_so_far": 1763701666691384.0,
      "budget_used_percent": 1.7637016666913838
    },
    {
      "type": "training",
      "description": "Training step 1229",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:29",
      "total_flops_so_far": 1765132233728312.0,
      "budget_used_percent": 1.765132233728312
    },
    {
      "type": "training",
      "description": "Training step 1230",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:30",
      "total_flops_so_far": 1766562800765240.0,
      "budget_used_percent": 1.7665628007652399
    },
    {
      "type": "training",
      "description": "Training step 1231",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:30",
      "total_flops_so_far": 1767993367802168.0,
      "budget_used_percent": 1.767993367802168
    },
    {
      "type": "training",
      "description": "Training step 1232",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:30",
      "total_flops_so_far": 1769423934839096.0,
      "budget_used_percent": 1.7694239348390959
    },
    {
      "type": "training",
      "description": "Training step 1233",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:30",
      "total_flops_so_far": 1770854501876024.0,
      "budget_used_percent": 1.770854501876024
    },
    {
      "type": "training",
      "description": "Training step 1234",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:30",
      "total_flops_so_far": 1772285068912952.0,
      "budget_used_percent": 1.772285068912952
    },
    {
      "type": "training",
      "description": "Training step 1235",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:30",
      "total_flops_so_far": 1773715635949880.0,
      "budget_used_percent": 1.77371563594988
    },
    {
      "type": "training",
      "description": "Training step 1236",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:30",
      "total_flops_so_far": 1775146202986808.0,
      "budget_used_percent": 1.775146202986808
    },
    {
      "type": "training",
      "description": "Training step 1237",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:31",
      "total_flops_so_far": 1776576770023736.0,
      "budget_used_percent": 1.776576770023736
    },
    {
      "type": "training",
      "description": "Training step 1238",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:31",
      "total_flops_so_far": 1778007337060664.0,
      "budget_used_percent": 1.778007337060664
    },
    {
      "type": "training",
      "description": "Training step 1239",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:31",
      "total_flops_so_far": 1779437904097592.0,
      "budget_used_percent": 1.779437904097592
    },
    {
      "type": "training",
      "description": "Training step 1240",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:31",
      "total_flops_so_far": 1780868471134520.0,
      "budget_used_percent": 1.78086847113452
    },
    {
      "type": "training",
      "description": "Training step 1241",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:31",
      "total_flops_so_far": 1782299038171448.0,
      "budget_used_percent": 1.782299038171448
    },
    {
      "type": "training",
      "description": "Training step 1242",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:31",
      "total_flops_so_far": 1783729605208376.0,
      "budget_used_percent": 1.783729605208376
    },
    {
      "type": "training",
      "description": "Training step 1243",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:31",
      "total_flops_so_far": 1785160172245304.0,
      "budget_used_percent": 1.785160172245304
    },
    {
      "type": "training",
      "description": "Training step 1244",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:32",
      "total_flops_so_far": 1786590739282232.0,
      "budget_used_percent": 1.786590739282232
    },
    {
      "type": "training",
      "description": "Training step 1245",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:32",
      "total_flops_so_far": 1788021306319160.0,
      "budget_used_percent": 1.7880213063191601
    },
    {
      "type": "training",
      "description": "Training step 1246",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:32",
      "total_flops_so_far": 1789451873356088.0,
      "budget_used_percent": 1.789451873356088
    },
    {
      "type": "training",
      "description": "Training step 1247",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:32",
      "total_flops_so_far": 1790882440393016.0,
      "budget_used_percent": 1.7908824403930161
    },
    {
      "type": "training",
      "description": "Training step 1248",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:32",
      "total_flops_so_far": 1792313007429944.0,
      "budget_used_percent": 1.792313007429944
    },
    {
      "type": "training",
      "description": "Training step 1249",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:32",
      "total_flops_so_far": 1793743574466872.0,
      "budget_used_percent": 1.7937435744668722
    },
    {
      "type": "training",
      "description": "Training step 1250",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:32",
      "total_flops_so_far": 1795174141503800.0,
      "budget_used_percent": 1.7951741415038
    },
    {
      "type": "training",
      "description": "Training step 1251",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:33",
      "total_flops_so_far": 1796604708540728.0,
      "budget_used_percent": 1.7966047085407282
    },
    {
      "type": "training",
      "description": "Training step 1252",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:33",
      "total_flops_so_far": 1798035275577656.0,
      "budget_used_percent": 1.798035275577656
    },
    {
      "type": "training",
      "description": "Training step 1253",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:33",
      "total_flops_so_far": 1799465842614584.0,
      "budget_used_percent": 1.7994658426145842
    },
    {
      "type": "training",
      "description": "Training step 1254",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:33",
      "total_flops_so_far": 1800896409651512.0,
      "budget_used_percent": 1.800896409651512
    },
    {
      "type": "training",
      "description": "Training step 1255",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:33",
      "total_flops_so_far": 1802326976688440.0,
      "budget_used_percent": 1.8023269766884402
    },
    {
      "type": "training",
      "description": "Training step 1256",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:33",
      "total_flops_so_far": 1803757543725368.0,
      "budget_used_percent": 1.8037575437253681
    },
    {
      "type": "training",
      "description": "Training step 1257",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:33",
      "total_flops_so_far": 1805188110762296.0,
      "budget_used_percent": 1.8051881107622962
    },
    {
      "type": "training",
      "description": "Training step 1258",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:34",
      "total_flops_so_far": 1806618677799224.0,
      "budget_used_percent": 1.8066186777992241
    },
    {
      "type": "training",
      "description": "Training step 1259",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:34",
      "total_flops_so_far": 1808049244836152.0,
      "budget_used_percent": 1.8080492448361523
    },
    {
      "type": "training",
      "description": "Training step 1260",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:34",
      "total_flops_so_far": 1809479811873080.0,
      "budget_used_percent": 1.8094798118730802
    },
    {
      "type": "training",
      "description": "Training step 1261",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:34",
      "total_flops_so_far": 1810910378910008.0,
      "budget_used_percent": 1.8109103789100078
    },
    {
      "type": "training",
      "description": "Training step 1262",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:34",
      "total_flops_so_far": 1812340945946936.0,
      "budget_used_percent": 1.8123409459469357
    },
    {
      "type": "training",
      "description": "Training step 1263",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:34",
      "total_flops_so_far": 1813771512983864.0,
      "budget_used_percent": 1.8137715129838639
    },
    {
      "type": "training",
      "description": "Training step 1264",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:34",
      "total_flops_so_far": 1815202080020792.0,
      "budget_used_percent": 1.815202080020792
    },
    {
      "type": "training",
      "description": "Training step 1265",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:35",
      "total_flops_so_far": 1816632647057720.0,
      "budget_used_percent": 1.8166326470577199
    },
    {
      "type": "training",
      "description": "Training step 1266",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:35",
      "total_flops_so_far": 1818063214094648.0,
      "budget_used_percent": 1.8180632140946478
    },
    {
      "type": "training",
      "description": "Training step 1267",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:35",
      "total_flops_so_far": 1819493781131576.0,
      "budget_used_percent": 1.819493781131576
    },
    {
      "type": "training",
      "description": "Training step 1268",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:35",
      "total_flops_so_far": 1820924348168504.0,
      "budget_used_percent": 1.820924348168504
    },
    {
      "type": "training",
      "description": "Training step 1269",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:35",
      "total_flops_so_far": 1822354915205432.0,
      "budget_used_percent": 1.822354915205432
    },
    {
      "type": "training",
      "description": "Training step 1270",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:35",
      "total_flops_so_far": 1823785482242360.0,
      "budget_used_percent": 1.8237854822423598
    },
    {
      "type": "training",
      "description": "Training step 1271",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:35",
      "total_flops_so_far": 1825216049279288.0,
      "budget_used_percent": 1.825216049279288
    },
    {
      "type": "training",
      "description": "Training step 1272",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:36",
      "total_flops_so_far": 1826646616316216.0,
      "budget_used_percent": 1.826646616316216
    },
    {
      "type": "training",
      "description": "Training step 1273",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:36",
      "total_flops_so_far": 1828077183353144.0,
      "budget_used_percent": 1.828077183353144
    },
    {
      "type": "training",
      "description": "Training step 1274",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:36",
      "total_flops_so_far": 1829507750390072.0,
      "budget_used_percent": 1.8295077503900719
    },
    {
      "type": "training",
      "description": "Training step 1275",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:36",
      "total_flops_so_far": 1830938317427000.0,
      "budget_used_percent": 1.830938317427
    },
    {
      "type": "training",
      "description": "Training step 1276",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:36",
      "total_flops_so_far": 1832368884463928.0,
      "budget_used_percent": 1.832368884463928
    },
    {
      "type": "training",
      "description": "Training step 1277",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:36",
      "total_flops_so_far": 1833799451500856.0,
      "budget_used_percent": 1.833799451500856
    },
    {
      "type": "training",
      "description": "Training step 1278",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:37",
      "total_flops_so_far": 1835230018537784.0,
      "budget_used_percent": 1.835230018537784
    },
    {
      "type": "training",
      "description": "Training step 1279",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:37",
      "total_flops_so_far": 1836660585574712.0,
      "budget_used_percent": 1.836660585574712
    },
    {
      "type": "training",
      "description": "Training step 1280",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:37",
      "total_flops_so_far": 1838091152611640.0,
      "budget_used_percent": 1.8380911526116401
    },
    {
      "type": "training",
      "description": "Training step 1281",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:37",
      "total_flops_so_far": 1839521719648568.0,
      "budget_used_percent": 1.839521719648568
    },
    {
      "type": "training",
      "description": "Training step 1282",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:37",
      "total_flops_so_far": 1840952286685496.0,
      "budget_used_percent": 1.840952286685496
    },
    {
      "type": "training",
      "description": "Training step 1283",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:37",
      "total_flops_so_far": 1842382853722424.0,
      "budget_used_percent": 1.842382853722424
    },
    {
      "type": "training",
      "description": "Training step 1284",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:37",
      "total_flops_so_far": 1843813420759352.0,
      "budget_used_percent": 1.8438134207593522
    },
    {
      "type": "training",
      "description": "Training step 1285",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:38",
      "total_flops_so_far": 1845243987796280.0,
      "budget_used_percent": 1.84524398779628
    },
    {
      "type": "training",
      "description": "Training step 1286",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:38",
      "total_flops_so_far": 1846674554833208.0,
      "budget_used_percent": 1.846674554833208
    },
    {
      "type": "training",
      "description": "Training step 1287",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:38",
      "total_flops_so_far": 1848105121870136.0,
      "budget_used_percent": 1.848105121870136
    },
    {
      "type": "training",
      "description": "Training step 1288",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:38",
      "total_flops_so_far": 1849535688907064.0,
      "budget_used_percent": 1.8495356889070642
    },
    {
      "type": "training",
      "description": "Training step 1289",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:38",
      "total_flops_so_far": 1850966255943992.0,
      "budget_used_percent": 1.8509662559439921
    },
    {
      "type": "training",
      "description": "Training step 1290",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:38",
      "total_flops_so_far": 1852396822980920.0,
      "budget_used_percent": 1.85239682298092
    },
    {
      "type": "training",
      "description": "Training step 1291",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:38",
      "total_flops_so_far": 1853827390017848.0,
      "budget_used_percent": 1.8538273900178481
    },
    {
      "type": "training",
      "description": "Training step 1292",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:39",
      "total_flops_so_far": 1855257957054776.0,
      "budget_used_percent": 1.8552579570547763
    },
    {
      "type": "training",
      "description": "Training step 1293",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:39",
      "total_flops_so_far": 1856688524091704.0,
      "budget_used_percent": 1.8566885240917042
    },
    {
      "type": "training",
      "description": "Training step 1294",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:39",
      "total_flops_so_far": 1858119091128632.0,
      "budget_used_percent": 1.858119091128632
    },
    {
      "type": "training",
      "description": "Training step 1295",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:39",
      "total_flops_so_far": 1859549658165560.0,
      "budget_used_percent": 1.8595496581655602
    },
    {
      "type": "training",
      "description": "Training step 1296",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:39",
      "total_flops_so_far": 1860980225202488.0,
      "budget_used_percent": 1.8609802252024878
    },
    {
      "type": "training",
      "description": "Training step 1297",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:39",
      "total_flops_so_far": 1862410792239416.0,
      "budget_used_percent": 1.8624107922394157
    },
    {
      "type": "training",
      "description": "Training step 1298",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:39",
      "total_flops_so_far": 1863841359276344.0,
      "budget_used_percent": 1.8638413592763439
    },
    {
      "type": "training",
      "description": "Training step 1299",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:40",
      "total_flops_so_far": 1865271926313272.0,
      "budget_used_percent": 1.8652719263132718
    },
    {
      "type": "training",
      "description": "Training step 1300",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:40",
      "total_flops_so_far": 1866702493350200.0,
      "budget_used_percent": 1.8667024933501999
    },
    {
      "type": "training",
      "description": "Training step 1301",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:40",
      "total_flops_so_far": 1868133060387128.0,
      "budget_used_percent": 1.8681330603871278
    },
    {
      "type": "training",
      "description": "Training step 1302",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:40",
      "total_flops_so_far": 1869563627424056.0,
      "budget_used_percent": 1.869563627424056
    },
    {
      "type": "training",
      "description": "Training step 1303",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:40",
      "total_flops_so_far": 1870994194460984.0,
      "budget_used_percent": 1.8709941944609838
    },
    {
      "type": "training",
      "description": "Training step 1304",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:40",
      "total_flops_so_far": 1872424761497912.0,
      "budget_used_percent": 1.872424761497912
    },
    {
      "type": "training",
      "description": "Training step 1305",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:40",
      "total_flops_so_far": 1873855328534840.0,
      "budget_used_percent": 1.8738553285348398
    },
    {
      "type": "training",
      "description": "Training step 1306",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:41",
      "total_flops_so_far": 1875285895571768.0,
      "budget_used_percent": 1.875285895571768
    },
    {
      "type": "training",
      "description": "Training step 1307",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:41",
      "total_flops_so_far": 1876716462608696.0,
      "budget_used_percent": 1.8767164626086958
    },
    {
      "type": "training",
      "description": "Training step 1308",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:41",
      "total_flops_so_far": 1878147029645624.0,
      "budget_used_percent": 1.878147029645624
    },
    {
      "type": "training",
      "description": "Training step 1309",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:41",
      "total_flops_so_far": 1879577596682552.0,
      "budget_used_percent": 1.8795775966825519
    },
    {
      "type": "training",
      "description": "Training step 1310",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:41",
      "total_flops_so_far": 1881008163719480.0,
      "budget_used_percent": 1.88100816371948
    },
    {
      "type": "training",
      "description": "Training step 1311",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:41",
      "total_flops_so_far": 1882438730756408.0,
      "budget_used_percent": 1.8824387307564079
    },
    {
      "type": "training",
      "description": "Training step 1312",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:41",
      "total_flops_so_far": 1883869297793336.0,
      "budget_used_percent": 1.883869297793336
    },
    {
      "type": "training",
      "description": "Training step 1313",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:42",
      "total_flops_so_far": 1885299864830264.0,
      "budget_used_percent": 1.885299864830264
    },
    {
      "type": "training",
      "description": "Training step 1314",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:42",
      "total_flops_so_far": 1886730431867192.0,
      "budget_used_percent": 1.886730431867192
    },
    {
      "type": "training",
      "description": "Training step 1315",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:42",
      "total_flops_so_far": 1888160998904120.0,
      "budget_used_percent": 1.88816099890412
    },
    {
      "type": "training",
      "description": "Training step 1316",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:42",
      "total_flops_so_far": 1889591565941048.0,
      "budget_used_percent": 1.889591565941048
    },
    {
      "type": "training",
      "description": "Training step 1317",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:42",
      "total_flops_so_far": 1891022132977976.0,
      "budget_used_percent": 1.891022132977976
    },
    {
      "type": "training",
      "description": "Training step 1318",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:42",
      "total_flops_so_far": 1892452700014904.0,
      "budget_used_percent": 1.892452700014904
    },
    {
      "type": "training",
      "description": "Training step 1319",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:43",
      "total_flops_so_far": 1893883267051832.0,
      "budget_used_percent": 1.893883267051832
    },
    {
      "type": "training",
      "description": "Training step 1320",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:43",
      "total_flops_so_far": 1895313834088760.0,
      "budget_used_percent": 1.89531383408876
    },
    {
      "type": "training",
      "description": "Training step 1321",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:43",
      "total_flops_so_far": 1896744401125688.0,
      "budget_used_percent": 1.896744401125688
    },
    {
      "type": "training",
      "description": "Training step 1322",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:43",
      "total_flops_so_far": 1898174968162616.0,
      "budget_used_percent": 1.898174968162616
    },
    {
      "type": "training",
      "description": "Training step 1323",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:43",
      "total_flops_so_far": 1899605535199544.0,
      "budget_used_percent": 1.899605535199544
    },
    {
      "type": "training",
      "description": "Training step 1324",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:43",
      "total_flops_so_far": 1901036102236472.0,
      "budget_used_percent": 1.9010361022364721
    },
    {
      "type": "training",
      "description": "Training step 1325",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:43",
      "total_flops_so_far": 1902466669273400.0,
      "budget_used_percent": 1.9024666692734
    },
    {
      "type": "training",
      "description": "Training step 1326",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:44",
      "total_flops_so_far": 1903897236310328.0,
      "budget_used_percent": 1.9038972363103281
    },
    {
      "type": "training",
      "description": "Training step 1327",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:44",
      "total_flops_so_far": 1905327803347256.0,
      "budget_used_percent": 1.905327803347256
    },
    {
      "type": "training",
      "description": "Training step 1328",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:44",
      "total_flops_so_far": 1906758370384184.0,
      "budget_used_percent": 1.9067583703841842
    },
    {
      "type": "training",
      "description": "Training step 1329",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:44",
      "total_flops_so_far": 1908188937421112.0,
      "budget_used_percent": 1.908188937421112
    },
    {
      "type": "training",
      "description": "Training step 1330",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:44",
      "total_flops_so_far": 1909619504458040.0,
      "budget_used_percent": 1.9096195044580402
    },
    {
      "type": "training",
      "description": "Training step 1331",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:44",
      "total_flops_so_far": 1911050071494968.0,
      "budget_used_percent": 1.9110500714949679
    },
    {
      "type": "training",
      "description": "Training step 1332",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:44",
      "total_flops_so_far": 1912480638531896.0,
      "budget_used_percent": 1.9124806385318958
    },
    {
      "type": "training",
      "description": "Training step 1333",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:45",
      "total_flops_so_far": 1913911205568824.0,
      "budget_used_percent": 1.9139112055688239
    },
    {
      "type": "training",
      "description": "Training step 1334",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:45",
      "total_flops_so_far": 1915341772605752.0,
      "budget_used_percent": 1.9153417726057518
    },
    {
      "type": "training",
      "description": "Training step 1335",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:45",
      "total_flops_so_far": 1916772339642680.0,
      "budget_used_percent": 1.91677233964268
    },
    {
      "type": "training",
      "description": "Training step 1336",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:45",
      "total_flops_so_far": 1918202906679608.0,
      "budget_used_percent": 1.9182029066796078
    },
    {
      "type": "training",
      "description": "Training step 1337",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:45",
      "total_flops_so_far": 1919633473716536.0,
      "budget_used_percent": 1.919633473716536
    },
    {
      "type": "training",
      "description": "Training step 1338",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:45",
      "total_flops_so_far": 1921064040753464.0,
      "budget_used_percent": 1.9210640407534638
    },
    {
      "type": "training",
      "description": "Training step 1339",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:45",
      "total_flops_so_far": 1922494607790392.0,
      "budget_used_percent": 1.922494607790392
    },
    {
      "type": "training",
      "description": "Training step 1340",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:46",
      "total_flops_so_far": 1923925174827320.0,
      "budget_used_percent": 1.9239251748273198
    },
    {
      "type": "training",
      "description": "Training step 1341",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:46",
      "total_flops_so_far": 1925355741864248.0,
      "budget_used_percent": 1.925355741864248
    },
    {
      "type": "training",
      "description": "Training step 1342",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:46",
      "total_flops_so_far": 1926786308901176.0,
      "budget_used_percent": 1.9267863089011759
    },
    {
      "type": "training",
      "description": "Training step 1343",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:46",
      "total_flops_so_far": 1928216875938104.0,
      "budget_used_percent": 1.928216875938104
    },
    {
      "type": "training",
      "description": "Training step 1344",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:46",
      "total_flops_so_far": 1929647442975032.0,
      "budget_used_percent": 1.9296474429750319
    },
    {
      "type": "training",
      "description": "Training step 1345",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:46",
      "total_flops_so_far": 1931078010011960.0,
      "budget_used_percent": 1.93107801001196
    },
    {
      "type": "training",
      "description": "Training step 1346",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:47",
      "total_flops_so_far": 1932508577048888.0,
      "budget_used_percent": 1.932508577048888
    },
    {
      "type": "training",
      "description": "Training step 1347",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:47",
      "total_flops_so_far": 1933939144085816.0,
      "budget_used_percent": 1.933939144085816
    },
    {
      "type": "training",
      "description": "Training step 1348",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:47",
      "total_flops_so_far": 1935369711122744.0,
      "budget_used_percent": 1.935369711122744
    },
    {
      "type": "training",
      "description": "Training step 1349",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:47",
      "total_flops_so_far": 1936800278159672.0,
      "budget_used_percent": 1.936800278159672
    },
    {
      "type": "training",
      "description": "Training step 1350",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:47",
      "total_flops_so_far": 1938230845196600.0,
      "budget_used_percent": 1.9382308451966
    },
    {
      "type": "training",
      "description": "Training step 1351",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:47",
      "total_flops_so_far": 1939661412233528.0,
      "budget_used_percent": 1.939661412233528
    },
    {
      "type": "training",
      "description": "Training step 1352",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:47",
      "total_flops_so_far": 1941091979270456.0,
      "budget_used_percent": 1.941091979270456
    },
    {
      "type": "training",
      "description": "Training step 1353",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:48",
      "total_flops_so_far": 1942522546307384.0,
      "budget_used_percent": 1.942522546307384
    },
    {
      "type": "training",
      "description": "Training step 1354",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:48",
      "total_flops_so_far": 1943953113344312.0,
      "budget_used_percent": 1.943953113344312
    },
    {
      "type": "training",
      "description": "Training step 1355",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:48",
      "total_flops_so_far": 1945383680381240.0,
      "budget_used_percent": 1.94538368038124
    },
    {
      "type": "training",
      "description": "Training step 1356",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:48",
      "total_flops_so_far": 1946814247418168.0,
      "budget_used_percent": 1.946814247418168
    },
    {
      "type": "training",
      "description": "Training step 1357",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:48",
      "total_flops_so_far": 1948244814455096.0,
      "budget_used_percent": 1.9482448144550961
    },
    {
      "type": "training",
      "description": "Training step 1358",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:48",
      "total_flops_so_far": 1949675381492024.0,
      "budget_used_percent": 1.949675381492024
    },
    {
      "type": "training",
      "description": "Training step 1359",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:48",
      "total_flops_so_far": 1951105948528952.0,
      "budget_used_percent": 1.9511059485289521
    },
    {
      "type": "training",
      "description": "Training step 1360",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:49",
      "total_flops_so_far": 1952536515565880.0,
      "budget_used_percent": 1.95253651556588
    },
    {
      "type": "training",
      "description": "Training step 1361",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:49",
      "total_flops_so_far": 1953967082602808.0,
      "budget_used_percent": 1.9539670826028082
    },
    {
      "type": "training",
      "description": "Training step 1362",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:49",
      "total_flops_so_far": 1955397649639736.0,
      "budget_used_percent": 1.955397649639736
    },
    {
      "type": "training",
      "description": "Training step 1363",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:49",
      "total_flops_so_far": 1956828216676664.0,
      "budget_used_percent": 1.9568282166766642
    },
    {
      "type": "training",
      "description": "Training step 1364",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:49",
      "total_flops_so_far": 1958258783713592.0,
      "budget_used_percent": 1.958258783713592
    },
    {
      "type": "training",
      "description": "Training step 1365",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:49",
      "total_flops_so_far": 1959689350750520.0,
      "budget_used_percent": 1.9596893507505202
    },
    {
      "type": "training",
      "description": "Training step 1366",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:49",
      "total_flops_so_far": 1961119917787448.0,
      "budget_used_percent": 1.9611199177874479
    },
    {
      "type": "training",
      "description": "Training step 1367",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:50",
      "total_flops_so_far": 1962550484824376.0,
      "budget_used_percent": 1.9625504848243758
    },
    {
      "type": "training",
      "description": "Training step 1368",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:50",
      "total_flops_so_far": 1963981051861304.0,
      "budget_used_percent": 1.963981051861304
    },
    {
      "type": "training",
      "description": "Training step 1369",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:50",
      "total_flops_so_far": 1965411618898232.0,
      "budget_used_percent": 1.9654116188982318
    },
    {
      "type": "training",
      "description": "Training step 1370",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:50",
      "total_flops_so_far": 1966842185935160.0,
      "budget_used_percent": 1.96684218593516
    },
    {
      "type": "training",
      "description": "Training step 1371",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:50",
      "total_flops_so_far": 1968272752972088.0,
      "budget_used_percent": 1.9682727529720878
    },
    {
      "type": "training",
      "description": "Training step 1372",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:50",
      "total_flops_so_far": 1969703320009016.0,
      "budget_used_percent": 1.969703320009016
    },
    {
      "type": "training",
      "description": "Training step 1373",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:50",
      "total_flops_so_far": 1971133887045944.0,
      "budget_used_percent": 1.9711338870459438
    },
    {
      "type": "training",
      "description": "Training step 1374",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:51",
      "total_flops_so_far": 1972564454082872.0,
      "budget_used_percent": 1.972564454082872
    },
    {
      "type": "training",
      "description": "Training step 1375",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:51",
      "total_flops_so_far": 1973995021119800.0,
      "budget_used_percent": 1.9739950211197999
    },
    {
      "type": "training",
      "description": "Training step 1376",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:51",
      "total_flops_so_far": 1975425588156728.0,
      "budget_used_percent": 1.975425588156728
    },
    {
      "type": "training",
      "description": "Training step 1377",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:51",
      "total_flops_so_far": 1976856155193656.0,
      "budget_used_percent": 1.9768561551936559
    },
    {
      "type": "training",
      "description": "Training step 1378",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:51",
      "total_flops_so_far": 1978286722230584.0,
      "budget_used_percent": 1.978286722230584
    },
    {
      "type": "training",
      "description": "Training step 1379",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:51",
      "total_flops_so_far": 1979717289267512.0,
      "budget_used_percent": 1.979717289267512
    },
    {
      "type": "training",
      "description": "Training step 1380",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:52",
      "total_flops_so_far": 1981147856304440.0,
      "budget_used_percent": 1.98114785630444
    },
    {
      "type": "training",
      "description": "Training step 1381",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:52",
      "total_flops_so_far": 1982578423341368.0,
      "budget_used_percent": 1.982578423341368
    },
    {
      "type": "training",
      "description": "Training step 1382",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:52",
      "total_flops_so_far": 1984008990378296.0,
      "budget_used_percent": 1.984008990378296
    },
    {
      "type": "training",
      "description": "Training step 1383",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:52",
      "total_flops_so_far": 1985439557415224.0,
      "budget_used_percent": 1.985439557415224
    },
    {
      "type": "training",
      "description": "Training step 1384",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:52",
      "total_flops_so_far": 1986870124452152.0,
      "budget_used_percent": 1.986870124452152
    },
    {
      "type": "training",
      "description": "Training step 1385",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:52",
      "total_flops_so_far": 1988300691489080.0,
      "budget_used_percent": 1.98830069148908
    },
    {
      "type": "training",
      "description": "Training step 1386",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:52",
      "total_flops_so_far": 1989731258526008.0,
      "budget_used_percent": 1.989731258526008
    },
    {
      "type": "training",
      "description": "Training step 1387",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:53",
      "total_flops_so_far": 1991161825562936.0,
      "budget_used_percent": 1.991161825562936
    },
    {
      "type": "training",
      "description": "Training step 1388",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:53",
      "total_flops_so_far": 1992592392599864.0,
      "budget_used_percent": 1.992592392599864
    },
    {
      "type": "training",
      "description": "Training step 1389",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:53",
      "total_flops_so_far": 1994022959636792.0,
      "budget_used_percent": 1.994022959636792
    },
    {
      "type": "training",
      "description": "Training step 1390",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:53",
      "total_flops_so_far": 1995453526673720.0,
      "budget_used_percent": 1.9954535266737201
    },
    {
      "type": "training",
      "description": "Training step 1391",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:53",
      "total_flops_so_far": 1996884093710648.0,
      "budget_used_percent": 1.996884093710648
    },
    {
      "type": "training",
      "description": "Training step 1392",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:53",
      "total_flops_so_far": 1998314660747576.0,
      "budget_used_percent": 1.9983146607475761
    },
    {
      "type": "training",
      "description": "Training step 1393",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:53",
      "total_flops_so_far": 1999745227784504.0,
      "budget_used_percent": 1.999745227784504
    },
    {
      "type": "training",
      "description": "Training step 1394",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:54",
      "total_flops_so_far": 2001175794821432.0,
      "budget_used_percent": 2.001175794821432
    },
    {
      "type": "training",
      "description": "Training step 1395",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:54",
      "total_flops_so_far": 2002606361858360.0,
      "budget_used_percent": 2.0026063618583603
    },
    {
      "type": "training",
      "description": "Training step 1396",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:54",
      "total_flops_so_far": 2004036928895288.0,
      "budget_used_percent": 2.004036928895288
    },
    {
      "type": "training",
      "description": "Training step 1397",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:54",
      "total_flops_so_far": 2005467495932216.0,
      "budget_used_percent": 2.005467495932216
    },
    {
      "type": "training",
      "description": "Training step 1398",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:54",
      "total_flops_so_far": 2006898062969144.0,
      "budget_used_percent": 2.006898062969144
    },
    {
      "type": "training",
      "description": "Training step 1399",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:54",
      "total_flops_so_far": 2008328630006072.0,
      "budget_used_percent": 2.0083286300060723
    },
    {
      "type": "training",
      "description": "Training step 1400",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:54",
      "total_flops_so_far": 2009759197043000.0,
      "budget_used_percent": 2.009759197043
    },
    {
      "type": "training",
      "description": "Training step 1401",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:55",
      "total_flops_so_far": 2011189764079928.0,
      "budget_used_percent": 2.0111897640799277
    },
    {
      "type": "training",
      "description": "Training step 1402",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:55",
      "total_flops_so_far": 2012620331116856.0,
      "budget_used_percent": 2.012620331116856
    },
    {
      "type": "training",
      "description": "Training step 1403",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:55",
      "total_flops_so_far": 2014050898153784.0,
      "budget_used_percent": 2.014050898153784
    },
    {
      "type": "training",
      "description": "Training step 1404",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:55",
      "total_flops_so_far": 2015481465190712.0,
      "budget_used_percent": 2.015481465190712
    },
    {
      "type": "training",
      "description": "Training step 1405",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:55",
      "total_flops_so_far": 2016912032227640.0,
      "budget_used_percent": 2.0169120322276397
    },
    {
      "type": "training",
      "description": "Training step 1406",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:55",
      "total_flops_so_far": 2018342599264568.0,
      "budget_used_percent": 2.018342599264568
    },
    {
      "type": "training",
      "description": "Training step 1407",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:56",
      "total_flops_so_far": 2019773166301496.0,
      "budget_used_percent": 2.019773166301496
    },
    {
      "type": "training",
      "description": "Training step 1408",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:56",
      "total_flops_so_far": 2021203733338424.0,
      "budget_used_percent": 2.021203733338424
    },
    {
      "type": "training",
      "description": "Training step 1409",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:56",
      "total_flops_so_far": 2022634300375352.0,
      "budget_used_percent": 2.0226343003753517
    },
    {
      "type": "training",
      "description": "Training step 1410",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:56",
      "total_flops_so_far": 2024064867412280.0,
      "budget_used_percent": 2.02406486741228
    },
    {
      "type": "training",
      "description": "Training step 1411",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:56",
      "total_flops_so_far": 2025495434449208.0,
      "budget_used_percent": 2.025495434449208
    },
    {
      "type": "training",
      "description": "Training step 1412",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:56",
      "total_flops_so_far": 2026926001486136.0,
      "budget_used_percent": 2.026926001486136
    },
    {
      "type": "training",
      "description": "Training step 1413",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:56",
      "total_flops_so_far": 2028356568523064.0,
      "budget_used_percent": 2.028356568523064
    },
    {
      "type": "training",
      "description": "Training step 1414",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:57",
      "total_flops_so_far": 2029787135559992.0,
      "budget_used_percent": 2.029787135559992
    },
    {
      "type": "training",
      "description": "Training step 1415",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:57",
      "total_flops_so_far": 2031217702596920.0,
      "budget_used_percent": 2.03121770259692
    },
    {
      "type": "training",
      "description": "Training step 1416",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:57",
      "total_flops_so_far": 2032648269633848.0,
      "budget_used_percent": 2.032648269633848
    },
    {
      "type": "training",
      "description": "Training step 1417",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:57",
      "total_flops_so_far": 2034078836670776.0,
      "budget_used_percent": 2.034078836670776
    },
    {
      "type": "training",
      "description": "Training step 1418",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:57",
      "total_flops_so_far": 2035509403707704.0,
      "budget_used_percent": 2.035509403707704
    },
    {
      "type": "training",
      "description": "Training step 1419",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:57",
      "total_flops_so_far": 2036939970744632.0,
      "budget_used_percent": 2.036939970744632
    },
    {
      "type": "training",
      "description": "Training step 1420",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:57",
      "total_flops_so_far": 2038370537781560.0,
      "budget_used_percent": 2.03837053778156
    },
    {
      "type": "training",
      "description": "Training step 1421",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:58",
      "total_flops_so_far": 2039801104818488.0,
      "budget_used_percent": 2.039801104818488
    },
    {
      "type": "training",
      "description": "Training step 1422",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:58",
      "total_flops_so_far": 2041231671855416.0,
      "budget_used_percent": 2.041231671855416
    },
    {
      "type": "training",
      "description": "Training step 1423",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:58",
      "total_flops_so_far": 2042662238892344.0,
      "budget_used_percent": 2.042662238892344
    },
    {
      "type": "training",
      "description": "Training step 1424",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:58",
      "total_flops_so_far": 2044092805929272.0,
      "budget_used_percent": 2.0440928059292722
    },
    {
      "type": "training",
      "description": "Training step 1425",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:58",
      "total_flops_so_far": 2045523372966200.0,
      "budget_used_percent": 2.0455233729662
    },
    {
      "type": "training",
      "description": "Training step 1426",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:58",
      "total_flops_so_far": 2046953940003128.0,
      "budget_used_percent": 2.046953940003128
    },
    {
      "type": "training",
      "description": "Training step 1427",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:59",
      "total_flops_so_far": 2048384507040056.0,
      "budget_used_percent": 2.048384507040056
    },
    {
      "type": "training",
      "description": "Training step 1428",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:59",
      "total_flops_so_far": 2049815074076984.0,
      "budget_used_percent": 2.0498150740769843
    },
    {
      "type": "training",
      "description": "Training step 1429",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:59",
      "total_flops_so_far": 2051245641113912.0,
      "budget_used_percent": 2.051245641113912
    },
    {
      "type": "training",
      "description": "Training step 1430",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:59",
      "total_flops_so_far": 2052676208150840.0,
      "budget_used_percent": 2.05267620815084
    },
    {
      "type": "training",
      "description": "Training step 1431",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:59",
      "total_flops_so_far": 2054106775187768.0,
      "budget_used_percent": 2.054106775187768
    },
    {
      "type": "training",
      "description": "Training step 1432",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:59",
      "total_flops_so_far": 2055537342224696.0,
      "budget_used_percent": 2.0555373422246963
    },
    {
      "type": "training",
      "description": "Training step 1433",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:03:59",
      "total_flops_so_far": 2056967909261624.0,
      "budget_used_percent": 2.056967909261624
    },
    {
      "type": "training",
      "description": "Training step 1434",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:00",
      "total_flops_so_far": 2058398476298552.0,
      "budget_used_percent": 2.058398476298552
    },
    {
      "type": "training",
      "description": "Training step 1435",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:00",
      "total_flops_so_far": 2059829043335480.0,
      "budget_used_percent": 2.0598290433354802
    },
    {
      "type": "training",
      "description": "Training step 1436",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:00",
      "total_flops_so_far": 2061259610372408.0,
      "budget_used_percent": 2.061259610372408
    },
    {
      "type": "training",
      "description": "Training step 1437",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:00",
      "total_flops_so_far": 2062690177409336.0,
      "budget_used_percent": 2.062690177409336
    },
    {
      "type": "training",
      "description": "Training step 1438",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:00",
      "total_flops_so_far": 2064120744446264.0,
      "budget_used_percent": 2.0641207444462637
    },
    {
      "type": "training",
      "description": "Training step 1439",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:00",
      "total_flops_so_far": 2065551311483192.0,
      "budget_used_percent": 2.065551311483192
    },
    {
      "type": "training",
      "description": "Training step 1440",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:00",
      "total_flops_so_far": 2066981878520120.0,
      "budget_used_percent": 2.06698187852012
    },
    {
      "type": "training",
      "description": "Training step 1441",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:01",
      "total_flops_so_far": 2068412445557048.0,
      "budget_used_percent": 2.068412445557048
    },
    {
      "type": "training",
      "description": "Training step 1442",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:01",
      "total_flops_so_far": 2069843012593976.0,
      "budget_used_percent": 2.0698430125939757
    },
    {
      "type": "training",
      "description": "Training step 1443",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:01",
      "total_flops_so_far": 2071273579630904.0,
      "budget_used_percent": 2.071273579630904
    },
    {
      "type": "training",
      "description": "Training step 1444",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:01",
      "total_flops_so_far": 2072704146667832.0,
      "budget_used_percent": 2.072704146667832
    },
    {
      "type": "training",
      "description": "Training step 1445",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:01",
      "total_flops_so_far": 2074134713704760.0,
      "budget_used_percent": 2.07413471370476
    },
    {
      "type": "training",
      "description": "Training step 1446",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:01",
      "total_flops_so_far": 2075565280741688.0,
      "budget_used_percent": 2.075565280741688
    },
    {
      "type": "training",
      "description": "Training step 1447",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:02",
      "total_flops_so_far": 2076995847778616.0,
      "budget_used_percent": 2.076995847778616
    },
    {
      "type": "training",
      "description": "Training step 1448",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:02",
      "total_flops_so_far": 2078426414815544.0,
      "budget_used_percent": 2.078426414815544
    },
    {
      "type": "training",
      "description": "Training step 1449",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:02",
      "total_flops_so_far": 2079856981852472.0,
      "budget_used_percent": 2.079856981852472
    },
    {
      "type": "training",
      "description": "Training step 1450",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:02",
      "total_flops_so_far": 2081287548889400.0,
      "budget_used_percent": 2.0812875488894
    },
    {
      "type": "training",
      "description": "Training step 1451",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:02",
      "total_flops_so_far": 2082718115926328.0,
      "budget_used_percent": 2.082718115926328
    },
    {
      "type": "training",
      "description": "Training step 1452",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:02",
      "total_flops_so_far": 2084148682963256.0,
      "budget_used_percent": 2.084148682963256
    },
    {
      "type": "training",
      "description": "Training step 1453",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:02",
      "total_flops_so_far": 2085579250000184.0,
      "budget_used_percent": 2.085579250000184
    },
    {
      "type": "training",
      "description": "Training step 1454",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:03",
      "total_flops_so_far": 2087009817037112.0,
      "budget_used_percent": 2.087009817037112
    },
    {
      "type": "training",
      "description": "Training step 1455",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:03",
      "total_flops_so_far": 2088440384074040.0,
      "budget_used_percent": 2.08844038407404
    },
    {
      "type": "training",
      "description": "Training step 1456",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:03",
      "total_flops_so_far": 2089870951110968.0,
      "budget_used_percent": 2.089870951110968
    },
    {
      "type": "training",
      "description": "Training step 1457",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:03",
      "total_flops_so_far": 2091301518147896.0,
      "budget_used_percent": 2.091301518147896
    },
    {
      "type": "training",
      "description": "Training step 1458",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:03",
      "total_flops_so_far": 2092732085184824.0,
      "budget_used_percent": 2.092732085184824
    },
    {
      "type": "training",
      "description": "Training step 1459",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:03",
      "total_flops_so_far": 2094162652221752.0,
      "budget_used_percent": 2.094162652221752
    },
    {
      "type": "training",
      "description": "Training step 1460",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:03",
      "total_flops_so_far": 2095593219258680.0,
      "budget_used_percent": 2.09559321925868
    },
    {
      "type": "training",
      "description": "Training step 1461",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:04",
      "total_flops_so_far": 2097023786295608.0,
      "budget_used_percent": 2.0970237862956083
    },
    {
      "type": "training",
      "description": "Training step 1462",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:04",
      "total_flops_so_far": 2098454353332536.0,
      "budget_used_percent": 2.098454353332536
    },
    {
      "type": "training",
      "description": "Training step 1463",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:04",
      "total_flops_so_far": 2099884920369464.0,
      "budget_used_percent": 2.099884920369464
    },
    {
      "type": "training",
      "description": "Training step 1464",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:04",
      "total_flops_so_far": 2101315487406392.0,
      "budget_used_percent": 2.101315487406392
    },
    {
      "type": "training",
      "description": "Training step 1465",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:04",
      "total_flops_so_far": 2102746054443320.0,
      "budget_used_percent": 2.1027460544433203
    },
    {
      "type": "training",
      "description": "Training step 1466",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:04",
      "total_flops_so_far": 2104176621480248.0,
      "budget_used_percent": 2.104176621480248
    },
    {
      "type": "training",
      "description": "Training step 1467",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:05",
      "total_flops_so_far": 2105607188517176.0,
      "budget_used_percent": 2.105607188517176
    },
    {
      "type": "training",
      "description": "Training step 1468",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:05",
      "total_flops_so_far": 2107037755554104.0,
      "budget_used_percent": 2.107037755554104
    },
    {
      "type": "training",
      "description": "Training step 1469",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:05",
      "total_flops_so_far": 2108468322591032.0,
      "budget_used_percent": 2.1084683225910323
    },
    {
      "type": "training",
      "description": "Training step 1470",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:05",
      "total_flops_so_far": 2109898889627960.0,
      "budget_used_percent": 2.10989888962796
    },
    {
      "type": "training",
      "description": "Training step 1471",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:05",
      "total_flops_so_far": 2111329456664888.0,
      "budget_used_percent": 2.1113294566648877
    },
    {
      "type": "training",
      "description": "Training step 1472",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:05",
      "total_flops_so_far": 2112760023701816.0,
      "budget_used_percent": 2.112760023701816
    },
    {
      "type": "training",
      "description": "Training step 1473",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:05",
      "total_flops_so_far": 2114190590738744.0,
      "budget_used_percent": 2.114190590738744
    },
    {
      "type": "training",
      "description": "Training step 1474",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:06",
      "total_flops_so_far": 2115621157775672.0,
      "budget_used_percent": 2.115621157775672
    },
    {
      "type": "training",
      "description": "Training step 1475",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:06",
      "total_flops_so_far": 2117051724812600.0,
      "budget_used_percent": 2.1170517248125997
    },
    {
      "type": "training",
      "description": "Training step 1476",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:06",
      "total_flops_so_far": 2118482291849528.0,
      "budget_used_percent": 2.118482291849528
    },
    {
      "type": "training",
      "description": "Training step 1477",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:06",
      "total_flops_so_far": 2119912858886456.0,
      "budget_used_percent": 2.119912858886456
    },
    {
      "type": "training",
      "description": "Training step 1478",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:06",
      "total_flops_so_far": 2121343425923384.0,
      "budget_used_percent": 2.121343425923384
    },
    {
      "type": "training",
      "description": "Training step 1479",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:06",
      "total_flops_so_far": 2122773992960312.0,
      "budget_used_percent": 2.1227739929603118
    },
    {
      "type": "training",
      "description": "Training step 1480",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:07",
      "total_flops_so_far": 2124204559997240.0,
      "budget_used_percent": 2.12420455999724
    },
    {
      "type": "training",
      "description": "Training step 1481",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:07",
      "total_flops_so_far": 2125635127034168.0,
      "budget_used_percent": 2.125635127034168
    },
    {
      "type": "training",
      "description": "Training step 1482",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:07",
      "total_flops_so_far": 2127065694071096.0,
      "budget_used_percent": 2.127065694071096
    },
    {
      "type": "training",
      "description": "Training step 1483",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:07",
      "total_flops_so_far": 2128496261108024.0,
      "budget_used_percent": 2.128496261108024
    },
    {
      "type": "training",
      "description": "Training step 1484",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:07",
      "total_flops_so_far": 2129926828144952.0,
      "budget_used_percent": 2.129926828144952
    },
    {
      "type": "training",
      "description": "Training step 1485",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:07",
      "total_flops_so_far": 2131357395181880.0,
      "budget_used_percent": 2.13135739518188
    },
    {
      "type": "training",
      "description": "Training step 1486",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:08",
      "total_flops_so_far": 2132787962218808.0,
      "budget_used_percent": 2.132787962218808
    },
    {
      "type": "training",
      "description": "Training step 1487",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:08",
      "total_flops_so_far": 2134218529255736.0,
      "budget_used_percent": 2.134218529255736
    },
    {
      "type": "training",
      "description": "Training step 1488",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:08",
      "total_flops_so_far": 2135649096292664.0,
      "budget_used_percent": 2.135649096292664
    },
    {
      "type": "training",
      "description": "Training step 1489",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:08",
      "total_flops_so_far": 2137079663329592.0,
      "budget_used_percent": 2.137079663329592
    },
    {
      "type": "training",
      "description": "Training step 1490",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:08",
      "total_flops_so_far": 2138510230366520.0,
      "budget_used_percent": 2.13851023036652
    },
    {
      "type": "training",
      "description": "Training step 1491",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:08",
      "total_flops_so_far": 2139940797403448.0,
      "budget_used_percent": 2.139940797403448
    },
    {
      "type": "training",
      "description": "Training step 1492",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:08",
      "total_flops_so_far": 2141371364440376.0,
      "budget_used_percent": 2.141371364440376
    },
    {
      "type": "training",
      "description": "Training step 1493",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:09",
      "total_flops_so_far": 2142801931477304.0,
      "budget_used_percent": 2.142801931477304
    },
    {
      "type": "training",
      "description": "Training step 1494",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:09",
      "total_flops_so_far": 2144232498514232.0,
      "budget_used_percent": 2.1442324985142323
    },
    {
      "type": "training",
      "description": "Training step 1495",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:09",
      "total_flops_so_far": 2145663065551160.0,
      "budget_used_percent": 2.14566306555116
    },
    {
      "type": "training",
      "description": "Training step 1496",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:09",
      "total_flops_so_far": 2147093632588088.0,
      "budget_used_percent": 2.147093632588088
    },
    {
      "type": "training",
      "description": "Training step 1497",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:09",
      "total_flops_so_far": 2148524199625016.0,
      "budget_used_percent": 2.148524199625016
    },
    {
      "type": "training",
      "description": "Training step 1498",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:09",
      "total_flops_so_far": 2149954766661944.0,
      "budget_used_percent": 2.1499547666619443
    },
    {
      "type": "training",
      "description": "Training step 1499",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:09",
      "total_flops_so_far": 2151385333698872.0,
      "budget_used_percent": 2.151385333698872
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 551857894496.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:11",
      "total_flops_so_far": 2151937191593368.0,
      "budget_used_percent": 2.151937191593368
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 555458032848.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:12",
      "total_flops_so_far": 2152492649626216.0,
      "budget_used_percent": 2.152492649626216
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 553657603480.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:13",
      "total_flops_so_far": 2153046307229696.0,
      "budget_used_percent": 2.153046307229696
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 551857894496.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:15",
      "total_flops_so_far": 2153598165124192.0,
      "budget_used_percent": 2.153598165124192
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 554557728116.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:16",
      "total_flops_so_far": 2154152722852308.0,
      "budget_used_percent": 2.154152722852308
    },
    {
      "type": "training",
      "description": "Training step 1500",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:16",
      "total_flops_so_far": 2155583289889236.0,
      "budget_used_percent": 2.155583289889236
    },
    {
      "type": "training",
      "description": "Training step 1501",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:16",
      "total_flops_so_far": 2157013856926164.0,
      "budget_used_percent": 2.157013856926164
    },
    {
      "type": "training",
      "description": "Training step 1502",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:17",
      "total_flops_so_far": 2158444423963092.0,
      "budget_used_percent": 2.158444423963092
    },
    {
      "type": "training",
      "description": "Training step 1503",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:17",
      "total_flops_so_far": 2159874991000020.0,
      "budget_used_percent": 2.15987499100002
    },
    {
      "type": "training",
      "description": "Training step 1504",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:17",
      "total_flops_so_far": 2161305558036948.0,
      "budget_used_percent": 2.1613055580369482
    },
    {
      "type": "training",
      "description": "Training step 1505",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:17",
      "total_flops_so_far": 2162736125073876.0,
      "budget_used_percent": 2.162736125073876
    },
    {
      "type": "training",
      "description": "Training step 1506",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:17",
      "total_flops_so_far": 2164166692110804.0,
      "budget_used_percent": 2.164166692110804
    },
    {
      "type": "training",
      "description": "Training step 1507",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:17",
      "total_flops_so_far": 2165597259147732.0,
      "budget_used_percent": 2.165597259147732
    },
    {
      "type": "training",
      "description": "Training step 1508",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:18",
      "total_flops_so_far": 2167027826184660.0,
      "budget_used_percent": 2.1670278261846603
    },
    {
      "type": "training",
      "description": "Training step 1509",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:18",
      "total_flops_so_far": 2168458393221588.0,
      "budget_used_percent": 2.168458393221588
    },
    {
      "type": "training",
      "description": "Training step 1510",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:18",
      "total_flops_so_far": 2169888960258516.0,
      "budget_used_percent": 2.169888960258516
    },
    {
      "type": "training",
      "description": "Training step 1511",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:18",
      "total_flops_so_far": 2171319527295444.0,
      "budget_used_percent": 2.171319527295444
    },
    {
      "type": "training",
      "description": "Training step 1512",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:18",
      "total_flops_so_far": 2172750094332372.0,
      "budget_used_percent": 2.1727500943323723
    },
    {
      "type": "training",
      "description": "Training step 1513",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:18",
      "total_flops_so_far": 2174180661369300.0,
      "budget_used_percent": 2.1741806613693
    },
    {
      "type": "training",
      "description": "Training step 1514",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:18",
      "total_flops_so_far": 2175611228406228.0,
      "budget_used_percent": 2.175611228406228
    },
    {
      "type": "training",
      "description": "Training step 1515",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:19",
      "total_flops_so_far": 2177041795443156.0,
      "budget_used_percent": 2.177041795443156
    },
    {
      "type": "training",
      "description": "Training step 1516",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:19",
      "total_flops_so_far": 2178472362480084.0,
      "budget_used_percent": 2.178472362480084
    },
    {
      "type": "training",
      "description": "Training step 1517",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:19",
      "total_flops_so_far": 2179902929517012.0,
      "budget_used_percent": 2.179902929517012
    },
    {
      "type": "training",
      "description": "Training step 1518",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:19",
      "total_flops_so_far": 2181333496553940.0,
      "budget_used_percent": 2.1813334965539397
    },
    {
      "type": "training",
      "description": "Training step 1519",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:19",
      "total_flops_so_far": 2182764063590868.0,
      "budget_used_percent": 2.182764063590868
    },
    {
      "type": "training",
      "description": "Training step 1520",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:19",
      "total_flops_so_far": 2184194630627796.0,
      "budget_used_percent": 2.184194630627796
    },
    {
      "type": "training",
      "description": "Training step 1521",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:19",
      "total_flops_so_far": 2185625197664724.0,
      "budget_used_percent": 2.185625197664724
    },
    {
      "type": "training",
      "description": "Training step 1522",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:20",
      "total_flops_so_far": 2187055764701652.0,
      "budget_used_percent": 2.1870557647016517
    },
    {
      "type": "training",
      "description": "Training step 1523",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:20",
      "total_flops_so_far": 2188486331738580.0,
      "budget_used_percent": 2.18848633173858
    },
    {
      "type": "training",
      "description": "Training step 1524",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:20",
      "total_flops_so_far": 2189916898775508.0,
      "budget_used_percent": 2.189916898775508
    },
    {
      "type": "training",
      "description": "Training step 1525",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:20",
      "total_flops_so_far": 2191347465812436.0,
      "budget_used_percent": 2.191347465812436
    },
    {
      "type": "training",
      "description": "Training step 1526",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:20",
      "total_flops_so_far": 2192778032849364.0,
      "budget_used_percent": 2.192778032849364
    },
    {
      "type": "training",
      "description": "Training step 1527",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:20",
      "total_flops_so_far": 2194208599886292.0,
      "budget_used_percent": 2.194208599886292
    },
    {
      "type": "training",
      "description": "Training step 1528",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:21",
      "total_flops_so_far": 2195639166923220.0,
      "budget_used_percent": 2.19563916692322
    },
    {
      "type": "training",
      "description": "Training step 1529",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:21",
      "total_flops_so_far": 2197069733960148.0,
      "budget_used_percent": 2.197069733960148
    },
    {
      "type": "training",
      "description": "Training step 1530",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:21",
      "total_flops_so_far": 2198500300997076.0,
      "budget_used_percent": 2.198500300997076
    },
    {
      "type": "training",
      "description": "Training step 1531",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:21",
      "total_flops_so_far": 2199930868034004.0,
      "budget_used_percent": 2.199930868034004
    },
    {
      "type": "training",
      "description": "Training step 1532",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:21",
      "total_flops_so_far": 2201361435070932.0,
      "budget_used_percent": 2.201361435070932
    },
    {
      "type": "training",
      "description": "Training step 1533",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:21",
      "total_flops_so_far": 2202792002107860.0,
      "budget_used_percent": 2.20279200210786
    },
    {
      "type": "training",
      "description": "Training step 1534",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:21",
      "total_flops_so_far": 2204222569144788.0,
      "budget_used_percent": 2.204222569144788
    },
    {
      "type": "training",
      "description": "Training step 1535",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:22",
      "total_flops_so_far": 2205653136181716.0,
      "budget_used_percent": 2.205653136181716
    },
    {
      "type": "training",
      "description": "Training step 1536",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:22",
      "total_flops_so_far": 2207083703218644.0,
      "budget_used_percent": 2.207083703218644
    },
    {
      "type": "training",
      "description": "Training step 1537",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:22",
      "total_flops_so_far": 2208514270255572.0,
      "budget_used_percent": 2.2085142702555722
    },
    {
      "type": "training",
      "description": "Training step 1538",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:22",
      "total_flops_so_far": 2209944837292500.0,
      "budget_used_percent": 2.2099448372925
    },
    {
      "type": "training",
      "description": "Training step 1539",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:22",
      "total_flops_so_far": 2211375404329428.0,
      "budget_used_percent": 2.211375404329428
    },
    {
      "type": "training",
      "description": "Training step 1540",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:22",
      "total_flops_so_far": 2212805971366356.0,
      "budget_used_percent": 2.212805971366356
    },
    {
      "type": "training",
      "description": "Training step 1541",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:23",
      "total_flops_so_far": 2214236538403284.0,
      "budget_used_percent": 2.2142365384032843
    },
    {
      "type": "training",
      "description": "Training step 1542",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:23",
      "total_flops_so_far": 2215667105440212.0,
      "budget_used_percent": 2.215667105440212
    },
    {
      "type": "training",
      "description": "Training step 1543",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:23",
      "total_flops_so_far": 2217097672477140.0,
      "budget_used_percent": 2.21709767247714
    },
    {
      "type": "training",
      "description": "Training step 1544",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:23",
      "total_flops_so_far": 2218528239514068.0,
      "budget_used_percent": 2.218528239514068
    },
    {
      "type": "training",
      "description": "Training step 1545",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:23",
      "total_flops_so_far": 2219958806550996.0,
      "budget_used_percent": 2.2199588065509963
    },
    {
      "type": "training",
      "description": "Training step 1546",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:23",
      "total_flops_so_far": 2221389373587924.0,
      "budget_used_percent": 2.221389373587924
    },
    {
      "type": "training",
      "description": "Training step 1547",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:23",
      "total_flops_so_far": 2222819940624852.0,
      "budget_used_percent": 2.222819940624852
    },
    {
      "type": "training",
      "description": "Training step 1548",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:24",
      "total_flops_so_far": 2224250507661780.0,
      "budget_used_percent": 2.2242505076617802
    },
    {
      "type": "training",
      "description": "Training step 1549",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:24",
      "total_flops_so_far": 2225681074698708.0,
      "budget_used_percent": 2.2256810746987084
    },
    {
      "type": "training",
      "description": "Training step 1550",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:24",
      "total_flops_so_far": 2227111641735636.0,
      "budget_used_percent": 2.227111641735636
    },
    {
      "type": "training",
      "description": "Training step 1551",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:24",
      "total_flops_so_far": 2228542208772564.0,
      "budget_used_percent": 2.2285422087725637
    },
    {
      "type": "training",
      "description": "Training step 1552",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:24",
      "total_flops_so_far": 2229972775809492.0,
      "budget_used_percent": 2.229972775809492
    },
    {
      "type": "training",
      "description": "Training step 1553",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:24",
      "total_flops_so_far": 2231403342846420.0,
      "budget_used_percent": 2.23140334284642
    },
    {
      "type": "training",
      "description": "Training step 1554",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:24",
      "total_flops_so_far": 2232833909883348.0,
      "budget_used_percent": 2.232833909883348
    },
    {
      "type": "training",
      "description": "Training step 1555",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:25",
      "total_flops_so_far": 2234264476920276.0,
      "budget_used_percent": 2.2342644769202757
    },
    {
      "type": "training",
      "description": "Training step 1556",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:25",
      "total_flops_so_far": 2235695043957204.0,
      "budget_used_percent": 2.235695043957204
    },
    {
      "type": "training",
      "description": "Training step 1557",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:25",
      "total_flops_so_far": 2237125610994132.0,
      "budget_used_percent": 2.237125610994132
    },
    {
      "type": "training",
      "description": "Training step 1558",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:25",
      "total_flops_so_far": 2238556178031060.0,
      "budget_used_percent": 2.23855617803106
    },
    {
      "type": "training",
      "description": "Training step 1559",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:25",
      "total_flops_so_far": 2239986745067988.0,
      "budget_used_percent": 2.239986745067988
    },
    {
      "type": "training",
      "description": "Training step 1560",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:25",
      "total_flops_so_far": 2241417312104916.0,
      "budget_used_percent": 2.241417312104916
    },
    {
      "type": "training",
      "description": "Training step 1561",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:26",
      "total_flops_so_far": 2242847879141844.0,
      "budget_used_percent": 2.242847879141844
    },
    {
      "type": "training",
      "description": "Training step 1562",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:26",
      "total_flops_so_far": 2244278446178772.0,
      "budget_used_percent": 2.244278446178772
    },
    {
      "type": "training",
      "description": "Training step 1563",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:26",
      "total_flops_so_far": 2245709013215700.0,
      "budget_used_percent": 2.2457090132157
    },
    {
      "type": "training",
      "description": "Training step 1564",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:26",
      "total_flops_so_far": 2247139580252628.0,
      "budget_used_percent": 2.247139580252628
    },
    {
      "type": "training",
      "description": "Training step 1565",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:26",
      "total_flops_so_far": 2248570147289556.0,
      "budget_used_percent": 2.248570147289556
    },
    {
      "type": "training",
      "description": "Training step 1566",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:26",
      "total_flops_so_far": 2250000714326484.0,
      "budget_used_percent": 2.250000714326484
    },
    {
      "type": "training",
      "description": "Training step 1567",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:26",
      "total_flops_so_far": 2251431281363412.0,
      "budget_used_percent": 2.251431281363412
    },
    {
      "type": "training",
      "description": "Training step 1568",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:27",
      "total_flops_so_far": 2252861848400340.0,
      "budget_used_percent": 2.25286184840034
    },
    {
      "type": "training",
      "description": "Training step 1569",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:27",
      "total_flops_so_far": 2254292415437268.0,
      "budget_used_percent": 2.254292415437268
    },
    {
      "type": "training",
      "description": "Training step 1570",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:27",
      "total_flops_so_far": 2255722982474196.0,
      "budget_used_percent": 2.2557229824741962
    },
    {
      "type": "training",
      "description": "Training step 1571",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:27",
      "total_flops_so_far": 2257153549511124.0,
      "budget_used_percent": 2.257153549511124
    },
    {
      "type": "training",
      "description": "Training step 1572",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:27",
      "total_flops_so_far": 2258584116548052.0,
      "budget_used_percent": 2.258584116548052
    },
    {
      "type": "training",
      "description": "Training step 1573",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:27",
      "total_flops_so_far": 2260014683584980.0,
      "budget_used_percent": 2.26001468358498
    },
    {
      "type": "training",
      "description": "Training step 1574",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:28",
      "total_flops_so_far": 2261445250621908.0,
      "budget_used_percent": 2.2614452506219083
    },
    {
      "type": "training",
      "description": "Training step 1575",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:28",
      "total_flops_so_far": 2262875817658836.0,
      "budget_used_percent": 2.262875817658836
    },
    {
      "type": "training",
      "description": "Training step 1576",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:28",
      "total_flops_so_far": 2264306384695764.0,
      "budget_used_percent": 2.264306384695764
    },
    {
      "type": "training",
      "description": "Training step 1577",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:28",
      "total_flops_so_far": 2265736951732692.0,
      "budget_used_percent": 2.265736951732692
    },
    {
      "type": "training",
      "description": "Training step 1578",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:28",
      "total_flops_so_far": 2267167518769620.0,
      "budget_used_percent": 2.2671675187696203
    },
    {
      "type": "training",
      "description": "Training step 1579",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:28",
      "total_flops_so_far": 2268598085806548.0,
      "budget_used_percent": 2.268598085806548
    },
    {
      "type": "training",
      "description": "Training step 1580",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:28",
      "total_flops_so_far": 2270028652843476.0,
      "budget_used_percent": 2.270028652843476
    },
    {
      "type": "training",
      "description": "Training step 1581",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:29",
      "total_flops_so_far": 2271459219880404.0,
      "budget_used_percent": 2.2714592198804042
    },
    {
      "type": "training",
      "description": "Training step 1582",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:29",
      "total_flops_so_far": 2272889786917332.0,
      "budget_used_percent": 2.2728897869173323
    },
    {
      "type": "training",
      "description": "Training step 1583",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:29",
      "total_flops_so_far": 2274320353954260.0,
      "budget_used_percent": 2.27432035395426
    },
    {
      "type": "training",
      "description": "Training step 1584",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:29",
      "total_flops_so_far": 2275750920991188.0,
      "budget_used_percent": 2.275750920991188
    },
    {
      "type": "training",
      "description": "Training step 1585",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:29",
      "total_flops_so_far": 2277181488028116.0,
      "budget_used_percent": 2.277181488028116
    },
    {
      "type": "training",
      "description": "Training step 1586",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:29",
      "total_flops_so_far": 2278612055065044.0,
      "budget_used_percent": 2.278612055065044
    },
    {
      "type": "training",
      "description": "Training step 1587",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:30",
      "total_flops_so_far": 2280042622101972.0,
      "budget_used_percent": 2.280042622101972
    },
    {
      "type": "training",
      "description": "Training step 1588",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:30",
      "total_flops_so_far": 2281473189138900.0,
      "budget_used_percent": 2.2814731891388997
    },
    {
      "type": "training",
      "description": "Training step 1589",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:30",
      "total_flops_so_far": 2282903756175828.0,
      "budget_used_percent": 2.282903756175828
    },
    {
      "type": "training",
      "description": "Training step 1590",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:30",
      "total_flops_so_far": 2284334323212756.0,
      "budget_used_percent": 2.284334323212756
    },
    {
      "type": "training",
      "description": "Training step 1591",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:30",
      "total_flops_so_far": 2285764890249684.0,
      "budget_used_percent": 2.285764890249684
    },
    {
      "type": "training",
      "description": "Training step 1592",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:30",
      "total_flops_so_far": 2287195457286612.0,
      "budget_used_percent": 2.2871954572866118
    },
    {
      "type": "training",
      "description": "Training step 1593",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:30",
      "total_flops_so_far": 2288626024323540.0,
      "budget_used_percent": 2.28862602432354
    },
    {
      "type": "training",
      "description": "Training step 1594",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:31",
      "total_flops_so_far": 2290056591360468.0,
      "budget_used_percent": 2.290056591360468
    },
    {
      "type": "training",
      "description": "Training step 1595",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:31",
      "total_flops_so_far": 2291487158397396.0,
      "budget_used_percent": 2.291487158397396
    },
    {
      "type": "training",
      "description": "Training step 1596",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:31",
      "total_flops_so_far": 2292917725434324.0,
      "budget_used_percent": 2.292917725434324
    },
    {
      "type": "training",
      "description": "Training step 1597",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:31",
      "total_flops_so_far": 2294348292471252.0,
      "budget_used_percent": 2.294348292471252
    },
    {
      "type": "training",
      "description": "Training step 1598",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:31",
      "total_flops_so_far": 2295778859508180.0,
      "budget_used_percent": 2.29577885950818
    },
    {
      "type": "training",
      "description": "Training step 1599",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:31",
      "total_flops_so_far": 2297209426545108.0,
      "budget_used_percent": 2.297209426545108
    },
    {
      "type": "training",
      "description": "Training step 1600",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:32",
      "total_flops_so_far": 2298639993582036.0,
      "budget_used_percent": 2.298639993582036
    },
    {
      "type": "training",
      "description": "Training step 1601",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:32",
      "total_flops_so_far": 2300070560618964.0,
      "budget_used_percent": 2.300070560618964
    },
    {
      "type": "training",
      "description": "Training step 1602",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:32",
      "total_flops_so_far": 2301501127655892.0,
      "budget_used_percent": 2.301501127655892
    },
    {
      "type": "training",
      "description": "Training step 1603",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:32",
      "total_flops_so_far": 2302931694692820.0,
      "budget_used_percent": 2.30293169469282
    },
    {
      "type": "training",
      "description": "Training step 1604",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:32",
      "total_flops_so_far": 2304362261729748.0,
      "budget_used_percent": 2.304362261729748
    },
    {
      "type": "training",
      "description": "Training step 1605",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:32",
      "total_flops_so_far": 2305792828766676.0,
      "budget_used_percent": 2.305792828766676
    },
    {
      "type": "training",
      "description": "Training step 1606",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:32",
      "total_flops_so_far": 2307223395803604.0,
      "budget_used_percent": 2.307223395803604
    },
    {
      "type": "training",
      "description": "Training step 1607",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:33",
      "total_flops_so_far": 2308653962840532.0,
      "budget_used_percent": 2.3086539628405323
    },
    {
      "type": "training",
      "description": "Training step 1608",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:33",
      "total_flops_so_far": 2310084529877460.0,
      "budget_used_percent": 2.31008452987746
    },
    {
      "type": "training",
      "description": "Training step 1609",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:33",
      "total_flops_so_far": 2311515096914388.0,
      "budget_used_percent": 2.311515096914388
    },
    {
      "type": "training",
      "description": "Training step 1610",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:33",
      "total_flops_so_far": 2312945663951316.0,
      "budget_used_percent": 2.312945663951316
    },
    {
      "type": "training",
      "description": "Training step 1611",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:33",
      "total_flops_so_far": 2314376230988244.0,
      "budget_used_percent": 2.3143762309882443
    },
    {
      "type": "training",
      "description": "Training step 1612",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:33",
      "total_flops_so_far": 2315806798025172.0,
      "budget_used_percent": 2.315806798025172
    },
    {
      "type": "training",
      "description": "Training step 1613",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:34",
      "total_flops_so_far": 2317237365062100.0,
      "budget_used_percent": 2.3172373650621
    },
    {
      "type": "training",
      "description": "Training step 1614",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:34",
      "total_flops_so_far": 2318667932099028.0,
      "budget_used_percent": 2.318667932099028
    },
    {
      "type": "training",
      "description": "Training step 1615",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:34",
      "total_flops_so_far": 2320098499135956.0,
      "budget_used_percent": 2.3200984991359563
    },
    {
      "type": "training",
      "description": "Training step 1616",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:34",
      "total_flops_so_far": 2321529066172884.0,
      "budget_used_percent": 2.321529066172884
    },
    {
      "type": "training",
      "description": "Training step 1617",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:34",
      "total_flops_so_far": 2322959633209812.0,
      "budget_used_percent": 2.322959633209812
    },
    {
      "type": "training",
      "description": "Training step 1618",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:34",
      "total_flops_so_far": 2324390200246740.0,
      "budget_used_percent": 2.3243902002467403
    },
    {
      "type": "training",
      "description": "Training step 1619",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:34",
      "total_flops_so_far": 2325820767283668.0,
      "budget_used_percent": 2.3258207672836684
    },
    {
      "type": "training",
      "description": "Training step 1620",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:35",
      "total_flops_so_far": 2327251334320596.0,
      "budget_used_percent": 2.3272513343205956
    },
    {
      "type": "training",
      "description": "Training step 1621",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:35",
      "total_flops_so_far": 2328681901357524.0,
      "budget_used_percent": 2.3286819013575237
    },
    {
      "type": "training",
      "description": "Training step 1622",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:35",
      "total_flops_so_far": 2330112468394452.0,
      "budget_used_percent": 2.330112468394452
    },
    {
      "type": "training",
      "description": "Training step 1623",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:35",
      "total_flops_so_far": 2331543035431380.0,
      "budget_used_percent": 2.33154303543138
    },
    {
      "type": "training",
      "description": "Training step 1624",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:35",
      "total_flops_so_far": 2332973602468308.0,
      "budget_used_percent": 2.332973602468308
    },
    {
      "type": "training",
      "description": "Training step 1625",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:35",
      "total_flops_so_far": 2334404169505236.0,
      "budget_used_percent": 2.3344041695052358
    },
    {
      "type": "training",
      "description": "Training step 1626",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:36",
      "total_flops_so_far": 2335834736542164.0,
      "budget_used_percent": 2.335834736542164
    },
    {
      "type": "training",
      "description": "Training step 1627",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:36",
      "total_flops_so_far": 2337265303579092.0,
      "budget_used_percent": 2.337265303579092
    },
    {
      "type": "training",
      "description": "Training step 1628",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:36",
      "total_flops_so_far": 2338695870616020.0,
      "budget_used_percent": 2.3386958706160197
    },
    {
      "type": "training",
      "description": "Training step 1629",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:36",
      "total_flops_so_far": 2340126437652948.0,
      "budget_used_percent": 2.340126437652948
    },
    {
      "type": "training",
      "description": "Training step 1630",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:36",
      "total_flops_so_far": 2341557004689876.0,
      "budget_used_percent": 2.341557004689876
    },
    {
      "type": "training",
      "description": "Training step 1631",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:36",
      "total_flops_so_far": 2342987571726804.0,
      "budget_used_percent": 2.342987571726804
    },
    {
      "type": "training",
      "description": "Training step 1632",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:36",
      "total_flops_so_far": 2344418138763732.0,
      "budget_used_percent": 2.344418138763732
    },
    {
      "type": "training",
      "description": "Training step 1633",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:37",
      "total_flops_so_far": 2345848705800660.0,
      "budget_used_percent": 2.34584870580066
    },
    {
      "type": "training",
      "description": "Training step 1634",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:37",
      "total_flops_so_far": 2347279272837588.0,
      "budget_used_percent": 2.347279272837588
    },
    {
      "type": "training",
      "description": "Training step 1635",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:37",
      "total_flops_so_far": 2348709839874516.0,
      "budget_used_percent": 2.348709839874516
    },
    {
      "type": "training",
      "description": "Training step 1636",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:37",
      "total_flops_so_far": 2350140406911444.0,
      "budget_used_percent": 2.3501404069114438
    },
    {
      "type": "training",
      "description": "Training step 1637",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:37",
      "total_flops_so_far": 2351570973948372.0,
      "budget_used_percent": 2.351570973948372
    },
    {
      "type": "training",
      "description": "Training step 1638",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:37",
      "total_flops_so_far": 2353001540985300.0,
      "budget_used_percent": 2.3530015409853
    },
    {
      "type": "training",
      "description": "Training step 1639",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:38",
      "total_flops_so_far": 2354432108022228.0,
      "budget_used_percent": 2.354432108022228
    },
    {
      "type": "training",
      "description": "Training step 1640",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:38",
      "total_flops_so_far": 2355862675059156.0,
      "budget_used_percent": 2.3558626750591563
    },
    {
      "type": "training",
      "description": "Training step 1641",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:38",
      "total_flops_so_far": 2357293242096084.0,
      "budget_used_percent": 2.357293242096084
    },
    {
      "type": "training",
      "description": "Training step 1642",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:38",
      "total_flops_so_far": 2358723809133012.0,
      "budget_used_percent": 2.358723809133012
    },
    {
      "type": "training",
      "description": "Training step 1643",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:38",
      "total_flops_so_far": 2360154376169940.0,
      "budget_used_percent": 2.36015437616994
    },
    {
      "type": "training",
      "description": "Training step 1644",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:38",
      "total_flops_so_far": 2361584943206868.0,
      "budget_used_percent": 2.361584943206868
    },
    {
      "type": "training",
      "description": "Training step 1645",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:38",
      "total_flops_so_far": 2363015510243796.0,
      "budget_used_percent": 2.363015510243796
    },
    {
      "type": "training",
      "description": "Training step 1646",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:39",
      "total_flops_so_far": 2364446077280724.0,
      "budget_used_percent": 2.364446077280724
    },
    {
      "type": "training",
      "description": "Training step 1647",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:39",
      "total_flops_so_far": 2365876644317652.0,
      "budget_used_percent": 2.365876644317652
    },
    {
      "type": "training",
      "description": "Training step 1648",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:39",
      "total_flops_so_far": 2367307211354580.0,
      "budget_used_percent": 2.3673072113545803
    },
    {
      "type": "training",
      "description": "Training step 1649",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:39",
      "total_flops_so_far": 2368737778391508.0,
      "budget_used_percent": 2.368737778391508
    },
    {
      "type": "training",
      "description": "Training step 1650",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:39",
      "total_flops_so_far": 2370168345428436.0,
      "budget_used_percent": 2.370168345428436
    },
    {
      "type": "training",
      "description": "Training step 1651",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:39",
      "total_flops_so_far": 2371598912465364.0,
      "budget_used_percent": 2.3715989124653643
    },
    {
      "type": "training",
      "description": "Training step 1652",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:40",
      "total_flops_so_far": 2373029479502292.0,
      "budget_used_percent": 2.373029479502292
    },
    {
      "type": "training",
      "description": "Training step 1653",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:40",
      "total_flops_so_far": 2374460046539220.0,
      "budget_used_percent": 2.37446004653922
    },
    {
      "type": "training",
      "description": "Training step 1654",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:40",
      "total_flops_so_far": 2375890613576148.0,
      "budget_used_percent": 2.375890613576148
    },
    {
      "type": "training",
      "description": "Training step 1655",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:40",
      "total_flops_so_far": 2377321180613076.0,
      "budget_used_percent": 2.377321180613076
    },
    {
      "type": "training",
      "description": "Training step 1656",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:40",
      "total_flops_so_far": 2378751747650004.0,
      "budget_used_percent": 2.378751747650004
    },
    {
      "type": "training",
      "description": "Training step 1657",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:40",
      "total_flops_so_far": 2380182314686932.0,
      "budget_used_percent": 2.3801823146869316
    },
    {
      "type": "training",
      "description": "Training step 1658",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:41",
      "total_flops_so_far": 2381612881723860.0,
      "budget_used_percent": 2.3816128817238598
    },
    {
      "type": "training",
      "description": "Training step 1659",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:41",
      "total_flops_so_far": 2383043448760788.0,
      "budget_used_percent": 2.383043448760788
    },
    {
      "type": "training",
      "description": "Training step 1660",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:41",
      "total_flops_so_far": 2384474015797716.0,
      "budget_used_percent": 2.384474015797716
    },
    {
      "type": "training",
      "description": "Training step 1661",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:41",
      "total_flops_so_far": 2385904582834644.0,
      "budget_used_percent": 2.3859045828346437
    },
    {
      "type": "training",
      "description": "Training step 1662",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:41",
      "total_flops_so_far": 2387335149871572.0,
      "budget_used_percent": 2.387335149871572
    },
    {
      "type": "training",
      "description": "Training step 1663",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:41",
      "total_flops_so_far": 2388765716908500.0,
      "budget_used_percent": 2.3887657169085
    },
    {
      "type": "training",
      "description": "Training step 1664",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:42",
      "total_flops_so_far": 2390196283945428.0,
      "budget_used_percent": 2.390196283945428
    },
    {
      "type": "training",
      "description": "Training step 1665",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:42",
      "total_flops_so_far": 2391626850982356.0,
      "budget_used_percent": 2.3916268509823557
    },
    {
      "type": "training",
      "description": "Training step 1666",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:42",
      "total_flops_so_far": 2393057418019284.0,
      "budget_used_percent": 2.393057418019284
    },
    {
      "type": "training",
      "description": "Training step 1667",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:42",
      "total_flops_so_far": 2394487985056212.0,
      "budget_used_percent": 2.394487985056212
    },
    {
      "type": "training",
      "description": "Training step 1668",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:42",
      "total_flops_so_far": 2395918552093140.0,
      "budget_used_percent": 2.39591855209314
    },
    {
      "type": "training",
      "description": "Training step 1669",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:42",
      "total_flops_so_far": 2397349119130068.0,
      "budget_used_percent": 2.3973491191300678
    },
    {
      "type": "training",
      "description": "Training step 1670",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:42",
      "total_flops_so_far": 2398779686166996.0,
      "budget_used_percent": 2.398779686166996
    },
    {
      "type": "training",
      "description": "Training step 1671",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:43",
      "total_flops_so_far": 2400210253203924.0,
      "budget_used_percent": 2.400210253203924
    },
    {
      "type": "training",
      "description": "Training step 1672",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:43",
      "total_flops_so_far": 2401640820240852.0,
      "budget_used_percent": 2.401640820240852
    },
    {
      "type": "training",
      "description": "Training step 1673",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:43",
      "total_flops_so_far": 2403071387277780.0,
      "budget_used_percent": 2.40307138727778
    },
    {
      "type": "training",
      "description": "Training step 1674",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:43",
      "total_flops_so_far": 2404501954314708.0,
      "budget_used_percent": 2.404501954314708
    },
    {
      "type": "training",
      "description": "Training step 1675",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:43",
      "total_flops_so_far": 2405932521351636.0,
      "budget_used_percent": 2.405932521351636
    },
    {
      "type": "training",
      "description": "Training step 1676",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:43",
      "total_flops_so_far": 2407363088388564.0,
      "budget_used_percent": 2.407363088388564
    },
    {
      "type": "training",
      "description": "Training step 1677",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:44",
      "total_flops_so_far": 2408793655425492.0,
      "budget_used_percent": 2.408793655425492
    },
    {
      "type": "training",
      "description": "Training step 1678",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:44",
      "total_flops_so_far": 2410224222462420.0,
      "budget_used_percent": 2.41022422246242
    },
    {
      "type": "training",
      "description": "Training step 1679",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:44",
      "total_flops_so_far": 2411654789499348.0,
      "budget_used_percent": 2.411654789499348
    },
    {
      "type": "training",
      "description": "Training step 1680",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:44",
      "total_flops_so_far": 2413085356536276.0,
      "budget_used_percent": 2.413085356536276
    },
    {
      "type": "training",
      "description": "Training step 1681",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:44",
      "total_flops_so_far": 2414515923573204.0,
      "budget_used_percent": 2.414515923573204
    },
    {
      "type": "training",
      "description": "Training step 1682",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:44",
      "total_flops_so_far": 2415946490610132.0,
      "budget_used_percent": 2.415946490610132
    },
    {
      "type": "training",
      "description": "Training step 1683",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:44",
      "total_flops_so_far": 2417377057647060.0,
      "budget_used_percent": 2.41737705764706
    },
    {
      "type": "training",
      "description": "Training step 1684",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:45",
      "total_flops_so_far": 2418807624683988.0,
      "budget_used_percent": 2.4188076246839882
    },
    {
      "type": "training",
      "description": "Training step 1685",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:45",
      "total_flops_so_far": 2420238191720916.0,
      "budget_used_percent": 2.420238191720916
    },
    {
      "type": "training",
      "description": "Training step 1686",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:45",
      "total_flops_so_far": 2421668758757844.0,
      "budget_used_percent": 2.421668758757844
    },
    {
      "type": "training",
      "description": "Training step 1687",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:45",
      "total_flops_so_far": 2423099325794772.0,
      "budget_used_percent": 2.423099325794772
    },
    {
      "type": "training",
      "description": "Training step 1688",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:45",
      "total_flops_so_far": 2424529892831700.0,
      "budget_used_percent": 2.4245298928317003
    },
    {
      "type": "training",
      "description": "Training step 1689",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:45",
      "total_flops_so_far": 2425960459868628.0,
      "budget_used_percent": 2.425960459868628
    },
    {
      "type": "training",
      "description": "Training step 1690",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:46",
      "total_flops_so_far": 2427391026905556.0,
      "budget_used_percent": 2.4273910269055556
    },
    {
      "type": "training",
      "description": "Training step 1691",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:46",
      "total_flops_so_far": 2428821593942484.0,
      "budget_used_percent": 2.4288215939424838
    },
    {
      "type": "training",
      "description": "Training step 1692",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:46",
      "total_flops_so_far": 2430252160979412.0,
      "budget_used_percent": 2.430252160979412
    },
    {
      "type": "training",
      "description": "Training step 1693",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:46",
      "total_flops_so_far": 2431682728016340.0,
      "budget_used_percent": 2.43168272801634
    },
    {
      "type": "training",
      "description": "Training step 1694",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:46",
      "total_flops_so_far": 2433113295053268.0,
      "budget_used_percent": 2.4331132950532677
    },
    {
      "type": "training",
      "description": "Training step 1695",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:46",
      "total_flops_so_far": 2434543862090196.0,
      "budget_used_percent": 2.434543862090196
    },
    {
      "type": "training",
      "description": "Training step 1696",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:47",
      "total_flops_so_far": 2435974429127124.0,
      "budget_used_percent": 2.435974429127124
    },
    {
      "type": "training",
      "description": "Training step 1697",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:47",
      "total_flops_so_far": 2437404996164052.0,
      "budget_used_percent": 2.437404996164052
    },
    {
      "type": "training",
      "description": "Training step 1698",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:47",
      "total_flops_so_far": 2438835563200980.0,
      "budget_used_percent": 2.4388355632009797
    },
    {
      "type": "training",
      "description": "Training step 1699",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:47",
      "total_flops_so_far": 2440266130237908.0,
      "budget_used_percent": 2.440266130237908
    },
    {
      "type": "training",
      "description": "Training step 1700",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:47",
      "total_flops_so_far": 2441696697274836.0,
      "budget_used_percent": 2.441696697274836
    },
    {
      "type": "training",
      "description": "Training step 1701",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:47",
      "total_flops_so_far": 2443127264311764.0,
      "budget_used_percent": 2.443127264311764
    },
    {
      "type": "training",
      "description": "Training step 1702",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:47",
      "total_flops_so_far": 2444557831348692.0,
      "budget_used_percent": 2.4445578313486918
    },
    {
      "type": "training",
      "description": "Training step 1703",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:48",
      "total_flops_so_far": 2445988398385620.0,
      "budget_used_percent": 2.44598839838562
    },
    {
      "type": "training",
      "description": "Training step 1704",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:48",
      "total_flops_so_far": 2447418965422548.0,
      "budget_used_percent": 2.447418965422548
    },
    {
      "type": "training",
      "description": "Training step 1705",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:48",
      "total_flops_so_far": 2448849532459476.0,
      "budget_used_percent": 2.448849532459476
    },
    {
      "type": "training",
      "description": "Training step 1706",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:48",
      "total_flops_so_far": 2450280099496404.0,
      "budget_used_percent": 2.450280099496404
    },
    {
      "type": "training",
      "description": "Training step 1707",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:48",
      "total_flops_so_far": 2451710666533332.0,
      "budget_used_percent": 2.451710666533332
    },
    {
      "type": "training",
      "description": "Training step 1708",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:48",
      "total_flops_so_far": 2453141233570260.0,
      "budget_used_percent": 2.45314123357026
    },
    {
      "type": "training",
      "description": "Training step 1709",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:49",
      "total_flops_so_far": 2454571800607188.0,
      "budget_used_percent": 2.454571800607188
    },
    {
      "type": "training",
      "description": "Training step 1710",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:49",
      "total_flops_so_far": 2456002367644116.0,
      "budget_used_percent": 2.456002367644116
    },
    {
      "type": "training",
      "description": "Training step 1711",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:49",
      "total_flops_so_far": 2457432934681044.0,
      "budget_used_percent": 2.457432934681044
    },
    {
      "type": "training",
      "description": "Training step 1712",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:49",
      "total_flops_so_far": 2458863501717972.0,
      "budget_used_percent": 2.458863501717972
    },
    {
      "type": "training",
      "description": "Training step 1713",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:49",
      "total_flops_so_far": 2460294068754900.0,
      "budget_used_percent": 2.4602940687549
    },
    {
      "type": "training",
      "description": "Training step 1714",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:49",
      "total_flops_so_far": 2461724635791828.0,
      "budget_used_percent": 2.461724635791828
    },
    {
      "type": "training",
      "description": "Training step 1715",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:49",
      "total_flops_so_far": 2463155202828756.0,
      "budget_used_percent": 2.463155202828756
    },
    {
      "type": "training",
      "description": "Training step 1716",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:50",
      "total_flops_so_far": 2464585769865684.0,
      "budget_used_percent": 2.464585769865684
    },
    {
      "type": "training",
      "description": "Training step 1717",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:50",
      "total_flops_so_far": 2466016336902612.0,
      "budget_used_percent": 2.4660163369026122
    },
    {
      "type": "training",
      "description": "Training step 1718",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:50",
      "total_flops_so_far": 2467446903939540.0,
      "budget_used_percent": 2.46744690393954
    },
    {
      "type": "training",
      "description": "Training step 1719",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:50",
      "total_flops_so_far": 2468877470976468.0,
      "budget_used_percent": 2.468877470976468
    },
    {
      "type": "training",
      "description": "Training step 1720",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:50",
      "total_flops_so_far": 2470308038013396.0,
      "budget_used_percent": 2.470308038013396
    },
    {
      "type": "training",
      "description": "Training step 1721",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:50",
      "total_flops_so_far": 2471738605050324.0,
      "budget_used_percent": 2.4717386050503243
    },
    {
      "type": "training",
      "description": "Training step 1722",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:51",
      "total_flops_so_far": 2473169172087252.0,
      "budget_used_percent": 2.473169172087252
    },
    {
      "type": "training",
      "description": "Training step 1723",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:51",
      "total_flops_so_far": 2474599739124180.0,
      "budget_used_percent": 2.47459973912418
    },
    {
      "type": "training",
      "description": "Training step 1724",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:51",
      "total_flops_so_far": 2476030306161108.0,
      "budget_used_percent": 2.476030306161108
    },
    {
      "type": "training",
      "description": "Training step 1725",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:51",
      "total_flops_so_far": 2477460873198036.0,
      "budget_used_percent": 2.477460873198036
    },
    {
      "type": "training",
      "description": "Training step 1726",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:51",
      "total_flops_so_far": 2478891440234964.0,
      "budget_used_percent": 2.478891440234964
    },
    {
      "type": "training",
      "description": "Training step 1727",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:51",
      "total_flops_so_far": 2480322007271892.0,
      "budget_used_percent": 2.4803220072718917
    },
    {
      "type": "training",
      "description": "Training step 1728",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:52",
      "total_flops_so_far": 2481752574308820.0,
      "budget_used_percent": 2.48175257430882
    },
    {
      "type": "training",
      "description": "Training step 1729",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:52",
      "total_flops_so_far": 2483183141345748.0,
      "budget_used_percent": 2.483183141345748
    },
    {
      "type": "training",
      "description": "Training step 1730",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:52",
      "total_flops_so_far": 2484613708382676.0,
      "budget_used_percent": 2.484613708382676
    },
    {
      "type": "training",
      "description": "Training step 1731",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:52",
      "total_flops_so_far": 2486044275419604.0,
      "budget_used_percent": 2.4860442754196037
    },
    {
      "type": "training",
      "description": "Training step 1732",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:52",
      "total_flops_so_far": 2487474842456532.0,
      "budget_used_percent": 2.487474842456532
    },
    {
      "type": "training",
      "description": "Training step 1733",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:52",
      "total_flops_so_far": 2488905409493460.0,
      "budget_used_percent": 2.48890540949346
    },
    {
      "type": "training",
      "description": "Training step 1734",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:52",
      "total_flops_so_far": 2490335976530388.0,
      "budget_used_percent": 2.490335976530388
    },
    {
      "type": "training",
      "description": "Training step 1735",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:53",
      "total_flops_so_far": 2491766543567316.0,
      "budget_used_percent": 2.4917665435673158
    },
    {
      "type": "training",
      "description": "Training step 1736",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:53",
      "total_flops_so_far": 2493197110604244.0,
      "budget_used_percent": 2.493197110604244
    },
    {
      "type": "training",
      "description": "Training step 1737",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:53",
      "total_flops_so_far": 2494627677641172.0,
      "budget_used_percent": 2.494627677641172
    },
    {
      "type": "training",
      "description": "Training step 1738",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:53",
      "total_flops_so_far": 2496058244678100.0,
      "budget_used_percent": 2.4960582446781
    },
    {
      "type": "training",
      "description": "Training step 1739",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:53",
      "total_flops_so_far": 2497488811715028.0,
      "budget_used_percent": 2.497488811715028
    },
    {
      "type": "training",
      "description": "Training step 1740",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:53",
      "total_flops_so_far": 2498919378751956.0,
      "budget_used_percent": 2.498919378751956
    },
    {
      "type": "training",
      "description": "Training step 1741",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:54",
      "total_flops_so_far": 2500349945788884.0,
      "budget_used_percent": 2.500349945788884
    },
    {
      "type": "training",
      "description": "Training step 1742",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:54",
      "total_flops_so_far": 2501780512825812.0,
      "budget_used_percent": 2.501780512825812
    },
    {
      "type": "training",
      "description": "Training step 1743",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:54",
      "total_flops_so_far": 2503211079862740.0,
      "budget_used_percent": 2.50321107986274
    },
    {
      "type": "training",
      "description": "Training step 1744",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:54",
      "total_flops_so_far": 2504641646899668.0,
      "budget_used_percent": 2.504641646899668
    },
    {
      "type": "training",
      "description": "Training step 1745",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:54",
      "total_flops_so_far": 2506072213936596.0,
      "budget_used_percent": 2.506072213936596
    },
    {
      "type": "training",
      "description": "Training step 1746",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:54",
      "total_flops_so_far": 2507502780973524.0,
      "budget_used_percent": 2.507502780973524
    },
    {
      "type": "training",
      "description": "Training step 1747",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:54",
      "total_flops_so_far": 2508933348010452.0,
      "budget_used_percent": 2.508933348010452
    },
    {
      "type": "training",
      "description": "Training step 1748",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:55",
      "total_flops_so_far": 2510363915047380.0,
      "budget_used_percent": 2.51036391504738
    },
    {
      "type": "training",
      "description": "Training step 1749",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:55",
      "total_flops_so_far": 2511794482084308.0,
      "budget_used_percent": 2.511794482084308
    },
    {
      "type": "training",
      "description": "Training step 1750",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:55",
      "total_flops_so_far": 2513225049121236.0,
      "budget_used_percent": 2.5132250491212362
    },
    {
      "type": "training",
      "description": "Training step 1751",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:55",
      "total_flops_so_far": 2514655616158164.0,
      "budget_used_percent": 2.514655616158164
    },
    {
      "type": "training",
      "description": "Training step 1752",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:55",
      "total_flops_so_far": 2516086183195092.0,
      "budget_used_percent": 2.516086183195092
    },
    {
      "type": "training",
      "description": "Training step 1753",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:55",
      "total_flops_so_far": 2517516750232020.0,
      "budget_used_percent": 2.51751675023202
    },
    {
      "type": "training",
      "description": "Training step 1754",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:56",
      "total_flops_so_far": 2518947317268948.0,
      "budget_used_percent": 2.5189473172689483
    },
    {
      "type": "training",
      "description": "Training step 1755",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:56",
      "total_flops_so_far": 2520377884305876.0,
      "budget_used_percent": 2.520377884305876
    },
    {
      "type": "training",
      "description": "Training step 1756",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:56",
      "total_flops_so_far": 2521808451342804.0,
      "budget_used_percent": 2.521808451342804
    },
    {
      "type": "training",
      "description": "Training step 1757",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:56",
      "total_flops_so_far": 2523239018379732.0,
      "budget_used_percent": 2.523239018379732
    },
    {
      "type": "training",
      "description": "Training step 1758",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:56",
      "total_flops_so_far": 2524669585416660.0,
      "budget_used_percent": 2.5246695854166603
    },
    {
      "type": "training",
      "description": "Training step 1759",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:56",
      "total_flops_so_far": 2526100152453588.0,
      "budget_used_percent": 2.526100152453588
    },
    {
      "type": "training",
      "description": "Training step 1760",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:57",
      "total_flops_so_far": 2527530719490516.0,
      "budget_used_percent": 2.5275307194905157
    },
    {
      "type": "training",
      "description": "Training step 1761",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:57",
      "total_flops_so_far": 2528961286527444.0,
      "budget_used_percent": 2.528961286527444
    },
    {
      "type": "training",
      "description": "Training step 1762",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:57",
      "total_flops_so_far": 2530391853564372.0,
      "budget_used_percent": 2.530391853564372
    },
    {
      "type": "training",
      "description": "Training step 1763",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:57",
      "total_flops_so_far": 2531822420601300.0,
      "budget_used_percent": 2.5318224206013
    },
    {
      "type": "training",
      "description": "Training step 1764",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:57",
      "total_flops_so_far": 2533252987638228.0,
      "budget_used_percent": 2.5332529876382277
    },
    {
      "type": "training",
      "description": "Training step 1765",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:57",
      "total_flops_so_far": 2534683554675156.0,
      "budget_used_percent": 2.534683554675156
    },
    {
      "type": "training",
      "description": "Training step 1766",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:57",
      "total_flops_so_far": 2536114121712084.0,
      "budget_used_percent": 2.536114121712084
    },
    {
      "type": "training",
      "description": "Training step 1767",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:58",
      "total_flops_so_far": 2537544688749012.0,
      "budget_used_percent": 2.537544688749012
    },
    {
      "type": "training",
      "description": "Training step 1768",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:58",
      "total_flops_so_far": 2538975255785940.0,
      "budget_used_percent": 2.5389752557859397
    },
    {
      "type": "training",
      "description": "Training step 1769",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:58",
      "total_flops_so_far": 2540405822822868.0,
      "budget_used_percent": 2.540405822822868
    },
    {
      "type": "training",
      "description": "Training step 1770",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:58",
      "total_flops_so_far": 2541836389859796.0,
      "budget_used_percent": 2.541836389859796
    },
    {
      "type": "training",
      "description": "Training step 1771",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:58",
      "total_flops_so_far": 2543266956896724.0,
      "budget_used_percent": 2.543266956896724
    },
    {
      "type": "training",
      "description": "Training step 1772",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:58",
      "total_flops_so_far": 2544697523933652.0,
      "budget_used_percent": 2.544697523933652
    },
    {
      "type": "training",
      "description": "Training step 1773",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:59",
      "total_flops_so_far": 2546128090970580.0,
      "budget_used_percent": 2.54612809097058
    },
    {
      "type": "training",
      "description": "Training step 1774",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:59",
      "total_flops_so_far": 2547558658007508.0,
      "budget_used_percent": 2.547558658007508
    },
    {
      "type": "training",
      "description": "Training step 1775",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:59",
      "total_flops_so_far": 2548989225044436.0,
      "budget_used_percent": 2.548989225044436
    },
    {
      "type": "training",
      "description": "Training step 1776",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:59",
      "total_flops_so_far": 2550419792081364.0,
      "budget_used_percent": 2.550419792081364
    },
    {
      "type": "training",
      "description": "Training step 1777",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:59",
      "total_flops_so_far": 2551850359118292.0,
      "budget_used_percent": 2.551850359118292
    },
    {
      "type": "training",
      "description": "Training step 1778",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:04:59",
      "total_flops_so_far": 2553280926155220.0,
      "budget_used_percent": 2.55328092615522
    },
    {
      "type": "training",
      "description": "Training step 1779",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:00",
      "total_flops_so_far": 2554711493192148.0,
      "budget_used_percent": 2.554711493192148
    },
    {
      "type": "training",
      "description": "Training step 1780",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:00",
      "total_flops_so_far": 2556142060229076.0,
      "budget_used_percent": 2.556142060229076
    },
    {
      "type": "training",
      "description": "Training step 1781",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:00",
      "total_flops_so_far": 2557572627266004.0,
      "budget_used_percent": 2.557572627266004
    },
    {
      "type": "training",
      "description": "Training step 1782",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:00",
      "total_flops_so_far": 2559003194302932.0,
      "budget_used_percent": 2.559003194302932
    },
    {
      "type": "training",
      "description": "Training step 1783",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:00",
      "total_flops_so_far": 2560433761339860.0,
      "budget_used_percent": 2.5604337613398602
    },
    {
      "type": "training",
      "description": "Training step 1784",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:00",
      "total_flops_so_far": 2561864328376788.0,
      "budget_used_percent": 2.561864328376788
    },
    {
      "type": "training",
      "description": "Training step 1785",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:00",
      "total_flops_so_far": 2563294895413716.0,
      "budget_used_percent": 2.563294895413716
    },
    {
      "type": "training",
      "description": "Training step 1786",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:01",
      "total_flops_so_far": 2564725462450644.0,
      "budget_used_percent": 2.564725462450644
    },
    {
      "type": "training",
      "description": "Training step 1787",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:01",
      "total_flops_so_far": 2566156029487572.0,
      "budget_used_percent": 2.5661560294875723
    },
    {
      "type": "training",
      "description": "Training step 1788",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:01",
      "total_flops_so_far": 2567586596524500.0,
      "budget_used_percent": 2.5675865965245
    },
    {
      "type": "training",
      "description": "Training step 1789",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:01",
      "total_flops_so_far": 2569017163561428.0,
      "budget_used_percent": 2.569017163561428
    },
    {
      "type": "training",
      "description": "Training step 1790",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:01",
      "total_flops_so_far": 2570447730598356.0,
      "budget_used_percent": 2.570447730598356
    },
    {
      "type": "training",
      "description": "Training step 1791",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:02",
      "total_flops_so_far": 2571878297635284.0,
      "budget_used_percent": 2.5718782976352843
    },
    {
      "type": "training",
      "description": "Training step 1792",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:02",
      "total_flops_so_far": 2573308864672212.0,
      "budget_used_percent": 2.573308864672212
    },
    {
      "type": "training",
      "description": "Training step 1793",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:02",
      "total_flops_so_far": 2574739431709140.0,
      "budget_used_percent": 2.57473943170914
    },
    {
      "type": "training",
      "description": "Training step 1794",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:02",
      "total_flops_so_far": 2576169998746068.0,
      "budget_used_percent": 2.5761699987460682
    },
    {
      "type": "training",
      "description": "Training step 1795",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:02",
      "total_flops_so_far": 2577600565782996.0,
      "budget_used_percent": 2.577600565782996
    },
    {
      "type": "training",
      "description": "Training step 1796",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:02",
      "total_flops_so_far": 2579031132819924.0,
      "budget_used_percent": 2.579031132819924
    },
    {
      "type": "training",
      "description": "Training step 1797",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:03",
      "total_flops_so_far": 2580461699856852.0,
      "budget_used_percent": 2.5804616998568517
    },
    {
      "type": "training",
      "description": "Training step 1798",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:03",
      "total_flops_so_far": 2581892266893780.0,
      "budget_used_percent": 2.58189226689378
    },
    {
      "type": "training",
      "description": "Training step 1799",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:03",
      "total_flops_so_far": 2583322833930708.0,
      "budget_used_percent": 2.583322833930708
    },
    {
      "type": "training",
      "description": "Training step 1800",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:03",
      "total_flops_so_far": 2584753400967636.0,
      "budget_used_percent": 2.584753400967636
    },
    {
      "type": "training",
      "description": "Training step 1801",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:03",
      "total_flops_so_far": 2586183968004564.0,
      "budget_used_percent": 2.5861839680045637
    },
    {
      "type": "training",
      "description": "Training step 1802",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:03",
      "total_flops_so_far": 2587614535041492.0,
      "budget_used_percent": 2.587614535041492
    },
    {
      "type": "training",
      "description": "Training step 1803",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:03",
      "total_flops_so_far": 2589045102078420.0,
      "budget_used_percent": 2.58904510207842
    },
    {
      "type": "training",
      "description": "Training step 1804",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:04",
      "total_flops_so_far": 2590475669115348.0,
      "budget_used_percent": 2.590475669115348
    },
    {
      "type": "training",
      "description": "Training step 1805",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:04",
      "total_flops_so_far": 2591906236152276.0,
      "budget_used_percent": 2.5919062361522758
    },
    {
      "type": "training",
      "description": "Training step 1806",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:04",
      "total_flops_so_far": 2593336803189204.0,
      "budget_used_percent": 2.593336803189204
    },
    {
      "type": "training",
      "description": "Training step 1807",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:04",
      "total_flops_so_far": 2594767370226132.0,
      "budget_used_percent": 2.594767370226132
    },
    {
      "type": "training",
      "description": "Training step 1808",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:04",
      "total_flops_so_far": 2596197937263060.0,
      "budget_used_percent": 2.59619793726306
    },
    {
      "type": "training",
      "description": "Training step 1809",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:04",
      "total_flops_so_far": 2597628504299988.0,
      "budget_used_percent": 2.597628504299988
    },
    {
      "type": "training",
      "description": "Training step 1810",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:05",
      "total_flops_so_far": 2599059071336916.0,
      "budget_used_percent": 2.599059071336916
    },
    {
      "type": "training",
      "description": "Training step 1811",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:05",
      "total_flops_so_far": 2600489638373844.0,
      "budget_used_percent": 2.600489638373844
    },
    {
      "type": "training",
      "description": "Training step 1812",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:05",
      "total_flops_so_far": 2601920205410772.0,
      "budget_used_percent": 2.601920205410772
    },
    {
      "type": "training",
      "description": "Training step 1813",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:05",
      "total_flops_so_far": 2603350772447700.0,
      "budget_used_percent": 2.6033507724477
    },
    {
      "type": "training",
      "description": "Training step 1814",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:05",
      "total_flops_so_far": 2604781339484628.0,
      "budget_used_percent": 2.604781339484628
    },
    {
      "type": "training",
      "description": "Training step 1815",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:05",
      "total_flops_so_far": 2606211906521556.0,
      "budget_used_percent": 2.606211906521556
    },
    {
      "type": "training",
      "description": "Training step 1816",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:06",
      "total_flops_so_far": 2607642473558484.0,
      "budget_used_percent": 2.607642473558484
    },
    {
      "type": "training",
      "description": "Training step 1817",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:06",
      "total_flops_so_far": 2609073040595412.0,
      "budget_used_percent": 2.609073040595412
    },
    {
      "type": "training",
      "description": "Training step 1818",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:06",
      "total_flops_so_far": 2610503607632340.0,
      "budget_used_percent": 2.61050360763234
    },
    {
      "type": "training",
      "description": "Training step 1819",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:06",
      "total_flops_so_far": 2611934174669268.0,
      "budget_used_percent": 2.611934174669268
    },
    {
      "type": "training",
      "description": "Training step 1820",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:06",
      "total_flops_so_far": 2613364741706196.0,
      "budget_used_percent": 2.6133647417061963
    },
    {
      "type": "training",
      "description": "Training step 1821",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:06",
      "total_flops_so_far": 2614795308743124.0,
      "budget_used_percent": 2.614795308743124
    },
    {
      "type": "training",
      "description": "Training step 1822",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:07",
      "total_flops_so_far": 2616225875780052.0,
      "budget_used_percent": 2.616225875780052
    },
    {
      "type": "training",
      "description": "Training step 1823",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:07",
      "total_flops_so_far": 2617656442816980.0,
      "budget_used_percent": 2.61765644281698
    },
    {
      "type": "training",
      "description": "Training step 1824",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:07",
      "total_flops_so_far": 2619087009853908.0,
      "budget_used_percent": 2.6190870098539083
    },
    {
      "type": "training",
      "description": "Training step 1825",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:07",
      "total_flops_so_far": 2620517576890836.0,
      "budget_used_percent": 2.620517576890836
    },
    {
      "type": "training",
      "description": "Training step 1826",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:07",
      "total_flops_so_far": 2621948143927764.0,
      "budget_used_percent": 2.621948143927764
    },
    {
      "type": "training",
      "description": "Training step 1827",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:07",
      "total_flops_so_far": 2623378710964692.0,
      "budget_used_percent": 2.623378710964692
    },
    {
      "type": "training",
      "description": "Training step 1828",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:07",
      "total_flops_so_far": 2624809278001620.0,
      "budget_used_percent": 2.6248092780016203
    },
    {
      "type": "training",
      "description": "Training step 1829",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:08",
      "total_flops_so_far": 2626239845038548.0,
      "budget_used_percent": 2.626239845038548
    },
    {
      "type": "training",
      "description": "Training step 1830",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:08",
      "total_flops_so_far": 2627670412075476.0,
      "budget_used_percent": 2.6276704120754757
    },
    {
      "type": "training",
      "description": "Training step 1831",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:08",
      "total_flops_so_far": 2629100979112404.0,
      "budget_used_percent": 2.629100979112404
    },
    {
      "type": "training",
      "description": "Training step 1832",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:08",
      "total_flops_so_far": 2630531546149332.0,
      "budget_used_percent": 2.630531546149332
    },
    {
      "type": "training",
      "description": "Training step 1833",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:08",
      "total_flops_so_far": 2631962113186260.0,
      "budget_used_percent": 2.63196211318626
    },
    {
      "type": "training",
      "description": "Training step 1834",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:08",
      "total_flops_so_far": 2633392680223188.0,
      "budget_used_percent": 2.6333926802231877
    },
    {
      "type": "training",
      "description": "Training step 1835",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:09",
      "total_flops_so_far": 2634823247260116.0,
      "budget_used_percent": 2.634823247260116
    },
    {
      "type": "training",
      "description": "Training step 1836",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:09",
      "total_flops_so_far": 2636253814297044.0,
      "budget_used_percent": 2.636253814297044
    },
    {
      "type": "training",
      "description": "Training step 1837",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:09",
      "total_flops_so_far": 2637684381333972.0,
      "budget_used_percent": 2.637684381333972
    },
    {
      "type": "training",
      "description": "Training step 1838",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:09",
      "total_flops_so_far": 2639114948370900.0,
      "budget_used_percent": 2.6391149483708998
    },
    {
      "type": "training",
      "description": "Training step 1839",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:09",
      "total_flops_so_far": 2640545515407828.0,
      "budget_used_percent": 2.640545515407828
    },
    {
      "type": "training",
      "description": "Training step 1840",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:09",
      "total_flops_so_far": 2641976082444756.0,
      "budget_used_percent": 2.641976082444756
    },
    {
      "type": "training",
      "description": "Training step 1841",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:10",
      "total_flops_so_far": 2643406649481684.0,
      "budget_used_percent": 2.643406649481684
    },
    {
      "type": "training",
      "description": "Training step 1842",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:10",
      "total_flops_so_far": 2644837216518612.0,
      "budget_used_percent": 2.644837216518612
    },
    {
      "type": "training",
      "description": "Training step 1843",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:10",
      "total_flops_so_far": 2646267783555540.0,
      "budget_used_percent": 2.64626778355554
    },
    {
      "type": "training",
      "description": "Training step 1844",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:10",
      "total_flops_so_far": 2647698350592468.0,
      "budget_used_percent": 2.647698350592468
    },
    {
      "type": "training",
      "description": "Training step 1845",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:10",
      "total_flops_so_far": 2649128917629396.0,
      "budget_used_percent": 2.649128917629396
    },
    {
      "type": "training",
      "description": "Training step 1846",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:10",
      "total_flops_so_far": 2650559484666324.0,
      "budget_used_percent": 2.650559484666324
    },
    {
      "type": "training",
      "description": "Training step 1847",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:10",
      "total_flops_so_far": 2651990051703252.0,
      "budget_used_percent": 2.651990051703252
    },
    {
      "type": "training",
      "description": "Training step 1848",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:11",
      "total_flops_so_far": 2653420618740180.0,
      "budget_used_percent": 2.65342061874018
    },
    {
      "type": "training",
      "description": "Training step 1849",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:11",
      "total_flops_so_far": 2654851185777108.0,
      "budget_used_percent": 2.654851185777108
    },
    {
      "type": "training",
      "description": "Training step 1850",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:11",
      "total_flops_so_far": 2656281752814036.0,
      "budget_used_percent": 2.656281752814036
    },
    {
      "type": "training",
      "description": "Training step 1851",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:12",
      "total_flops_so_far": 2657712319850964.0,
      "budget_used_percent": 2.657712319850964
    },
    {
      "type": "training",
      "description": "Training step 1852",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:12",
      "total_flops_so_far": 2659142886887892.0,
      "budget_used_percent": 2.659142886887892
    },
    {
      "type": "training",
      "description": "Training step 1853",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:12",
      "total_flops_so_far": 2660573453924820.0,
      "budget_used_percent": 2.6605734539248203
    },
    {
      "type": "training",
      "description": "Training step 1854",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:12",
      "total_flops_so_far": 2662004020961748.0,
      "budget_used_percent": 2.662004020961748
    },
    {
      "type": "training",
      "description": "Training step 1855",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:12",
      "total_flops_so_far": 2663434587998676.0,
      "budget_used_percent": 2.663434587998676
    },
    {
      "type": "training",
      "description": "Training step 1856",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:12",
      "total_flops_so_far": 2664865155035604.0,
      "budget_used_percent": 2.664865155035604
    },
    {
      "type": "training",
      "description": "Training step 1857",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:13",
      "total_flops_so_far": 2666295722072532.0,
      "budget_used_percent": 2.6662957220725323
    },
    {
      "type": "training",
      "description": "Training step 1858",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:13",
      "total_flops_so_far": 2667726289109460.0,
      "budget_used_percent": 2.66772628910946
    },
    {
      "type": "training",
      "description": "Training step 1859",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:13",
      "total_flops_so_far": 2669156856146388.0,
      "budget_used_percent": 2.669156856146388
    },
    {
      "type": "training",
      "description": "Training step 1860",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:13",
      "total_flops_so_far": 2670587423183316.0,
      "budget_used_percent": 2.670587423183316
    },
    {
      "type": "training",
      "description": "Training step 1861",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:13",
      "total_flops_so_far": 2672017990220244.0,
      "budget_used_percent": 2.6720179902202443
    },
    {
      "type": "training",
      "description": "Training step 1862",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:13",
      "total_flops_so_far": 2673448557257172.0,
      "budget_used_percent": 2.673448557257172
    },
    {
      "type": "training",
      "description": "Training step 1863",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:13",
      "total_flops_so_far": 2674879124294100.0,
      "budget_used_percent": 2.6748791242941
    },
    {
      "type": "training",
      "description": "Training step 1864",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:14",
      "total_flops_so_far": 2676309691331028.0,
      "budget_used_percent": 2.6763096913310283
    },
    {
      "type": "training",
      "description": "Training step 1865",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:14",
      "total_flops_so_far": 2677740258367956.0,
      "budget_used_percent": 2.677740258367956
    },
    {
      "type": "training",
      "description": "Training step 1866",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:14",
      "total_flops_so_far": 2679170825404884.0,
      "budget_used_percent": 2.679170825404884
    },
    {
      "type": "training",
      "description": "Training step 1867",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:14",
      "total_flops_so_far": 2680601392441812.0,
      "budget_used_percent": 2.6806013924418117
    },
    {
      "type": "training",
      "description": "Training step 1868",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:14",
      "total_flops_so_far": 2682031959478740.0,
      "budget_used_percent": 2.68203195947874
    },
    {
      "type": "training",
      "description": "Training step 1869",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:14",
      "total_flops_so_far": 2683462526515668.0,
      "budget_used_percent": 2.683462526515668
    },
    {
      "type": "training",
      "description": "Training step 1870",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:15",
      "total_flops_so_far": 2684893093552596.0,
      "budget_used_percent": 2.684893093552596
    },
    {
      "type": "training",
      "description": "Training step 1871",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:15",
      "total_flops_so_far": 2686323660589524.0,
      "budget_used_percent": 2.6863236605895238
    },
    {
      "type": "training",
      "description": "Training step 1872",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:15",
      "total_flops_so_far": 2687754227626452.0,
      "budget_used_percent": 2.687754227626452
    },
    {
      "type": "training",
      "description": "Training step 1873",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:15",
      "total_flops_so_far": 2689184794663380.0,
      "budget_used_percent": 2.68918479466338
    },
    {
      "type": "training",
      "description": "Training step 1874",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:15",
      "total_flops_so_far": 2690615361700308.0,
      "budget_used_percent": 2.690615361700308
    },
    {
      "type": "training",
      "description": "Training step 1875",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:15",
      "total_flops_so_far": 2692045928737236.0,
      "budget_used_percent": 2.692045928737236
    },
    {
      "type": "training",
      "description": "Training step 1876",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:16",
      "total_flops_so_far": 2693476495774164.0,
      "budget_used_percent": 2.693476495774164
    },
    {
      "type": "training",
      "description": "Training step 1877",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:16",
      "total_flops_so_far": 2694907062811092.0,
      "budget_used_percent": 2.694907062811092
    },
    {
      "type": "training",
      "description": "Training step 1878",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:16",
      "total_flops_so_far": 2696337629848020.0,
      "budget_used_percent": 2.69633762984802
    },
    {
      "type": "training",
      "description": "Training step 1879",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:16",
      "total_flops_so_far": 2697768196884948.0,
      "budget_used_percent": 2.697768196884948
    },
    {
      "type": "training",
      "description": "Training step 1880",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:16",
      "total_flops_so_far": 2699198763921876.0,
      "budget_used_percent": 2.699198763921876
    },
    {
      "type": "training",
      "description": "Training step 1881",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:16",
      "total_flops_so_far": 2700629330958804.0,
      "budget_used_percent": 2.700629330958804
    },
    {
      "type": "training",
      "description": "Training step 1882",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:17",
      "total_flops_so_far": 2702059897995732.0,
      "budget_used_percent": 2.702059897995732
    },
    {
      "type": "training",
      "description": "Training step 1883",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:17",
      "total_flops_so_far": 2703490465032660.0,
      "budget_used_percent": 2.70349046503266
    },
    {
      "type": "training",
      "description": "Training step 1884",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:17",
      "total_flops_so_far": 2704921032069588.0,
      "budget_used_percent": 2.704921032069588
    },
    {
      "type": "training",
      "description": "Training step 1885",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:17",
      "total_flops_so_far": 2706351599106516.0,
      "budget_used_percent": 2.706351599106516
    },
    {
      "type": "training",
      "description": "Training step 1886",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:17",
      "total_flops_so_far": 2707782166143444.0,
      "budget_used_percent": 2.7077821661434442
    },
    {
      "type": "training",
      "description": "Training step 1887",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:17",
      "total_flops_so_far": 2709212733180372.0,
      "budget_used_percent": 2.709212733180372
    },
    {
      "type": "training",
      "description": "Training step 1888",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:17",
      "total_flops_so_far": 2710643300217300.0,
      "budget_used_percent": 2.7106433002173
    },
    {
      "type": "training",
      "description": "Training step 1889",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:18",
      "total_flops_so_far": 2712073867254228.0,
      "budget_used_percent": 2.712073867254228
    },
    {
      "type": "training",
      "description": "Training step 1890",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:18",
      "total_flops_so_far": 2713504434291156.0,
      "budget_used_percent": 2.7135044342911563
    },
    {
      "type": "training",
      "description": "Training step 1891",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:18",
      "total_flops_so_far": 2714935001328084.0,
      "budget_used_percent": 2.714935001328084
    },
    {
      "type": "training",
      "description": "Training step 1892",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:18",
      "total_flops_so_far": 2716365568365012.0,
      "budget_used_percent": 2.716365568365012
    },
    {
      "type": "training",
      "description": "Training step 1893",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:18",
      "total_flops_so_far": 2717796135401940.0,
      "budget_used_percent": 2.71779613540194
    },
    {
      "type": "training",
      "description": "Training step 1894",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:18",
      "total_flops_so_far": 2719226702438868.0,
      "budget_used_percent": 2.7192267024388683
    },
    {
      "type": "training",
      "description": "Training step 1895",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:19",
      "total_flops_so_far": 2720657269475796.0,
      "budget_used_percent": 2.720657269475796
    },
    {
      "type": "training",
      "description": "Training step 1896",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:19",
      "total_flops_so_far": 2722087836512724.0,
      "budget_used_percent": 2.722087836512724
    },
    {
      "type": "training",
      "description": "Training step 1897",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:19",
      "total_flops_so_far": 2723518403549652.0,
      "budget_used_percent": 2.7235184035496522
    },
    {
      "type": "training",
      "description": "Training step 1898",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:19",
      "total_flops_so_far": 2724948970586580.0,
      "budget_used_percent": 2.7249489705865804
    },
    {
      "type": "training",
      "description": "Training step 1899",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:19",
      "total_flops_so_far": 2726379537623508.0,
      "budget_used_percent": 2.726379537623508
    },
    {
      "type": "training",
      "description": "Training step 1900",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:19",
      "total_flops_so_far": 2727810104660436.0,
      "budget_used_percent": 2.7278101046604357
    },
    {
      "type": "training",
      "description": "Training step 1901",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:20",
      "total_flops_so_far": 2729240671697364.0,
      "budget_used_percent": 2.729240671697364
    },
    {
      "type": "training",
      "description": "Training step 1902",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:20",
      "total_flops_so_far": 2730671238734292.0,
      "budget_used_percent": 2.730671238734292
    },
    {
      "type": "training",
      "description": "Training step 1903",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:20",
      "total_flops_so_far": 2732101805771220.0,
      "budget_used_percent": 2.7321018057712196
    },
    {
      "type": "training",
      "description": "Training step 1904",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:20",
      "total_flops_so_far": 2733532372808148.0,
      "budget_used_percent": 2.7335323728081478
    },
    {
      "type": "training",
      "description": "Training step 1905",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:20",
      "total_flops_so_far": 2734962939845076.0,
      "budget_used_percent": 2.734962939845076
    },
    {
      "type": "training",
      "description": "Training step 1906",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:20",
      "total_flops_so_far": 2736393506882004.0,
      "budget_used_percent": 2.736393506882004
    },
    {
      "type": "training",
      "description": "Training step 1907",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:21",
      "total_flops_so_far": 2737824073918932.0,
      "budget_used_percent": 2.737824073918932
    },
    {
      "type": "training",
      "description": "Training step 1908",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:21",
      "total_flops_so_far": 2739254640955860.0,
      "budget_used_percent": 2.73925464095586
    },
    {
      "type": "training",
      "description": "Training step 1909",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:21",
      "total_flops_so_far": 2740685207992788.0,
      "budget_used_percent": 2.740685207992788
    },
    {
      "type": "training",
      "description": "Training step 1910",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:21",
      "total_flops_so_far": 2742115775029716.0,
      "budget_used_percent": 2.742115775029716
    },
    {
      "type": "training",
      "description": "Training step 1911",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:21",
      "total_flops_so_far": 2743546342066644.0,
      "budget_used_percent": 2.7435463420666437
    },
    {
      "type": "training",
      "description": "Training step 1912",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:21",
      "total_flops_so_far": 2744976909103572.0,
      "budget_used_percent": 2.744976909103572
    },
    {
      "type": "training",
      "description": "Training step 1913",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:21",
      "total_flops_so_far": 2746407476140500.0,
      "budget_used_percent": 2.7464074761405
    },
    {
      "type": "training",
      "description": "Training step 1914",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:22",
      "total_flops_so_far": 2747838043177428.0,
      "budget_used_percent": 2.747838043177428
    },
    {
      "type": "training",
      "description": "Training step 1915",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:22",
      "total_flops_so_far": 2749268610214356.0,
      "budget_used_percent": 2.749268610214356
    },
    {
      "type": "training",
      "description": "Training step 1916",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:22",
      "total_flops_so_far": 2750699177251284.0,
      "budget_used_percent": 2.750699177251284
    },
    {
      "type": "training",
      "description": "Training step 1917",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:22",
      "total_flops_so_far": 2752129744288212.0,
      "budget_used_percent": 2.752129744288212
    },
    {
      "type": "training",
      "description": "Training step 1918",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:22",
      "total_flops_so_far": 2753560311325140.0,
      "budget_used_percent": 2.75356031132514
    },
    {
      "type": "training",
      "description": "Training step 1919",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:23",
      "total_flops_so_far": 2754990878362068.0,
      "budget_used_percent": 2.754990878362068
    },
    {
      "type": "training",
      "description": "Training step 1920",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:23",
      "total_flops_so_far": 2756421445398996.0,
      "budget_used_percent": 2.756421445398996
    },
    {
      "type": "training",
      "description": "Training step 1921",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:23",
      "total_flops_so_far": 2757852012435924.0,
      "budget_used_percent": 2.757852012435924
    },
    {
      "type": "training",
      "description": "Training step 1922",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:23",
      "total_flops_so_far": 2759282579472852.0,
      "budget_used_percent": 2.759282579472852
    },
    {
      "type": "training",
      "description": "Training step 1923",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:23",
      "total_flops_so_far": 2760713146509780.0,
      "budget_used_percent": 2.7607131465097803
    },
    {
      "type": "training",
      "description": "Training step 1924",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:23",
      "total_flops_so_far": 2762143713546708.0,
      "budget_used_percent": 2.762143713546708
    },
    {
      "type": "training",
      "description": "Training step 1925",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:24",
      "total_flops_so_far": 2763574280583636.0,
      "budget_used_percent": 2.763574280583636
    },
    {
      "type": "training",
      "description": "Training step 1926",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:24",
      "total_flops_so_far": 2765004847620564.0,
      "budget_used_percent": 2.765004847620564
    },
    {
      "type": "training",
      "description": "Training step 1927",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:24",
      "total_flops_so_far": 2766435414657492.0,
      "budget_used_percent": 2.766435414657492
    },
    {
      "type": "training",
      "description": "Training step 1928",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:24",
      "total_flops_so_far": 2767865981694420.0,
      "budget_used_percent": 2.76786598169442
    },
    {
      "type": "training",
      "description": "Training step 1929",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:24",
      "total_flops_so_far": 2769296548731348.0,
      "budget_used_percent": 2.769296548731348
    },
    {
      "type": "training",
      "description": "Training step 1930",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:24",
      "total_flops_so_far": 2770727115768276.0,
      "budget_used_percent": 2.7707271157682762
    },
    {
      "type": "training",
      "description": "Training step 1931",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:25",
      "total_flops_so_far": 2772157682805204.0,
      "budget_used_percent": 2.7721576828052044
    },
    {
      "type": "training",
      "description": "Training step 1932",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:25",
      "total_flops_so_far": 2773588249842132.0,
      "budget_used_percent": 2.773588249842132
    },
    {
      "type": "training",
      "description": "Training step 1933",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:25",
      "total_flops_so_far": 2775018816879060.0,
      "budget_used_percent": 2.77501881687906
    },
    {
      "type": "training",
      "description": "Training step 1934",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:25",
      "total_flops_so_far": 2776449383915988.0,
      "budget_used_percent": 2.7764493839159883
    },
    {
      "type": "training",
      "description": "Training step 1935",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:25",
      "total_flops_so_far": 2777879950952916.0,
      "budget_used_percent": 2.777879950952916
    },
    {
      "type": "training",
      "description": "Training step 1936",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:25",
      "total_flops_so_far": 2779310517989844.0,
      "budget_used_percent": 2.7793105179898436
    },
    {
      "type": "training",
      "description": "Training step 1937",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:26",
      "total_flops_so_far": 2780741085026772.0,
      "budget_used_percent": 2.7807410850267718
    },
    {
      "type": "training",
      "description": "Training step 1938",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:26",
      "total_flops_so_far": 2782171652063700.0,
      "budget_used_percent": 2.7821716520637
    },
    {
      "type": "training",
      "description": "Training step 1939",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:26",
      "total_flops_so_far": 2783602219100628.0,
      "budget_used_percent": 2.783602219100628
    },
    {
      "type": "training",
      "description": "Training step 1940",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:26",
      "total_flops_so_far": 2785032786137556.0,
      "budget_used_percent": 2.7850327861375557
    },
    {
      "type": "training",
      "description": "Training step 1941",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:26",
      "total_flops_so_far": 2786463353174484.0,
      "budget_used_percent": 2.786463353174484
    },
    {
      "type": "training",
      "description": "Training step 1942",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:26",
      "total_flops_so_far": 2787893920211412.0,
      "budget_used_percent": 2.787893920211412
    },
    {
      "type": "training",
      "description": "Training step 1943",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:26",
      "total_flops_so_far": 2789324487248340.0,
      "budget_used_percent": 2.78932448724834
    },
    {
      "type": "training",
      "description": "Training step 1944",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:27",
      "total_flops_so_far": 2790755054285268.0,
      "budget_used_percent": 2.7907550542852677
    },
    {
      "type": "training",
      "description": "Training step 1945",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:27",
      "total_flops_so_far": 2792185621322196.0,
      "budget_used_percent": 2.792185621322196
    },
    {
      "type": "training",
      "description": "Training step 1946",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:27",
      "total_flops_so_far": 2793616188359124.0,
      "budget_used_percent": 2.793616188359124
    },
    {
      "type": "training",
      "description": "Training step 1947",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:27",
      "total_flops_so_far": 2795046755396052.0,
      "budget_used_percent": 2.795046755396052
    },
    {
      "type": "training",
      "description": "Training step 1948",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:27",
      "total_flops_so_far": 2796477322432980.0,
      "budget_used_percent": 2.7964773224329798
    },
    {
      "type": "training",
      "description": "Training step 1949",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:27",
      "total_flops_so_far": 2797907889469908.0,
      "budget_used_percent": 2.797907889469908
    },
    {
      "type": "training",
      "description": "Training step 1950",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:28",
      "total_flops_so_far": 2799338456506836.0,
      "budget_used_percent": 2.799338456506836
    },
    {
      "type": "training",
      "description": "Training step 1951",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:28",
      "total_flops_so_far": 2800769023543764.0,
      "budget_used_percent": 2.800769023543764
    },
    {
      "type": "training",
      "description": "Training step 1952",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:28",
      "total_flops_so_far": 2802199590580692.0,
      "budget_used_percent": 2.802199590580692
    },
    {
      "type": "training",
      "description": "Training step 1953",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:28",
      "total_flops_so_far": 2803630157617620.0,
      "budget_used_percent": 2.80363015761762
    },
    {
      "type": "training",
      "description": "Training step 1954",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:28",
      "total_flops_so_far": 2805060724654548.0,
      "budget_used_percent": 2.805060724654548
    },
    {
      "type": "training",
      "description": "Training step 1955",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:28",
      "total_flops_so_far": 2806491291691476.0,
      "budget_used_percent": 2.806491291691476
    },
    {
      "type": "training",
      "description": "Training step 1956",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:29",
      "total_flops_so_far": 2807921858728404.0,
      "budget_used_percent": 2.807921858728404
    },
    {
      "type": "training",
      "description": "Training step 1957",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:29",
      "total_flops_so_far": 2809352425765332.0,
      "budget_used_percent": 2.809352425765332
    },
    {
      "type": "training",
      "description": "Training step 1958",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:29",
      "total_flops_so_far": 2810782992802260.0,
      "budget_used_percent": 2.81078299280226
    },
    {
      "type": "training",
      "description": "Training step 1959",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:29",
      "total_flops_so_far": 2812213559839188.0,
      "budget_used_percent": 2.812213559839188
    },
    {
      "type": "training",
      "description": "Training step 1960",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:29",
      "total_flops_so_far": 2813644126876116.0,
      "budget_used_percent": 2.813644126876116
    },
    {
      "type": "training",
      "description": "Training step 1961",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:29",
      "total_flops_so_far": 2815074693913044.0,
      "budget_used_percent": 2.815074693913044
    },
    {
      "type": "training",
      "description": "Training step 1962",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:30",
      "total_flops_so_far": 2816505260949972.0,
      "budget_used_percent": 2.816505260949972
    },
    {
      "type": "training",
      "description": "Training step 1963",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:30",
      "total_flops_so_far": 2817935827986900.0,
      "budget_used_percent": 2.8179358279869002
    },
    {
      "type": "training",
      "description": "Training step 1964",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:30",
      "total_flops_so_far": 2819366395023828.0,
      "budget_used_percent": 2.819366395023828
    },
    {
      "type": "training",
      "description": "Training step 1965",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:30",
      "total_flops_so_far": 2820796962060756.0,
      "budget_used_percent": 2.820796962060756
    },
    {
      "type": "training",
      "description": "Training step 1966",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:30",
      "total_flops_so_far": 2822227529097684.0,
      "budget_used_percent": 2.822227529097684
    },
    {
      "type": "training",
      "description": "Training step 1967",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:30",
      "total_flops_so_far": 2823658096134612.0,
      "budget_used_percent": 2.8236580961346123
    },
    {
      "type": "training",
      "description": "Training step 1968",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:31",
      "total_flops_so_far": 2825088663171540.0,
      "budget_used_percent": 2.82508866317154
    },
    {
      "type": "training",
      "description": "Training step 1969",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:31",
      "total_flops_so_far": 2826519230208468.0,
      "budget_used_percent": 2.826519230208468
    },
    {
      "type": "training",
      "description": "Training step 1970",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:31",
      "total_flops_so_far": 2827949797245396.0,
      "budget_used_percent": 2.8279497972453957
    },
    {
      "type": "training",
      "description": "Training step 1971",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:31",
      "total_flops_so_far": 2829380364282324.0,
      "budget_used_percent": 2.829380364282324
    },
    {
      "type": "training",
      "description": "Training step 1972",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:31",
      "total_flops_so_far": 2830810931319252.0,
      "budget_used_percent": 2.830810931319252
    },
    {
      "type": "training",
      "description": "Training step 1973",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:31",
      "total_flops_so_far": 2832241498356180.0,
      "budget_used_percent": 2.8322414983561797
    },
    {
      "type": "training",
      "description": "Training step 1974",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:31",
      "total_flops_so_far": 2833672065393108.0,
      "budget_used_percent": 2.833672065393108
    },
    {
      "type": "training",
      "description": "Training step 1975",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:32",
      "total_flops_so_far": 2835102632430036.0,
      "budget_used_percent": 2.835102632430036
    },
    {
      "type": "training",
      "description": "Training step 1976",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:32",
      "total_flops_so_far": 2836533199466964.0,
      "budget_used_percent": 2.836533199466964
    },
    {
      "type": "training",
      "description": "Training step 1977",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:32",
      "total_flops_so_far": 2837963766503892.0,
      "budget_used_percent": 2.8379637665038917
    },
    {
      "type": "training",
      "description": "Training step 1978",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:32",
      "total_flops_so_far": 2839394333540820.0,
      "budget_used_percent": 2.83939433354082
    },
    {
      "type": "training",
      "description": "Training step 1979",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:32",
      "total_flops_so_far": 2840824900577748.0,
      "budget_used_percent": 2.840824900577748
    },
    {
      "type": "training",
      "description": "Training step 1980",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:32",
      "total_flops_so_far": 2842255467614676.0,
      "budget_used_percent": 2.842255467614676
    },
    {
      "type": "training",
      "description": "Training step 1981",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:33",
      "total_flops_so_far": 2843686034651604.0,
      "budget_used_percent": 2.8436860346516037
    },
    {
      "type": "training",
      "description": "Training step 1982",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:33",
      "total_flops_so_far": 2845116601688532.0,
      "budget_used_percent": 2.845116601688532
    },
    {
      "type": "training",
      "description": "Training step 1983",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:33",
      "total_flops_so_far": 2846547168725460.0,
      "budget_used_percent": 2.84654716872546
    },
    {
      "type": "training",
      "description": "Training step 1984",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:33",
      "total_flops_so_far": 2847977735762388.0,
      "budget_used_percent": 2.847977735762388
    },
    {
      "type": "training",
      "description": "Training step 1985",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:33",
      "total_flops_so_far": 2849408302799316.0,
      "budget_used_percent": 2.849408302799316
    },
    {
      "type": "training",
      "description": "Training step 1986",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:33",
      "total_flops_so_far": 2850838869836244.0,
      "budget_used_percent": 2.850838869836244
    },
    {
      "type": "training",
      "description": "Training step 1987",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:34",
      "total_flops_so_far": 2852269436873172.0,
      "budget_used_percent": 2.852269436873172
    },
    {
      "type": "training",
      "description": "Training step 1988",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:34",
      "total_flops_so_far": 2853700003910100.0,
      "budget_used_percent": 2.8537000039101
    },
    {
      "type": "training",
      "description": "Training step 1989",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:34",
      "total_flops_so_far": 2855130570947028.0,
      "budget_used_percent": 2.855130570947028
    },
    {
      "type": "training",
      "description": "Training step 1990",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:34",
      "total_flops_so_far": 2856561137983956.0,
      "budget_used_percent": 2.856561137983956
    },
    {
      "type": "training",
      "description": "Training step 1991",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:34",
      "total_flops_so_far": 2857991705020884.0,
      "budget_used_percent": 2.857991705020884
    },
    {
      "type": "training",
      "description": "Training step 1992",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:34",
      "total_flops_so_far": 2859422272057812.0,
      "budget_used_percent": 2.859422272057812
    },
    {
      "type": "training",
      "description": "Training step 1993",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:35",
      "total_flops_so_far": 2860852839094740.0,
      "budget_used_percent": 2.86085283909474
    },
    {
      "type": "training",
      "description": "Training step 1994",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:35",
      "total_flops_so_far": 2862283406131668.0,
      "budget_used_percent": 2.862283406131668
    },
    {
      "type": "training",
      "description": "Training step 1995",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:35",
      "total_flops_so_far": 2863713973168596.0,
      "budget_used_percent": 2.863713973168596
    },
    {
      "type": "training",
      "description": "Training step 1996",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:35",
      "total_flops_so_far": 2865144540205524.0,
      "budget_used_percent": 2.8651445402055242
    },
    {
      "type": "training",
      "description": "Training step 1997",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:35",
      "total_flops_so_far": 2866575107242452.0,
      "budget_used_percent": 2.866575107242452
    },
    {
      "type": "training",
      "description": "Training step 1998",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:35",
      "total_flops_so_far": 2868005674279380.0,
      "budget_used_percent": 2.86800567427938
    },
    {
      "type": "training",
      "description": "Training step 1999",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:36",
      "total_flops_so_far": 2869436241316308.0,
      "budget_used_percent": 2.869436241316308
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 551857894496.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:37",
      "total_flops_so_far": 2869988099210804.0,
      "budget_used_percent": 2.869988099210804
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 555458032848.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:38",
      "total_flops_so_far": 2870543557243652.0,
      "budget_used_percent": 2.870543557243652
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 553657603480.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:40",
      "total_flops_so_far": 2871097214847132.0,
      "budget_used_percent": 2.871097214847132
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 551857894496.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:41",
      "total_flops_so_far": 2871649072741628.0,
      "budget_used_percent": 2.871649072741628
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 554557728116.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:42",
      "total_flops_so_far": 2872203630469744.0,
      "budget_used_percent": 2.872203630469744
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 551857894496.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:44",
      "total_flops_so_far": 2872755488364240.0,
      "budget_used_percent": 2.8727554883642403
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 555458032848.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:45",
      "total_flops_so_far": 2873310946397088.0,
      "budget_used_percent": 2.873310946397088
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 553657603480.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:46",
      "total_flops_so_far": 2873864604000568.0,
      "budget_used_percent": 2.873864604000568
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 551857894496.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:48",
      "total_flops_so_far": 2874416461895064.0,
      "budget_used_percent": 2.874416461895064
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 554557728116.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 14:05:49",
      "total_flops_so_far": 2874971019623180.0,
      "budget_used_percent": 2.87497101962318
    }
  ],
  "total_flops": 2874971019623180.0,
  "budget_used_percent": 2.87497101962318
}