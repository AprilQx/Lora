{
  "experiment_name": "lr1e-05_rank2_ctx128",
  "model_config": {
    "hidden_size": 896,
    "num_attention_heads": 14,
    "num_hidden_layers": 24,
    "intermediate_size": 4864,
    "head_dim": 64,
    "vocab_size": 151936,
    "lora_r": 2,
    "lora_target_modules": [
      "q_proj",
      "v_proj"
    ]
  },
  "max_budget": 1e+17,
  "start_time": "2025-03-17 19:50:09",
  "operations": [
    {
      "type": "training",
      "description": "Training step 0",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-17 19:50:16",
      "total_flops_so_far": 1430567036928.0,
      "budget_used_percent": 0.0014305670369280001
    },
    {
      "type": "training",
      "description": "Training step 1",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-17 19:50:18",
      "total_flops_so_far": 2861134073856.0,
      "budget_used_percent": 0.0028611340738560003
    },
    {
      "type": "training",
      "description": "Training step 2",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-17 19:50:19",
      "total_flops_so_far": 4291701110784.0,
      "budget_used_percent": 0.004291701110784
    },
    {
      "type": "training",
      "description": "Training step 3",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-17 19:50:19",
      "total_flops_so_far": 5722268147712.0,
      "budget_used_percent": 0.005722268147712001
    },
    {
      "type": "training",
      "description": "Training step 4",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-17 19:50:20",
      "total_flops_so_far": 7152835184640.0,
      "budget_used_percent": 0.007152835184639999
    },
    {
      "type": "training",
      "description": "Training step 5",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-17 19:50:21",
      "total_flops_so_far": 8583402221568.0,
      "budget_used_percent": 0.008583402221568
    },
    {
      "type": "training",
      "description": "Training step 6",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-17 19:50:21",
      "total_flops_so_far": 10013969258496.0,
      "budget_used_percent": 0.010013969258496
    },
    {
      "type": "training",
      "description": "Training step 7",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-17 19:50:22",
      "total_flops_so_far": 11444536295424.0,
      "budget_used_percent": 0.011444536295424001
    },
    {
      "type": "training",
      "description": "Training step 8",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-17 19:50:23",
      "total_flops_so_far": 12875103332352.0,
      "budget_used_percent": 0.012875103332351999
    },
    {
      "type": "training",
      "description": "Training step 9",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-17 19:50:23",
      "total_flops_so_far": 14305670369280.0,
      "budget_used_percent": 0.014305670369279998
    },
    {
      "type": "training",
      "description": "Training step 10",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-17 19:50:24",
      "total_flops_so_far": 15736237406208.0,
      "budget_used_percent": 0.015736237406208
    },
    {
      "type": "training",
      "description": "Training step 11",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-17 19:50:24",
      "total_flops_so_far": 17166804443136.0,
      "budget_used_percent": 0.017166804443136
    },
    {
      "type": "training",
      "description": "Training step 12",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-17 19:50:25",
      "total_flops_so_far": 18597371480064.0,
      "budget_used_percent": 0.018597371480063997
    },
    {
      "type": "training",
      "description": "Training step 13",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-17 19:50:26",
      "total_flops_so_far": 20027938516992.0,
      "budget_used_percent": 0.020027938516992
    },
    {
      "type": "training",
      "description": "Training step 14",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-17 19:50:26",
      "total_flops_so_far": 21458505553920.0,
      "budget_used_percent": 0.02145850555392
    },
    {
      "type": "training",
      "description": "Training step 15",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-17 19:50:27",
      "total_flops_so_far": 22889072590848.0,
      "budget_used_percent": 0.022889072590848002
    },
    {
      "type": "training",
      "description": "Training step 16",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-17 19:50:27",
      "total_flops_so_far": 24319639627776.0,
      "budget_used_percent": 0.024319639627776002
    },
    {
      "type": "training",
      "description": "Training step 17",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-17 19:50:28",
      "total_flops_so_far": 25750206664704.0,
      "budget_used_percent": 0.025750206664703998
    },
    {
      "type": "training",
      "description": "Training step 18",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-17 19:50:29",
      "total_flops_so_far": 27180773701632.0,
      "budget_used_percent": 0.027180773701631997
    },
    {
      "type": "training",
      "description": "Training step 19",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-17 19:50:29",
      "total_flops_so_far": 28611340738560.0,
      "budget_used_percent": 0.028611340738559997
    },
    {
      "type": "training",
      "description": "Training step 20",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-17 19:50:30",
      "total_flops_so_far": 30041907775488.0,
      "budget_used_percent": 0.030041907775488003
    },
    {
      "type": "training",
      "description": "Training step 21",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-17 19:50:31",
      "total_flops_so_far": 31472474812416.0,
      "budget_used_percent": 0.031472474812416
    },
    {
      "type": "training",
      "description": "Training step 22",
      "seq_len": 128,
      "batch_size": 4,
      "forward_flops": 476855678976.0,
      "backward_flops": 953711357952.0,
      "flops": 1430567036928.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-17 19:50:31",
      "total_flops_so_far": 32903041849344.0,
      "budget_used_percent": 0.032903041849344
    }
  ],
  "total_flops": 32903041849344.0,
  "budget_used_percent": 0.032903041849344
}