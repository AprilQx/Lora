{
  "experiment_name": "lr1e-04_rank4",
  "model_config": {
    "hidden_size": 896,
    "num_attention_heads": 14,
    "num_hidden_layers": 24,
    "intermediate_size": 4864,
    "head_dim": 64,
    "vocab_size": 151936,
    "lora_r": 4,
    "lora_target_modules": [
      "q_proj",
      "v_proj"
    ]
  },
  "max_budget": 1e+17,
  "start_time": "2025-03-20 05:42:54",
  "operations": [
    {
      "type": "training",
      "description": "Training step 0",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:42:56",
      "total_flops_so_far": 23747325493248.0,
      "budget_used_percent": 0.023747325493248
    },
    {
      "type": "training",
      "description": "Training step 1",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:42:58",
      "total_flops_so_far": 47494650986496.0,
      "budget_used_percent": 0.047494650986496
    },
    {
      "type": "training",
      "description": "Training step 2",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:42:59",
      "total_flops_so_far": 71241976479744.0,
      "budget_used_percent": 0.07124197647974399
    },
    {
      "type": "training",
      "description": "Training step 3",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:00",
      "total_flops_so_far": 94989301972992.0,
      "budget_used_percent": 0.094989301972992
    },
    {
      "type": "training",
      "description": "Training step 4",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:01",
      "total_flops_so_far": 118736627466240.0,
      "budget_used_percent": 0.11873662746624
    },
    {
      "type": "training",
      "description": "Training step 5",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:03",
      "total_flops_so_far": 142483952959488.0,
      "budget_used_percent": 0.14248395295948799
    },
    {
      "type": "training",
      "description": "Training step 6",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:04",
      "total_flops_so_far": 166231278452736.0,
      "budget_used_percent": 0.166231278452736
    },
    {
      "type": "training",
      "description": "Training step 7",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:05",
      "total_flops_so_far": 189978603945984.0,
      "budget_used_percent": 0.189978603945984
    },
    {
      "type": "training",
      "description": "Training step 8",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:07",
      "total_flops_so_far": 213725929439232.0,
      "budget_used_percent": 0.213725929439232
    },
    {
      "type": "training",
      "description": "Training step 9",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:08",
      "total_flops_so_far": 237473254932480.0,
      "budget_used_percent": 0.23747325493248
    },
    {
      "type": "training",
      "description": "Training step 10",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:09",
      "total_flops_so_far": 261220580425728.0,
      "budget_used_percent": 0.261220580425728
    },
    {
      "type": "training",
      "description": "Training step 11",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:10",
      "total_flops_so_far": 284967905918976.0,
      "budget_used_percent": 0.28496790591897597
    },
    {
      "type": "training",
      "description": "Training step 12",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:12",
      "total_flops_so_far": 308715231412224.0,
      "budget_used_percent": 0.308715231412224
    },
    {
      "type": "training",
      "description": "Training step 13",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:13",
      "total_flops_so_far": 332462556905472.0,
      "budget_used_percent": 0.332462556905472
    },
    {
      "type": "training",
      "description": "Training step 14",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:14",
      "total_flops_so_far": 356209882398720.0,
      "budget_used_percent": 0.35620988239872003
    },
    {
      "type": "training",
      "description": "Training step 15",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:16",
      "total_flops_so_far": 379957207891968.0,
      "budget_used_percent": 0.379957207891968
    },
    {
      "type": "training",
      "description": "Training step 16",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:17",
      "total_flops_so_far": 403704533385216.0,
      "budget_used_percent": 0.403704533385216
    },
    {
      "type": "training",
      "description": "Training step 17",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:18",
      "total_flops_so_far": 427451858878464.0,
      "budget_used_percent": 0.427451858878464
    },
    {
      "type": "training",
      "description": "Training step 18",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:19",
      "total_flops_so_far": 451199184371712.0,
      "budget_used_percent": 0.451199184371712
    },
    {
      "type": "training",
      "description": "Training step 19",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:21",
      "total_flops_so_far": 474946509864960.0,
      "budget_used_percent": 0.47494650986496
    },
    {
      "type": "training",
      "description": "Training step 20",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:22",
      "total_flops_so_far": 498693835358208.0,
      "budget_used_percent": 0.498693835358208
    },
    {
      "type": "training",
      "description": "Training step 21",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:23",
      "total_flops_so_far": 522441160851456.0,
      "budget_used_percent": 0.522441160851456
    },
    {
      "type": "training",
      "description": "Training step 22",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:25",
      "total_flops_so_far": 546188486344704.0,
      "budget_used_percent": 0.546188486344704
    },
    {
      "type": "training",
      "description": "Training step 23",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:26",
      "total_flops_so_far": 569935811837952.0,
      "budget_used_percent": 0.5699358118379519
    },
    {
      "type": "training",
      "description": "Training step 24",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:27",
      "total_flops_so_far": 593683137331200.0,
      "budget_used_percent": 0.5936831373312
    },
    {
      "type": "training",
      "description": "Training step 25",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:28",
      "total_flops_so_far": 617430462824448.0,
      "budget_used_percent": 0.617430462824448
    },
    {
      "type": "training",
      "description": "Training step 26",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:30",
      "total_flops_so_far": 641177788317696.0,
      "budget_used_percent": 0.641177788317696
    },
    {
      "type": "training",
      "description": "Training step 27",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:31",
      "total_flops_so_far": 664925113810944.0,
      "budget_used_percent": 0.664925113810944
    },
    {
      "type": "training",
      "description": "Training step 28",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:32",
      "total_flops_so_far": 688672439304192.0,
      "budget_used_percent": 0.688672439304192
    },
    {
      "type": "training",
      "description": "Training step 29",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:34",
      "total_flops_so_far": 712419764797440.0,
      "budget_used_percent": 0.7124197647974401
    },
    {
      "type": "training",
      "description": "Training step 30",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:35",
      "total_flops_so_far": 736167090290688.0,
      "budget_used_percent": 0.736167090290688
    },
    {
      "type": "training",
      "description": "Training step 31",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:36",
      "total_flops_so_far": 759914415783936.0,
      "budget_used_percent": 0.759914415783936
    },
    {
      "type": "training",
      "description": "Training step 32",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:37",
      "total_flops_so_far": 783661741277184.0,
      "budget_used_percent": 0.783661741277184
    },
    {
      "type": "training",
      "description": "Training step 33",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:39",
      "total_flops_so_far": 807409066770432.0,
      "budget_used_percent": 0.807409066770432
    },
    {
      "type": "training",
      "description": "Training step 34",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:40",
      "total_flops_so_far": 831156392263680.0,
      "budget_used_percent": 0.83115639226368
    },
    {
      "type": "training",
      "description": "Training step 35",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:41",
      "total_flops_so_far": 854903717756928.0,
      "budget_used_percent": 0.854903717756928
    },
    {
      "type": "training",
      "description": "Training step 36",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:42",
      "total_flops_so_far": 878651043250176.0,
      "budget_used_percent": 0.878651043250176
    },
    {
      "type": "training",
      "description": "Training step 37",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:44",
      "total_flops_so_far": 902398368743424.0,
      "budget_used_percent": 0.902398368743424
    },
    {
      "type": "training",
      "description": "Training step 38",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:45",
      "total_flops_so_far": 926145694236672.0,
      "budget_used_percent": 0.926145694236672
    },
    {
      "type": "training",
      "description": "Training step 39",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:46",
      "total_flops_so_far": 949893019729920.0,
      "budget_used_percent": 0.94989301972992
    },
    {
      "type": "training",
      "description": "Training step 40",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:48",
      "total_flops_so_far": 973640345223168.0,
      "budget_used_percent": 0.973640345223168
    },
    {
      "type": "training",
      "description": "Training step 41",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:49",
      "total_flops_so_far": 997387670716416.0,
      "budget_used_percent": 0.997387670716416
    },
    {
      "type": "training",
      "description": "Training step 42",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:50",
      "total_flops_so_far": 1021134996209664.0,
      "budget_used_percent": 1.021134996209664
    },
    {
      "type": "training",
      "description": "Training step 43",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:51",
      "total_flops_so_far": 1044882321702912.0,
      "budget_used_percent": 1.044882321702912
    },
    {
      "type": "training",
      "description": "Training step 44",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:53",
      "total_flops_so_far": 1068629647196160.0,
      "budget_used_percent": 1.06862964719616
    },
    {
      "type": "training",
      "description": "Training step 45",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:54",
      "total_flops_so_far": 1092376972689408.0,
      "budget_used_percent": 1.092376972689408
    },
    {
      "type": "training",
      "description": "Training step 46",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:55",
      "total_flops_so_far": 1116124298182656.0,
      "budget_used_percent": 1.116124298182656
    },
    {
      "type": "training",
      "description": "Training step 47",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:57",
      "total_flops_so_far": 1139871623675904.0,
      "budget_used_percent": 1.1398716236759039
    },
    {
      "type": "training",
      "description": "Training step 48",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:58",
      "total_flops_so_far": 1163618949169152.0,
      "budget_used_percent": 1.163618949169152
    },
    {
      "type": "training",
      "description": "Training step 49",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:43:59",
      "total_flops_so_far": 1187366274662400.0,
      "budget_used_percent": 1.1873662746624
    },
    {
      "type": "training",
      "description": "Training step 50",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:00",
      "total_flops_so_far": 1211113600155648.0,
      "budget_used_percent": 1.211113600155648
    },
    {
      "type": "training",
      "description": "Training step 51",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:02",
      "total_flops_so_far": 1234860925648896.0,
      "budget_used_percent": 1.234860925648896
    },
    {
      "type": "training",
      "description": "Training step 52",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:03",
      "total_flops_so_far": 1258608251142144.0,
      "budget_used_percent": 1.258608251142144
    },
    {
      "type": "training",
      "description": "Training step 53",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:04",
      "total_flops_so_far": 1282355576635392.0,
      "budget_used_percent": 1.282355576635392
    },
    {
      "type": "training",
      "description": "Training step 54",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:06",
      "total_flops_so_far": 1306102902128640.0,
      "budget_used_percent": 1.3061029021286399
    },
    {
      "type": "training",
      "description": "Training step 55",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:07",
      "total_flops_so_far": 1329850227621888.0,
      "budget_used_percent": 1.329850227621888
    },
    {
      "type": "training",
      "description": "Training step 56",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:08",
      "total_flops_so_far": 1353597553115136.0,
      "budget_used_percent": 1.3535975531151359
    },
    {
      "type": "training",
      "description": "Training step 57",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:09",
      "total_flops_so_far": 1377344878608384.0,
      "budget_used_percent": 1.377344878608384
    },
    {
      "type": "training",
      "description": "Training step 58",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:11",
      "total_flops_so_far": 1401092204101632.0,
      "budget_used_percent": 1.401092204101632
    },
    {
      "type": "training",
      "description": "Training step 59",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:12",
      "total_flops_so_far": 1424839529594880.0,
      "budget_used_percent": 1.4248395295948801
    },
    {
      "type": "training",
      "description": "Training step 60",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:13",
      "total_flops_so_far": 1448586855088128.0,
      "budget_used_percent": 1.448586855088128
    },
    {
      "type": "training",
      "description": "Training step 61",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:14",
      "total_flops_so_far": 1472334180581376.0,
      "budget_used_percent": 1.472334180581376
    },
    {
      "type": "training",
      "description": "Training step 62",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:16",
      "total_flops_so_far": 1496081506074624.0,
      "budget_used_percent": 1.496081506074624
    },
    {
      "type": "training",
      "description": "Training step 63",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:17",
      "total_flops_so_far": 1519828831567872.0,
      "budget_used_percent": 1.519828831567872
    },
    {
      "type": "training",
      "description": "Training step 64",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:18",
      "total_flops_so_far": 1543576157061120.0,
      "budget_used_percent": 1.54357615706112
    },
    {
      "type": "training",
      "description": "Training step 65",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:20",
      "total_flops_so_far": 1567323482554368.0,
      "budget_used_percent": 1.567323482554368
    },
    {
      "type": "training",
      "description": "Training step 66",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:21",
      "total_flops_so_far": 1591070808047616.0,
      "budget_used_percent": 1.591070808047616
    },
    {
      "type": "training",
      "description": "Training step 67",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:22",
      "total_flops_so_far": 1614818133540864.0,
      "budget_used_percent": 1.614818133540864
    },
    {
      "type": "training",
      "description": "Training step 68",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:23",
      "total_flops_so_far": 1638565459034112.0,
      "budget_used_percent": 1.6385654590341119
    },
    {
      "type": "training",
      "description": "Training step 69",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:25",
      "total_flops_so_far": 1662312784527360.0,
      "budget_used_percent": 1.66231278452736
    },
    {
      "type": "training",
      "description": "Training step 70",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:26",
      "total_flops_so_far": 1686060110020608.0,
      "budget_used_percent": 1.6860601100206078
    },
    {
      "type": "training",
      "description": "Training step 71",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:27",
      "total_flops_so_far": 1709807435513856.0,
      "budget_used_percent": 1.709807435513856
    },
    {
      "type": "training",
      "description": "Training step 72",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:29",
      "total_flops_so_far": 1733554761007104.0,
      "budget_used_percent": 1.7335547610071038
    },
    {
      "type": "training",
      "description": "Training step 73",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:30",
      "total_flops_so_far": 1757302086500352.0,
      "budget_used_percent": 1.757302086500352
    },
    {
      "type": "training",
      "description": "Training step 74",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:31",
      "total_flops_so_far": 1781049411993600.0,
      "budget_used_percent": 1.7810494119936002
    },
    {
      "type": "training",
      "description": "Training step 75",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:32",
      "total_flops_so_far": 1804796737486848.0,
      "budget_used_percent": 1.804796737486848
    },
    {
      "type": "training",
      "description": "Training step 76",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:34",
      "total_flops_so_far": 1828544062980096.0,
      "budget_used_percent": 1.8285440629800962
    },
    {
      "type": "training",
      "description": "Training step 77",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:35",
      "total_flops_so_far": 1852291388473344.0,
      "budget_used_percent": 1.852291388473344
    },
    {
      "type": "training",
      "description": "Training step 78",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:36",
      "total_flops_so_far": 1876038713966592.0,
      "budget_used_percent": 1.8760387139665922
    },
    {
      "type": "training",
      "description": "Training step 79",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:38",
      "total_flops_so_far": 1899786039459840.0,
      "budget_used_percent": 1.89978603945984
    },
    {
      "type": "training",
      "description": "Training step 80",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:39",
      "total_flops_so_far": 1923533364953088.0,
      "budget_used_percent": 1.9235333649530881
    },
    {
      "type": "training",
      "description": "Training step 81",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:40",
      "total_flops_so_far": 1947280690446336.0,
      "budget_used_percent": 1.947280690446336
    },
    {
      "type": "training",
      "description": "Training step 82",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:41",
      "total_flops_so_far": 1971028015939584.0,
      "budget_used_percent": 1.971028015939584
    },
    {
      "type": "training",
      "description": "Training step 83",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:43",
      "total_flops_so_far": 1994775341432832.0,
      "budget_used_percent": 1.994775341432832
    },
    {
      "type": "training",
      "description": "Training step 84",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:44",
      "total_flops_so_far": 2018522666926080.0,
      "budget_used_percent": 2.01852266692608
    },
    {
      "type": "training",
      "description": "Training step 85",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:45",
      "total_flops_so_far": 2042269992419328.0,
      "budget_used_percent": 2.042269992419328
    },
    {
      "type": "training",
      "description": "Training step 86",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:47",
      "total_flops_so_far": 2066017317912576.0,
      "budget_used_percent": 2.066017317912576
    },
    {
      "type": "training",
      "description": "Training step 87",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:48",
      "total_flops_so_far": 2089764643405824.0,
      "budget_used_percent": 2.089764643405824
    },
    {
      "type": "training",
      "description": "Training step 88",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:49",
      "total_flops_so_far": 2113511968899072.0,
      "budget_used_percent": 2.113511968899072
    },
    {
      "type": "training",
      "description": "Training step 89",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:50",
      "total_flops_so_far": 2137259294392320.0,
      "budget_used_percent": 2.13725929439232
    },
    {
      "type": "training",
      "description": "Training step 90",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:52",
      "total_flops_so_far": 2161006619885568.0,
      "budget_used_percent": 2.1610066198855677
    },
    {
      "type": "training",
      "description": "Training step 91",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:53",
      "total_flops_so_far": 2184753945378816.0,
      "budget_used_percent": 2.184753945378816
    },
    {
      "type": "training",
      "description": "Training step 92",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:54",
      "total_flops_so_far": 2208501270872064.0,
      "budget_used_percent": 2.208501270872064
    },
    {
      "type": "training",
      "description": "Training step 93",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:55",
      "total_flops_so_far": 2232248596365312.0,
      "budget_used_percent": 2.232248596365312
    },
    {
      "type": "training",
      "description": "Training step 94",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:57",
      "total_flops_so_far": 2255995921858560.0,
      "budget_used_percent": 2.2559959218585597
    },
    {
      "type": "training",
      "description": "Training step 95",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:58",
      "total_flops_so_far": 2279743247351808.0,
      "budget_used_percent": 2.2797432473518078
    },
    {
      "type": "training",
      "description": "Training step 96",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:44:59",
      "total_flops_so_far": 2303490572845056.0,
      "budget_used_percent": 2.3034905728450563
    },
    {
      "type": "training",
      "description": "Training step 97",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:01",
      "total_flops_so_far": 2327237898338304.0,
      "budget_used_percent": 2.327237898338304
    },
    {
      "type": "training",
      "description": "Training step 98",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:02",
      "total_flops_so_far": 2350985223831552.0,
      "budget_used_percent": 2.350985223831552
    },
    {
      "type": "training",
      "description": "Training step 99",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:03",
      "total_flops_so_far": 2374732549324800.0,
      "budget_used_percent": 2.3747325493248
    },
    {
      "type": "training",
      "description": "Training step 100",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:04",
      "total_flops_so_far": 2398479874818048.0,
      "budget_used_percent": 2.3984798748180483
    },
    {
      "type": "training",
      "description": "Training step 101",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:06",
      "total_flops_so_far": 2422227200311296.0,
      "budget_used_percent": 2.422227200311296
    },
    {
      "type": "training",
      "description": "Training step 102",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:07",
      "total_flops_so_far": 2445974525804544.0,
      "budget_used_percent": 2.445974525804544
    },
    {
      "type": "training",
      "description": "Training step 103",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:08",
      "total_flops_so_far": 2469721851297792.0,
      "budget_used_percent": 2.469721851297792
    },
    {
      "type": "training",
      "description": "Training step 104",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:10",
      "total_flops_so_far": 2493469176791040.0,
      "budget_used_percent": 2.49346917679104
    },
    {
      "type": "training",
      "description": "Training step 105",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:11",
      "total_flops_so_far": 2517216502284288.0,
      "budget_used_percent": 2.517216502284288
    },
    {
      "type": "training",
      "description": "Training step 106",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:12",
      "total_flops_so_far": 2540963827777536.0,
      "budget_used_percent": 2.540963827777536
    },
    {
      "type": "training",
      "description": "Training step 107",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:13",
      "total_flops_so_far": 2564711153270784.0,
      "budget_used_percent": 2.564711153270784
    },
    {
      "type": "training",
      "description": "Training step 108",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:15",
      "total_flops_so_far": 2588458478764032.0,
      "budget_used_percent": 2.588458478764032
    },
    {
      "type": "training",
      "description": "Training step 109",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:16",
      "total_flops_so_far": 2612205804257280.0,
      "budget_used_percent": 2.6122058042572798
    },
    {
      "type": "training",
      "description": "Training step 110",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:17",
      "total_flops_so_far": 2635953129750528.0,
      "budget_used_percent": 2.635953129750528
    },
    {
      "type": "training",
      "description": "Training step 111",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:19",
      "total_flops_so_far": 2659700455243776.0,
      "budget_used_percent": 2.659700455243776
    },
    {
      "type": "training",
      "description": "Training step 112",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:20",
      "total_flops_so_far": 2683447780737024.0,
      "budget_used_percent": 2.683447780737024
    },
    {
      "type": "training",
      "description": "Training step 113",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:21",
      "total_flops_so_far": 2707195106230272.0,
      "budget_used_percent": 2.7071951062302717
    },
    {
      "type": "training",
      "description": "Training step 114",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:22",
      "total_flops_so_far": 2730942431723520.0,
      "budget_used_percent": 2.73094243172352
    },
    {
      "type": "training",
      "description": "Training step 115",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:24",
      "total_flops_so_far": 2754689757216768.0,
      "budget_used_percent": 2.754689757216768
    },
    {
      "type": "training",
      "description": "Training step 116",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:25",
      "total_flops_so_far": 2778437082710016.0,
      "budget_used_percent": 2.778437082710016
    },
    {
      "type": "training",
      "description": "Training step 117",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:26",
      "total_flops_so_far": 2802184408203264.0,
      "budget_used_percent": 2.802184408203264
    },
    {
      "type": "training",
      "description": "Training step 118",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:28",
      "total_flops_so_far": 2825931733696512.0,
      "budget_used_percent": 2.825931733696512
    },
    {
      "type": "training",
      "description": "Training step 119",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:29",
      "total_flops_so_far": 2849679059189760.0,
      "budget_used_percent": 2.8496790591897603
    },
    {
      "type": "training",
      "description": "Training step 120",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:30",
      "total_flops_so_far": 2873426384683008.0,
      "budget_used_percent": 2.873426384683008
    },
    {
      "type": "training",
      "description": "Training step 121",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:31",
      "total_flops_so_far": 2897173710176256.0,
      "budget_used_percent": 2.897173710176256
    },
    {
      "type": "training",
      "description": "Training step 122",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:33",
      "total_flops_so_far": 2920921035669504.0,
      "budget_used_percent": 2.920921035669504
    },
    {
      "type": "training",
      "description": "Training step 123",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:34",
      "total_flops_so_far": 2944668361162752.0,
      "budget_used_percent": 2.944668361162752
    },
    {
      "type": "training",
      "description": "Training step 124",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:35",
      "total_flops_so_far": 2968415686656000.0,
      "budget_used_percent": 2.968415686656
    },
    {
      "type": "training",
      "description": "Training step 125",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:36",
      "total_flops_so_far": 2992163012149248.0,
      "budget_used_percent": 2.992163012149248
    },
    {
      "type": "training",
      "description": "Training step 126",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:38",
      "total_flops_so_far": 3015910337642496.0,
      "budget_used_percent": 3.015910337642496
    },
    {
      "type": "training",
      "description": "Training step 127",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:39",
      "total_flops_so_far": 3039657663135744.0,
      "budget_used_percent": 3.039657663135744
    },
    {
      "type": "training",
      "description": "Training step 128",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:40",
      "total_flops_so_far": 3063404988628992.0,
      "budget_used_percent": 3.063404988628992
    },
    {
      "type": "training",
      "description": "Training step 129",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:42",
      "total_flops_so_far": 3087152314122240.0,
      "budget_used_percent": 3.08715231412224
    },
    {
      "type": "training",
      "description": "Training step 130",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:43",
      "total_flops_so_far": 3110899639615488.0,
      "budget_used_percent": 3.110899639615488
    },
    {
      "type": "training",
      "description": "Training step 131",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:44",
      "total_flops_so_far": 3134646965108736.0,
      "budget_used_percent": 3.134646965108736
    },
    {
      "type": "training",
      "description": "Training step 132",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:45",
      "total_flops_so_far": 3158394290601984.0,
      "budget_used_percent": 3.1583942906019837
    },
    {
      "type": "training",
      "description": "Training step 133",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:46",
      "total_flops_so_far": 3182141616095232.0,
      "budget_used_percent": 3.182141616095232
    },
    {
      "type": "training",
      "description": "Training step 134",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:47",
      "total_flops_so_far": 3205888941588480.0,
      "budget_used_percent": 3.20588894158848
    },
    {
      "type": "training",
      "description": "Training step 135",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:48",
      "total_flops_so_far": 3229636267081728.0,
      "budget_used_percent": 3.229636267081728
    },
    {
      "type": "training",
      "description": "Training step 136",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:50",
      "total_flops_so_far": 3253383592574976.0,
      "budget_used_percent": 3.2533835925749757
    },
    {
      "type": "training",
      "description": "Training step 137",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:51",
      "total_flops_so_far": 3277130918068224.0,
      "budget_used_percent": 3.2771309180682238
    },
    {
      "type": "training",
      "description": "Training step 138",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:52",
      "total_flops_so_far": 3300878243561472.0,
      "budget_used_percent": 3.300878243561472
    },
    {
      "type": "training",
      "description": "Training step 139",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:54",
      "total_flops_so_far": 3324625569054720.0,
      "budget_used_percent": 3.32462556905472
    },
    {
      "type": "training",
      "description": "Training step 140",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:55",
      "total_flops_so_far": 3348372894547968.0,
      "budget_used_percent": 3.3483728945479676
    },
    {
      "type": "training",
      "description": "Training step 141",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:56",
      "total_flops_so_far": 3372120220041216.0,
      "budget_used_percent": 3.3721202200412157
    },
    {
      "type": "training",
      "description": "Training step 142",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:57",
      "total_flops_so_far": 3395867545534464.0,
      "budget_used_percent": 3.395867545534464
    },
    {
      "type": "training",
      "description": "Training step 143",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:45:59",
      "total_flops_so_far": 3419614871027712.0,
      "budget_used_percent": 3.419614871027712
    },
    {
      "type": "training",
      "description": "Training step 144",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:00",
      "total_flops_so_far": 3443362196520960.0,
      "budget_used_percent": 3.4433621965209595
    },
    {
      "type": "training",
      "description": "Training step 145",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:01",
      "total_flops_so_far": 3467109522014208.0,
      "budget_used_percent": 3.4671095220142076
    },
    {
      "type": "training",
      "description": "Training step 146",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:03",
      "total_flops_so_far": 3490856847507456.0,
      "budget_used_percent": 3.4908568475074557
    },
    {
      "type": "training",
      "description": "Training step 147",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:04",
      "total_flops_so_far": 3514604173000704.0,
      "budget_used_percent": 3.514604173000704
    },
    {
      "type": "training",
      "description": "Training step 148",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:05",
      "total_flops_so_far": 3538351498493952.0,
      "budget_used_percent": 3.5383514984939515
    },
    {
      "type": "training",
      "description": "Training step 149",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:06",
      "total_flops_so_far": 3562098823987200.0,
      "budget_used_percent": 3.5620988239872005
    },
    {
      "type": "training",
      "description": "Training step 150",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:08",
      "total_flops_so_far": 3585846149480448.0,
      "budget_used_percent": 3.5858461494804486
    },
    {
      "type": "training",
      "description": "Training step 151",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:09",
      "total_flops_so_far": 3609593474973696.0,
      "budget_used_percent": 3.609593474973696
    },
    {
      "type": "training",
      "description": "Training step 152",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:10",
      "total_flops_so_far": 3633340800466944.0,
      "budget_used_percent": 3.6333408004669443
    },
    {
      "type": "training",
      "description": "Training step 153",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:12",
      "total_flops_so_far": 3657088125960192.0,
      "budget_used_percent": 3.6570881259601924
    },
    {
      "type": "training",
      "description": "Training step 154",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:13",
      "total_flops_so_far": 3680835451453440.0,
      "budget_used_percent": 3.68083545145344
    },
    {
      "type": "training",
      "description": "Training step 155",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:14",
      "total_flops_so_far": 3704582776946688.0,
      "budget_used_percent": 3.704582776946688
    },
    {
      "type": "training",
      "description": "Training step 156",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:15",
      "total_flops_so_far": 3728330102439936.0,
      "budget_used_percent": 3.7283301024399362
    },
    {
      "type": "training",
      "description": "Training step 157",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:17",
      "total_flops_so_far": 3752077427933184.0,
      "budget_used_percent": 3.7520774279331843
    },
    {
      "type": "training",
      "description": "Training step 158",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:18",
      "total_flops_so_far": 3775824753426432.0,
      "budget_used_percent": 3.7758247534264324
    },
    {
      "type": "training",
      "description": "Training step 159",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:19",
      "total_flops_so_far": 3799572078919680.0,
      "budget_used_percent": 3.79957207891968
    },
    {
      "type": "training",
      "description": "Training step 160",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:21",
      "total_flops_so_far": 3823319404412928.0,
      "budget_used_percent": 3.823319404412928
    },
    {
      "type": "training",
      "description": "Training step 161",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:22",
      "total_flops_so_far": 3847066729906176.0,
      "budget_used_percent": 3.8470667299061763
    },
    {
      "type": "training",
      "description": "Training step 162",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:23",
      "total_flops_so_far": 3870814055399424.0,
      "budget_used_percent": 3.870814055399424
    },
    {
      "type": "training",
      "description": "Training step 163",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:24",
      "total_flops_so_far": 3894561380892672.0,
      "budget_used_percent": 3.894561380892672
    },
    {
      "type": "training",
      "description": "Training step 164",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:26",
      "total_flops_so_far": 3918308706385920.0,
      "budget_used_percent": 3.91830870638592
    },
    {
      "type": "training",
      "description": "Training step 165",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:27",
      "total_flops_so_far": 3942056031879168.0,
      "budget_used_percent": 3.942056031879168
    },
    {
      "type": "training",
      "description": "Training step 166",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:28",
      "total_flops_so_far": 3965803357372416.0,
      "budget_used_percent": 3.9658033573724163
    },
    {
      "type": "training",
      "description": "Training step 167",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:29",
      "total_flops_so_far": 3989550682865664.0,
      "budget_used_percent": 3.989550682865664
    },
    {
      "type": "training",
      "description": "Training step 168",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:31",
      "total_flops_so_far": 4013298008358912.0,
      "budget_used_percent": 4.0132980083589125
    },
    {
      "type": "training",
      "description": "Training step 169",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:32",
      "total_flops_so_far": 4037045333852160.0,
      "budget_used_percent": 4.03704533385216
    },
    {
      "type": "training",
      "description": "Training step 170",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:33",
      "total_flops_so_far": 4060792659345408.0,
      "budget_used_percent": 4.060792659345408
    },
    {
      "type": "training",
      "description": "Training step 171",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:35",
      "total_flops_so_far": 4084539984838656.0,
      "budget_used_percent": 4.084539984838656
    },
    {
      "type": "training",
      "description": "Training step 172",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:36",
      "total_flops_so_far": 4108287310331904.0,
      "budget_used_percent": 4.108287310331904
    },
    {
      "type": "training",
      "description": "Training step 173",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:37",
      "total_flops_so_far": 4132034635825152.0,
      "budget_used_percent": 4.132034635825152
    },
    {
      "type": "training",
      "description": "Training step 174",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:38",
      "total_flops_so_far": 4155781961318400.0,
      "budget_used_percent": 4.1557819613184
    },
    {
      "type": "training",
      "description": "Training step 175",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:40",
      "total_flops_so_far": 4179529286811648.0,
      "budget_used_percent": 4.179529286811648
    },
    {
      "type": "training",
      "description": "Training step 176",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:41",
      "total_flops_so_far": 4203276612304896.0,
      "budget_used_percent": 4.2032766123048955
    },
    {
      "type": "training",
      "description": "Training step 177",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:42",
      "total_flops_so_far": 4227023937798144.0,
      "budget_used_percent": 4.227023937798144
    },
    {
      "type": "training",
      "description": "Training step 178",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:44",
      "total_flops_so_far": 4250771263291392.0,
      "budget_used_percent": 4.250771263291392
    },
    {
      "type": "training",
      "description": "Training step 179",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:45",
      "total_flops_so_far": 4274518588784640.0,
      "budget_used_percent": 4.27451858878464
    },
    {
      "type": "training",
      "description": "Training step 180",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:46",
      "total_flops_so_far": 4298265914277888.0,
      "budget_used_percent": 4.298265914277888
    },
    {
      "type": "training",
      "description": "Training step 181",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:47",
      "total_flops_so_far": 4322013239771136.0,
      "budget_used_percent": 4.3220132397711355
    },
    {
      "type": "training",
      "description": "Training step 182",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:49",
      "total_flops_so_far": 4345760565264384.0,
      "budget_used_percent": 4.345760565264384
    },
    {
      "type": "training",
      "description": "Training step 183",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:50",
      "total_flops_so_far": 4369507890757632.0,
      "budget_used_percent": 4.369507890757632
    },
    {
      "type": "training",
      "description": "Training step 184",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:51",
      "total_flops_so_far": 4393255216250880.0,
      "budget_used_percent": 4.39325521625088
    },
    {
      "type": "training",
      "description": "Training step 185",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:53",
      "total_flops_so_far": 4417002541744128.0,
      "budget_used_percent": 4.417002541744128
    },
    {
      "type": "training",
      "description": "Training step 186",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:54",
      "total_flops_so_far": 4440749867237376.0,
      "budget_used_percent": 4.4407498672373755
    },
    {
      "type": "training",
      "description": "Training step 187",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:55",
      "total_flops_so_far": 4464497192730624.0,
      "budget_used_percent": 4.464497192730624
    },
    {
      "type": "training",
      "description": "Training step 188",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:56",
      "total_flops_so_far": 4488244518223872.0,
      "budget_used_percent": 4.488244518223872
    },
    {
      "type": "training",
      "description": "Training step 189",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:58",
      "total_flops_so_far": 4511991843717120.0,
      "budget_used_percent": 4.511991843717119
    },
    {
      "type": "training",
      "description": "Training step 190",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:46:59",
      "total_flops_so_far": 4535739169210368.0,
      "budget_used_percent": 4.535739169210368
    },
    {
      "type": "training",
      "description": "Training step 191",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:00",
      "total_flops_so_far": 4559486494703616.0,
      "budget_used_percent": 4.5594864947036156
    },
    {
      "type": "training",
      "description": "Training step 192",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:02",
      "total_flops_so_far": 4583233820196864.0,
      "budget_used_percent": 4.583233820196864
    },
    {
      "type": "training",
      "description": "Training step 193",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:03",
      "total_flops_so_far": 4606981145690112.0,
      "budget_used_percent": 4.606981145690113
    },
    {
      "type": "training",
      "description": "Training step 194",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:04",
      "total_flops_so_far": 4630728471183360.0,
      "budget_used_percent": 4.63072847118336
    },
    {
      "type": "training",
      "description": "Training step 195",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:05",
      "total_flops_so_far": 4654475796676608.0,
      "budget_used_percent": 4.654475796676608
    },
    {
      "type": "training",
      "description": "Training step 196",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:07",
      "total_flops_so_far": 4678223122169856.0,
      "budget_used_percent": 4.6782231221698565
    },
    {
      "type": "training",
      "description": "Training step 197",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:08",
      "total_flops_so_far": 4701970447663104.0,
      "budget_used_percent": 4.701970447663104
    },
    {
      "type": "training",
      "description": "Training step 198",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:09",
      "total_flops_so_far": 4725717773156352.0,
      "budget_used_percent": 4.725717773156353
    },
    {
      "type": "training",
      "description": "Training step 199",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:11",
      "total_flops_so_far": 4749465098649600.0,
      "budget_used_percent": 4.7494650986496
    },
    {
      "type": "training",
      "description": "Training step 200",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:12",
      "total_flops_so_far": 4773212424142848.0,
      "budget_used_percent": 4.773212424142848
    },
    {
      "type": "training",
      "description": "Training step 201",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:13",
      "total_flops_so_far": 4796959749636096.0,
      "budget_used_percent": 4.7969597496360965
    },
    {
      "type": "training",
      "description": "Training step 202",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:14",
      "total_flops_so_far": 4820707075129344.0,
      "budget_used_percent": 4.820707075129344
    },
    {
      "type": "training",
      "description": "Training step 203",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:16",
      "total_flops_so_far": 4844454400622592.0,
      "budget_used_percent": 4.844454400622592
    },
    {
      "type": "training",
      "description": "Training step 204",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:17",
      "total_flops_so_far": 4868201726115840.0,
      "budget_used_percent": 4.86820172611584
    },
    {
      "type": "training",
      "description": "Training step 205",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:18",
      "total_flops_so_far": 4891949051609088.0,
      "budget_used_percent": 4.891949051609088
    },
    {
      "type": "training",
      "description": "Training step 206",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:20",
      "total_flops_so_far": 4915696377102336.0,
      "budget_used_percent": 4.9156963771023365
    },
    {
      "type": "training",
      "description": "Training step 207",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:21",
      "total_flops_so_far": 4939443702595584.0,
      "budget_used_percent": 4.939443702595584
    },
    {
      "type": "training",
      "description": "Training step 208",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:22",
      "total_flops_so_far": 4963191028088832.0,
      "budget_used_percent": 4.963191028088832
    },
    {
      "type": "training",
      "description": "Training step 209",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:23",
      "total_flops_so_far": 4986938353582080.0,
      "budget_used_percent": 4.98693835358208
    },
    {
      "type": "training",
      "description": "Training step 210",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:25",
      "total_flops_so_far": 5010685679075328.0,
      "budget_used_percent": 5.010685679075328
    },
    {
      "type": "training",
      "description": "Training step 211",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:26",
      "total_flops_so_far": 5034433004568576.0,
      "budget_used_percent": 5.034433004568576
    },
    {
      "type": "training",
      "description": "Training step 212",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:27",
      "total_flops_so_far": 5058180330061824.0,
      "budget_used_percent": 5.058180330061824
    },
    {
      "type": "training",
      "description": "Training step 213",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:29",
      "total_flops_so_far": 5081927655555072.0,
      "budget_used_percent": 5.081927655555072
    },
    {
      "type": "training",
      "description": "Training step 214",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:30",
      "total_flops_so_far": 5105674981048320.0,
      "budget_used_percent": 5.10567498104832
    },
    {
      "type": "training",
      "description": "Training step 215",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:31",
      "total_flops_so_far": 5129422306541568.0,
      "budget_used_percent": 5.129422306541568
    },
    {
      "type": "training",
      "description": "Training step 216",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:32",
      "total_flops_so_far": 5153169632034816.0,
      "budget_used_percent": 5.153169632034816
    },
    {
      "type": "training",
      "description": "Training step 217",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:34",
      "total_flops_so_far": 5176916957528064.0,
      "budget_used_percent": 5.176916957528064
    },
    {
      "type": "training",
      "description": "Training step 218",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:35",
      "total_flops_so_far": 5200664283021312.0,
      "budget_used_percent": 5.200664283021312
    },
    {
      "type": "training",
      "description": "Training step 219",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:36",
      "total_flops_so_far": 5224411608514560.0,
      "budget_used_percent": 5.2244116085145595
    },
    {
      "type": "training",
      "description": "Training step 220",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:38",
      "total_flops_so_far": 5248158934007808.0,
      "budget_used_percent": 5.248158934007808
    },
    {
      "type": "training",
      "description": "Training step 221",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:39",
      "total_flops_so_far": 5271906259501056.0,
      "budget_used_percent": 5.271906259501056
    },
    {
      "type": "training",
      "description": "Training step 222",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:40",
      "total_flops_so_far": 5295653584994304.0,
      "budget_used_percent": 5.295653584994304
    },
    {
      "type": "training",
      "description": "Training step 223",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:41",
      "total_flops_so_far": 5319400910487552.0,
      "budget_used_percent": 5.319400910487552
    },
    {
      "type": "training",
      "description": "Training step 224",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:43",
      "total_flops_so_far": 5343148235980800.0,
      "budget_used_percent": 5.3431482359808
    },
    {
      "type": "training",
      "description": "Training step 225",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:44",
      "total_flops_so_far": 5366895561474048.0,
      "budget_used_percent": 5.366895561474048
    },
    {
      "type": "training",
      "description": "Training step 226",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:45",
      "total_flops_so_far": 5390642886967296.0,
      "budget_used_percent": 5.390642886967296
    },
    {
      "type": "training",
      "description": "Training step 227",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:47",
      "total_flops_so_far": 5414390212460544.0,
      "budget_used_percent": 5.414390212460543
    },
    {
      "type": "training",
      "description": "Training step 228",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:48",
      "total_flops_so_far": 5438137537953792.0,
      "budget_used_percent": 5.438137537953792
    },
    {
      "type": "training",
      "description": "Training step 229",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:49",
      "total_flops_so_far": 5461884863447040.0,
      "budget_used_percent": 5.46188486344704
    },
    {
      "type": "training",
      "description": "Training step 230",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:50",
      "total_flops_so_far": 5485632188940288.0,
      "budget_used_percent": 5.485632188940288
    },
    {
      "type": "training",
      "description": "Training step 231",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:52",
      "total_flops_so_far": 5509379514433536.0,
      "budget_used_percent": 5.509379514433536
    },
    {
      "type": "training",
      "description": "Training step 232",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:53",
      "total_flops_so_far": 5533126839926784.0,
      "budget_used_percent": 5.5331268399267834
    },
    {
      "type": "training",
      "description": "Training step 233",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:54",
      "total_flops_so_far": 5556874165420032.0,
      "budget_used_percent": 5.556874165420032
    },
    {
      "type": "training",
      "description": "Training step 234",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:56",
      "total_flops_so_far": 5580621490913280.0,
      "budget_used_percent": 5.58062149091328
    },
    {
      "type": "training",
      "description": "Training step 235",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:57",
      "total_flops_so_far": 5604368816406528.0,
      "budget_used_percent": 5.604368816406528
    },
    {
      "type": "training",
      "description": "Training step 236",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:58",
      "total_flops_so_far": 5628116141899776.0,
      "budget_used_percent": 5.628116141899777
    },
    {
      "type": "training",
      "description": "Training step 237",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:47:59",
      "total_flops_so_far": 5651863467393024.0,
      "budget_used_percent": 5.651863467393024
    },
    {
      "type": "training",
      "description": "Training step 238",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:01",
      "total_flops_so_far": 5675610792886272.0,
      "budget_used_percent": 5.675610792886272
    },
    {
      "type": "training",
      "description": "Training step 239",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:02",
      "total_flops_so_far": 5699358118379520.0,
      "budget_used_percent": 5.6993581183795206
    },
    {
      "type": "training",
      "description": "Training step 240",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:03",
      "total_flops_so_far": 5723105443872768.0,
      "budget_used_percent": 5.723105443872768
    },
    {
      "type": "training",
      "description": "Training step 241",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:05",
      "total_flops_so_far": 5746852769366016.0,
      "budget_used_percent": 5.746852769366016
    },
    {
      "type": "training",
      "description": "Training step 242",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:06",
      "total_flops_so_far": 5770600094859264.0,
      "budget_used_percent": 5.770600094859264
    },
    {
      "type": "training",
      "description": "Training step 243",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:07",
      "total_flops_so_far": 5794347420352512.0,
      "budget_used_percent": 5.794347420352512
    },
    {
      "type": "training",
      "description": "Training step 244",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:08",
      "total_flops_so_far": 5818094745845760.0,
      "budget_used_percent": 5.818094745845761
    },
    {
      "type": "training",
      "description": "Training step 245",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:10",
      "total_flops_so_far": 5841842071339008.0,
      "budget_used_percent": 5.841842071339008
    },
    {
      "type": "training",
      "description": "Training step 246",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:11",
      "total_flops_so_far": 5865589396832256.0,
      "budget_used_percent": 5.865589396832256
    },
    {
      "type": "training",
      "description": "Training step 247",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:12",
      "total_flops_so_far": 5889336722325504.0,
      "budget_used_percent": 5.889336722325504
    },
    {
      "type": "training",
      "description": "Training step 248",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:14",
      "total_flops_so_far": 5913084047818752.0,
      "budget_used_percent": 5.913084047818752
    },
    {
      "type": "training",
      "description": "Training step 249",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:15",
      "total_flops_so_far": 5936831373312000.0,
      "budget_used_percent": 5.936831373312
    },
    {
      "type": "training",
      "description": "Training step 250",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:16",
      "total_flops_so_far": 5960578698805248.0,
      "budget_used_percent": 5.960578698805248
    },
    {
      "type": "training",
      "description": "Training step 251",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:17",
      "total_flops_so_far": 5984326024298496.0,
      "budget_used_percent": 5.984326024298496
    },
    {
      "type": "training",
      "description": "Training step 252",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:19",
      "total_flops_so_far": 6008073349791744.0,
      "budget_used_percent": 6.0080733497917445
    },
    {
      "type": "training",
      "description": "Training step 253",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:20",
      "total_flops_so_far": 6031820675284992.0,
      "budget_used_percent": 6.031820675284992
    },
    {
      "type": "training",
      "description": "Training step 254",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:21",
      "total_flops_so_far": 6055568000778240.0,
      "budget_used_percent": 6.05556800077824
    },
    {
      "type": "training",
      "description": "Training step 255",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:23",
      "total_flops_so_far": 6079315326271488.0,
      "budget_used_percent": 6.079315326271488
    },
    {
      "type": "training",
      "description": "Training step 256",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:24",
      "total_flops_so_far": 6103062651764736.0,
      "budget_used_percent": 6.103062651764736
    },
    {
      "type": "training",
      "description": "Training step 257",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:25",
      "total_flops_so_far": 6126809977257984.0,
      "budget_used_percent": 6.126809977257984
    },
    {
      "type": "training",
      "description": "Training step 258",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:26",
      "total_flops_so_far": 6150557302751232.0,
      "budget_used_percent": 6.150557302751232
    },
    {
      "type": "training",
      "description": "Training step 259",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:28",
      "total_flops_so_far": 6174304628244480.0,
      "budget_used_percent": 6.17430462824448
    },
    {
      "type": "training",
      "description": "Training step 260",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:29",
      "total_flops_so_far": 6198051953737728.0,
      "budget_used_percent": 6.198051953737728
    },
    {
      "type": "training",
      "description": "Training step 261",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:30",
      "total_flops_so_far": 6221799279230976.0,
      "budget_used_percent": 6.221799279230976
    },
    {
      "type": "training",
      "description": "Training step 262",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:32",
      "total_flops_so_far": 6245546604724224.0,
      "budget_used_percent": 6.245546604724224
    },
    {
      "type": "training",
      "description": "Training step 263",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:33",
      "total_flops_so_far": 6269293930217472.0,
      "budget_used_percent": 6.269293930217472
    },
    {
      "type": "training",
      "description": "Training step 264",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:33",
      "total_flops_so_far": 6293041255710720.0,
      "budget_used_percent": 6.29304125571072
    },
    {
      "type": "training",
      "description": "Training step 265",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:35",
      "total_flops_so_far": 6316788581203968.0,
      "budget_used_percent": 6.3167885812039675
    },
    {
      "type": "training",
      "description": "Training step 266",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:36",
      "total_flops_so_far": 6340535906697216.0,
      "budget_used_percent": 6.340535906697216
    },
    {
      "type": "training",
      "description": "Training step 267",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:37",
      "total_flops_so_far": 6364283232190464.0,
      "budget_used_percent": 6.364283232190464
    },
    {
      "type": "training",
      "description": "Training step 268",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:38",
      "total_flops_so_far": 6388030557683712.0,
      "budget_used_percent": 6.388030557683712
    },
    {
      "type": "training",
      "description": "Training step 269",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:40",
      "total_flops_so_far": 6411777883176960.0,
      "budget_used_percent": 6.41177788317696
    },
    {
      "type": "training",
      "description": "Training step 270",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:41",
      "total_flops_so_far": 6435525208670208.0,
      "budget_used_percent": 6.4355252086702075
    },
    {
      "type": "training",
      "description": "Training step 271",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:42",
      "total_flops_so_far": 6459272534163456.0,
      "budget_used_percent": 6.459272534163456
    },
    {
      "type": "training",
      "description": "Training step 272",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:44",
      "total_flops_so_far": 6483019859656704.0,
      "budget_used_percent": 6.483019859656704
    },
    {
      "type": "training",
      "description": "Training step 273",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:45",
      "total_flops_so_far": 6506767185149952.0,
      "budget_used_percent": 6.506767185149951
    },
    {
      "type": "training",
      "description": "Training step 274",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:46",
      "total_flops_so_far": 6530514510643200.0,
      "budget_used_percent": 6.5305145106432
    },
    {
      "type": "training",
      "description": "Training step 275",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:47",
      "total_flops_so_far": 6554261836136448.0,
      "budget_used_percent": 6.5542618361364475
    },
    {
      "type": "training",
      "description": "Training step 276",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:49",
      "total_flops_so_far": 6578009161629696.0,
      "budget_used_percent": 6.578009161629696
    },
    {
      "type": "training",
      "description": "Training step 277",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:50",
      "total_flops_so_far": 6601756487122944.0,
      "budget_used_percent": 6.601756487122944
    },
    {
      "type": "training",
      "description": "Training step 278",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:51",
      "total_flops_so_far": 6625503812616192.0,
      "budget_used_percent": 6.625503812616191
    },
    {
      "type": "training",
      "description": "Training step 279",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:53",
      "total_flops_so_far": 6649251138109440.0,
      "budget_used_percent": 6.64925113810944
    },
    {
      "type": "training",
      "description": "Training step 280",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:54",
      "total_flops_so_far": 6672998463602688.0,
      "budget_used_percent": 6.672998463602688
    },
    {
      "type": "training",
      "description": "Training step 281",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:55",
      "total_flops_so_far": 6696745789095936.0,
      "budget_used_percent": 6.696745789095935
    },
    {
      "type": "training",
      "description": "Training step 282",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:56",
      "total_flops_so_far": 6720493114589184.0,
      "budget_used_percent": 6.720493114589184
    },
    {
      "type": "training",
      "description": "Training step 283",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:58",
      "total_flops_so_far": 6744240440082432.0,
      "budget_used_percent": 6.744240440082431
    },
    {
      "type": "training",
      "description": "Training step 284",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:48:59",
      "total_flops_so_far": 6767987765575680.0,
      "budget_used_percent": 6.76798776557568
    },
    {
      "type": "training",
      "description": "Training step 285",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:00",
      "total_flops_so_far": 6791735091068928.0,
      "budget_used_percent": 6.791735091068928
    },
    {
      "type": "training",
      "description": "Training step 286",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:02",
      "total_flops_so_far": 6815482416562176.0,
      "budget_used_percent": 6.815482416562175
    },
    {
      "type": "training",
      "description": "Training step 287",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:03",
      "total_flops_so_far": 6839229742055424.0,
      "budget_used_percent": 6.839229742055424
    },
    {
      "type": "training",
      "description": "Training step 288",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:04",
      "total_flops_so_far": 6862977067548672.0,
      "budget_used_percent": 6.862977067548671
    },
    {
      "type": "training",
      "description": "Training step 289",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:05",
      "total_flops_so_far": 6886724393041920.0,
      "budget_used_percent": 6.886724393041919
    },
    {
      "type": "training",
      "description": "Training step 290",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:07",
      "total_flops_so_far": 6910471718535168.0,
      "budget_used_percent": 6.910471718535168
    },
    {
      "type": "training",
      "description": "Training step 291",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:08",
      "total_flops_so_far": 6934219044028416.0,
      "budget_used_percent": 6.934219044028415
    },
    {
      "type": "training",
      "description": "Training step 292",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:09",
      "total_flops_so_far": 6957966369521664.0,
      "budget_used_percent": 6.957966369521664
    },
    {
      "type": "training",
      "description": "Training step 293",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:11",
      "total_flops_so_far": 6981713695014912.0,
      "budget_used_percent": 6.9817136950149115
    },
    {
      "type": "training",
      "description": "Training step 294",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:12",
      "total_flops_so_far": 7005461020508160.0,
      "budget_used_percent": 7.005461020508159
    },
    {
      "type": "training",
      "description": "Training step 295",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:13",
      "total_flops_so_far": 7029208346001408.0,
      "budget_used_percent": 7.029208346001408
    },
    {
      "type": "training",
      "description": "Training step 296",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:14",
      "total_flops_so_far": 7052955671494656.0,
      "budget_used_percent": 7.052955671494655
    },
    {
      "type": "training",
      "description": "Training step 297",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:16",
      "total_flops_so_far": 7076702996987904.0,
      "budget_used_percent": 7.076702996987903
    },
    {
      "type": "training",
      "description": "Training step 298",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:17",
      "total_flops_so_far": 7100450322481152.0,
      "budget_used_percent": 7.1004503224811515
    },
    {
      "type": "training",
      "description": "Training step 299",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:18",
      "total_flops_so_far": 7124197647974400.0,
      "budget_used_percent": 7.124197647974401
    },
    {
      "type": "training",
      "description": "Training step 300",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:20",
      "total_flops_so_far": 7147944973467648.0,
      "budget_used_percent": 7.147944973467649
    },
    {
      "type": "training",
      "description": "Training step 301",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:21",
      "total_flops_so_far": 7171692298960896.0,
      "budget_used_percent": 7.171692298960897
    },
    {
      "type": "training",
      "description": "Training step 302",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:22",
      "total_flops_so_far": 7195439624454144.0,
      "budget_used_percent": 7.195439624454145
    },
    {
      "type": "training",
      "description": "Training step 303",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:23",
      "total_flops_so_far": 7219186949947392.0,
      "budget_used_percent": 7.219186949947392
    },
    {
      "type": "training",
      "description": "Training step 304",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:25",
      "total_flops_so_far": 7242934275440640.0,
      "budget_used_percent": 7.242934275440641
    },
    {
      "type": "training",
      "description": "Training step 305",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:26",
      "total_flops_so_far": 7266681600933888.0,
      "budget_used_percent": 7.266681600933889
    },
    {
      "type": "training",
      "description": "Training step 306",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:27",
      "total_flops_so_far": 7290428926427136.0,
      "budget_used_percent": 7.290428926427136
    },
    {
      "type": "training",
      "description": "Training step 307",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:29",
      "total_flops_so_far": 7314176251920384.0,
      "budget_used_percent": 7.314176251920385
    },
    {
      "type": "training",
      "description": "Training step 308",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:30",
      "total_flops_so_far": 7337923577413632.0,
      "budget_used_percent": 7.337923577413632
    },
    {
      "type": "training",
      "description": "Training step 309",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:31",
      "total_flops_so_far": 7361670902906880.0,
      "budget_used_percent": 7.36167090290688
    },
    {
      "type": "training",
      "description": "Training step 310",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:32",
      "total_flops_so_far": 7385418228400128.0,
      "budget_used_percent": 7.385418228400129
    },
    {
      "type": "training",
      "description": "Training step 311",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:34",
      "total_flops_so_far": 7409165553893376.0,
      "budget_used_percent": 7.409165553893376
    },
    {
      "type": "training",
      "description": "Training step 312",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:35",
      "total_flops_so_far": 7432912879386624.0,
      "budget_used_percent": 7.432912879386625
    },
    {
      "type": "training",
      "description": "Training step 313",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:36",
      "total_flops_so_far": 7456660204879872.0,
      "budget_used_percent": 7.4566602048798725
    },
    {
      "type": "training",
      "description": "Training step 314",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:38",
      "total_flops_so_far": 7480407530373120.0,
      "budget_used_percent": 7.48040753037312
    },
    {
      "type": "training",
      "description": "Training step 315",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:39",
      "total_flops_so_far": 7504154855866368.0,
      "budget_used_percent": 7.504154855866369
    },
    {
      "type": "training",
      "description": "Training step 316",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:40",
      "total_flops_so_far": 7527902181359616.0,
      "budget_used_percent": 7.527902181359616
    },
    {
      "type": "training",
      "description": "Training step 317",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:41",
      "total_flops_so_far": 7551649506852864.0,
      "budget_used_percent": 7.551649506852865
    },
    {
      "type": "training",
      "description": "Training step 318",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:43",
      "total_flops_so_far": 7575396832346112.0,
      "budget_used_percent": 7.5753968323461125
    },
    {
      "type": "training",
      "description": "Training step 319",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:44",
      "total_flops_so_far": 7599144157839360.0,
      "budget_used_percent": 7.59914415783936
    },
    {
      "type": "training",
      "description": "Training step 320",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:45",
      "total_flops_so_far": 7622891483332608.0,
      "budget_used_percent": 7.622891483332609
    },
    {
      "type": "training",
      "description": "Training step 321",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:47",
      "total_flops_so_far": 7646638808825856.0,
      "budget_used_percent": 7.646638808825856
    },
    {
      "type": "training",
      "description": "Training step 322",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:48",
      "total_flops_so_far": 7670386134319104.0,
      "budget_used_percent": 7.670386134319104
    },
    {
      "type": "training",
      "description": "Training step 323",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:49",
      "total_flops_so_far": 7694133459812352.0,
      "budget_used_percent": 7.6941334598123525
    },
    {
      "type": "training",
      "description": "Training step 324",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:50",
      "total_flops_so_far": 7717880785305600.0,
      "budget_used_percent": 7.7178807853056
    },
    {
      "type": "training",
      "description": "Training step 325",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:52",
      "total_flops_so_far": 7741628110798848.0,
      "budget_used_percent": 7.741628110798848
    },
    {
      "type": "training",
      "description": "Training step 326",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:53",
      "total_flops_so_far": 7765375436292096.0,
      "budget_used_percent": 7.765375436292096
    },
    {
      "type": "training",
      "description": "Training step 327",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:54",
      "total_flops_so_far": 7789122761785344.0,
      "budget_used_percent": 7.789122761785344
    },
    {
      "type": "training",
      "description": "Training step 328",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:56",
      "total_flops_so_far": 7812870087278592.0,
      "budget_used_percent": 7.812870087278593
    },
    {
      "type": "training",
      "description": "Training step 329",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:57",
      "total_flops_so_far": 7836617412771840.0,
      "budget_used_percent": 7.83661741277184
    },
    {
      "type": "training",
      "description": "Training step 330",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:58",
      "total_flops_so_far": 7860364738265088.0,
      "budget_used_percent": 7.860364738265088
    },
    {
      "type": "training",
      "description": "Training step 331",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:49:59",
      "total_flops_so_far": 7884112063758336.0,
      "budget_used_percent": 7.884112063758336
    },
    {
      "type": "training",
      "description": "Training step 332",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:01",
      "total_flops_so_far": 7907859389251584.0,
      "budget_used_percent": 7.907859389251584
    },
    {
      "type": "training",
      "description": "Training step 333",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:02",
      "total_flops_so_far": 7931606714744832.0,
      "budget_used_percent": 7.931606714744833
    },
    {
      "type": "training",
      "description": "Training step 334",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:03",
      "total_flops_so_far": 7955354040238080.0,
      "budget_used_percent": 7.95535404023808
    },
    {
      "type": "training",
      "description": "Training step 335",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:05",
      "total_flops_so_far": 7979101365731328.0,
      "budget_used_percent": 7.979101365731328
    },
    {
      "type": "training",
      "description": "Training step 336",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:06",
      "total_flops_so_far": 8002848691224576.0,
      "budget_used_percent": 8.002848691224576
    },
    {
      "type": "training",
      "description": "Training step 337",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:07",
      "total_flops_so_far": 8026596016717824.0,
      "budget_used_percent": 8.026596016717825
    },
    {
      "type": "training",
      "description": "Training step 338",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:08",
      "total_flops_so_far": 8050343342211072.0,
      "budget_used_percent": 8.050343342211072
    },
    {
      "type": "training",
      "description": "Training step 339",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:10",
      "total_flops_so_far": 8074090667704320.0,
      "budget_used_percent": 8.07409066770432
    },
    {
      "type": "training",
      "description": "Training step 340",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:11",
      "total_flops_so_far": 8097837993197568.0,
      "budget_used_percent": 8.097837993197569
    },
    {
      "type": "training",
      "description": "Training step 341",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:12",
      "total_flops_so_far": 8121585318690816.0,
      "budget_used_percent": 8.121585318690816
    },
    {
      "type": "training",
      "description": "Training step 342",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:14",
      "total_flops_so_far": 8145332644184064.0,
      "budget_used_percent": 8.145332644184064
    },
    {
      "type": "training",
      "description": "Training step 343",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:15",
      "total_flops_so_far": 8169079969677312.0,
      "budget_used_percent": 8.169079969677313
    },
    {
      "type": "training",
      "description": "Training step 344",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:16",
      "total_flops_so_far": 8192827295170560.0,
      "budget_used_percent": 8.19282729517056
    },
    {
      "type": "training",
      "description": "Training step 345",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:18",
      "total_flops_so_far": 8216574620663808.0,
      "budget_used_percent": 8.216574620663808
    },
    {
      "type": "training",
      "description": "Training step 346",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:19",
      "total_flops_so_far": 8240321946157056.0,
      "budget_used_percent": 8.240321946157056
    },
    {
      "type": "training",
      "description": "Training step 347",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:20",
      "total_flops_so_far": 8264069271650304.0,
      "budget_used_percent": 8.264069271650303
    },
    {
      "type": "training",
      "description": "Training step 348",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:21",
      "total_flops_so_far": 8287816597143552.0,
      "budget_used_percent": 8.287816597143552
    },
    {
      "type": "training",
      "description": "Training step 349",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:23",
      "total_flops_so_far": 8311563922636800.0,
      "budget_used_percent": 8.3115639226368
    },
    {
      "type": "training",
      "description": "Training step 350",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:24",
      "total_flops_so_far": 8335311248130048.0,
      "budget_used_percent": 8.335311248130047
    },
    {
      "type": "training",
      "description": "Training step 351",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:25",
      "total_flops_so_far": 8359058573623296.0,
      "budget_used_percent": 8.359058573623296
    },
    {
      "type": "training",
      "description": "Training step 352",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:27",
      "total_flops_so_far": 8382805899116544.0,
      "budget_used_percent": 8.382805899116544
    },
    {
      "type": "training",
      "description": "Training step 353",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:28",
      "total_flops_so_far": 8406553224609792.0,
      "budget_used_percent": 8.406553224609791
    },
    {
      "type": "training",
      "description": "Training step 354",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:29",
      "total_flops_so_far": 8430300550103040.0,
      "budget_used_percent": 8.43030055010304
    },
    {
      "type": "training",
      "description": "Training step 355",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:30",
      "total_flops_so_far": 8454047875596288.0,
      "budget_used_percent": 8.454047875596288
    },
    {
      "type": "training",
      "description": "Training step 356",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:32",
      "total_flops_so_far": 8477795201089536.0,
      "budget_used_percent": 8.477795201089537
    },
    {
      "type": "training",
      "description": "Training step 357",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:33",
      "total_flops_so_far": 8501542526582784.0,
      "budget_used_percent": 8.501542526582783
    },
    {
      "type": "training",
      "description": "Training step 358",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:34",
      "total_flops_so_far": 8525289852076032.0,
      "budget_used_percent": 8.525289852076032
    },
    {
      "type": "training",
      "description": "Training step 359",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:36",
      "total_flops_so_far": 8549037177569280.0,
      "budget_used_percent": 8.54903717756928
    },
    {
      "type": "training",
      "description": "Training step 360",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:37",
      "total_flops_so_far": 8572784503062528.0,
      "budget_used_percent": 8.572784503062527
    },
    {
      "type": "training",
      "description": "Training step 361",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:38",
      "total_flops_so_far": 8596531828555776.0,
      "budget_used_percent": 8.596531828555776
    },
    {
      "type": "training",
      "description": "Training step 362",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:39",
      "total_flops_so_far": 8620279154049024.0,
      "budget_used_percent": 8.620279154049024
    },
    {
      "type": "training",
      "description": "Training step 363",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:41",
      "total_flops_so_far": 8644026479542272.0,
      "budget_used_percent": 8.644026479542271
    },
    {
      "type": "training",
      "description": "Training step 364",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:42",
      "total_flops_so_far": 8667773805035520.0,
      "budget_used_percent": 8.66777380503552
    },
    {
      "type": "training",
      "description": "Training step 365",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:43",
      "total_flops_so_far": 8691521130528768.0,
      "budget_used_percent": 8.691521130528768
    },
    {
      "type": "training",
      "description": "Training step 366",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:45",
      "total_flops_so_far": 8715268456022016.0,
      "budget_used_percent": 8.715268456022015
    },
    {
      "type": "training",
      "description": "Training step 367",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:46",
      "total_flops_so_far": 8739015781515264.0,
      "budget_used_percent": 8.739015781515263
    },
    {
      "type": "training",
      "description": "Training step 368",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:47",
      "total_flops_so_far": 8762763107008512.0,
      "budget_used_percent": 8.762763107008512
    },
    {
      "type": "training",
      "description": "Training step 369",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:48",
      "total_flops_so_far": 8786510432501760.0,
      "budget_used_percent": 8.78651043250176
    },
    {
      "type": "training",
      "description": "Training step 370",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:50",
      "total_flops_so_far": 8810257757995008.0,
      "budget_used_percent": 8.810257757995007
    },
    {
      "type": "training",
      "description": "Training step 371",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:51",
      "total_flops_so_far": 8834005083488256.0,
      "budget_used_percent": 8.834005083488256
    },
    {
      "type": "training",
      "description": "Training step 372",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:52",
      "total_flops_so_far": 8857752408981504.0,
      "budget_used_percent": 8.857752408981504
    },
    {
      "type": "training",
      "description": "Training step 373",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:54",
      "total_flops_so_far": 8881499734474752.0,
      "budget_used_percent": 8.881499734474751
    },
    {
      "type": "training",
      "description": "Training step 374",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:55",
      "total_flops_so_far": 8905247059968000.0,
      "budget_used_percent": 8.905247059968
    },
    {
      "type": "training",
      "description": "Training step 375",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:56",
      "total_flops_so_far": 8928994385461248.0,
      "budget_used_percent": 8.928994385461248
    },
    {
      "type": "training",
      "description": "Training step 376",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:57",
      "total_flops_so_far": 8952741710954496.0,
      "budget_used_percent": 8.952741710954495
    },
    {
      "type": "training",
      "description": "Training step 377",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:50:59",
      "total_flops_so_far": 8976489036447744.0,
      "budget_used_percent": 8.976489036447743
    },
    {
      "type": "training",
      "description": "Training step 378",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:00",
      "total_flops_so_far": 9000236361940992.0,
      "budget_used_percent": 9.000236361940992
    },
    {
      "type": "training",
      "description": "Training step 379",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:01",
      "total_flops_so_far": 9023983687434240.0,
      "budget_used_percent": 9.023983687434239
    },
    {
      "type": "training",
      "description": "Training step 380",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:03",
      "total_flops_so_far": 9047731012927488.0,
      "budget_used_percent": 9.047731012927487
    },
    {
      "type": "training",
      "description": "Training step 381",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:04",
      "total_flops_so_far": 9071478338420736.0,
      "budget_used_percent": 9.071478338420736
    },
    {
      "type": "training",
      "description": "Training step 382",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:05",
      "total_flops_so_far": 9095225663913984.0,
      "budget_used_percent": 9.095225663913983
    },
    {
      "type": "training",
      "description": "Training step 383",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:07",
      "total_flops_so_far": 9118972989407232.0,
      "budget_used_percent": 9.118972989407231
    },
    {
      "type": "training",
      "description": "Training step 384",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:08",
      "total_flops_so_far": 9142720314900480.0,
      "budget_used_percent": 9.14272031490048
    },
    {
      "type": "training",
      "description": "Training step 385",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:09",
      "total_flops_so_far": 9166467640393728.0,
      "budget_used_percent": 9.166467640393728
    },
    {
      "type": "training",
      "description": "Training step 386",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:10",
      "total_flops_so_far": 9190214965886976.0,
      "budget_used_percent": 9.190214965886977
    },
    {
      "type": "training",
      "description": "Training step 387",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:12",
      "total_flops_so_far": 9213962291380224.0,
      "budget_used_percent": 9.213962291380225
    },
    {
      "type": "training",
      "description": "Training step 388",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:13",
      "total_flops_so_far": 9237709616873472.0,
      "budget_used_percent": 9.237709616873472
    },
    {
      "type": "training",
      "description": "Training step 389",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:14",
      "total_flops_so_far": 9261456942366720.0,
      "budget_used_percent": 9.26145694236672
    },
    {
      "type": "training",
      "description": "Training step 390",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:16",
      "total_flops_so_far": 9285204267859968.0,
      "budget_used_percent": 9.285204267859969
    },
    {
      "type": "training",
      "description": "Training step 391",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:17",
      "total_flops_so_far": 9308951593353216.0,
      "budget_used_percent": 9.308951593353216
    },
    {
      "type": "training",
      "description": "Training step 392",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:18",
      "total_flops_so_far": 9332698918846464.0,
      "budget_used_percent": 9.332698918846464
    },
    {
      "type": "training",
      "description": "Training step 393",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:19",
      "total_flops_so_far": 9356446244339712.0,
      "budget_used_percent": 9.356446244339713
    },
    {
      "type": "training",
      "description": "Training step 394",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:21",
      "total_flops_so_far": 9380193569832960.0,
      "budget_used_percent": 9.38019356983296
    },
    {
      "type": "training",
      "description": "Training step 395",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:22",
      "total_flops_so_far": 9403940895326208.0,
      "budget_used_percent": 9.403940895326208
    },
    {
      "type": "training",
      "description": "Training step 396",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:22",
      "total_flops_so_far": 9427688220819456.0,
      "budget_used_percent": 9.427688220819457
    },
    {
      "type": "training",
      "description": "Training step 397",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:24",
      "total_flops_so_far": 9451435546312704.0,
      "budget_used_percent": 9.451435546312705
    },
    {
      "type": "training",
      "description": "Training step 398",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:25",
      "total_flops_so_far": 9475182871805952.0,
      "budget_used_percent": 9.475182871805952
    },
    {
      "type": "training",
      "description": "Training step 399",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:26",
      "total_flops_so_far": 9498930197299200.0,
      "budget_used_percent": 9.4989301972992
    },
    {
      "type": "training",
      "description": "Training step 400",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:28",
      "total_flops_so_far": 9522677522792448.0,
      "budget_used_percent": 9.52267752279245
    },
    {
      "type": "training",
      "description": "Training step 401",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:29",
      "total_flops_so_far": 9546424848285696.0,
      "budget_used_percent": 9.546424848285696
    },
    {
      "type": "training",
      "description": "Training step 402",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:30",
      "total_flops_so_far": 9570172173778944.0,
      "budget_used_percent": 9.570172173778944
    },
    {
      "type": "training",
      "description": "Training step 403",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:31",
      "total_flops_so_far": 9593919499272192.0,
      "budget_used_percent": 9.593919499272193
    },
    {
      "type": "training",
      "description": "Training step 404",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:33",
      "total_flops_so_far": 9617666824765440.0,
      "budget_used_percent": 9.61766682476544
    },
    {
      "type": "training",
      "description": "Training step 405",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:34",
      "total_flops_so_far": 9641414150258688.0,
      "budget_used_percent": 9.641414150258688
    },
    {
      "type": "training",
      "description": "Training step 406",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:35",
      "total_flops_so_far": 9665161475751936.0,
      "budget_used_percent": 9.665161475751937
    },
    {
      "type": "training",
      "description": "Training step 407",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:37",
      "total_flops_so_far": 9688908801245184.0,
      "budget_used_percent": 9.688908801245184
    },
    {
      "type": "training",
      "description": "Training step 408",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:38",
      "total_flops_so_far": 9712656126738432.0,
      "budget_used_percent": 9.712656126738432
    },
    {
      "type": "training",
      "description": "Training step 409",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:39",
      "total_flops_so_far": 9736403452231680.0,
      "budget_used_percent": 9.73640345223168
    },
    {
      "type": "training",
      "description": "Training step 410",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:40",
      "total_flops_so_far": 9760150777724928.0,
      "budget_used_percent": 9.760150777724927
    },
    {
      "type": "training",
      "description": "Training step 411",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:42",
      "total_flops_so_far": 9783898103218176.0,
      "budget_used_percent": 9.783898103218176
    },
    {
      "type": "training",
      "description": "Training step 412",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:43",
      "total_flops_so_far": 9807645428711424.0,
      "budget_used_percent": 9.807645428711425
    },
    {
      "type": "training",
      "description": "Training step 413",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:44",
      "total_flops_so_far": 9831392754204672.0,
      "budget_used_percent": 9.831392754204673
    },
    {
      "type": "training",
      "description": "Training step 414",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:46",
      "total_flops_so_far": 9855140079697920.0,
      "budget_used_percent": 9.85514007969792
    },
    {
      "type": "training",
      "description": "Training step 415",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:47",
      "total_flops_so_far": 9878887405191168.0,
      "budget_used_percent": 9.878887405191168
    },
    {
      "type": "training",
      "description": "Training step 416",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:48",
      "total_flops_so_far": 9902634730684416.0,
      "budget_used_percent": 9.902634730684417
    },
    {
      "type": "training",
      "description": "Training step 417",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:50",
      "total_flops_so_far": 9926382056177664.0,
      "budget_used_percent": 9.926382056177664
    },
    {
      "type": "training",
      "description": "Training step 418",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:51",
      "total_flops_so_far": 9950129381670912.0,
      "budget_used_percent": 9.950129381670912
    },
    {
      "type": "training",
      "description": "Training step 419",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:52",
      "total_flops_so_far": 9973876707164160.0,
      "budget_used_percent": 9.97387670716416
    },
    {
      "type": "training",
      "description": "Training step 420",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:53",
      "total_flops_so_far": 9997624032657408.0,
      "budget_used_percent": 9.997624032657408
    },
    {
      "type": "training",
      "description": "Training step 421",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:55",
      "total_flops_so_far": 1.0021371358150656e+16,
      "budget_used_percent": 10.021371358150656
    },
    {
      "type": "training",
      "description": "Training step 422",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:56",
      "total_flops_so_far": 1.0045118683643904e+16,
      "budget_used_percent": 10.045118683643905
    },
    {
      "type": "training",
      "description": "Training step 423",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:57",
      "total_flops_so_far": 1.0068866009137152e+16,
      "budget_used_percent": 10.068866009137151
    },
    {
      "type": "training",
      "description": "Training step 424",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:51:59",
      "total_flops_so_far": 1.00926133346304e+16,
      "budget_used_percent": 10.0926133346304
    },
    {
      "type": "training",
      "description": "Training step 425",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:00",
      "total_flops_so_far": 1.0116360660123648e+16,
      "budget_used_percent": 10.116360660123648
    },
    {
      "type": "training",
      "description": "Training step 426",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:01",
      "total_flops_so_far": 1.0140107985616896e+16,
      "budget_used_percent": 10.140107985616895
    },
    {
      "type": "training",
      "description": "Training step 427",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:02",
      "total_flops_so_far": 1.0163855311110144e+16,
      "budget_used_percent": 10.163855311110144
    },
    {
      "type": "training",
      "description": "Training step 428",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:04",
      "total_flops_so_far": 1.0187602636603392e+16,
      "budget_used_percent": 10.187602636603392
    },
    {
      "type": "training",
      "description": "Training step 429",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:05",
      "total_flops_so_far": 1.021134996209664e+16,
      "budget_used_percent": 10.21134996209664
    },
    {
      "type": "training",
      "description": "Training step 430",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:06",
      "total_flops_so_far": 1.0235097287589888e+16,
      "budget_used_percent": 10.235097287589888
    },
    {
      "type": "training",
      "description": "Training step 431",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:08",
      "total_flops_so_far": 1.0258844613083136e+16,
      "budget_used_percent": 10.258844613083136
    },
    {
      "type": "training",
      "description": "Training step 432",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:09",
      "total_flops_so_far": 1.0282591938576384e+16,
      "budget_used_percent": 10.282591938576385
    },
    {
      "type": "training",
      "description": "Training step 433",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:10",
      "total_flops_so_far": 1.0306339264069632e+16,
      "budget_used_percent": 10.306339264069631
    },
    {
      "type": "training",
      "description": "Training step 434",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:11",
      "total_flops_so_far": 1.033008658956288e+16,
      "budget_used_percent": 10.33008658956288
    },
    {
      "type": "training",
      "description": "Training step 435",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:13",
      "total_flops_so_far": 1.0353833915056128e+16,
      "budget_used_percent": 10.353833915056128
    },
    {
      "type": "training",
      "description": "Training step 436",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:14",
      "total_flops_so_far": 1.0377581240549376e+16,
      "budget_used_percent": 10.377581240549375
    },
    {
      "type": "training",
      "description": "Training step 437",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:15",
      "total_flops_so_far": 1.0401328566042624e+16,
      "budget_used_percent": 10.401328566042624
    },
    {
      "type": "training",
      "description": "Training step 438",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:17",
      "total_flops_so_far": 1.0425075891535872e+16,
      "budget_used_percent": 10.425075891535872
    },
    {
      "type": "training",
      "description": "Training step 439",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:18",
      "total_flops_so_far": 1.044882321702912e+16,
      "budget_used_percent": 10.448823217029119
    },
    {
      "type": "training",
      "description": "Training step 440",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:19",
      "total_flops_so_far": 1.0472570542522368e+16,
      "budget_used_percent": 10.472570542522368
    },
    {
      "type": "training",
      "description": "Training step 441",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:20",
      "total_flops_so_far": 1.0496317868015616e+16,
      "budget_used_percent": 10.496317868015616
    },
    {
      "type": "training",
      "description": "Training step 442",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:22",
      "total_flops_so_far": 1.0520065193508864e+16,
      "budget_used_percent": 10.520065193508863
    },
    {
      "type": "training",
      "description": "Training step 443",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:23",
      "total_flops_so_far": 1.0543812519002112e+16,
      "budget_used_percent": 10.543812519002111
    },
    {
      "type": "training",
      "description": "Training step 444",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:24",
      "total_flops_so_far": 1.056755984449536e+16,
      "budget_used_percent": 10.56755984449536
    },
    {
      "type": "training",
      "description": "Training step 445",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:26",
      "total_flops_so_far": 1.0591307169988608e+16,
      "budget_used_percent": 10.591307169988609
    },
    {
      "type": "training",
      "description": "Training step 446",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:27",
      "total_flops_so_far": 1.0615054495481856e+16,
      "budget_used_percent": 10.615054495481855
    },
    {
      "type": "training",
      "description": "Training step 447",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:28",
      "total_flops_so_far": 1.0638801820975104e+16,
      "budget_used_percent": 10.638801820975104
    },
    {
      "type": "training",
      "description": "Training step 448",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:29",
      "total_flops_so_far": 1.0662549146468352e+16,
      "budget_used_percent": 10.662549146468352
    },
    {
      "type": "training",
      "description": "Training step 449",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:31",
      "total_flops_so_far": 1.06862964719616e+16,
      "budget_used_percent": 10.6862964719616
    },
    {
      "type": "training",
      "description": "Training step 450",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:32",
      "total_flops_so_far": 1.0710043797454848e+16,
      "budget_used_percent": 10.710043797454848
    },
    {
      "type": "training",
      "description": "Training step 451",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:33",
      "total_flops_so_far": 1.0733791122948096e+16,
      "budget_used_percent": 10.733791122948096
    },
    {
      "type": "training",
      "description": "Training step 452",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:35",
      "total_flops_so_far": 1.0757538448441344e+16,
      "budget_used_percent": 10.757538448441343
    },
    {
      "type": "training",
      "description": "Training step 453",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:36",
      "total_flops_so_far": 1.0781285773934592e+16,
      "budget_used_percent": 10.781285773934592
    },
    {
      "type": "training",
      "description": "Training step 454",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:37",
      "total_flops_so_far": 1.080503309942784e+16,
      "budget_used_percent": 10.80503309942784
    },
    {
      "type": "training",
      "description": "Training step 455",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:39",
      "total_flops_so_far": 1.0828780424921088e+16,
      "budget_used_percent": 10.828780424921087
    },
    {
      "type": "training",
      "description": "Training step 456",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:40",
      "total_flops_so_far": 1.0852527750414336e+16,
      "budget_used_percent": 10.852527750414335
    },
    {
      "type": "training",
      "description": "Training step 457",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:41",
      "total_flops_so_far": 1.0876275075907584e+16,
      "budget_used_percent": 10.876275075907584
    },
    {
      "type": "training",
      "description": "Training step 458",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:42",
      "total_flops_so_far": 1.0900022401400832e+16,
      "budget_used_percent": 10.90002240140083
    },
    {
      "type": "training",
      "description": "Training step 459",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:44",
      "total_flops_so_far": 1.092376972689408e+16,
      "budget_used_percent": 10.92376972689408
    },
    {
      "type": "training",
      "description": "Training step 460",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:45",
      "total_flops_so_far": 1.0947517052387328e+16,
      "budget_used_percent": 10.947517052387328
    },
    {
      "type": "training",
      "description": "Training step 461",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:46",
      "total_flops_so_far": 1.0971264377880576e+16,
      "budget_used_percent": 10.971264377880576
    },
    {
      "type": "training",
      "description": "Training step 462",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:48",
      "total_flops_so_far": 1.0995011703373824e+16,
      "budget_used_percent": 10.995011703373823
    },
    {
      "type": "training",
      "description": "Training step 463",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:49",
      "total_flops_so_far": 1.1018759028867072e+16,
      "budget_used_percent": 11.018759028867072
    },
    {
      "type": "training",
      "description": "Training step 464",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:50",
      "total_flops_so_far": 1.104250635436032e+16,
      "budget_used_percent": 11.04250635436032
    },
    {
      "type": "training",
      "description": "Training step 465",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:51",
      "total_flops_so_far": 1.1066253679853568e+16,
      "budget_used_percent": 11.066253679853567
    },
    {
      "type": "training",
      "description": "Training step 466",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:53",
      "total_flops_so_far": 1.1090001005346816e+16,
      "budget_used_percent": 11.090001005346815
    },
    {
      "type": "training",
      "description": "Training step 467",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:54",
      "total_flops_so_far": 1.1113748330840064e+16,
      "budget_used_percent": 11.113748330840064
    },
    {
      "type": "training",
      "description": "Training step 468",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:55",
      "total_flops_so_far": 1.1137495656333312e+16,
      "budget_used_percent": 11.13749565633331
    },
    {
      "type": "training",
      "description": "Training step 469",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:57",
      "total_flops_so_far": 1.116124298182656e+16,
      "budget_used_percent": 11.16124298182656
    },
    {
      "type": "training",
      "description": "Training step 470",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:58",
      "total_flops_so_far": 1.1184990307319808e+16,
      "budget_used_percent": 11.184990307319808
    },
    {
      "type": "training",
      "description": "Training step 471",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:52:59",
      "total_flops_so_far": 1.1208737632813056e+16,
      "budget_used_percent": 11.208737632813056
    },
    {
      "type": "training",
      "description": "Training step 472",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:00",
      "total_flops_so_far": 1.1232484958306304e+16,
      "budget_used_percent": 11.232484958306305
    },
    {
      "type": "training",
      "description": "Training step 473",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:02",
      "total_flops_so_far": 1.1256232283799552e+16,
      "budget_used_percent": 11.256232283799553
    },
    {
      "type": "training",
      "description": "Training step 474",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:03",
      "total_flops_so_far": 1.12799796092928e+16,
      "budget_used_percent": 11.2799796092928
    },
    {
      "type": "training",
      "description": "Training step 475",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:04",
      "total_flops_so_far": 1.1303726934786048e+16,
      "budget_used_percent": 11.303726934786049
    },
    {
      "type": "training",
      "description": "Training step 476",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:06",
      "total_flops_so_far": 1.1327474260279296e+16,
      "budget_used_percent": 11.327474260279297
    },
    {
      "type": "training",
      "description": "Training step 477",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:07",
      "total_flops_so_far": 1.1351221585772544e+16,
      "budget_used_percent": 11.351221585772544
    },
    {
      "type": "training",
      "description": "Training step 478",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:08",
      "total_flops_so_far": 1.1374968911265792e+16,
      "budget_used_percent": 11.374968911265793
    },
    {
      "type": "training",
      "description": "Training step 479",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:10",
      "total_flops_so_far": 1.139871623675904e+16,
      "budget_used_percent": 11.398716236759041
    },
    {
      "type": "training",
      "description": "Training step 480",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:11",
      "total_flops_so_far": 1.1422463562252288e+16,
      "budget_used_percent": 11.422463562252288
    },
    {
      "type": "training",
      "description": "Training step 481",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:12",
      "total_flops_so_far": 1.1446210887745536e+16,
      "budget_used_percent": 11.446210887745536
    },
    {
      "type": "training",
      "description": "Training step 482",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:13",
      "total_flops_so_far": 1.1469958213238784e+16,
      "budget_used_percent": 11.469958213238785
    },
    {
      "type": "training",
      "description": "Training step 483",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:15",
      "total_flops_so_far": 1.1493705538732032e+16,
      "budget_used_percent": 11.493705538732032
    },
    {
      "type": "training",
      "description": "Training step 484",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:16",
      "total_flops_so_far": 1.151745286422528e+16,
      "budget_used_percent": 11.51745286422528
    },
    {
      "type": "training",
      "description": "Training step 485",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:17",
      "total_flops_so_far": 1.1541200189718528e+16,
      "budget_used_percent": 11.541200189718529
    },
    {
      "type": "training",
      "description": "Training step 486",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:19",
      "total_flops_so_far": 1.1564947515211776e+16,
      "budget_used_percent": 11.564947515211777
    },
    {
      "type": "training",
      "description": "Training step 487",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:20",
      "total_flops_so_far": 1.1588694840705024e+16,
      "budget_used_percent": 11.588694840705024
    },
    {
      "type": "training",
      "description": "Training step 488",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:21",
      "total_flops_so_far": 1.1612442166198272e+16,
      "budget_used_percent": 11.612442166198273
    },
    {
      "type": "training",
      "description": "Training step 489",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:22",
      "total_flops_so_far": 1.163618949169152e+16,
      "budget_used_percent": 11.636189491691521
    },
    {
      "type": "training",
      "description": "Training step 490",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:24",
      "total_flops_so_far": 1.1659936817184768e+16,
      "budget_used_percent": 11.659936817184768
    },
    {
      "type": "training",
      "description": "Training step 491",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:25",
      "total_flops_so_far": 1.1683684142678016e+16,
      "budget_used_percent": 11.683684142678016
    },
    {
      "type": "training",
      "description": "Training step 492",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:26",
      "total_flops_so_far": 1.1707431468171264e+16,
      "budget_used_percent": 11.707431468171265
    },
    {
      "type": "training",
      "description": "Training step 493",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:28",
      "total_flops_so_far": 1.1731178793664512e+16,
      "budget_used_percent": 11.731178793664512
    },
    {
      "type": "training",
      "description": "Training step 494",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:29",
      "total_flops_so_far": 1.175492611915776e+16,
      "budget_used_percent": 11.75492611915776
    },
    {
      "type": "training",
      "description": "Training step 495",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:30",
      "total_flops_so_far": 1.1778673444651008e+16,
      "budget_used_percent": 11.778673444651009
    },
    {
      "type": "training",
      "description": "Training step 496",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:31",
      "total_flops_so_far": 1.1802420770144256e+16,
      "budget_used_percent": 11.802420770144256
    },
    {
      "type": "training",
      "description": "Training step 497",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:33",
      "total_flops_so_far": 1.1826168095637504e+16,
      "budget_used_percent": 11.826168095637504
    },
    {
      "type": "training",
      "description": "Training step 498",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:34",
      "total_flops_so_far": 1.1849915421130752e+16,
      "budget_used_percent": 11.849915421130753
    },
    {
      "type": "training",
      "description": "Training step 499",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:35",
      "total_flops_so_far": 1.1873662746624e+16,
      "budget_used_percent": 11.873662746624
    },
    {
      "type": "training",
      "description": "Training step 500",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:37",
      "total_flops_so_far": 1.1897410072117248e+16,
      "budget_used_percent": 11.897410072117248
    },
    {
      "type": "training",
      "description": "Training step 501",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:38",
      "total_flops_so_far": 1.1921157397610496e+16,
      "budget_used_percent": 11.921157397610497
    },
    {
      "type": "training",
      "description": "Training step 502",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:39",
      "total_flops_so_far": 1.1944904723103744e+16,
      "budget_used_percent": 11.944904723103743
    },
    {
      "type": "training",
      "description": "Training step 503",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:41",
      "total_flops_so_far": 1.1968652048596992e+16,
      "budget_used_percent": 11.968652048596992
    },
    {
      "type": "training",
      "description": "Training step 504",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:42",
      "total_flops_so_far": 1.199239937409024e+16,
      "budget_used_percent": 11.99239937409024
    },
    {
      "type": "training",
      "description": "Training step 505",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:43",
      "total_flops_so_far": 1.2016146699583488e+16,
      "budget_used_percent": 12.016146699583489
    },
    {
      "type": "training",
      "description": "Training step 506",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:44",
      "total_flops_so_far": 1.2039894025076736e+16,
      "budget_used_percent": 12.039894025076736
    },
    {
      "type": "training",
      "description": "Training step 507",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:46",
      "total_flops_so_far": 1.2063641350569984e+16,
      "budget_used_percent": 12.063641350569984
    },
    {
      "type": "training",
      "description": "Training step 508",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:47",
      "total_flops_so_far": 1.2087388676063232e+16,
      "budget_used_percent": 12.087388676063233
    },
    {
      "type": "training",
      "description": "Training step 509",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:48",
      "total_flops_so_far": 1.211113600155648e+16,
      "budget_used_percent": 12.11113600155648
    },
    {
      "type": "training",
      "description": "Training step 510",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:50",
      "total_flops_so_far": 1.2134883327049728e+16,
      "budget_used_percent": 12.134883327049728
    },
    {
      "type": "training",
      "description": "Training step 511",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:51",
      "total_flops_so_far": 1.2158630652542976e+16,
      "budget_used_percent": 12.158630652542977
    },
    {
      "type": "training",
      "description": "Training step 512",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:52",
      "total_flops_so_far": 1.2182377978036224e+16,
      "budget_used_percent": 12.182377978036223
    },
    {
      "type": "training",
      "description": "Training step 513",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:53",
      "total_flops_so_far": 1.2206125303529472e+16,
      "budget_used_percent": 12.206125303529472
    },
    {
      "type": "training",
      "description": "Training step 514",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:55",
      "total_flops_so_far": 1.222987262902272e+16,
      "budget_used_percent": 12.22987262902272
    },
    {
      "type": "training",
      "description": "Training step 515",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:56",
      "total_flops_so_far": 1.2253619954515968e+16,
      "budget_used_percent": 12.253619954515967
    },
    {
      "type": "training",
      "description": "Training step 516",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:57",
      "total_flops_so_far": 1.2277367280009216e+16,
      "budget_used_percent": 12.277367280009216
    },
    {
      "type": "training",
      "description": "Training step 517",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:53:59",
      "total_flops_so_far": 1.2301114605502464e+16,
      "budget_used_percent": 12.301114605502464
    },
    {
      "type": "training",
      "description": "Training step 518",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:00",
      "total_flops_so_far": 1.2324861930995712e+16,
      "budget_used_percent": 12.324861930995713
    },
    {
      "type": "training",
      "description": "Training step 519",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:01",
      "total_flops_so_far": 1.234860925648896e+16,
      "budget_used_percent": 12.34860925648896
    },
    {
      "type": "training",
      "description": "Training step 520",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:02",
      "total_flops_so_far": 1.2372356581982208e+16,
      "budget_used_percent": 12.372356581982208
    },
    {
      "type": "training",
      "description": "Training step 521",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:04",
      "total_flops_so_far": 1.2396103907475456e+16,
      "budget_used_percent": 12.396103907475457
    },
    {
      "type": "training",
      "description": "Training step 522",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:05",
      "total_flops_so_far": 1.2419851232968704e+16,
      "budget_used_percent": 12.419851232968703
    },
    {
      "type": "training",
      "description": "Training step 523",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:06",
      "total_flops_so_far": 1.2443598558461952e+16,
      "budget_used_percent": 12.443598558461952
    },
    {
      "type": "training",
      "description": "Training step 524",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:08",
      "total_flops_so_far": 1.24673458839552e+16,
      "budget_used_percent": 12.4673458839552
    },
    {
      "type": "training",
      "description": "Training step 525",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:09",
      "total_flops_so_far": 1.2491093209448448e+16,
      "budget_used_percent": 12.491093209448447
    },
    {
      "type": "training",
      "description": "Training step 526",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:10",
      "total_flops_so_far": 1.2514840534941696e+16,
      "budget_used_percent": 12.514840534941696
    },
    {
      "type": "training",
      "description": "Training step 527",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:12",
      "total_flops_so_far": 1.2538587860434944e+16,
      "budget_used_percent": 12.538587860434944
    },
    {
      "type": "training",
      "description": "Training step 528",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:12",
      "total_flops_so_far": 1.2562335185928192e+16,
      "budget_used_percent": 12.562335185928191
    },
    {
      "type": "training",
      "description": "Training step 529",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:13",
      "total_flops_so_far": 1.258608251142144e+16,
      "budget_used_percent": 12.58608251142144
    },
    {
      "type": "training",
      "description": "Training step 530",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:14",
      "total_flops_so_far": 1.2609829836914688e+16,
      "budget_used_percent": 12.609829836914688
    },
    {
      "type": "training",
      "description": "Training step 531",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:16",
      "total_flops_so_far": 1.2633577162407936e+16,
      "budget_used_percent": 12.633577162407935
    },
    {
      "type": "training",
      "description": "Training step 532",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:17",
      "total_flops_so_far": 1.2657324487901184e+16,
      "budget_used_percent": 12.657324487901183
    },
    {
      "type": "training",
      "description": "Training step 533",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:18",
      "total_flops_so_far": 1.2681071813394432e+16,
      "budget_used_percent": 12.681071813394432
    },
    {
      "type": "training",
      "description": "Training step 534",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:20",
      "total_flops_so_far": 1.270481913888768e+16,
      "budget_used_percent": 12.704819138887679
    },
    {
      "type": "training",
      "description": "Training step 535",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:21",
      "total_flops_so_far": 1.2728566464380928e+16,
      "budget_used_percent": 12.728566464380927
    },
    {
      "type": "training",
      "description": "Training step 536",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:22",
      "total_flops_so_far": 1.2752313789874176e+16,
      "budget_used_percent": 12.752313789874176
    },
    {
      "type": "training",
      "description": "Training step 537",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:24",
      "total_flops_so_far": 1.2776061115367424e+16,
      "budget_used_percent": 12.776061115367424
    },
    {
      "type": "training",
      "description": "Training step 538",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:25",
      "total_flops_so_far": 1.2799808440860672e+16,
      "budget_used_percent": 12.799808440860671
    },
    {
      "type": "training",
      "description": "Training step 539",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:26",
      "total_flops_so_far": 1.282355576635392e+16,
      "budget_used_percent": 12.82355576635392
    },
    {
      "type": "training",
      "description": "Training step 540",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:27",
      "total_flops_so_far": 1.2847303091847168e+16,
      "budget_used_percent": 12.847303091847168
    },
    {
      "type": "training",
      "description": "Training step 541",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:29",
      "total_flops_so_far": 1.2871050417340416e+16,
      "budget_used_percent": 12.871050417340415
    },
    {
      "type": "training",
      "description": "Training step 542",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:30",
      "total_flops_so_far": 1.2894797742833664e+16,
      "budget_used_percent": 12.894797742833664
    },
    {
      "type": "training",
      "description": "Training step 543",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:31",
      "total_flops_so_far": 1.2918545068326912e+16,
      "budget_used_percent": 12.918545068326912
    },
    {
      "type": "training",
      "description": "Training step 544",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:33",
      "total_flops_so_far": 1.294229239382016e+16,
      "budget_used_percent": 12.942292393820159
    },
    {
      "type": "training",
      "description": "Training step 545",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:34",
      "total_flops_so_far": 1.2966039719313408e+16,
      "budget_used_percent": 12.966039719313407
    },
    {
      "type": "training",
      "description": "Training step 546",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:35",
      "total_flops_so_far": 1.2989787044806656e+16,
      "budget_used_percent": 12.989787044806656
    },
    {
      "type": "training",
      "description": "Training step 547",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:36",
      "total_flops_so_far": 1.3013534370299904e+16,
      "budget_used_percent": 13.013534370299903
    },
    {
      "type": "training",
      "description": "Training step 548",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:38",
      "total_flops_so_far": 1.3037281695793152e+16,
      "budget_used_percent": 13.037281695793151
    },
    {
      "type": "training",
      "description": "Training step 549",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:39",
      "total_flops_so_far": 1.30610290212864e+16,
      "budget_used_percent": 13.0610290212864
    },
    {
      "type": "training",
      "description": "Training step 550",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:40",
      "total_flops_so_far": 1.3084776346779648e+16,
      "budget_used_percent": 13.084776346779648
    },
    {
      "type": "training",
      "description": "Training step 551",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:42",
      "total_flops_so_far": 1.3108523672272896e+16,
      "budget_used_percent": 13.108523672272895
    },
    {
      "type": "training",
      "description": "Training step 552",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:43",
      "total_flops_so_far": 1.3132270997766144e+16,
      "budget_used_percent": 13.132270997766144
    },
    {
      "type": "training",
      "description": "Training step 553",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:44",
      "total_flops_so_far": 1.3156018323259392e+16,
      "budget_used_percent": 13.156018323259392
    },
    {
      "type": "training",
      "description": "Training step 554",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:46",
      "total_flops_so_far": 1.317976564875264e+16,
      "budget_used_percent": 13.179765648752639
    },
    {
      "type": "training",
      "description": "Training step 555",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:47",
      "total_flops_so_far": 1.3203512974245888e+16,
      "budget_used_percent": 13.203512974245887
    },
    {
      "type": "training",
      "description": "Training step 556",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:48",
      "total_flops_so_far": 1.3227260299739136e+16,
      "budget_used_percent": 13.227260299739136
    },
    {
      "type": "training",
      "description": "Training step 557",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:49",
      "total_flops_so_far": 1.3251007625232384e+16,
      "budget_used_percent": 13.251007625232383
    },
    {
      "type": "training",
      "description": "Training step 558",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:51",
      "total_flops_so_far": 1.3274754950725632e+16,
      "budget_used_percent": 13.274754950725631
    },
    {
      "type": "training",
      "description": "Training step 559",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:52",
      "total_flops_so_far": 1.329850227621888e+16,
      "budget_used_percent": 13.29850227621888
    },
    {
      "type": "training",
      "description": "Training step 560",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:53",
      "total_flops_so_far": 1.3322249601712128e+16,
      "budget_used_percent": 13.322249601712127
    },
    {
      "type": "training",
      "description": "Training step 561",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:55",
      "total_flops_so_far": 1.3345996927205376e+16,
      "budget_used_percent": 13.345996927205375
    },
    {
      "type": "training",
      "description": "Training step 562",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:56",
      "total_flops_so_far": 1.3369744252698624e+16,
      "budget_used_percent": 13.369744252698624
    },
    {
      "type": "training",
      "description": "Training step 563",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:57",
      "total_flops_so_far": 1.3393491578191872e+16,
      "budget_used_percent": 13.39349157819187
    },
    {
      "type": "training",
      "description": "Training step 564",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:54:58",
      "total_flops_so_far": 1.341723890368512e+16,
      "budget_used_percent": 13.417238903685119
    },
    {
      "type": "training",
      "description": "Training step 565",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:00",
      "total_flops_so_far": 1.3440986229178368e+16,
      "budget_used_percent": 13.440986229178367
    },
    {
      "type": "training",
      "description": "Training step 566",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:01",
      "total_flops_so_far": 1.3464733554671616e+16,
      "budget_used_percent": 13.464733554671614
    },
    {
      "type": "training",
      "description": "Training step 567",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:02",
      "total_flops_so_far": 1.3488480880164864e+16,
      "budget_used_percent": 13.488480880164863
    },
    {
      "type": "training",
      "description": "Training step 568",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:04",
      "total_flops_so_far": 1.3512228205658112e+16,
      "budget_used_percent": 13.512228205658111
    },
    {
      "type": "training",
      "description": "Training step 569",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:05",
      "total_flops_so_far": 1.353597553115136e+16,
      "budget_used_percent": 13.53597553115136
    },
    {
      "type": "training",
      "description": "Training step 570",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:06",
      "total_flops_so_far": 1.3559722856644608e+16,
      "budget_used_percent": 13.559722856644607
    },
    {
      "type": "training",
      "description": "Training step 571",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:07",
      "total_flops_so_far": 1.3583470182137856e+16,
      "budget_used_percent": 13.583470182137855
    },
    {
      "type": "training",
      "description": "Training step 572",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:09",
      "total_flops_so_far": 1.3607217507631104e+16,
      "budget_used_percent": 13.607217507631104
    },
    {
      "type": "training",
      "description": "Training step 573",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:11",
      "total_flops_so_far": 1.3630964833124352e+16,
      "budget_used_percent": 13.63096483312435
    },
    {
      "type": "training",
      "description": "Training step 574",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:13",
      "total_flops_so_far": 1.36547121586176e+16,
      "budget_used_percent": 13.654712158617599
    },
    {
      "type": "training",
      "description": "Training step 575",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:14",
      "total_flops_so_far": 1.3678459484110848e+16,
      "budget_used_percent": 13.678459484110848
    },
    {
      "type": "training",
      "description": "Training step 576",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:15",
      "total_flops_so_far": 1.3702206809604096e+16,
      "budget_used_percent": 13.702206809604094
    },
    {
      "type": "training",
      "description": "Training step 577",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:17",
      "total_flops_so_far": 1.3725954135097344e+16,
      "budget_used_percent": 13.725954135097343
    },
    {
      "type": "training",
      "description": "Training step 578",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:18",
      "total_flops_so_far": 1.3749701460590592e+16,
      "budget_used_percent": 13.749701460590591
    },
    {
      "type": "training",
      "description": "Training step 579",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:19",
      "total_flops_so_far": 1.377344878608384e+16,
      "budget_used_percent": 13.773448786083838
    },
    {
      "type": "training",
      "description": "Training step 580",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:20",
      "total_flops_so_far": 1.3797196111577088e+16,
      "budget_used_percent": 13.797196111577087
    },
    {
      "type": "training",
      "description": "Training step 581",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:22",
      "total_flops_so_far": 1.3820943437070336e+16,
      "budget_used_percent": 13.820943437070335
    },
    {
      "type": "training",
      "description": "Training step 582",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:23",
      "total_flops_so_far": 1.3844690762563584e+16,
      "budget_used_percent": 13.844690762563584
    },
    {
      "type": "training",
      "description": "Training step 583",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:24",
      "total_flops_so_far": 1.3868438088056832e+16,
      "budget_used_percent": 13.86843808805683
    },
    {
      "type": "training",
      "description": "Training step 584",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:26",
      "total_flops_so_far": 1.389218541355008e+16,
      "budget_used_percent": 13.892185413550079
    },
    {
      "type": "training",
      "description": "Training step 585",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:27",
      "total_flops_so_far": 1.3915932739043328e+16,
      "budget_used_percent": 13.915932739043328
    },
    {
      "type": "training",
      "description": "Training step 586",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:28",
      "total_flops_so_far": 1.3939680064536576e+16,
      "budget_used_percent": 13.939680064536574
    },
    {
      "type": "training",
      "description": "Training step 587",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:30",
      "total_flops_so_far": 1.3963427390029824e+16,
      "budget_used_percent": 13.963427390029823
    },
    {
      "type": "training",
      "description": "Training step 588",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:31",
      "total_flops_so_far": 1.3987174715523072e+16,
      "budget_used_percent": 13.987174715523071
    },
    {
      "type": "training",
      "description": "Training step 589",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:32",
      "total_flops_so_far": 1.401092204101632e+16,
      "budget_used_percent": 14.010922041016318
    },
    {
      "type": "training",
      "description": "Training step 590",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:33",
      "total_flops_so_far": 1.4034669366509568e+16,
      "budget_used_percent": 14.034669366509567
    },
    {
      "type": "training",
      "description": "Training step 591",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:35",
      "total_flops_so_far": 1.4058416692002816e+16,
      "budget_used_percent": 14.058416692002815
    },
    {
      "type": "training",
      "description": "Training step 592",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:36",
      "total_flops_so_far": 1.4082164017496064e+16,
      "budget_used_percent": 14.082164017496062
    },
    {
      "type": "training",
      "description": "Training step 593",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:37",
      "total_flops_so_far": 1.4105911342989312e+16,
      "budget_used_percent": 14.10591134298931
    },
    {
      "type": "training",
      "description": "Training step 594",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:39",
      "total_flops_so_far": 1.412965866848256e+16,
      "budget_used_percent": 14.12965866848256
    },
    {
      "type": "training",
      "description": "Training step 595",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:40",
      "total_flops_so_far": 1.4153405993975808e+16,
      "budget_used_percent": 14.153405993975806
    },
    {
      "type": "training",
      "description": "Training step 596",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:41",
      "total_flops_so_far": 1.4177153319469056e+16,
      "budget_used_percent": 14.177153319469054
    },
    {
      "type": "training",
      "description": "Training step 597",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:42",
      "total_flops_so_far": 1.4200900644962304e+16,
      "budget_used_percent": 14.200900644962303
    },
    {
      "type": "training",
      "description": "Training step 598",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:44",
      "total_flops_so_far": 1.4224647970455552e+16,
      "budget_used_percent": 14.22464797045555
    },
    {
      "type": "training",
      "description": "Training step 599",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:45",
      "total_flops_so_far": 1.42483952959488e+16,
      "budget_used_percent": 14.248395295948802
    },
    {
      "type": "training",
      "description": "Training step 600",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:46",
      "total_flops_so_far": 1.4272142621442048e+16,
      "budget_used_percent": 14.272142621442049
    },
    {
      "type": "training",
      "description": "Training step 601",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:48",
      "total_flops_so_far": 1.4295889946935296e+16,
      "budget_used_percent": 14.295889946935297
    },
    {
      "type": "training",
      "description": "Training step 602",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:49",
      "total_flops_so_far": 1.4319637272428544e+16,
      "budget_used_percent": 14.319637272428546
    },
    {
      "type": "training",
      "description": "Training step 603",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:50",
      "total_flops_so_far": 1.4343384597921792e+16,
      "budget_used_percent": 14.343384597921794
    },
    {
      "type": "training",
      "description": "Training step 604",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:52",
      "total_flops_so_far": 1.436713192341504e+16,
      "budget_used_percent": 14.367131923415041
    },
    {
      "type": "training",
      "description": "Training step 605",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:53",
      "total_flops_so_far": 1.4390879248908288e+16,
      "budget_used_percent": 14.39087924890829
    },
    {
      "type": "training",
      "description": "Training step 606",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:54",
      "total_flops_so_far": 1.4414626574401536e+16,
      "budget_used_percent": 14.414626574401538
    },
    {
      "type": "training",
      "description": "Training step 607",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:55",
      "total_flops_so_far": 1.4438373899894784e+16,
      "budget_used_percent": 14.438373899894785
    },
    {
      "type": "training",
      "description": "Training step 608",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:57",
      "total_flops_so_far": 1.4462121225388032e+16,
      "budget_used_percent": 14.462121225388033
    },
    {
      "type": "training",
      "description": "Training step 609",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:58",
      "total_flops_so_far": 1.448586855088128e+16,
      "budget_used_percent": 14.485868550881282
    },
    {
      "type": "training",
      "description": "Training step 610",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:55:59",
      "total_flops_so_far": 1.4509615876374528e+16,
      "budget_used_percent": 14.509615876374529
    },
    {
      "type": "training",
      "description": "Training step 611",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:01",
      "total_flops_so_far": 1.4533363201867776e+16,
      "budget_used_percent": 14.533363201867777
    },
    {
      "type": "training",
      "description": "Training step 612",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:02",
      "total_flops_so_far": 1.4557110527361024e+16,
      "budget_used_percent": 14.557110527361026
    },
    {
      "type": "training",
      "description": "Training step 613",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:03",
      "total_flops_so_far": 1.4580857852854272e+16,
      "budget_used_percent": 14.580857852854272
    },
    {
      "type": "training",
      "description": "Training step 614",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:04",
      "total_flops_so_far": 1.460460517834752e+16,
      "budget_used_percent": 14.604605178347521
    },
    {
      "type": "training",
      "description": "Training step 615",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:06",
      "total_flops_so_far": 1.4628352503840768e+16,
      "budget_used_percent": 14.62835250384077
    },
    {
      "type": "training",
      "description": "Training step 616",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:07",
      "total_flops_so_far": 1.4652099829334016e+16,
      "budget_used_percent": 14.652099829334016
    },
    {
      "type": "training",
      "description": "Training step 617",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:08",
      "total_flops_so_far": 1.4675847154827264e+16,
      "budget_used_percent": 14.675847154827265
    },
    {
      "type": "training",
      "description": "Training step 618",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:10",
      "total_flops_so_far": 1.4699594480320512e+16,
      "budget_used_percent": 14.699594480320513
    },
    {
      "type": "training",
      "description": "Training step 619",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:11",
      "total_flops_so_far": 1.472334180581376e+16,
      "budget_used_percent": 14.72334180581376
    },
    {
      "type": "training",
      "description": "Training step 620",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:12",
      "total_flops_so_far": 1.4747089131307008e+16,
      "budget_used_percent": 14.747089131307009
    },
    {
      "type": "training",
      "description": "Training step 621",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:14",
      "total_flops_so_far": 1.4770836456800256e+16,
      "budget_used_percent": 14.770836456800257
    },
    {
      "type": "training",
      "description": "Training step 622",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:15",
      "total_flops_so_far": 1.4794583782293504e+16,
      "budget_used_percent": 14.794583782293506
    },
    {
      "type": "training",
      "description": "Training step 623",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:16",
      "total_flops_so_far": 1.4818331107786752e+16,
      "budget_used_percent": 14.818331107786753
    },
    {
      "type": "training",
      "description": "Training step 624",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:18",
      "total_flops_so_far": 1.484207843328e+16,
      "budget_used_percent": 14.842078433280001
    },
    {
      "type": "training",
      "description": "Training step 625",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:19",
      "total_flops_so_far": 1.4865825758773248e+16,
      "budget_used_percent": 14.86582575877325
    },
    {
      "type": "training",
      "description": "Training step 626",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:20",
      "total_flops_so_far": 1.4889573084266496e+16,
      "budget_used_percent": 14.889573084266496
    },
    {
      "type": "training",
      "description": "Training step 627",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:21",
      "total_flops_so_far": 1.4913320409759744e+16,
      "budget_used_percent": 14.913320409759745
    },
    {
      "type": "training",
      "description": "Training step 628",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:23",
      "total_flops_so_far": 1.4937067735252992e+16,
      "budget_used_percent": 14.937067735252993
    },
    {
      "type": "training",
      "description": "Training step 629",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:24",
      "total_flops_so_far": 1.496081506074624e+16,
      "budget_used_percent": 14.96081506074624
    },
    {
      "type": "training",
      "description": "Training step 630",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:25",
      "total_flops_so_far": 1.4984562386239488e+16,
      "budget_used_percent": 14.984562386239489
    },
    {
      "type": "training",
      "description": "Training step 631",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:27",
      "total_flops_so_far": 1.5008309711732736e+16,
      "budget_used_percent": 15.008309711732737
    },
    {
      "type": "training",
      "description": "Training step 632",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:28",
      "total_flops_so_far": 1.5032057037225984e+16,
      "budget_used_percent": 15.032057037225984
    },
    {
      "type": "training",
      "description": "Training step 633",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:29",
      "total_flops_so_far": 1.5055804362719232e+16,
      "budget_used_percent": 15.055804362719233
    },
    {
      "type": "training",
      "description": "Training step 634",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:30",
      "total_flops_so_far": 1.507955168821248e+16,
      "budget_used_percent": 15.079551688212481
    },
    {
      "type": "training",
      "description": "Training step 635",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:32",
      "total_flops_so_far": 1.5103299013705728e+16,
      "budget_used_percent": 15.10329901370573
    },
    {
      "type": "training",
      "description": "Training step 636",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:33",
      "total_flops_so_far": 1.5127046339198976e+16,
      "budget_used_percent": 15.127046339198976
    },
    {
      "type": "training",
      "description": "Training step 637",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:34",
      "total_flops_so_far": 1.5150793664692224e+16,
      "budget_used_percent": 15.150793664692225
    },
    {
      "type": "training",
      "description": "Training step 638",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:36",
      "total_flops_so_far": 1.5174540990185472e+16,
      "budget_used_percent": 15.174540990185474
    },
    {
      "type": "training",
      "description": "Training step 639",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:37",
      "total_flops_so_far": 1.519828831567872e+16,
      "budget_used_percent": 15.19828831567872
    },
    {
      "type": "training",
      "description": "Training step 640",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:38",
      "total_flops_so_far": 1.5222035641171968e+16,
      "budget_used_percent": 15.222035641171969
    },
    {
      "type": "training",
      "description": "Training step 641",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:40",
      "total_flops_so_far": 1.5245782966665216e+16,
      "budget_used_percent": 15.245782966665217
    },
    {
      "type": "training",
      "description": "Training step 642",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:41",
      "total_flops_so_far": 1.5269530292158464e+16,
      "budget_used_percent": 15.269530292158464
    },
    {
      "type": "training",
      "description": "Training step 643",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:42",
      "total_flops_so_far": 1.5293277617651712e+16,
      "budget_used_percent": 15.293277617651713
    },
    {
      "type": "training",
      "description": "Training step 644",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:43",
      "total_flops_so_far": 1.531702494314496e+16,
      "budget_used_percent": 15.317024943144961
    },
    {
      "type": "training",
      "description": "Training step 645",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:45",
      "total_flops_so_far": 1.5340772268638208e+16,
      "budget_used_percent": 15.340772268638208
    },
    {
      "type": "training",
      "description": "Training step 646",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:46",
      "total_flops_so_far": 1.5364519594131456e+16,
      "budget_used_percent": 15.364519594131457
    },
    {
      "type": "training",
      "description": "Training step 647",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:47",
      "total_flops_so_far": 1.5388266919624704e+16,
      "budget_used_percent": 15.388266919624705
    },
    {
      "type": "training",
      "description": "Training step 648",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:49",
      "total_flops_so_far": 1.5412014245117952e+16,
      "budget_used_percent": 15.412014245117952
    },
    {
      "type": "training",
      "description": "Training step 649",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:50",
      "total_flops_so_far": 1.54357615706112e+16,
      "budget_used_percent": 15.4357615706112
    },
    {
      "type": "training",
      "description": "Training step 650",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:51",
      "total_flops_so_far": 1.5459508896104448e+16,
      "budget_used_percent": 15.459508896104449
    },
    {
      "type": "training",
      "description": "Training step 651",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:52",
      "total_flops_so_far": 1.5483256221597696e+16,
      "budget_used_percent": 15.483256221597696
    },
    {
      "type": "training",
      "description": "Training step 652",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:54",
      "total_flops_so_far": 1.5507003547090944e+16,
      "budget_used_percent": 15.507003547090944
    },
    {
      "type": "training",
      "description": "Training step 653",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:55",
      "total_flops_so_far": 1.5530750872584192e+16,
      "budget_used_percent": 15.530750872584193
    },
    {
      "type": "training",
      "description": "Training step 654",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:56",
      "total_flops_so_far": 1.555449819807744e+16,
      "budget_used_percent": 15.554498198077441
    },
    {
      "type": "training",
      "description": "Training step 655",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:58",
      "total_flops_so_far": 1.5578245523570688e+16,
      "budget_used_percent": 15.578245523570688
    },
    {
      "type": "training",
      "description": "Training step 656",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:56:59",
      "total_flops_so_far": 1.5601992849063936e+16,
      "budget_used_percent": 15.601992849063937
    },
    {
      "type": "training",
      "description": "Training step 657",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:00",
      "total_flops_so_far": 1.5625740174557184e+16,
      "budget_used_percent": 15.625740174557185
    },
    {
      "type": "training",
      "description": "Training step 658",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:02",
      "total_flops_so_far": 1.5649487500050432e+16,
      "budget_used_percent": 15.649487500050432
    },
    {
      "type": "training",
      "description": "Training step 659",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:03",
      "total_flops_so_far": 1.567323482554368e+16,
      "budget_used_percent": 15.67323482554368
    },
    {
      "type": "training",
      "description": "Training step 660",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:03",
      "total_flops_so_far": 1.5696982151036928e+16,
      "budget_used_percent": 15.696982151036929
    },
    {
      "type": "training",
      "description": "Training step 661",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:05",
      "total_flops_so_far": 1.5720729476530176e+16,
      "budget_used_percent": 15.720729476530176
    },
    {
      "type": "training",
      "description": "Training step 662",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:06",
      "total_flops_so_far": 1.5744476802023424e+16,
      "budget_used_percent": 15.744476802023424
    },
    {
      "type": "training",
      "description": "Training step 663",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:07",
      "total_flops_so_far": 1.5768224127516672e+16,
      "budget_used_percent": 15.768224127516673
    },
    {
      "type": "training",
      "description": "Training step 664",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:08",
      "total_flops_so_far": 1.579197145300992e+16,
      "budget_used_percent": 15.79197145300992
    },
    {
      "type": "training",
      "description": "Training step 665",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:10",
      "total_flops_so_far": 1.5815718778503168e+16,
      "budget_used_percent": 15.815718778503168
    },
    {
      "type": "training",
      "description": "Training step 666",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:11",
      "total_flops_so_far": 1.5839466103996416e+16,
      "budget_used_percent": 15.839466103996417
    },
    {
      "type": "training",
      "description": "Training step 667",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:12",
      "total_flops_so_far": 1.5863213429489664e+16,
      "budget_used_percent": 15.863213429489665
    },
    {
      "type": "training",
      "description": "Training step 668",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:14",
      "total_flops_so_far": 1.5886960754982912e+16,
      "budget_used_percent": 15.886960754982912
    },
    {
      "type": "training",
      "description": "Training step 669",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:15",
      "total_flops_so_far": 1.591070808047616e+16,
      "budget_used_percent": 15.91070808047616
    },
    {
      "type": "training",
      "description": "Training step 670",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:16",
      "total_flops_so_far": 1.5934455405969408e+16,
      "budget_used_percent": 15.934455405969409
    },
    {
      "type": "training",
      "description": "Training step 671",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:17",
      "total_flops_so_far": 1.5958202731462656e+16,
      "budget_used_percent": 15.958202731462656
    },
    {
      "type": "training",
      "description": "Training step 672",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:19",
      "total_flops_so_far": 1.5981950056955904e+16,
      "budget_used_percent": 15.981950056955904
    },
    {
      "type": "training",
      "description": "Training step 673",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:20",
      "total_flops_so_far": 1.6005697382449152e+16,
      "budget_used_percent": 16.005697382449153
    },
    {
      "type": "training",
      "description": "Training step 674",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:21",
      "total_flops_so_far": 1.60294447079424e+16,
      "budget_used_percent": 16.0294447079424
    },
    {
      "type": "training",
      "description": "Training step 675",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:23",
      "total_flops_so_far": 1.6053192033435648e+16,
      "budget_used_percent": 16.05319203343565
    },
    {
      "type": "training",
      "description": "Training step 676",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:24",
      "total_flops_so_far": 1.6076939358928896e+16,
      "budget_used_percent": 16.076939358928897
    },
    {
      "type": "training",
      "description": "Training step 677",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:25",
      "total_flops_so_far": 1.6100686684422144e+16,
      "budget_used_percent": 16.100686684422143
    },
    {
      "type": "training",
      "description": "Training step 678",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:27",
      "total_flops_so_far": 1.6124434009915392e+16,
      "budget_used_percent": 16.124434009915394
    },
    {
      "type": "training",
      "description": "Training step 679",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:28",
      "total_flops_so_far": 1.614818133540864e+16,
      "budget_used_percent": 16.14818133540864
    },
    {
      "type": "training",
      "description": "Training step 680",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:29",
      "total_flops_so_far": 1.6171928660901888e+16,
      "budget_used_percent": 16.171928660901887
    },
    {
      "type": "training",
      "description": "Training step 681",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:30",
      "total_flops_so_far": 1.6195675986395136e+16,
      "budget_used_percent": 16.195675986395138
    },
    {
      "type": "training",
      "description": "Training step 682",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:32",
      "total_flops_so_far": 1.6219423311888384e+16,
      "budget_used_percent": 16.219423311888384
    },
    {
      "type": "training",
      "description": "Training step 683",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:33",
      "total_flops_so_far": 1.6243170637381632e+16,
      "budget_used_percent": 16.24317063738163
    },
    {
      "type": "training",
      "description": "Training step 684",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:34",
      "total_flops_so_far": 1.626691796287488e+16,
      "budget_used_percent": 16.26691796287488
    },
    {
      "type": "training",
      "description": "Training step 685",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:36",
      "total_flops_so_far": 1.6290665288368128e+16,
      "budget_used_percent": 16.290665288368128
    },
    {
      "type": "training",
      "description": "Training step 686",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:37",
      "total_flops_so_far": 1.6314412613861376e+16,
      "budget_used_percent": 16.314412613861375
    },
    {
      "type": "training",
      "description": "Training step 687",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:38",
      "total_flops_so_far": 1.6338159939354624e+16,
      "budget_used_percent": 16.338159939354625
    },
    {
      "type": "training",
      "description": "Training step 688",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:40",
      "total_flops_so_far": 1.6361907264847872e+16,
      "budget_used_percent": 16.361907264847872
    },
    {
      "type": "training",
      "description": "Training step 689",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:41",
      "total_flops_so_far": 1.638565459034112e+16,
      "budget_used_percent": 16.38565459034112
    },
    {
      "type": "training",
      "description": "Training step 690",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:42",
      "total_flops_so_far": 1.6409401915834368e+16,
      "budget_used_percent": 16.40940191583437
    },
    {
      "type": "training",
      "description": "Training step 691",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:43",
      "total_flops_so_far": 1.6433149241327616e+16,
      "budget_used_percent": 16.433149241327616
    },
    {
      "type": "training",
      "description": "Training step 692",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:45",
      "total_flops_so_far": 1.6456896566820864e+16,
      "budget_used_percent": 16.456896566820863
    },
    {
      "type": "training",
      "description": "Training step 693",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:46",
      "total_flops_so_far": 1.6480643892314112e+16,
      "budget_used_percent": 16.480643892314113
    },
    {
      "type": "training",
      "description": "Training step 694",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:47",
      "total_flops_so_far": 1.650439121780736e+16,
      "budget_used_percent": 16.50439121780736
    },
    {
      "type": "training",
      "description": "Training step 695",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:49",
      "total_flops_so_far": 1.6528138543300608e+16,
      "budget_used_percent": 16.528138543300606
    },
    {
      "type": "training",
      "description": "Training step 696",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:50",
      "total_flops_so_far": 1.6551885868793856e+16,
      "budget_used_percent": 16.551885868793857
    },
    {
      "type": "training",
      "description": "Training step 697",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:51",
      "total_flops_so_far": 1.6575633194287104e+16,
      "budget_used_percent": 16.575633194287104
    },
    {
      "type": "training",
      "description": "Training step 698",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:52",
      "total_flops_so_far": 1.6599380519780352e+16,
      "budget_used_percent": 16.59938051978035
    },
    {
      "type": "training",
      "description": "Training step 699",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:54",
      "total_flops_so_far": 1.66231278452736e+16,
      "budget_used_percent": 16.6231278452736
    },
    {
      "type": "training",
      "description": "Training step 700",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:55",
      "total_flops_so_far": 1.6646875170766848e+16,
      "budget_used_percent": 16.646875170766847
    },
    {
      "type": "training",
      "description": "Training step 701",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:56",
      "total_flops_so_far": 1.6670622496260096e+16,
      "budget_used_percent": 16.670622496260094
    },
    {
      "type": "training",
      "description": "Training step 702",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:58",
      "total_flops_so_far": 1.6694369821753344e+16,
      "budget_used_percent": 16.694369821753344
    },
    {
      "type": "training",
      "description": "Training step 703",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:57:59",
      "total_flops_so_far": 1.6718117147246592e+16,
      "budget_used_percent": 16.71811714724659
    },
    {
      "type": "training",
      "description": "Training step 704",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:00",
      "total_flops_so_far": 1.674186447273984e+16,
      "budget_used_percent": 16.741864472739838
    },
    {
      "type": "training",
      "description": "Training step 705",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:02",
      "total_flops_so_far": 1.6765611798233088e+16,
      "budget_used_percent": 16.76561179823309
    },
    {
      "type": "training",
      "description": "Training step 706",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:03",
      "total_flops_so_far": 1.6789359123726336e+16,
      "budget_used_percent": 16.789359123726335
    },
    {
      "type": "training",
      "description": "Training step 707",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:04",
      "total_flops_so_far": 1.6813106449219584e+16,
      "budget_used_percent": 16.813106449219582
    },
    {
      "type": "training",
      "description": "Training step 708",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:05",
      "total_flops_so_far": 1.6836853774712832e+16,
      "budget_used_percent": 16.836853774712832
    },
    {
      "type": "training",
      "description": "Training step 709",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:07",
      "total_flops_so_far": 1.686060110020608e+16,
      "budget_used_percent": 16.86060110020608
    },
    {
      "type": "training",
      "description": "Training step 710",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:08",
      "total_flops_so_far": 1.6884348425699328e+16,
      "budget_used_percent": 16.88434842569933
    },
    {
      "type": "training",
      "description": "Training step 711",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:09",
      "total_flops_so_far": 1.6908095751192576e+16,
      "budget_used_percent": 16.908095751192576
    },
    {
      "type": "training",
      "description": "Training step 712",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:11",
      "total_flops_so_far": 1.6931843076685824e+16,
      "budget_used_percent": 16.931843076685823
    },
    {
      "type": "training",
      "description": "Training step 713",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:12",
      "total_flops_so_far": 1.6955590402179072e+16,
      "budget_used_percent": 16.955590402179073
    },
    {
      "type": "training",
      "description": "Training step 714",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:13",
      "total_flops_so_far": 1.697933772767232e+16,
      "budget_used_percent": 16.97933772767232
    },
    {
      "type": "training",
      "description": "Training step 715",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:15",
      "total_flops_so_far": 1.7003085053165568e+16,
      "budget_used_percent": 17.003085053165567
    },
    {
      "type": "training",
      "description": "Training step 716",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:16",
      "total_flops_so_far": 1.7026832378658816e+16,
      "budget_used_percent": 17.026832378658817
    },
    {
      "type": "training",
      "description": "Training step 717",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:17",
      "total_flops_so_far": 1.7050579704152064e+16,
      "budget_used_percent": 17.050579704152064
    },
    {
      "type": "training",
      "description": "Training step 718",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:18",
      "total_flops_so_far": 1.7074327029645312e+16,
      "budget_used_percent": 17.07432702964531
    },
    {
      "type": "training",
      "description": "Training step 719",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:20",
      "total_flops_so_far": 1.709807435513856e+16,
      "budget_used_percent": 17.09807435513856
    },
    {
      "type": "training",
      "description": "Training step 720",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:21",
      "total_flops_so_far": 1.7121821680631808e+16,
      "budget_used_percent": 17.121821680631808
    },
    {
      "type": "training",
      "description": "Training step 721",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:22",
      "total_flops_so_far": 1.7145569006125056e+16,
      "budget_used_percent": 17.145569006125054
    },
    {
      "type": "training",
      "description": "Training step 722",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:24",
      "total_flops_so_far": 1.7169316331618304e+16,
      "budget_used_percent": 17.169316331618305
    },
    {
      "type": "training",
      "description": "Training step 723",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:25",
      "total_flops_so_far": 1.7193063657111552e+16,
      "budget_used_percent": 17.19306365711155
    },
    {
      "type": "training",
      "description": "Training step 724",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:26",
      "total_flops_so_far": 1.72168109826048e+16,
      "budget_used_percent": 17.216810982604798
    },
    {
      "type": "training",
      "description": "Training step 725",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:27",
      "total_flops_so_far": 1.7240558308098048e+16,
      "budget_used_percent": 17.24055830809805
    },
    {
      "type": "training",
      "description": "Training step 726",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:29",
      "total_flops_so_far": 1.7264305633591296e+16,
      "budget_used_percent": 17.264305633591295
    },
    {
      "type": "training",
      "description": "Training step 727",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:30",
      "total_flops_so_far": 1.7288052959084544e+16,
      "budget_used_percent": 17.288052959084542
    },
    {
      "type": "training",
      "description": "Training step 728",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:31",
      "total_flops_so_far": 1.7311800284577792e+16,
      "budget_used_percent": 17.311800284577792
    },
    {
      "type": "training",
      "description": "Training step 729",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:33",
      "total_flops_so_far": 1.733554761007104e+16,
      "budget_used_percent": 17.33554761007104
    },
    {
      "type": "training",
      "description": "Training step 730",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:34",
      "total_flops_so_far": 1.7359294935564288e+16,
      "budget_used_percent": 17.359294935564286
    },
    {
      "type": "training",
      "description": "Training step 731",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:35",
      "total_flops_so_far": 1.7383042261057536e+16,
      "budget_used_percent": 17.383042261057536
    },
    {
      "type": "training",
      "description": "Training step 732",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:37",
      "total_flops_so_far": 1.7406789586550784e+16,
      "budget_used_percent": 17.406789586550783
    },
    {
      "type": "training",
      "description": "Training step 733",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:38",
      "total_flops_so_far": 1.7430536912044032e+16,
      "budget_used_percent": 17.43053691204403
    },
    {
      "type": "training",
      "description": "Training step 734",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:39",
      "total_flops_so_far": 1.745428423753728e+16,
      "budget_used_percent": 17.45428423753728
    },
    {
      "type": "training",
      "description": "Training step 735",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:40",
      "total_flops_so_far": 1.7478031563030528e+16,
      "budget_used_percent": 17.478031563030527
    },
    {
      "type": "training",
      "description": "Training step 736",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:42",
      "total_flops_so_far": 1.7501778888523776e+16,
      "budget_used_percent": 17.501778888523774
    },
    {
      "type": "training",
      "description": "Training step 737",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:43",
      "total_flops_so_far": 1.7525526214017024e+16,
      "budget_used_percent": 17.525526214017024
    },
    {
      "type": "training",
      "description": "Training step 738",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:44",
      "total_flops_so_far": 1.7549273539510272e+16,
      "budget_used_percent": 17.54927353951027
    },
    {
      "type": "training",
      "description": "Training step 739",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:46",
      "total_flops_so_far": 1.757302086500352e+16,
      "budget_used_percent": 17.57302086500352
    },
    {
      "type": "training",
      "description": "Training step 740",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:47",
      "total_flops_so_far": 1.7596768190496768e+16,
      "budget_used_percent": 17.596768190496768
    },
    {
      "type": "training",
      "description": "Training step 741",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:48",
      "total_flops_so_far": 1.7620515515990016e+16,
      "budget_used_percent": 17.620515515990014
    },
    {
      "type": "training",
      "description": "Training step 742",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:50",
      "total_flops_so_far": 1.7644262841483264e+16,
      "budget_used_percent": 17.644262841483265
    },
    {
      "type": "training",
      "description": "Training step 743",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:51",
      "total_flops_so_far": 1.7668010166976512e+16,
      "budget_used_percent": 17.66801016697651
    },
    {
      "type": "training",
      "description": "Training step 744",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:52",
      "total_flops_so_far": 1.769175749246976e+16,
      "budget_used_percent": 17.69175749246976
    },
    {
      "type": "training",
      "description": "Training step 745",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:53",
      "total_flops_so_far": 1.7715504817963008e+16,
      "budget_used_percent": 17.71550481796301
    },
    {
      "type": "training",
      "description": "Training step 746",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:55",
      "total_flops_so_far": 1.7739252143456256e+16,
      "budget_used_percent": 17.739252143456255
    },
    {
      "type": "training",
      "description": "Training step 747",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:56",
      "total_flops_so_far": 1.7762999468949504e+16,
      "budget_used_percent": 17.762999468949502
    },
    {
      "type": "training",
      "description": "Training step 748",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:57",
      "total_flops_so_far": 1.7786746794442752e+16,
      "budget_used_percent": 17.786746794442752
    },
    {
      "type": "training",
      "description": "Training step 749",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:58:59",
      "total_flops_so_far": 1.7810494119936e+16,
      "budget_used_percent": 17.810494119936
    },
    {
      "type": "training",
      "description": "Training step 750",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:00",
      "total_flops_so_far": 1.7834241445429248e+16,
      "budget_used_percent": 17.834241445429246
    },
    {
      "type": "training",
      "description": "Training step 751",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:01",
      "total_flops_so_far": 1.7857988770922496e+16,
      "budget_used_percent": 17.857988770922496
    },
    {
      "type": "training",
      "description": "Training step 752",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:02",
      "total_flops_so_far": 1.7881736096415744e+16,
      "budget_used_percent": 17.881736096415743
    },
    {
      "type": "training",
      "description": "Training step 753",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:04",
      "total_flops_so_far": 1.7905483421908992e+16,
      "budget_used_percent": 17.90548342190899
    },
    {
      "type": "training",
      "description": "Training step 754",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:05",
      "total_flops_so_far": 1.792923074740224e+16,
      "budget_used_percent": 17.92923074740224
    },
    {
      "type": "training",
      "description": "Training step 755",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:06",
      "total_flops_so_far": 1.7952978072895488e+16,
      "budget_used_percent": 17.952978072895487
    },
    {
      "type": "training",
      "description": "Training step 756",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:08",
      "total_flops_so_far": 1.7976725398388736e+16,
      "budget_used_percent": 17.976725398388734
    },
    {
      "type": "training",
      "description": "Training step 757",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:09",
      "total_flops_so_far": 1.8000472723881984e+16,
      "budget_used_percent": 18.000472723881984
    },
    {
      "type": "training",
      "description": "Training step 758",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:10",
      "total_flops_so_far": 1.802422004937523e+16,
      "budget_used_percent": 18.02422004937523
    },
    {
      "type": "training",
      "description": "Training step 759",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:12",
      "total_flops_so_far": 1.804796737486848e+16,
      "budget_used_percent": 18.047967374868477
    },
    {
      "type": "training",
      "description": "Training step 760",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:13",
      "total_flops_so_far": 1.807171470036173e+16,
      "budget_used_percent": 18.071714700361728
    },
    {
      "type": "training",
      "description": "Training step 761",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:14",
      "total_flops_so_far": 1.8095462025854976e+16,
      "budget_used_percent": 18.095462025854975
    },
    {
      "type": "training",
      "description": "Training step 762",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:15",
      "total_flops_so_far": 1.8119209351348224e+16,
      "budget_used_percent": 18.11920935134822
    },
    {
      "type": "training",
      "description": "Training step 763",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:17",
      "total_flops_so_far": 1.814295667684147e+16,
      "budget_used_percent": 18.14295667684147
    },
    {
      "type": "training",
      "description": "Training step 764",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:18",
      "total_flops_so_far": 1.816670400233472e+16,
      "budget_used_percent": 18.16670400233472
    },
    {
      "type": "training",
      "description": "Training step 765",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:19",
      "total_flops_so_far": 1.819045132782797e+16,
      "budget_used_percent": 18.190451327827965
    },
    {
      "type": "training",
      "description": "Training step 766",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:21",
      "total_flops_so_far": 1.8214198653321216e+16,
      "budget_used_percent": 18.214198653321215
    },
    {
      "type": "training",
      "description": "Training step 767",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:22",
      "total_flops_so_far": 1.8237945978814464e+16,
      "budget_used_percent": 18.237945978814462
    },
    {
      "type": "training",
      "description": "Training step 768",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:23",
      "total_flops_so_far": 1.826169330430771e+16,
      "budget_used_percent": 18.26169330430771
    },
    {
      "type": "training",
      "description": "Training step 769",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:25",
      "total_flops_so_far": 1.828544062980096e+16,
      "budget_used_percent": 18.28544062980096
    },
    {
      "type": "training",
      "description": "Training step 770",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:26",
      "total_flops_so_far": 1.830918795529421e+16,
      "budget_used_percent": 18.309187955294206
    },
    {
      "type": "training",
      "description": "Training step 771",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:27",
      "total_flops_so_far": 1.8332935280787456e+16,
      "budget_used_percent": 18.332935280787456
    },
    {
      "type": "training",
      "description": "Training step 772",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:29",
      "total_flops_so_far": 1.8356682606280704e+16,
      "budget_used_percent": 18.356682606280707
    },
    {
      "type": "training",
      "description": "Training step 773",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:30",
      "total_flops_so_far": 1.838042993177395e+16,
      "budget_used_percent": 18.380429931773953
    },
    {
      "type": "training",
      "description": "Training step 774",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:31",
      "total_flops_so_far": 1.84041772572672e+16,
      "budget_used_percent": 18.4041772572672
    },
    {
      "type": "training",
      "description": "Training step 775",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:32",
      "total_flops_so_far": 1.842792458276045e+16,
      "budget_used_percent": 18.42792458276045
    },
    {
      "type": "training",
      "description": "Training step 776",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:34",
      "total_flops_so_far": 1.8451671908253696e+16,
      "budget_used_percent": 18.451671908253697
    },
    {
      "type": "training",
      "description": "Training step 777",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:35",
      "total_flops_so_far": 1.8475419233746944e+16,
      "budget_used_percent": 18.475419233746944
    },
    {
      "type": "training",
      "description": "Training step 778",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:36",
      "total_flops_so_far": 1.849916655924019e+16,
      "budget_used_percent": 18.499166559240194
    },
    {
      "type": "training",
      "description": "Training step 779",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:38",
      "total_flops_so_far": 1.852291388473344e+16,
      "budget_used_percent": 18.52291388473344
    },
    {
      "type": "training",
      "description": "Training step 780",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:39",
      "total_flops_so_far": 1.854666121022669e+16,
      "budget_used_percent": 18.546661210226688
    },
    {
      "type": "training",
      "description": "Training step 781",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:40",
      "total_flops_so_far": 1.8570408535719936e+16,
      "budget_used_percent": 18.570408535719938
    },
    {
      "type": "training",
      "description": "Training step 782",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:42",
      "total_flops_so_far": 1.8594155861213184e+16,
      "budget_used_percent": 18.594155861213185
    },
    {
      "type": "training",
      "description": "Training step 783",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:43",
      "total_flops_so_far": 1.861790318670643e+16,
      "budget_used_percent": 18.61790318670643
    },
    {
      "type": "training",
      "description": "Training step 784",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:44",
      "total_flops_so_far": 1.864165051219968e+16,
      "budget_used_percent": 18.641650512199682
    },
    {
      "type": "training",
      "description": "Training step 785",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:45",
      "total_flops_so_far": 1.866539783769293e+16,
      "budget_used_percent": 18.66539783769293
    },
    {
      "type": "training",
      "description": "Training step 786",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:47",
      "total_flops_so_far": 1.8689145163186176e+16,
      "budget_used_percent": 18.689145163186176
    },
    {
      "type": "training",
      "description": "Training step 787",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:48",
      "total_flops_so_far": 1.8712892488679424e+16,
      "budget_used_percent": 18.712892488679426
    },
    {
      "type": "training",
      "description": "Training step 788",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:49",
      "total_flops_so_far": 1.873663981417267e+16,
      "budget_used_percent": 18.736639814172673
    },
    {
      "type": "training",
      "description": "Training step 789",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:51",
      "total_flops_so_far": 1.876038713966592e+16,
      "budget_used_percent": 18.76038713966592
    },
    {
      "type": "training",
      "description": "Training step 790",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:52",
      "total_flops_so_far": 1.878413446515917e+16,
      "budget_used_percent": 18.78413446515917
    },
    {
      "type": "training",
      "description": "Training step 791",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:53",
      "total_flops_so_far": 1.8807881790652416e+16,
      "budget_used_percent": 18.807881790652416
    },
    {
      "type": "training",
      "description": "Training step 792",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:54",
      "total_flops_so_far": 1.8831629116145664e+16,
      "budget_used_percent": 18.831629116145663
    },
    {
      "type": "training",
      "description": "Training step 793",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:55",
      "total_flops_so_far": 1.885537644163891e+16,
      "budget_used_percent": 18.855376441638914
    },
    {
      "type": "training",
      "description": "Training step 794",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:56",
      "total_flops_so_far": 1.887912376713216e+16,
      "budget_used_percent": 18.87912376713216
    },
    {
      "type": "training",
      "description": "Training step 795",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:57",
      "total_flops_so_far": 1.890287109262541e+16,
      "budget_used_percent": 18.90287109262541
    },
    {
      "type": "training",
      "description": "Training step 796",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 05:59:59",
      "total_flops_so_far": 1.8926618418118656e+16,
      "budget_used_percent": 18.926618418118657
    },
    {
      "type": "training",
      "description": "Training step 797",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:00",
      "total_flops_so_far": 1.8950365743611904e+16,
      "budget_used_percent": 18.950365743611904
    },
    {
      "type": "training",
      "description": "Training step 798",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:01",
      "total_flops_so_far": 1.897411306910515e+16,
      "budget_used_percent": 18.974113069105154
    },
    {
      "type": "training",
      "description": "Training step 799",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:03",
      "total_flops_so_far": 1.89978603945984e+16,
      "budget_used_percent": 18.9978603945984
    },
    {
      "type": "training",
      "description": "Training step 800",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:04",
      "total_flops_so_far": 1.902160772009165e+16,
      "budget_used_percent": 19.021607720091648
    },
    {
      "type": "training",
      "description": "Training step 801",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:06",
      "total_flops_so_far": 1.9045355045584896e+16,
      "budget_used_percent": 19.0453550455849
    },
    {
      "type": "training",
      "description": "Training step 802",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:12",
      "total_flops_so_far": 1.9069102371078144e+16,
      "budget_used_percent": 19.069102371078145
    },
    {
      "type": "training",
      "description": "Training step 803",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:13",
      "total_flops_so_far": 1.909284969657139e+16,
      "budget_used_percent": 19.092849696571392
    },
    {
      "type": "training",
      "description": "Training step 804",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:14",
      "total_flops_so_far": 1.911659702206464e+16,
      "budget_used_percent": 19.116597022064642
    },
    {
      "type": "training",
      "description": "Training step 805",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:16",
      "total_flops_so_far": 1.914034434755789e+16,
      "budget_used_percent": 19.14034434755789
    },
    {
      "type": "training",
      "description": "Training step 806",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:17",
      "total_flops_so_far": 1.9164091673051136e+16,
      "budget_used_percent": 19.164091673051136
    },
    {
      "type": "training",
      "description": "Training step 807",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:18",
      "total_flops_so_far": 1.9187838998544384e+16,
      "budget_used_percent": 19.187838998544386
    },
    {
      "type": "training",
      "description": "Training step 808",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:20",
      "total_flops_so_far": 1.921158632403763e+16,
      "budget_used_percent": 19.211586324037633
    },
    {
      "type": "training",
      "description": "Training step 809",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:21",
      "total_flops_so_far": 1.923533364953088e+16,
      "budget_used_percent": 19.23533364953088
    },
    {
      "type": "training",
      "description": "Training step 810",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:22",
      "total_flops_so_far": 1.925908097502413e+16,
      "budget_used_percent": 19.25908097502413
    },
    {
      "type": "training",
      "description": "Training step 811",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:23",
      "total_flops_so_far": 1.9282828300517376e+16,
      "budget_used_percent": 19.282828300517377
    },
    {
      "type": "training",
      "description": "Training step 812",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:25",
      "total_flops_so_far": 1.9306575626010624e+16,
      "budget_used_percent": 19.306575626010623
    },
    {
      "type": "training",
      "description": "Training step 813",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:26",
      "total_flops_so_far": 1.933032295150387e+16,
      "budget_used_percent": 19.330322951503874
    },
    {
      "type": "training",
      "description": "Training step 814",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:27",
      "total_flops_so_far": 1.935407027699712e+16,
      "budget_used_percent": 19.35407027699712
    },
    {
      "type": "training",
      "description": "Training step 815",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:29",
      "total_flops_so_far": 1.937781760249037e+16,
      "budget_used_percent": 19.377817602490367
    },
    {
      "type": "training",
      "description": "Training step 816",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:30",
      "total_flops_so_far": 1.9401564927983616e+16,
      "budget_used_percent": 19.401564927983618
    },
    {
      "type": "training",
      "description": "Training step 817",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:31",
      "total_flops_so_far": 1.9425312253476864e+16,
      "budget_used_percent": 19.425312253476864
    },
    {
      "type": "training",
      "description": "Training step 818",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:33",
      "total_flops_so_far": 1.944905957897011e+16,
      "budget_used_percent": 19.44905957897011
    },
    {
      "type": "training",
      "description": "Training step 819",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:34",
      "total_flops_so_far": 1.947280690446336e+16,
      "budget_used_percent": 19.47280690446336
    },
    {
      "type": "training",
      "description": "Training step 820",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:35",
      "total_flops_so_far": 1.949655422995661e+16,
      "budget_used_percent": 19.496554229956608
    },
    {
      "type": "training",
      "description": "Training step 821",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:36",
      "total_flops_so_far": 1.9520301555449856e+16,
      "budget_used_percent": 19.520301555449855
    },
    {
      "type": "training",
      "description": "Training step 822",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:38",
      "total_flops_so_far": 1.9544048880943104e+16,
      "budget_used_percent": 19.544048880943105
    },
    {
      "type": "training",
      "description": "Training step 823",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:39",
      "total_flops_so_far": 1.956779620643635e+16,
      "budget_used_percent": 19.567796206436352
    },
    {
      "type": "training",
      "description": "Training step 824",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:40",
      "total_flops_so_far": 1.95915435319296e+16,
      "budget_used_percent": 19.591543531929602
    },
    {
      "type": "training",
      "description": "Training step 825",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:42",
      "total_flops_so_far": 1.961529085742285e+16,
      "budget_used_percent": 19.61529085742285
    },
    {
      "type": "training",
      "description": "Training step 826",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:43",
      "total_flops_so_far": 1.9639038182916096e+16,
      "budget_used_percent": 19.639038182916096
    },
    {
      "type": "training",
      "description": "Training step 827",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:44",
      "total_flops_so_far": 1.9662785508409344e+16,
      "budget_used_percent": 19.662785508409346
    },
    {
      "type": "training",
      "description": "Training step 828",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:46",
      "total_flops_so_far": 1.968653283390259e+16,
      "budget_used_percent": 19.686532833902593
    },
    {
      "type": "training",
      "description": "Training step 829",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:47",
      "total_flops_so_far": 1.971028015939584e+16,
      "budget_used_percent": 19.71028015939584
    },
    {
      "type": "training",
      "description": "Training step 830",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:48",
      "total_flops_so_far": 1.973402748488909e+16,
      "budget_used_percent": 19.73402748488909
    },
    {
      "type": "training",
      "description": "Training step 831",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:49",
      "total_flops_so_far": 1.9757774810382336e+16,
      "budget_used_percent": 19.757774810382337
    },
    {
      "type": "training",
      "description": "Training step 832",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:51",
      "total_flops_so_far": 1.9781522135875584e+16,
      "budget_used_percent": 19.781522135875584
    },
    {
      "type": "training",
      "description": "Training step 833",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:52",
      "total_flops_so_far": 1.980526946136883e+16,
      "budget_used_percent": 19.805269461368834
    },
    {
      "type": "training",
      "description": "Training step 834",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:53",
      "total_flops_so_far": 1.982901678686208e+16,
      "budget_used_percent": 19.82901678686208
    },
    {
      "type": "training",
      "description": "Training step 835",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:55",
      "total_flops_so_far": 1.985276411235533e+16,
      "budget_used_percent": 19.852764112355327
    },
    {
      "type": "training",
      "description": "Training step 836",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:56",
      "total_flops_so_far": 1.9876511437848576e+16,
      "budget_used_percent": 19.876511437848578
    },
    {
      "type": "training",
      "description": "Training step 837",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:57",
      "total_flops_so_far": 1.9900258763341824e+16,
      "budget_used_percent": 19.900258763341824
    },
    {
      "type": "training",
      "description": "Training step 838",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:00:59",
      "total_flops_so_far": 1.992400608883507e+16,
      "budget_used_percent": 19.92400608883507
    },
    {
      "type": "training",
      "description": "Training step 839",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:00",
      "total_flops_so_far": 1.994775341432832e+16,
      "budget_used_percent": 19.94775341432832
    },
    {
      "type": "training",
      "description": "Training step 840",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:01",
      "total_flops_so_far": 1.997150073982157e+16,
      "budget_used_percent": 19.97150073982157
    },
    {
      "type": "training",
      "description": "Training step 841",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:02",
      "total_flops_so_far": 1.9995248065314816e+16,
      "budget_used_percent": 19.995248065314815
    },
    {
      "type": "training",
      "description": "Training step 842",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:04",
      "total_flops_so_far": 2.0018995390808064e+16,
      "budget_used_percent": 20.018995390808065
    },
    {
      "type": "training",
      "description": "Training step 843",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:05",
      "total_flops_so_far": 2.004274271630131e+16,
      "budget_used_percent": 20.042742716301312
    },
    {
      "type": "training",
      "description": "Training step 844",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:06",
      "total_flops_so_far": 2.006649004179456e+16,
      "budget_used_percent": 20.06649004179456
    },
    {
      "type": "training",
      "description": "Training step 845",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:08",
      "total_flops_so_far": 2.009023736728781e+16,
      "budget_used_percent": 20.09023736728781
    },
    {
      "type": "training",
      "description": "Training step 846",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:09",
      "total_flops_so_far": 2.0113984692781056e+16,
      "budget_used_percent": 20.113984692781056
    },
    {
      "type": "training",
      "description": "Training step 847",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:10",
      "total_flops_so_far": 2.0137732018274304e+16,
      "budget_used_percent": 20.137732018274303
    },
    {
      "type": "training",
      "description": "Training step 848",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:12",
      "total_flops_so_far": 2.016147934376755e+16,
      "budget_used_percent": 20.161479343767553
    },
    {
      "type": "training",
      "description": "Training step 849",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:13",
      "total_flops_so_far": 2.01852266692608e+16,
      "budget_used_percent": 20.1852266692608
    },
    {
      "type": "training",
      "description": "Training step 850",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:14",
      "total_flops_so_far": 2.020897399475405e+16,
      "budget_used_percent": 20.208973994754047
    },
    {
      "type": "training",
      "description": "Training step 851",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:15",
      "total_flops_so_far": 2.0232721320247296e+16,
      "budget_used_percent": 20.232721320247297
    },
    {
      "type": "training",
      "description": "Training step 852",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:17",
      "total_flops_so_far": 2.0256468645740544e+16,
      "budget_used_percent": 20.256468645740544
    },
    {
      "type": "training",
      "description": "Training step 853",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:18",
      "total_flops_so_far": 2.028021597123379e+16,
      "budget_used_percent": 20.28021597123379
    },
    {
      "type": "training",
      "description": "Training step 854",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:19",
      "total_flops_so_far": 2.030396329672704e+16,
      "budget_used_percent": 20.30396329672704
    },
    {
      "type": "training",
      "description": "Training step 855",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:21",
      "total_flops_so_far": 2.032771062222029e+16,
      "budget_used_percent": 20.327710622220287
    },
    {
      "type": "training",
      "description": "Training step 856",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:22",
      "total_flops_so_far": 2.0351457947713536e+16,
      "budget_used_percent": 20.351457947713534
    },
    {
      "type": "training",
      "description": "Training step 857",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:23",
      "total_flops_so_far": 2.0375205273206784e+16,
      "budget_used_percent": 20.375205273206785
    },
    {
      "type": "training",
      "description": "Training step 858",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:25",
      "total_flops_so_far": 2.039895259870003e+16,
      "budget_used_percent": 20.39895259870003
    },
    {
      "type": "training",
      "description": "Training step 859",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:26",
      "total_flops_so_far": 2.042269992419328e+16,
      "budget_used_percent": 20.42269992419328
    },
    {
      "type": "training",
      "description": "Training step 860",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:27",
      "total_flops_so_far": 2.044644724968653e+16,
      "budget_used_percent": 20.44644724968653
    },
    {
      "type": "training",
      "description": "Training step 861",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:28",
      "total_flops_so_far": 2.0470194575179776e+16,
      "budget_used_percent": 20.470194575179775
    },
    {
      "type": "training",
      "description": "Training step 862",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:30",
      "total_flops_so_far": 2.0493941900673024e+16,
      "budget_used_percent": 20.493941900673025
    },
    {
      "type": "training",
      "description": "Training step 863",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:31",
      "total_flops_so_far": 2.051768922616627e+16,
      "budget_used_percent": 20.517689226166272
    },
    {
      "type": "training",
      "description": "Training step 864",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:32",
      "total_flops_so_far": 2.054143655165952e+16,
      "budget_used_percent": 20.54143655165952
    },
    {
      "type": "training",
      "description": "Training step 865",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:34",
      "total_flops_so_far": 2.056518387715277e+16,
      "budget_used_percent": 20.56518387715277
    },
    {
      "type": "training",
      "description": "Training step 866",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:35",
      "total_flops_so_far": 2.0588931202646016e+16,
      "budget_used_percent": 20.588931202646016
    },
    {
      "type": "training",
      "description": "Training step 867",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:36",
      "total_flops_so_far": 2.0612678528139264e+16,
      "budget_used_percent": 20.612678528139263
    },
    {
      "type": "training",
      "description": "Training step 868",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:38",
      "total_flops_so_far": 2.063642585363251e+16,
      "budget_used_percent": 20.636425853632513
    },
    {
      "type": "training",
      "description": "Training step 869",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:39",
      "total_flops_so_far": 2.066017317912576e+16,
      "budget_used_percent": 20.66017317912576
    },
    {
      "type": "training",
      "description": "Training step 870",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:40",
      "total_flops_so_far": 2.068392050461901e+16,
      "budget_used_percent": 20.683920504619007
    },
    {
      "type": "training",
      "description": "Training step 871",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:41",
      "total_flops_so_far": 2.0707667830112256e+16,
      "budget_used_percent": 20.707667830112257
    },
    {
      "type": "training",
      "description": "Training step 872",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:43",
      "total_flops_so_far": 2.0731415155605504e+16,
      "budget_used_percent": 20.731415155605504
    },
    {
      "type": "training",
      "description": "Training step 873",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:44",
      "total_flops_so_far": 2.075516248109875e+16,
      "budget_used_percent": 20.75516248109875
    },
    {
      "type": "training",
      "description": "Training step 874",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:45",
      "total_flops_so_far": 2.0778909806592e+16,
      "budget_used_percent": 20.778909806592
    },
    {
      "type": "training",
      "description": "Training step 875",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:47",
      "total_flops_so_far": 2.080265713208525e+16,
      "budget_used_percent": 20.802657132085248
    },
    {
      "type": "training",
      "description": "Training step 876",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:48",
      "total_flops_so_far": 2.0826404457578496e+16,
      "budget_used_percent": 20.826404457578494
    },
    {
      "type": "training",
      "description": "Training step 877",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:49",
      "total_flops_so_far": 2.0850151783071744e+16,
      "budget_used_percent": 20.850151783071745
    },
    {
      "type": "training",
      "description": "Training step 878",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:51",
      "total_flops_so_far": 2.087389910856499e+16,
      "budget_used_percent": 20.87389910856499
    },
    {
      "type": "training",
      "description": "Training step 879",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:52",
      "total_flops_so_far": 2.089764643405824e+16,
      "budget_used_percent": 20.897646434058238
    },
    {
      "type": "training",
      "description": "Training step 880",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:53",
      "total_flops_so_far": 2.092139375955149e+16,
      "budget_used_percent": 20.92139375955149
    },
    {
      "type": "training",
      "description": "Training step 881",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:54",
      "total_flops_so_far": 2.0945141085044736e+16,
      "budget_used_percent": 20.945141085044735
    },
    {
      "type": "training",
      "description": "Training step 882",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:56",
      "total_flops_so_far": 2.0968888410537984e+16,
      "budget_used_percent": 20.968888410537982
    },
    {
      "type": "training",
      "description": "Training step 883",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:57",
      "total_flops_so_far": 2.099263573603123e+16,
      "budget_used_percent": 20.992635736031232
    },
    {
      "type": "training",
      "description": "Training step 884",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:01:58",
      "total_flops_so_far": 2.101638306152448e+16,
      "budget_used_percent": 21.01638306152448
    },
    {
      "type": "training",
      "description": "Training step 885",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:00",
      "total_flops_so_far": 2.104013038701773e+16,
      "budget_used_percent": 21.040130387017726
    },
    {
      "type": "training",
      "description": "Training step 886",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:01",
      "total_flops_so_far": 2.1063877712510976e+16,
      "budget_used_percent": 21.063877712510976
    },
    {
      "type": "training",
      "description": "Training step 887",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:02",
      "total_flops_so_far": 2.1087625038004224e+16,
      "budget_used_percent": 21.087625038004223
    },
    {
      "type": "training",
      "description": "Training step 888",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:04",
      "total_flops_so_far": 2.111137236349747e+16,
      "budget_used_percent": 21.111372363497473
    },
    {
      "type": "training",
      "description": "Training step 889",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:05",
      "total_flops_so_far": 2.113511968899072e+16,
      "budget_used_percent": 21.13511968899072
    },
    {
      "type": "training",
      "description": "Training step 890",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:06",
      "total_flops_so_far": 2.115886701448397e+16,
      "budget_used_percent": 21.158867014483967
    },
    {
      "type": "training",
      "description": "Training step 891",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:07",
      "total_flops_so_far": 2.1182614339977216e+16,
      "budget_used_percent": 21.182614339977217
    },
    {
      "type": "training",
      "description": "Training step 892",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:09",
      "total_flops_so_far": 2.1206361665470464e+16,
      "budget_used_percent": 21.206361665470464
    },
    {
      "type": "training",
      "description": "Training step 893",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:10",
      "total_flops_so_far": 2.123010899096371e+16,
      "budget_used_percent": 21.23010899096371
    },
    {
      "type": "training",
      "description": "Training step 894",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:12",
      "total_flops_so_far": 2.125385631645696e+16,
      "budget_used_percent": 21.25385631645696
    },
    {
      "type": "training",
      "description": "Training step 895",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:14",
      "total_flops_so_far": 2.127760364195021e+16,
      "budget_used_percent": 21.277603641950208
    },
    {
      "type": "training",
      "description": "Training step 896",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:15",
      "total_flops_so_far": 2.1301350967443456e+16,
      "budget_used_percent": 21.301350967443454
    },
    {
      "type": "training",
      "description": "Training step 897",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:16",
      "total_flops_so_far": 2.1325098292936704e+16,
      "budget_used_percent": 21.325098292936705
    },
    {
      "type": "training",
      "description": "Training step 898",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:18",
      "total_flops_so_far": 2.134884561842995e+16,
      "budget_used_percent": 21.34884561842995
    },
    {
      "type": "training",
      "description": "Training step 899",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:19",
      "total_flops_so_far": 2.13725929439232e+16,
      "budget_used_percent": 21.3725929439232
    },
    {
      "type": "training",
      "description": "Training step 900",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:20",
      "total_flops_so_far": 2.139634026941645e+16,
      "budget_used_percent": 21.39634026941645
    },
    {
      "type": "training",
      "description": "Training step 901",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:21",
      "total_flops_so_far": 2.1420087594909696e+16,
      "budget_used_percent": 21.420087594909695
    },
    {
      "type": "training",
      "description": "Training step 902",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:23",
      "total_flops_so_far": 2.1443834920402944e+16,
      "budget_used_percent": 21.443834920402942
    },
    {
      "type": "training",
      "description": "Training step 903",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:24",
      "total_flops_so_far": 2.146758224589619e+16,
      "budget_used_percent": 21.467582245896192
    },
    {
      "type": "training",
      "description": "Training step 904",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:25",
      "total_flops_so_far": 2.149132957138944e+16,
      "budget_used_percent": 21.49132957138944
    },
    {
      "type": "training",
      "description": "Training step 905",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:27",
      "total_flops_so_far": 2.151507689688269e+16,
      "budget_used_percent": 21.515076896882686
    },
    {
      "type": "training",
      "description": "Training step 906",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:28",
      "total_flops_so_far": 2.1538824222375936e+16,
      "budget_used_percent": 21.538824222375936
    },
    {
      "type": "training",
      "description": "Training step 907",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:29",
      "total_flops_so_far": 2.1562571547869184e+16,
      "budget_used_percent": 21.562571547869183
    },
    {
      "type": "training",
      "description": "Training step 908",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:31",
      "total_flops_so_far": 2.158631887336243e+16,
      "budget_used_percent": 21.58631887336243
    },
    {
      "type": "training",
      "description": "Training step 909",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:32",
      "total_flops_so_far": 2.161006619885568e+16,
      "budget_used_percent": 21.61006619885568
    },
    {
      "type": "training",
      "description": "Training step 910",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:33",
      "total_flops_so_far": 2.163381352434893e+16,
      "budget_used_percent": 21.633813524348927
    },
    {
      "type": "training",
      "description": "Training step 911",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:34",
      "total_flops_so_far": 2.1657560849842176e+16,
      "budget_used_percent": 21.657560849842174
    },
    {
      "type": "training",
      "description": "Training step 912",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:36",
      "total_flops_so_far": 2.1681308175335424e+16,
      "budget_used_percent": 21.681308175335424
    },
    {
      "type": "training",
      "description": "Training step 913",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:37",
      "total_flops_so_far": 2.170505550082867e+16,
      "budget_used_percent": 21.70505550082867
    },
    {
      "type": "training",
      "description": "Training step 914",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:38",
      "total_flops_so_far": 2.172880282632192e+16,
      "budget_used_percent": 21.728802826321918
    },
    {
      "type": "training",
      "description": "Training step 915",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:40",
      "total_flops_so_far": 2.175255015181517e+16,
      "budget_used_percent": 21.752550151815168
    },
    {
      "type": "training",
      "description": "Training step 916",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:41",
      "total_flops_so_far": 2.1776297477308416e+16,
      "budget_used_percent": 21.776297477308415
    },
    {
      "type": "training",
      "description": "Training step 917",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:42",
      "total_flops_so_far": 2.1800044802801664e+16,
      "budget_used_percent": 21.80004480280166
    },
    {
      "type": "training",
      "description": "Training step 918",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:44",
      "total_flops_so_far": 2.182379212829491e+16,
      "budget_used_percent": 21.82379212829491
    },
    {
      "type": "training",
      "description": "Training step 919",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:45",
      "total_flops_so_far": 2.184753945378816e+16,
      "budget_used_percent": 21.84753945378816
    },
    {
      "type": "training",
      "description": "Training step 920",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:46",
      "total_flops_so_far": 2.187128677928141e+16,
      "budget_used_percent": 21.871286779281405
    },
    {
      "type": "training",
      "description": "Training step 921",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:47",
      "total_flops_so_far": 2.1895034104774656e+16,
      "budget_used_percent": 21.895034104774656
    },
    {
      "type": "training",
      "description": "Training step 922",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:49",
      "total_flops_so_far": 2.1918781430267904e+16,
      "budget_used_percent": 21.918781430267902
    },
    {
      "type": "training",
      "description": "Training step 923",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:50",
      "total_flops_so_far": 2.194252875576115e+16,
      "budget_used_percent": 21.942528755761153
    },
    {
      "type": "training",
      "description": "Training step 924",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:50",
      "total_flops_so_far": 2.19662760812544e+16,
      "budget_used_percent": 21.9662760812544
    },
    {
      "type": "training",
      "description": "Training step 925",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:52",
      "total_flops_so_far": 2.199002340674765e+16,
      "budget_used_percent": 21.990023406747646
    },
    {
      "type": "training",
      "description": "Training step 926",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:53",
      "total_flops_so_far": 2.2013770732240896e+16,
      "budget_used_percent": 22.013770732240896
    },
    {
      "type": "training",
      "description": "Training step 927",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:54",
      "total_flops_so_far": 2.2037518057734144e+16,
      "budget_used_percent": 22.037518057734143
    },
    {
      "type": "training",
      "description": "Training step 928",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:56",
      "total_flops_so_far": 2.206126538322739e+16,
      "budget_used_percent": 22.06126538322739
    },
    {
      "type": "training",
      "description": "Training step 929",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:57",
      "total_flops_so_far": 2.208501270872064e+16,
      "budget_used_percent": 22.08501270872064
    },
    {
      "type": "training",
      "description": "Training step 930",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:02:58",
      "total_flops_so_far": 2.210876003421389e+16,
      "budget_used_percent": 22.108760034213887
    },
    {
      "type": "training",
      "description": "Training step 931",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:00",
      "total_flops_so_far": 2.2132507359707136e+16,
      "budget_used_percent": 22.132507359707134
    },
    {
      "type": "training",
      "description": "Training step 932",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:01",
      "total_flops_so_far": 2.2156254685200384e+16,
      "budget_used_percent": 22.156254685200384
    },
    {
      "type": "training",
      "description": "Training step 933",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:02",
      "total_flops_so_far": 2.218000201069363e+16,
      "budget_used_percent": 22.18000201069363
    },
    {
      "type": "training",
      "description": "Training step 934",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:03",
      "total_flops_so_far": 2.220374933618688e+16,
      "budget_used_percent": 22.203749336186878
    },
    {
      "type": "training",
      "description": "Training step 935",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:05",
      "total_flops_so_far": 2.222749666168013e+16,
      "budget_used_percent": 22.227496661680128
    },
    {
      "type": "training",
      "description": "Training step 936",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:06",
      "total_flops_so_far": 2.2251243987173376e+16,
      "budget_used_percent": 22.251243987173375
    },
    {
      "type": "training",
      "description": "Training step 937",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:07",
      "total_flops_so_far": 2.2274991312666624e+16,
      "budget_used_percent": 22.27499131266662
    },
    {
      "type": "training",
      "description": "Training step 938",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:09",
      "total_flops_so_far": 2.229873863815987e+16,
      "budget_used_percent": 22.298738638159872
    },
    {
      "type": "training",
      "description": "Training step 939",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:10",
      "total_flops_so_far": 2.232248596365312e+16,
      "budget_used_percent": 22.32248596365312
    },
    {
      "type": "training",
      "description": "Training step 940",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:11",
      "total_flops_so_far": 2.234623328914637e+16,
      "budget_used_percent": 22.346233289146365
    },
    {
      "type": "training",
      "description": "Training step 941",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:13",
      "total_flops_so_far": 2.2369980614639616e+16,
      "budget_used_percent": 22.369980614639616
    },
    {
      "type": "training",
      "description": "Training step 942",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:14",
      "total_flops_so_far": 2.2393727940132864e+16,
      "budget_used_percent": 22.393727940132866
    },
    {
      "type": "training",
      "description": "Training step 943",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:15",
      "total_flops_so_far": 2.241747526562611e+16,
      "budget_used_percent": 22.417475265626113
    },
    {
      "type": "training",
      "description": "Training step 944",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:16",
      "total_flops_so_far": 2.244122259111936e+16,
      "budget_used_percent": 22.441222591119363
    },
    {
      "type": "training",
      "description": "Training step 945",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:18",
      "total_flops_so_far": 2.246496991661261e+16,
      "budget_used_percent": 22.46496991661261
    },
    {
      "type": "training",
      "description": "Training step 946",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:19",
      "total_flops_so_far": 2.2488717242105856e+16,
      "budget_used_percent": 22.488717242105857
    },
    {
      "type": "training",
      "description": "Training step 947",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:20",
      "total_flops_so_far": 2.2512464567599104e+16,
      "budget_used_percent": 22.512464567599107
    },
    {
      "type": "training",
      "description": "Training step 948",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:22",
      "total_flops_so_far": 2.253621189309235e+16,
      "budget_used_percent": 22.536211893092354
    },
    {
      "type": "training",
      "description": "Training step 949",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:23",
      "total_flops_so_far": 2.25599592185856e+16,
      "budget_used_percent": 22.5599592185856
    },
    {
      "type": "training",
      "description": "Training step 950",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:24",
      "total_flops_so_far": 2.258370654407885e+16,
      "budget_used_percent": 22.58370654407885
    },
    {
      "type": "training",
      "description": "Training step 951",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:26",
      "total_flops_so_far": 2.2607453869572096e+16,
      "budget_used_percent": 22.607453869572097
    },
    {
      "type": "training",
      "description": "Training step 952",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:27",
      "total_flops_so_far": 2.2631201195065344e+16,
      "budget_used_percent": 22.631201195065344
    },
    {
      "type": "training",
      "description": "Training step 953",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:28",
      "total_flops_so_far": 2.265494852055859e+16,
      "budget_used_percent": 22.654948520558595
    },
    {
      "type": "training",
      "description": "Training step 954",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:29",
      "total_flops_so_far": 2.267869584605184e+16,
      "budget_used_percent": 22.67869584605184
    },
    {
      "type": "training",
      "description": "Training step 955",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:31",
      "total_flops_so_far": 2.270244317154509e+16,
      "budget_used_percent": 22.702443171545088
    },
    {
      "type": "training",
      "description": "Training step 956",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:32",
      "total_flops_so_far": 2.2726190497038336e+16,
      "budget_used_percent": 22.72619049703834
    },
    {
      "type": "training",
      "description": "Training step 957",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:33",
      "total_flops_so_far": 2.2749937822531584e+16,
      "budget_used_percent": 22.749937822531585
    },
    {
      "type": "training",
      "description": "Training step 958",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:35",
      "total_flops_so_far": 2.277368514802483e+16,
      "budget_used_percent": 22.773685148024832
    },
    {
      "type": "training",
      "description": "Training step 959",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:36",
      "total_flops_so_far": 2.279743247351808e+16,
      "budget_used_percent": 22.797432473518082
    },
    {
      "type": "training",
      "description": "Training step 960",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:37",
      "total_flops_so_far": 2.282117979901133e+16,
      "budget_used_percent": 22.82117979901133
    },
    {
      "type": "training",
      "description": "Training step 961",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:39",
      "total_flops_so_far": 2.2844927124504576e+16,
      "budget_used_percent": 22.844927124504576
    },
    {
      "type": "training",
      "description": "Training step 962",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:40",
      "total_flops_so_far": 2.2868674449997824e+16,
      "budget_used_percent": 22.868674449997826
    },
    {
      "type": "training",
      "description": "Training step 963",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:41",
      "total_flops_so_far": 2.289242177549107e+16,
      "budget_used_percent": 22.892421775491073
    },
    {
      "type": "training",
      "description": "Training step 964",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:43",
      "total_flops_so_far": 2.291616910098432e+16,
      "budget_used_percent": 22.91616910098432
    },
    {
      "type": "training",
      "description": "Training step 965",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:44",
      "total_flops_so_far": 2.293991642647757e+16,
      "budget_used_percent": 22.93991642647757
    },
    {
      "type": "training",
      "description": "Training step 966",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:45",
      "total_flops_so_far": 2.2963663751970816e+16,
      "budget_used_percent": 22.963663751970817
    },
    {
      "type": "training",
      "description": "Training step 967",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:46",
      "total_flops_so_far": 2.2987411077464064e+16,
      "budget_used_percent": 22.987411077464063
    },
    {
      "type": "training",
      "description": "Training step 968",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:48",
      "total_flops_so_far": 2.301115840295731e+16,
      "budget_used_percent": 23.011158402957314
    },
    {
      "type": "training",
      "description": "Training step 969",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:49",
      "total_flops_so_far": 2.303490572845056e+16,
      "budget_used_percent": 23.03490572845056
    },
    {
      "type": "training",
      "description": "Training step 970",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:50",
      "total_flops_so_far": 2.305865305394381e+16,
      "budget_used_percent": 23.058653053943807
    },
    {
      "type": "training",
      "description": "Training step 971",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:52",
      "total_flops_so_far": 2.3082400379437056e+16,
      "budget_used_percent": 23.082400379437058
    },
    {
      "type": "training",
      "description": "Training step 972",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:53",
      "total_flops_so_far": 2.3106147704930304e+16,
      "budget_used_percent": 23.106147704930304
    },
    {
      "type": "training",
      "description": "Training step 973",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:54",
      "total_flops_so_far": 2.312989503042355e+16,
      "budget_used_percent": 23.129895030423555
    },
    {
      "type": "training",
      "description": "Training step 974",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:56",
      "total_flops_so_far": 2.31536423559168e+16,
      "budget_used_percent": 23.1536423559168
    },
    {
      "type": "training",
      "description": "Training step 975",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:57",
      "total_flops_so_far": 2.317738968141005e+16,
      "budget_used_percent": 23.177389681410048
    },
    {
      "type": "training",
      "description": "Training step 976",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:58",
      "total_flops_so_far": 2.3201137006903296e+16,
      "budget_used_percent": 23.2011370069033
    },
    {
      "type": "training",
      "description": "Training step 977",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:03:59",
      "total_flops_so_far": 2.3224884332396544e+16,
      "budget_used_percent": 23.224884332396545
    },
    {
      "type": "training",
      "description": "Training step 978",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:01",
      "total_flops_so_far": 2.324863165788979e+16,
      "budget_used_percent": 23.248631657889792
    },
    {
      "type": "training",
      "description": "Training step 979",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:02",
      "total_flops_so_far": 2.327237898338304e+16,
      "budget_used_percent": 23.272378983383042
    },
    {
      "type": "training",
      "description": "Training step 980",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:03",
      "total_flops_so_far": 2.329612630887629e+16,
      "budget_used_percent": 23.29612630887629
    },
    {
      "type": "training",
      "description": "Training step 981",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:05",
      "total_flops_so_far": 2.3319873634369536e+16,
      "budget_used_percent": 23.319873634369536
    },
    {
      "type": "training",
      "description": "Training step 982",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:06",
      "total_flops_so_far": 2.3343620959862784e+16,
      "budget_used_percent": 23.343620959862786
    },
    {
      "type": "training",
      "description": "Training step 983",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:07",
      "total_flops_so_far": 2.336736828535603e+16,
      "budget_used_percent": 23.367368285356033
    },
    {
      "type": "training",
      "description": "Training step 984",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:09",
      "total_flops_so_far": 2.339111561084928e+16,
      "budget_used_percent": 23.39111561084928
    },
    {
      "type": "training",
      "description": "Training step 985",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:10",
      "total_flops_so_far": 2.341486293634253e+16,
      "budget_used_percent": 23.41486293634253
    },
    {
      "type": "training",
      "description": "Training step 986",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:11",
      "total_flops_so_far": 2.3438610261835776e+16,
      "budget_used_percent": 23.438610261835777
    },
    {
      "type": "training",
      "description": "Training step 987",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:12",
      "total_flops_so_far": 2.3462357587329024e+16,
      "budget_used_percent": 23.462357587329024
    },
    {
      "type": "training",
      "description": "Training step 988",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:14",
      "total_flops_so_far": 2.348610491282227e+16,
      "budget_used_percent": 23.486104912822274
    },
    {
      "type": "training",
      "description": "Training step 989",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:15",
      "total_flops_so_far": 2.350985223831552e+16,
      "budget_used_percent": 23.50985223831552
    },
    {
      "type": "training",
      "description": "Training step 990",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:16",
      "total_flops_so_far": 2.353359956380877e+16,
      "budget_used_percent": 23.533599563808767
    },
    {
      "type": "training",
      "description": "Training step 991",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:18",
      "total_flops_so_far": 2.3557346889302016e+16,
      "budget_used_percent": 23.557346889302018
    },
    {
      "type": "training",
      "description": "Training step 992",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:19",
      "total_flops_so_far": 2.3581094214795264e+16,
      "budget_used_percent": 23.581094214795264
    },
    {
      "type": "training",
      "description": "Training step 993",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:20",
      "total_flops_so_far": 2.360484154028851e+16,
      "budget_used_percent": 23.60484154028851
    },
    {
      "type": "training",
      "description": "Training step 994",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:22",
      "total_flops_so_far": 2.362858886578176e+16,
      "budget_used_percent": 23.62858886578176
    },
    {
      "type": "training",
      "description": "Training step 995",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:23",
      "total_flops_so_far": 2.365233619127501e+16,
      "budget_used_percent": 23.65233619127501
    },
    {
      "type": "training",
      "description": "Training step 996",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:24",
      "total_flops_so_far": 2.3676083516768256e+16,
      "budget_used_percent": 23.676083516768255
    },
    {
      "type": "training",
      "description": "Training step 997",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:26",
      "total_flops_so_far": 2.3699830842261504e+16,
      "budget_used_percent": 23.699830842261505
    },
    {
      "type": "training",
      "description": "Training step 998",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:27",
      "total_flops_so_far": 2.372357816775475e+16,
      "budget_used_percent": 23.723578167754752
    },
    {
      "type": "training",
      "description": "Training step 999",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:28",
      "total_flops_so_far": 2.3747325493248e+16,
      "budget_used_percent": 23.747325493248
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710078789056.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:34",
      "total_flops_so_far": 2.3748035572037056e+16,
      "budget_used_percent": 23.748035572037058
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 713780608688.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:39",
      "total_flops_so_far": 2.3748749352645744e+16,
      "budget_used_percent": 23.748749352645742
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711929338680.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:44",
      "total_flops_so_far": 2.3749461281984424e+16,
      "budget_used_percent": 23.749461281984424
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710078789056.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:49",
      "total_flops_so_far": 2.375017136077348e+16,
      "budget_used_percent": 23.750171360773482
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712854883636.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:54",
      "total_flops_so_far": 2.3750884215657116e+16,
      "budget_used_percent": 23.750884215657116
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 5",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710078789056.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:04:59",
      "total_flops_so_far": 2.3751594294446172e+16,
      "budget_used_percent": 23.75159429444617
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 6",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711929338680.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:05",
      "total_flops_so_far": 2.3752306223784852e+16,
      "budget_used_percent": 23.752306223784853
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 7",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711929338680.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:10",
      "total_flops_so_far": 2.3753018153123532e+16,
      "budget_used_percent": 23.75301815312353
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 8",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711929338680.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:15",
      "total_flops_so_far": 2.3753730082462212e+16,
      "budget_used_percent": 23.753730082462212
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 9",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711929338680.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:20",
      "total_flops_so_far": 2.3754442011800892e+16,
      "budget_used_percent": 23.75444201180089
    },
    {
      "type": "training",
      "description": "Training step 1000",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:20",
      "total_flops_so_far": 2.377818933729414e+16,
      "budget_used_percent": 23.77818933729414
    },
    {
      "type": "training",
      "description": "Training step 1001",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:21",
      "total_flops_so_far": 2.3801936662787388e+16,
      "budget_used_percent": 23.801936662787387
    },
    {
      "type": "training",
      "description": "Training step 1002",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:23",
      "total_flops_so_far": 2.3825683988280636e+16,
      "budget_used_percent": 23.825683988280634
    },
    {
      "type": "training",
      "description": "Training step 1003",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:24",
      "total_flops_so_far": 2.3849431313773884e+16,
      "budget_used_percent": 23.849431313773884
    },
    {
      "type": "training",
      "description": "Training step 1004",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:25",
      "total_flops_so_far": 2.3873178639267132e+16,
      "budget_used_percent": 23.87317863926713
    },
    {
      "type": "training",
      "description": "Training step 1005",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:27",
      "total_flops_so_far": 2.389692596476038e+16,
      "budget_used_percent": 23.896925964760378
    },
    {
      "type": "training",
      "description": "Training step 1006",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:28",
      "total_flops_so_far": 2.3920673290253628e+16,
      "budget_used_percent": 23.920673290253628
    },
    {
      "type": "training",
      "description": "Training step 1007",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:29",
      "total_flops_so_far": 2.3944420615746876e+16,
      "budget_used_percent": 23.944420615746875
    },
    {
      "type": "training",
      "description": "Training step 1008",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:31",
      "total_flops_so_far": 2.3968167941240124e+16,
      "budget_used_percent": 23.96816794124012
    },
    {
      "type": "training",
      "description": "Training step 1009",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:32",
      "total_flops_so_far": 2.3991915266733372e+16,
      "budget_used_percent": 23.991915266733372
    },
    {
      "type": "training",
      "description": "Training step 1010",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:33",
      "total_flops_so_far": 2.401566259222662e+16,
      "budget_used_percent": 24.01566259222662
    },
    {
      "type": "training",
      "description": "Training step 1011",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:34",
      "total_flops_so_far": 2.4039409917719868e+16,
      "budget_used_percent": 24.039409917719865
    },
    {
      "type": "training",
      "description": "Training step 1012",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:36",
      "total_flops_so_far": 2.4063157243213116e+16,
      "budget_used_percent": 24.063157243213116
    },
    {
      "type": "training",
      "description": "Training step 1013",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:37",
      "total_flops_so_far": 2.4086904568706364e+16,
      "budget_used_percent": 24.086904568706363
    },
    {
      "type": "training",
      "description": "Training step 1014",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:38",
      "total_flops_so_far": 2.4110651894199612e+16,
      "budget_used_percent": 24.110651894199613
    },
    {
      "type": "training",
      "description": "Training step 1015",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:40",
      "total_flops_so_far": 2.413439921969286e+16,
      "budget_used_percent": 24.13439921969286
    },
    {
      "type": "training",
      "description": "Training step 1016",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:41",
      "total_flops_so_far": 2.4158146545186108e+16,
      "budget_used_percent": 24.158146545186106
    },
    {
      "type": "training",
      "description": "Training step 1017",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:42",
      "total_flops_so_far": 2.4181893870679356e+16,
      "budget_used_percent": 24.181893870679357
    },
    {
      "type": "training",
      "description": "Training step 1018",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:44",
      "total_flops_so_far": 2.4205641196172604e+16,
      "budget_used_percent": 24.205641196172603
    },
    {
      "type": "training",
      "description": "Training step 1019",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:45",
      "total_flops_so_far": 2.4229388521665852e+16,
      "budget_used_percent": 24.22938852166585
    },
    {
      "type": "training",
      "description": "Training step 1020",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:46",
      "total_flops_so_far": 2.42531358471591e+16,
      "budget_used_percent": 24.2531358471591
    },
    {
      "type": "training",
      "description": "Training step 1021",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:48",
      "total_flops_so_far": 2.4276883172652348e+16,
      "budget_used_percent": 24.276883172652347
    },
    {
      "type": "training",
      "description": "Training step 1022",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:49",
      "total_flops_so_far": 2.4300630498145596e+16,
      "budget_used_percent": 24.300630498145594
    },
    {
      "type": "training",
      "description": "Training step 1023",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:50",
      "total_flops_so_far": 2.4324377823638844e+16,
      "budget_used_percent": 24.324377823638844
    },
    {
      "type": "training",
      "description": "Training step 1024",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:51",
      "total_flops_so_far": 2.4348125149132092e+16,
      "budget_used_percent": 24.34812514913209
    },
    {
      "type": "training",
      "description": "Training step 1025",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:53",
      "total_flops_so_far": 2.437187247462534e+16,
      "budget_used_percent": 24.371872474625338
    },
    {
      "type": "training",
      "description": "Training step 1026",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:54",
      "total_flops_so_far": 2.4395619800118588e+16,
      "budget_used_percent": 24.395619800118588
    },
    {
      "type": "training",
      "description": "Training step 1027",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:55",
      "total_flops_so_far": 2.4419367125611836e+16,
      "budget_used_percent": 24.419367125611835
    },
    {
      "type": "training",
      "description": "Training step 1028",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:57",
      "total_flops_so_far": 2.4443114451105084e+16,
      "budget_used_percent": 24.44311445110508
    },
    {
      "type": "training",
      "description": "Training step 1029",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:58",
      "total_flops_so_far": 2.4466861776598332e+16,
      "budget_used_percent": 24.466861776598332
    },
    {
      "type": "training",
      "description": "Training step 1030",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:05:59",
      "total_flops_so_far": 2.449060910209158e+16,
      "budget_used_percent": 24.49060910209158
    },
    {
      "type": "training",
      "description": "Training step 1031",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:01",
      "total_flops_so_far": 2.4514356427584828e+16,
      "budget_used_percent": 24.514356427584826
    },
    {
      "type": "training",
      "description": "Training step 1032",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:02",
      "total_flops_so_far": 2.4538103753078076e+16,
      "budget_used_percent": 24.538103753078076
    },
    {
      "type": "training",
      "description": "Training step 1033",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:03",
      "total_flops_so_far": 2.4561851078571324e+16,
      "budget_used_percent": 24.561851078571323
    },
    {
      "type": "training",
      "description": "Training step 1034",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:04",
      "total_flops_so_far": 2.4585598404064572e+16,
      "budget_used_percent": 24.58559840406457
    },
    {
      "type": "training",
      "description": "Training step 1035",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:06",
      "total_flops_so_far": 2.460934572955782e+16,
      "budget_used_percent": 24.60934572955782
    },
    {
      "type": "training",
      "description": "Training step 1036",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:07",
      "total_flops_so_far": 2.4633093055051068e+16,
      "budget_used_percent": 24.633093055051067
    },
    {
      "type": "training",
      "description": "Training step 1037",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:08",
      "total_flops_so_far": 2.4656840380544316e+16,
      "budget_used_percent": 24.656840380544313
    },
    {
      "type": "training",
      "description": "Training step 1038",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:10",
      "total_flops_so_far": 2.4680587706037564e+16,
      "budget_used_percent": 24.680587706037564
    },
    {
      "type": "training",
      "description": "Training step 1039",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:11",
      "total_flops_so_far": 2.4704335031530812e+16,
      "budget_used_percent": 24.70433503153081
    },
    {
      "type": "training",
      "description": "Training step 1040",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:12",
      "total_flops_so_far": 2.472808235702406e+16,
      "budget_used_percent": 24.728082357024057
    },
    {
      "type": "training",
      "description": "Training step 1041",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:14",
      "total_flops_so_far": 2.4751829682517308e+16,
      "budget_used_percent": 24.751829682517307
    },
    {
      "type": "training",
      "description": "Training step 1042",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:15",
      "total_flops_so_far": 2.4775577008010556e+16,
      "budget_used_percent": 24.775577008010554
    },
    {
      "type": "training",
      "description": "Training step 1043",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:16",
      "total_flops_so_far": 2.4799324333503804e+16,
      "budget_used_percent": 24.799324333503804
    },
    {
      "type": "training",
      "description": "Training step 1044",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:17",
      "total_flops_so_far": 2.4823071658997052e+16,
      "budget_used_percent": 24.82307165899705
    },
    {
      "type": "training",
      "description": "Training step 1045",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:19",
      "total_flops_so_far": 2.48468189844903e+16,
      "budget_used_percent": 24.846818984490298
    },
    {
      "type": "training",
      "description": "Training step 1046",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:20",
      "total_flops_so_far": 2.4870566309983548e+16,
      "budget_used_percent": 24.87056630998355
    },
    {
      "type": "training",
      "description": "Training step 1047",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:21",
      "total_flops_so_far": 2.4894313635476796e+16,
      "budget_used_percent": 24.894313635476795
    },
    {
      "type": "training",
      "description": "Training step 1048",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:23",
      "total_flops_so_far": 2.4918060960970044e+16,
      "budget_used_percent": 24.918060960970042
    },
    {
      "type": "training",
      "description": "Training step 1049",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:24",
      "total_flops_so_far": 2.4941808286463292e+16,
      "budget_used_percent": 24.941808286463292
    },
    {
      "type": "training",
      "description": "Training step 1050",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:25",
      "total_flops_so_far": 2.496555561195654e+16,
      "budget_used_percent": 24.965555611956542
    },
    {
      "type": "training",
      "description": "Training step 1051",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:27",
      "total_flops_so_far": 2.4989302937449788e+16,
      "budget_used_percent": 24.98930293744979
    },
    {
      "type": "training",
      "description": "Training step 1052",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:28",
      "total_flops_so_far": 2.5013050262943036e+16,
      "budget_used_percent": 25.013050262943036
    },
    {
      "type": "training",
      "description": "Training step 1053",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:29",
      "total_flops_so_far": 2.5036797588436284e+16,
      "budget_used_percent": 25.036797588436283
    },
    {
      "type": "training",
      "description": "Training step 1054",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:31",
      "total_flops_so_far": 2.5060544913929532e+16,
      "budget_used_percent": 25.060544913929533
    },
    {
      "type": "training",
      "description": "Training step 1055",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:32",
      "total_flops_so_far": 2.508429223942278e+16,
      "budget_used_percent": 25.08429223942278
    },
    {
      "type": "training",
      "description": "Training step 1056",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:32",
      "total_flops_so_far": 2.5108039564916028e+16,
      "budget_used_percent": 25.10803956491603
    },
    {
      "type": "training",
      "description": "Training step 1057",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:34",
      "total_flops_so_far": 2.5131786890409276e+16,
      "budget_used_percent": 25.131786890409273
    },
    {
      "type": "training",
      "description": "Training step 1058",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:35",
      "total_flops_so_far": 2.5155534215902524e+16,
      "budget_used_percent": 25.155534215902524
    },
    {
      "type": "training",
      "description": "Training step 1059",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:36",
      "total_flops_so_far": 2.5179281541395772e+16,
      "budget_used_percent": 25.17928154139577
    },
    {
      "type": "training",
      "description": "Training step 1060",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:38",
      "total_flops_so_far": 2.520302886688902e+16,
      "budget_used_percent": 25.20302886688902
    },
    {
      "type": "training",
      "description": "Training step 1061",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:39",
      "total_flops_so_far": 2.5226776192382268e+16,
      "budget_used_percent": 25.226776192382268
    },
    {
      "type": "training",
      "description": "Training step 1062",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:40",
      "total_flops_so_far": 2.5250523517875516e+16,
      "budget_used_percent": 25.250523517875518
    },
    {
      "type": "training",
      "description": "Training step 1063",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:42",
      "total_flops_so_far": 2.5274270843368764e+16,
      "budget_used_percent": 25.27427084336876
    },
    {
      "type": "training",
      "description": "Training step 1064",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:43",
      "total_flops_so_far": 2.5298018168862012e+16,
      "budget_used_percent": 25.298018168862015
    },
    {
      "type": "training",
      "description": "Training step 1065",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:44",
      "total_flops_so_far": 2.532176549435526e+16,
      "budget_used_percent": 25.321765494355258
    },
    {
      "type": "training",
      "description": "Training step 1066",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:46",
      "total_flops_so_far": 2.5345512819848508e+16,
      "budget_used_percent": 25.34551281984851
    },
    {
      "type": "training",
      "description": "Training step 1067",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:47",
      "total_flops_so_far": 2.5369260145341756e+16,
      "budget_used_percent": 25.369260145341755
    },
    {
      "type": "training",
      "description": "Training step 1068",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:48",
      "total_flops_so_far": 2.5393007470835004e+16,
      "budget_used_percent": 25.393007470835006
    },
    {
      "type": "training",
      "description": "Training step 1069",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:49",
      "total_flops_so_far": 2.5416754796328252e+16,
      "budget_used_percent": 25.41675479632825
    },
    {
      "type": "training",
      "description": "Training step 1070",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:51",
      "total_flops_so_far": 2.54405021218215e+16,
      "budget_used_percent": 25.440502121821503
    },
    {
      "type": "training",
      "description": "Training step 1071",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:52",
      "total_flops_so_far": 2.5464249447314748e+16,
      "budget_used_percent": 25.464249447314746
    },
    {
      "type": "training",
      "description": "Training step 1072",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:53",
      "total_flops_so_far": 2.5487996772807996e+16,
      "budget_used_percent": 25.487996772807996
    },
    {
      "type": "training",
      "description": "Training step 1073",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:55",
      "total_flops_so_far": 2.5511744098301244e+16,
      "budget_used_percent": 25.511744098301243
    },
    {
      "type": "training",
      "description": "Training step 1074",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:56",
      "total_flops_so_far": 2.5535491423794492e+16,
      "budget_used_percent": 25.535491423794493
    },
    {
      "type": "training",
      "description": "Training step 1075",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:57",
      "total_flops_so_far": 2.555923874928774e+16,
      "budget_used_percent": 25.559238749287736
    },
    {
      "type": "training",
      "description": "Training step 1076",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:06:59",
      "total_flops_so_far": 2.5582986074780988e+16,
      "budget_used_percent": 25.58298607478099
    },
    {
      "type": "training",
      "description": "Training step 1077",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:00",
      "total_flops_so_far": 2.5606733400274236e+16,
      "budget_used_percent": 25.606733400274234
    },
    {
      "type": "training",
      "description": "Training step 1078",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:01",
      "total_flops_so_far": 2.5630480725767484e+16,
      "budget_used_percent": 25.630480725767484
    },
    {
      "type": "training",
      "description": "Training step 1079",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:03",
      "total_flops_so_far": 2.5654228051260732e+16,
      "budget_used_percent": 25.65422805126073
    },
    {
      "type": "training",
      "description": "Training step 1080",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:04",
      "total_flops_so_far": 2.567797537675398e+16,
      "budget_used_percent": 25.67797537675398
    },
    {
      "type": "training",
      "description": "Training step 1081",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:05",
      "total_flops_so_far": 2.5701722702247228e+16,
      "budget_used_percent": 25.701722702247228
    },
    {
      "type": "training",
      "description": "Training step 1082",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:06",
      "total_flops_so_far": 2.5725470027740476e+16,
      "budget_used_percent": 25.725470027740478
    },
    {
      "type": "training",
      "description": "Training step 1083",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:08",
      "total_flops_so_far": 2.5749217353233724e+16,
      "budget_used_percent": 25.74921735323372
    },
    {
      "type": "training",
      "description": "Training step 1084",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:10",
      "total_flops_so_far": 2.5772964678726972e+16,
      "budget_used_percent": 25.77296467872697
    },
    {
      "type": "training",
      "description": "Training step 1085",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:12",
      "total_flops_so_far": 2.579671200422022e+16,
      "budget_used_percent": 25.79671200422022
    },
    {
      "type": "training",
      "description": "Training step 1086",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:13",
      "total_flops_so_far": 2.5820459329713468e+16,
      "budget_used_percent": 25.82045932971347
    },
    {
      "type": "training",
      "description": "Training step 1087",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:14",
      "total_flops_so_far": 2.5844206655206716e+16,
      "budget_used_percent": 25.844206655206715
    },
    {
      "type": "training",
      "description": "Training step 1088",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:15",
      "total_flops_so_far": 2.5867953980699964e+16,
      "budget_used_percent": 25.867953980699966
    },
    {
      "type": "training",
      "description": "Training step 1089",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:17",
      "total_flops_so_far": 2.5891701306193212e+16,
      "budget_used_percent": 25.89170130619321
    },
    {
      "type": "training",
      "description": "Training step 1090",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:18",
      "total_flops_so_far": 2.591544863168646e+16,
      "budget_used_percent": 25.91544863168646
    },
    {
      "type": "training",
      "description": "Training step 1091",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:19",
      "total_flops_so_far": 2.5939195957179708e+16,
      "budget_used_percent": 25.939195957179706
    },
    {
      "type": "training",
      "description": "Training step 1092",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:21",
      "total_flops_so_far": 2.5962943282672956e+16,
      "budget_used_percent": 25.962943282672956
    },
    {
      "type": "training",
      "description": "Training step 1093",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:22",
      "total_flops_so_far": 2.5986690608166204e+16,
      "budget_used_percent": 25.986690608166203
    },
    {
      "type": "training",
      "description": "Training step 1094",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:23",
      "total_flops_so_far": 2.6010437933659452e+16,
      "budget_used_percent": 26.010437933659453
    },
    {
      "type": "training",
      "description": "Training step 1095",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:25",
      "total_flops_so_far": 2.60341852591527e+16,
      "budget_used_percent": 26.034185259152697
    },
    {
      "type": "training",
      "description": "Training step 1096",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:26",
      "total_flops_so_far": 2.6057932584645948e+16,
      "budget_used_percent": 26.057932584645947
    },
    {
      "type": "training",
      "description": "Training step 1097",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:27",
      "total_flops_so_far": 2.6081679910139196e+16,
      "budget_used_percent": 26.081679910139194
    },
    {
      "type": "training",
      "description": "Training step 1098",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:28",
      "total_flops_so_far": 2.6105427235632444e+16,
      "budget_used_percent": 26.105427235632444
    },
    {
      "type": "training",
      "description": "Training step 1099",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:30",
      "total_flops_so_far": 2.6129174561125692e+16,
      "budget_used_percent": 26.12917456112569
    },
    {
      "type": "training",
      "description": "Training step 1100",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:31",
      "total_flops_so_far": 2.615292188661894e+16,
      "budget_used_percent": 26.15292188661894
    },
    {
      "type": "training",
      "description": "Training step 1101",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:32",
      "total_flops_so_far": 2.6176669212112188e+16,
      "budget_used_percent": 26.176669212112184
    },
    {
      "type": "training",
      "description": "Training step 1102",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:34",
      "total_flops_so_far": 2.6200416537605436e+16,
      "budget_used_percent": 26.200416537605438
    },
    {
      "type": "training",
      "description": "Training step 1103",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:35",
      "total_flops_so_far": 2.6224163863098684e+16,
      "budget_used_percent": 26.22416386309868
    },
    {
      "type": "training",
      "description": "Training step 1104",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:36",
      "total_flops_so_far": 2.6247911188591932e+16,
      "budget_used_percent": 26.24791118859193
    },
    {
      "type": "training",
      "description": "Training step 1105",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:38",
      "total_flops_so_far": 2.627165851408518e+16,
      "budget_used_percent": 26.27165851408518
    },
    {
      "type": "training",
      "description": "Training step 1106",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:39",
      "total_flops_so_far": 2.6295405839578428e+16,
      "budget_used_percent": 26.29540583957843
    },
    {
      "type": "training",
      "description": "Training step 1107",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:40",
      "total_flops_so_far": 2.6319153165071676e+16,
      "budget_used_percent": 26.319153165071675
    },
    {
      "type": "training",
      "description": "Training step 1108",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:42",
      "total_flops_so_far": 2.6342900490564924e+16,
      "budget_used_percent": 26.342900490564926
    },
    {
      "type": "training",
      "description": "Training step 1109",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:43",
      "total_flops_so_far": 2.6366647816058172e+16,
      "budget_used_percent": 26.36664781605817
    },
    {
      "type": "training",
      "description": "Training step 1110",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:44",
      "total_flops_so_far": 2.639039514155142e+16,
      "budget_used_percent": 26.39039514155142
    },
    {
      "type": "training",
      "description": "Training step 1111",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:45",
      "total_flops_so_far": 2.6414142467044668e+16,
      "budget_used_percent": 26.414142467044666
    },
    {
      "type": "training",
      "description": "Training step 1112",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:47",
      "total_flops_so_far": 2.6437889792537916e+16,
      "budget_used_percent": 26.437889792537916
    },
    {
      "type": "training",
      "description": "Training step 1113",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:48",
      "total_flops_so_far": 2.6461637118031164e+16,
      "budget_used_percent": 26.461637118031163
    },
    {
      "type": "training",
      "description": "Training step 1114",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:49",
      "total_flops_so_far": 2.6485384443524412e+16,
      "budget_used_percent": 26.485384443524413
    },
    {
      "type": "training",
      "description": "Training step 1115",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:51",
      "total_flops_so_far": 2.650913176901766e+16,
      "budget_used_percent": 26.509131769017657
    },
    {
      "type": "training",
      "description": "Training step 1116",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:52",
      "total_flops_so_far": 2.6532879094510908e+16,
      "budget_used_percent": 26.532879094510907
    },
    {
      "type": "training",
      "description": "Training step 1117",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:53",
      "total_flops_so_far": 2.6556626420004156e+16,
      "budget_used_percent": 26.556626420004154
    },
    {
      "type": "training",
      "description": "Training step 1118",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:55",
      "total_flops_so_far": 2.6580373745497404e+16,
      "budget_used_percent": 26.580373745497404
    },
    {
      "type": "training",
      "description": "Training step 1119",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:56",
      "total_flops_so_far": 2.6604121070990652e+16,
      "budget_used_percent": 26.60412107099065
    },
    {
      "type": "training",
      "description": "Training step 1120",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:57",
      "total_flops_so_far": 2.66278683964839e+16,
      "budget_used_percent": 26.6278683964839
    },
    {
      "type": "training",
      "description": "Training step 1121",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:07:59",
      "total_flops_so_far": 2.6651615721977148e+16,
      "budget_used_percent": 26.651615721977144
    },
    {
      "type": "training",
      "description": "Training step 1122",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:00",
      "total_flops_so_far": 2.6675363047470396e+16,
      "budget_used_percent": 26.675363047470395
    },
    {
      "type": "training",
      "description": "Training step 1123",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:01",
      "total_flops_so_far": 2.6699110372963644e+16,
      "budget_used_percent": 26.69911037296364
    },
    {
      "type": "training",
      "description": "Training step 1124",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:02",
      "total_flops_so_far": 2.6722857698456892e+16,
      "budget_used_percent": 26.72285769845689
    },
    {
      "type": "training",
      "description": "Training step 1125",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:04",
      "total_flops_so_far": 2.674660502395014e+16,
      "budget_used_percent": 26.74660502395014
    },
    {
      "type": "training",
      "description": "Training step 1126",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:05",
      "total_flops_so_far": 2.6770352349443388e+16,
      "budget_used_percent": 26.77035234944339
    },
    {
      "type": "training",
      "description": "Training step 1127",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:06",
      "total_flops_so_far": 2.6794099674936636e+16,
      "budget_used_percent": 26.794099674936632
    },
    {
      "type": "training",
      "description": "Training step 1128",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:08",
      "total_flops_so_far": 2.6817847000429884e+16,
      "budget_used_percent": 26.817847000429886
    },
    {
      "type": "training",
      "description": "Training step 1129",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:09",
      "total_flops_so_far": 2.6841594325923132e+16,
      "budget_used_percent": 26.84159432592313
    },
    {
      "type": "training",
      "description": "Training step 1130",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:10",
      "total_flops_so_far": 2.686534165141638e+16,
      "budget_used_percent": 26.86534165141638
    },
    {
      "type": "training",
      "description": "Training step 1131",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:12",
      "total_flops_so_far": 2.6889088976909628e+16,
      "budget_used_percent": 26.889088976909626
    },
    {
      "type": "training",
      "description": "Training step 1132",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:13",
      "total_flops_so_far": 2.6912836302402876e+16,
      "budget_used_percent": 26.912836302402876
    },
    {
      "type": "training",
      "description": "Training step 1133",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:14",
      "total_flops_so_far": 2.6936583627896124e+16,
      "budget_used_percent": 26.93658362789612
    },
    {
      "type": "training",
      "description": "Training step 1134",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:15",
      "total_flops_so_far": 2.6960330953389372e+16,
      "budget_used_percent": 26.960330953389374
    },
    {
      "type": "training",
      "description": "Training step 1135",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:17",
      "total_flops_so_far": 2.698407827888262e+16,
      "budget_used_percent": 26.984078278882617
    },
    {
      "type": "training",
      "description": "Training step 1136",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:18",
      "total_flops_so_far": 2.7007825604375868e+16,
      "budget_used_percent": 27.007825604375867
    },
    {
      "type": "training",
      "description": "Training step 1137",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:19",
      "total_flops_so_far": 2.7031572929869116e+16,
      "budget_used_percent": 27.031572929869117
    },
    {
      "type": "training",
      "description": "Training step 1138",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:21",
      "total_flops_so_far": 2.7055320255362364e+16,
      "budget_used_percent": 27.055320255362364
    },
    {
      "type": "training",
      "description": "Training step 1139",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:22",
      "total_flops_so_far": 2.7079067580855612e+16,
      "budget_used_percent": 27.079067580855614
    },
    {
      "type": "training",
      "description": "Training step 1140",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:23",
      "total_flops_so_far": 2.710281490634886e+16,
      "budget_used_percent": 27.10281490634886
    },
    {
      "type": "training",
      "description": "Training step 1141",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:25",
      "total_flops_so_far": 2.7126562231842108e+16,
      "budget_used_percent": 27.12656223184211
    },
    {
      "type": "training",
      "description": "Training step 1142",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:26",
      "total_flops_so_far": 2.7150309557335356e+16,
      "budget_used_percent": 27.150309557335355
    },
    {
      "type": "training",
      "description": "Training step 1143",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:27",
      "total_flops_so_far": 2.7174056882828604e+16,
      "budget_used_percent": 27.174056882828605
    },
    {
      "type": "training",
      "description": "Training step 1144",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:29",
      "total_flops_so_far": 2.7197804208321852e+16,
      "budget_used_percent": 27.197804208321852
    },
    {
      "type": "training",
      "description": "Training step 1145",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:30",
      "total_flops_so_far": 2.72215515338151e+16,
      "budget_used_percent": 27.221551533815102
    },
    {
      "type": "training",
      "description": "Training step 1146",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:31",
      "total_flops_so_far": 2.7245298859308348e+16,
      "budget_used_percent": 27.24529885930835
    },
    {
      "type": "training",
      "description": "Training step 1147",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:32",
      "total_flops_so_far": 2.7269046184801596e+16,
      "budget_used_percent": 27.2690461848016
    },
    {
      "type": "training",
      "description": "Training step 1148",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:34",
      "total_flops_so_far": 2.7292793510294844e+16,
      "budget_used_percent": 27.292793510294842
    },
    {
      "type": "training",
      "description": "Training step 1149",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:35",
      "total_flops_so_far": 2.7316540835788092e+16,
      "budget_used_percent": 27.316540835788096
    },
    {
      "type": "training",
      "description": "Training step 1150",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:36",
      "total_flops_so_far": 2.734028816128134e+16,
      "budget_used_percent": 27.34028816128134
    },
    {
      "type": "training",
      "description": "Training step 1151",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:38",
      "total_flops_so_far": 2.7364035486774588e+16,
      "budget_used_percent": 27.36403548677459
    },
    {
      "type": "training",
      "description": "Training step 1152",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:39",
      "total_flops_so_far": 2.7387782812267836e+16,
      "budget_used_percent": 27.387782812267837
    },
    {
      "type": "training",
      "description": "Training step 1153",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:40",
      "total_flops_so_far": 2.7411530137761084e+16,
      "budget_used_percent": 27.411530137761087
    },
    {
      "type": "training",
      "description": "Training step 1154",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:42",
      "total_flops_so_far": 2.7435277463254332e+16,
      "budget_used_percent": 27.43527746325433
    },
    {
      "type": "training",
      "description": "Training step 1155",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:43",
      "total_flops_so_far": 2.745902478874758e+16,
      "budget_used_percent": 27.459024788747584
    },
    {
      "type": "training",
      "description": "Training step 1156",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:44",
      "total_flops_so_far": 2.7482772114240828e+16,
      "budget_used_percent": 27.482772114240827
    },
    {
      "type": "training",
      "description": "Training step 1157",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:45",
      "total_flops_so_far": 2.7506519439734076e+16,
      "budget_used_percent": 27.506519439734078
    },
    {
      "type": "training",
      "description": "Training step 1158",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:47",
      "total_flops_so_far": 2.7530266765227324e+16,
      "budget_used_percent": 27.530266765227324
    },
    {
      "type": "training",
      "description": "Training step 1159",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:48",
      "total_flops_so_far": 2.7554014090720572e+16,
      "budget_used_percent": 27.554014090720575
    },
    {
      "type": "training",
      "description": "Training step 1160",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:49",
      "total_flops_so_far": 2.757776141621382e+16,
      "budget_used_percent": 27.577761416213818
    },
    {
      "type": "training",
      "description": "Training step 1161",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:51",
      "total_flops_so_far": 2.7601508741707068e+16,
      "budget_used_percent": 27.60150874170707
    },
    {
      "type": "training",
      "description": "Training step 1162",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:52",
      "total_flops_so_far": 2.7625256067200316e+16,
      "budget_used_percent": 27.625256067200315
    },
    {
      "type": "training",
      "description": "Training step 1163",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:53",
      "total_flops_so_far": 2.7649003392693564e+16,
      "budget_used_percent": 27.649003392693565
    },
    {
      "type": "training",
      "description": "Training step 1164",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:55",
      "total_flops_so_far": 2.7672750718186812e+16,
      "budget_used_percent": 27.672750718186812
    },
    {
      "type": "training",
      "description": "Training step 1165",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:56",
      "total_flops_so_far": 2.769649804368006e+16,
      "budget_used_percent": 27.696498043680062
    },
    {
      "type": "training",
      "description": "Training step 1166",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:57",
      "total_flops_so_far": 2.7720245369173308e+16,
      "budget_used_percent": 27.72024536917331
    },
    {
      "type": "training",
      "description": "Training step 1167",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:08:59",
      "total_flops_so_far": 2.7743992694666556e+16,
      "budget_used_percent": 27.74399269466656
    },
    {
      "type": "training",
      "description": "Training step 1168",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:00",
      "total_flops_so_far": 2.7767740020159804e+16,
      "budget_used_percent": 27.767740020159803
    },
    {
      "type": "training",
      "description": "Training step 1169",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:01",
      "total_flops_so_far": 2.7791487345653052e+16,
      "budget_used_percent": 27.791487345653053
    },
    {
      "type": "training",
      "description": "Training step 1170",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:02",
      "total_flops_so_far": 2.78152346711463e+16,
      "budget_used_percent": 27.8152346711463
    },
    {
      "type": "training",
      "description": "Training step 1171",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:04",
      "total_flops_so_far": 2.7838981996639548e+16,
      "budget_used_percent": 27.83898199663955
    },
    {
      "type": "training",
      "description": "Training step 1172",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:05",
      "total_flops_so_far": 2.7862729322132796e+16,
      "budget_used_percent": 27.862729322132797
    },
    {
      "type": "training",
      "description": "Training step 1173",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:06",
      "total_flops_so_far": 2.7886476647626044e+16,
      "budget_used_percent": 27.886476647626047
    },
    {
      "type": "training",
      "description": "Training step 1174",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:08",
      "total_flops_so_far": 2.7910223973119292e+16,
      "budget_used_percent": 27.91022397311929
    },
    {
      "type": "training",
      "description": "Training step 1175",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:09",
      "total_flops_so_far": 2.793397129861254e+16,
      "budget_used_percent": 27.93397129861254
    },
    {
      "type": "training",
      "description": "Training step 1176",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:10",
      "total_flops_so_far": 2.7957718624105788e+16,
      "budget_used_percent": 27.957718624105787
    },
    {
      "type": "training",
      "description": "Training step 1177",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:12",
      "total_flops_so_far": 2.7981465949599036e+16,
      "budget_used_percent": 27.981465949599038
    },
    {
      "type": "training",
      "description": "Training step 1178",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:13",
      "total_flops_so_far": 2.8005213275092284e+16,
      "budget_used_percent": 28.005213275092284
    },
    {
      "type": "training",
      "description": "Training step 1179",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:14",
      "total_flops_so_far": 2.8028960600585532e+16,
      "budget_used_percent": 28.028960600585535
    },
    {
      "type": "training",
      "description": "Training step 1180",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:16",
      "total_flops_so_far": 2.805270792607878e+16,
      "budget_used_percent": 28.052707926078778
    },
    {
      "type": "training",
      "description": "Training step 1181",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:17",
      "total_flops_so_far": 2.8076455251572028e+16,
      "budget_used_percent": 28.07645525157203
    },
    {
      "type": "training",
      "description": "Training step 1182",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:18",
      "total_flops_so_far": 2.8100202577065276e+16,
      "budget_used_percent": 28.100202577065275
    },
    {
      "type": "training",
      "description": "Training step 1183",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:19",
      "total_flops_so_far": 2.8123949902558524e+16,
      "budget_used_percent": 28.123949902558525
    },
    {
      "type": "training",
      "description": "Training step 1184",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:21",
      "total_flops_so_far": 2.8147697228051772e+16,
      "budget_used_percent": 28.147697228051772
    },
    {
      "type": "training",
      "description": "Training step 1185",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:22",
      "total_flops_so_far": 2.817144455354502e+16,
      "budget_used_percent": 28.171444553545022
    },
    {
      "type": "training",
      "description": "Training step 1186",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:23",
      "total_flops_so_far": 2.8195191879038268e+16,
      "budget_used_percent": 28.195191879038266
    },
    {
      "type": "training",
      "description": "Training step 1187",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:25",
      "total_flops_so_far": 2.8218939204531516e+16,
      "budget_used_percent": 28.21893920453152
    },
    {
      "type": "training",
      "description": "Training step 1188",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:25",
      "total_flops_so_far": 2.8242686530024764e+16,
      "budget_used_percent": 28.242686530024763
    },
    {
      "type": "training",
      "description": "Training step 1189",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:26",
      "total_flops_so_far": 2.8266433855518012e+16,
      "budget_used_percent": 28.266433855518013
    },
    {
      "type": "training",
      "description": "Training step 1190",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:28",
      "total_flops_so_far": 2.829018118101126e+16,
      "budget_used_percent": 28.29018118101126
    },
    {
      "type": "training",
      "description": "Training step 1191",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:29",
      "total_flops_so_far": 2.8313928506504508e+16,
      "budget_used_percent": 28.31392850650451
    },
    {
      "type": "training",
      "description": "Training step 1192",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:30",
      "total_flops_so_far": 2.8337675831997756e+16,
      "budget_used_percent": 28.337675831997757
    },
    {
      "type": "training",
      "description": "Training step 1193",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:32",
      "total_flops_so_far": 2.8361423157491004e+16,
      "budget_used_percent": 28.361423157491007
    },
    {
      "type": "training",
      "description": "Training step 1194",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:33",
      "total_flops_so_far": 2.8385170482984252e+16,
      "budget_used_percent": 28.38517048298425
    },
    {
      "type": "training",
      "description": "Training step 1195",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:34",
      "total_flops_so_far": 2.84089178084775e+16,
      "budget_used_percent": 28.4089178084775
    },
    {
      "type": "training",
      "description": "Training step 1196",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:35",
      "total_flops_so_far": 2.8432665133970748e+16,
      "budget_used_percent": 28.432665133970747
    },
    {
      "type": "training",
      "description": "Training step 1197",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:37",
      "total_flops_so_far": 2.8456412459463996e+16,
      "budget_used_percent": 28.456412459463998
    },
    {
      "type": "training",
      "description": "Training step 1198",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:38",
      "total_flops_so_far": 2.8480159784957244e+16,
      "budget_used_percent": 28.480159784957245
    },
    {
      "type": "training",
      "description": "Training step 1199",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:39",
      "total_flops_so_far": 2.8503907110450492e+16,
      "budget_used_percent": 28.503907110450495
    },
    {
      "type": "training",
      "description": "Training step 1200",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:41",
      "total_flops_so_far": 2.852765443594374e+16,
      "budget_used_percent": 28.527654435943738
    },
    {
      "type": "training",
      "description": "Training step 1201",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:42",
      "total_flops_so_far": 2.8551401761436988e+16,
      "budget_used_percent": 28.55140176143699
    },
    {
      "type": "training",
      "description": "Training step 1202",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:43",
      "total_flops_so_far": 2.8575149086930236e+16,
      "budget_used_percent": 28.575149086930235
    },
    {
      "type": "training",
      "description": "Training step 1203",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:45",
      "total_flops_so_far": 2.8598896412423484e+16,
      "budget_used_percent": 28.598896412423485
    },
    {
      "type": "training",
      "description": "Training step 1204",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:46",
      "total_flops_so_far": 2.8622643737916732e+16,
      "budget_used_percent": 28.622643737916732
    },
    {
      "type": "training",
      "description": "Training step 1205",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:47",
      "total_flops_so_far": 2.864639106340998e+16,
      "budget_used_percent": 28.646391063409983
    },
    {
      "type": "training",
      "description": "Training step 1206",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:49",
      "total_flops_so_far": 2.8670138388903228e+16,
      "budget_used_percent": 28.670138388903226
    },
    {
      "type": "training",
      "description": "Training step 1207",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:50",
      "total_flops_so_far": 2.8693885714396476e+16,
      "budget_used_percent": 28.693885714396476
    },
    {
      "type": "training",
      "description": "Training step 1208",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:51",
      "total_flops_so_far": 2.8717633039889724e+16,
      "budget_used_percent": 28.717633039889723
    },
    {
      "type": "training",
      "description": "Training step 1209",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:52",
      "total_flops_so_far": 2.8741380365382972e+16,
      "budget_used_percent": 28.741380365382973
    },
    {
      "type": "training",
      "description": "Training step 1210",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:54",
      "total_flops_so_far": 2.876512769087622e+16,
      "budget_used_percent": 28.76512769087622
    },
    {
      "type": "training",
      "description": "Training step 1211",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:55",
      "total_flops_so_far": 2.8788875016369468e+16,
      "budget_used_percent": 28.78887501636947
    },
    {
      "type": "training",
      "description": "Training step 1212",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:56",
      "total_flops_so_far": 2.8812622341862716e+16,
      "budget_used_percent": 28.812622341862713
    },
    {
      "type": "training",
      "description": "Training step 1213",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:58",
      "total_flops_so_far": 2.8836369667355964e+16,
      "budget_used_percent": 28.836369667355967
    },
    {
      "type": "training",
      "description": "Training step 1214",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:09:59",
      "total_flops_so_far": 2.8860116992849212e+16,
      "budget_used_percent": 28.86011699284921
    },
    {
      "type": "training",
      "description": "Training step 1215",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:00",
      "total_flops_so_far": 2.888386431834246e+16,
      "budget_used_percent": 28.88386431834246
    },
    {
      "type": "training",
      "description": "Training step 1216",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:02",
      "total_flops_so_far": 2.8907611643835708e+16,
      "budget_used_percent": 28.907611643835708
    },
    {
      "type": "training",
      "description": "Training step 1217",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:03",
      "total_flops_so_far": 2.8931358969328956e+16,
      "budget_used_percent": 28.931358969328958
    },
    {
      "type": "training",
      "description": "Training step 1218",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:04",
      "total_flops_so_far": 2.8955106294822204e+16,
      "budget_used_percent": 28.9551062948222
    },
    {
      "type": "training",
      "description": "Training step 1219",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:06",
      "total_flops_so_far": 2.8978853620315452e+16,
      "budget_used_percent": 28.978853620315455
    },
    {
      "type": "training",
      "description": "Training step 1220",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:07",
      "total_flops_so_far": 2.90026009458087e+16,
      "budget_used_percent": 29.002600945808698
    },
    {
      "type": "training",
      "description": "Training step 1221",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:08",
      "total_flops_so_far": 2.9026348271301948e+16,
      "budget_used_percent": 29.02634827130195
    },
    {
      "type": "training",
      "description": "Training step 1222",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:09",
      "total_flops_so_far": 2.9050095596795196e+16,
      "budget_used_percent": 29.050095596795195
    },
    {
      "type": "training",
      "description": "Training step 1223",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:11",
      "total_flops_so_far": 2.9073842922288444e+16,
      "budget_used_percent": 29.073842922288446
    },
    {
      "type": "training",
      "description": "Training step 1224",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:13",
      "total_flops_so_far": 2.9097590247781692e+16,
      "budget_used_percent": 29.09759024778169
    },
    {
      "type": "training",
      "description": "Training step 1225",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:15",
      "total_flops_so_far": 2.912133757327494e+16,
      "budget_used_percent": 29.121337573274943
    },
    {
      "type": "training",
      "description": "Training step 1226",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:16",
      "total_flops_so_far": 2.9145084898768188e+16,
      "budget_used_percent": 29.145084898768186
    },
    {
      "type": "training",
      "description": "Training step 1227",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:18",
      "total_flops_so_far": 2.9168832224261436e+16,
      "budget_used_percent": 29.168832224261436
    },
    {
      "type": "training",
      "description": "Training step 1228",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:19",
      "total_flops_so_far": 2.9192579549754684e+16,
      "budget_used_percent": 29.192579549754683
    },
    {
      "type": "training",
      "description": "Training step 1229",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:20",
      "total_flops_so_far": 2.9216326875247932e+16,
      "budget_used_percent": 29.216326875247933
    },
    {
      "type": "training",
      "description": "Training step 1230",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:21",
      "total_flops_so_far": 2.924007420074118e+16,
      "budget_used_percent": 29.24007420074118
    },
    {
      "type": "training",
      "description": "Training step 1231",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:23",
      "total_flops_so_far": 2.9263821526234428e+16,
      "budget_used_percent": 29.26382152623443
    },
    {
      "type": "training",
      "description": "Training step 1232",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:24",
      "total_flops_so_far": 2.9287568851727676e+16,
      "budget_used_percent": 29.287568851727674
    },
    {
      "type": "training",
      "description": "Training step 1233",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:25",
      "total_flops_so_far": 2.9311316177220924e+16,
      "budget_used_percent": 29.311316177220924
    },
    {
      "type": "training",
      "description": "Training step 1234",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:27",
      "total_flops_so_far": 2.9335063502714172e+16,
      "budget_used_percent": 29.33506350271417
    },
    {
      "type": "training",
      "description": "Training step 1235",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:28",
      "total_flops_so_far": 2.935881082820742e+16,
      "budget_used_percent": 29.35881082820742
    },
    {
      "type": "training",
      "description": "Training step 1236",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:29",
      "total_flops_so_far": 2.9382558153700668e+16,
      "budget_used_percent": 29.382558153700668
    },
    {
      "type": "training",
      "description": "Training step 1237",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:31",
      "total_flops_so_far": 2.9406305479193916e+16,
      "budget_used_percent": 29.406305479193918
    },
    {
      "type": "training",
      "description": "Training step 1238",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:32",
      "total_flops_so_far": 2.9430052804687164e+16,
      "budget_used_percent": 29.43005280468716
    },
    {
      "type": "training",
      "description": "Training step 1239",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:33",
      "total_flops_so_far": 2.9453800130180412e+16,
      "budget_used_percent": 29.45380013018041
    },
    {
      "type": "training",
      "description": "Training step 1240",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:35",
      "total_flops_so_far": 2.947754745567366e+16,
      "budget_used_percent": 29.47754745567366
    },
    {
      "type": "training",
      "description": "Training step 1241",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:36",
      "total_flops_so_far": 2.9501294781166908e+16,
      "budget_used_percent": 29.50129478116691
    },
    {
      "type": "training",
      "description": "Training step 1242",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:37",
      "total_flops_so_far": 2.9525042106660156e+16,
      "budget_used_percent": 29.525042106660155
    },
    {
      "type": "training",
      "description": "Training step 1243",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:38",
      "total_flops_so_far": 2.9548789432153404e+16,
      "budget_used_percent": 29.548789432153406
    },
    {
      "type": "training",
      "description": "Training step 1244",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:40",
      "total_flops_so_far": 2.9572536757646652e+16,
      "budget_used_percent": 29.57253675764665
    },
    {
      "type": "training",
      "description": "Training step 1245",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:41",
      "total_flops_so_far": 2.95962840831399e+16,
      "budget_used_percent": 29.5962840831399
    },
    {
      "type": "training",
      "description": "Training step 1246",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:42",
      "total_flops_so_far": 2.9620031408633148e+16,
      "budget_used_percent": 29.620031408633146
    },
    {
      "type": "training",
      "description": "Training step 1247",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:44",
      "total_flops_so_far": 2.9643778734126396e+16,
      "budget_used_percent": 29.643778734126396
    },
    {
      "type": "training",
      "description": "Training step 1248",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:45",
      "total_flops_so_far": 2.9667526059619644e+16,
      "budget_used_percent": 29.667526059619643
    },
    {
      "type": "training",
      "description": "Training step 1249",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:46",
      "total_flops_so_far": 2.9691273385112892e+16,
      "budget_used_percent": 29.691273385112893
    },
    {
      "type": "training",
      "description": "Training step 1250",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:48",
      "total_flops_so_far": 2.971502071060614e+16,
      "budget_used_percent": 29.715020710606137
    },
    {
      "type": "training",
      "description": "Training step 1251",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:49",
      "total_flops_so_far": 2.9738768036099388e+16,
      "budget_used_percent": 29.73876803609939
    },
    {
      "type": "training",
      "description": "Training step 1252",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:50",
      "total_flops_so_far": 2.9762515361592636e+16,
      "budget_used_percent": 29.762515361592634
    },
    {
      "type": "training",
      "description": "Training step 1253",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:52",
      "total_flops_so_far": 2.9786262687085884e+16,
      "budget_used_percent": 29.786262687085884
    },
    {
      "type": "training",
      "description": "Training step 1254",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:53",
      "total_flops_so_far": 2.9810010012579132e+16,
      "budget_used_percent": 29.81001001257913
    },
    {
      "type": "training",
      "description": "Training step 1255",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:54",
      "total_flops_so_far": 2.983375733807238e+16,
      "budget_used_percent": 29.83375733807238
    },
    {
      "type": "training",
      "description": "Training step 1256",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:55",
      "total_flops_so_far": 2.9857504663565628e+16,
      "budget_used_percent": 29.857504663565628
    },
    {
      "type": "training",
      "description": "Training step 1257",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:57",
      "total_flops_so_far": 2.9881251989058876e+16,
      "budget_used_percent": 29.881251989058878
    },
    {
      "type": "training",
      "description": "Training step 1258",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:58",
      "total_flops_so_far": 2.9904999314552124e+16,
      "budget_used_percent": 29.90499931455212
    },
    {
      "type": "training",
      "description": "Training step 1259",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:10:59",
      "total_flops_so_far": 2.9928746640045372e+16,
      "budget_used_percent": 29.92874664004537
    },
    {
      "type": "training",
      "description": "Training step 1260",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:01",
      "total_flops_so_far": 2.995249396553862e+16,
      "budget_used_percent": 29.95249396553862
    },
    {
      "type": "training",
      "description": "Training step 1261",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:02",
      "total_flops_so_far": 2.9976241291031868e+16,
      "budget_used_percent": 29.97624129103187
    },
    {
      "type": "training",
      "description": "Training step 1262",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:03",
      "total_flops_so_far": 2.9999988616525116e+16,
      "budget_used_percent": 29.999988616525116
    },
    {
      "type": "training",
      "description": "Training step 1263",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:05",
      "total_flops_so_far": 3.0023735942018364e+16,
      "budget_used_percent": 30.023735942018366
    },
    {
      "type": "training",
      "description": "Training step 1264",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:06",
      "total_flops_so_far": 3.0047483267511612e+16,
      "budget_used_percent": 30.04748326751161
    },
    {
      "type": "training",
      "description": "Training step 1265",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:07",
      "total_flops_so_far": 3.007123059300486e+16,
      "budget_used_percent": 30.07123059300486
    },
    {
      "type": "training",
      "description": "Training step 1266",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:09",
      "total_flops_so_far": 3.0094977918498108e+16,
      "budget_used_percent": 30.094977918498106
    },
    {
      "type": "training",
      "description": "Training step 1267",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:10",
      "total_flops_so_far": 3.0118725243991356e+16,
      "budget_used_percent": 30.118725243991356
    },
    {
      "type": "training",
      "description": "Training step 1268",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:11",
      "total_flops_so_far": 3.0142472569484604e+16,
      "budget_used_percent": 30.142472569484603
    },
    {
      "type": "training",
      "description": "Training step 1269",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:12",
      "total_flops_so_far": 3.0166219894977852e+16,
      "budget_used_percent": 30.166219894977853
    },
    {
      "type": "training",
      "description": "Training step 1270",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:14",
      "total_flops_so_far": 3.01899672204711e+16,
      "budget_used_percent": 30.189967220471097
    },
    {
      "type": "training",
      "description": "Training step 1271",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:15",
      "total_flops_so_far": 3.0213714545964348e+16,
      "budget_used_percent": 30.213714545964347
    },
    {
      "type": "training",
      "description": "Training step 1272",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:16",
      "total_flops_so_far": 3.0237461871457596e+16,
      "budget_used_percent": 30.237461871457594
    },
    {
      "type": "training",
      "description": "Training step 1273",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:18",
      "total_flops_so_far": 3.0261209196950844e+16,
      "budget_used_percent": 30.261209196950844
    },
    {
      "type": "training",
      "description": "Training step 1274",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:19",
      "total_flops_so_far": 3.0284956522444092e+16,
      "budget_used_percent": 30.28495652244409
    },
    {
      "type": "training",
      "description": "Training step 1275",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:20",
      "total_flops_so_far": 3.030870384793734e+16,
      "budget_used_percent": 30.30870384793734
    },
    {
      "type": "training",
      "description": "Training step 1276",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:22",
      "total_flops_so_far": 3.0332451173430588e+16,
      "budget_used_percent": 30.332451173430584
    },
    {
      "type": "training",
      "description": "Training step 1277",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:23",
      "total_flops_so_far": 3.0356198498923836e+16,
      "budget_used_percent": 30.35619849892384
    },
    {
      "type": "training",
      "description": "Training step 1278",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:24",
      "total_flops_so_far": 3.0379945824417084e+16,
      "budget_used_percent": 30.37994582441708
    },
    {
      "type": "training",
      "description": "Training step 1279",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:26",
      "total_flops_so_far": 3.0403693149910332e+16,
      "budget_used_percent": 30.403693149910332
    },
    {
      "type": "training",
      "description": "Training step 1280",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:27",
      "total_flops_so_far": 3.042744047540358e+16,
      "budget_used_percent": 30.42744047540358
    },
    {
      "type": "training",
      "description": "Training step 1281",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:28",
      "total_flops_so_far": 3.0451187800896828e+16,
      "budget_used_percent": 30.45118780089683
    },
    {
      "type": "training",
      "description": "Training step 1282",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:29",
      "total_flops_so_far": 3.0474935126390076e+16,
      "budget_used_percent": 30.474935126390072
    },
    {
      "type": "training",
      "description": "Training step 1283",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:31",
      "total_flops_so_far": 3.0498682451883324e+16,
      "budget_used_percent": 30.498682451883326
    },
    {
      "type": "training",
      "description": "Training step 1284",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:32",
      "total_flops_so_far": 3.0522429777376572e+16,
      "budget_used_percent": 30.52242977737657
    },
    {
      "type": "training",
      "description": "Training step 1285",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:33",
      "total_flops_so_far": 3.054617710286982e+16,
      "budget_used_percent": 30.54617710286982
    },
    {
      "type": "training",
      "description": "Training step 1286",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:35",
      "total_flops_so_far": 3.0569924428363068e+16,
      "budget_used_percent": 30.569924428363066
    },
    {
      "type": "training",
      "description": "Training step 1287",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:36",
      "total_flops_so_far": 3.0593671753856316e+16,
      "budget_used_percent": 30.593671753856317
    },
    {
      "type": "training",
      "description": "Training step 1288",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:37",
      "total_flops_so_far": 3.0617419079349564e+16,
      "budget_used_percent": 30.61741907934956
    },
    {
      "type": "training",
      "description": "Training step 1289",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:39",
      "total_flops_so_far": 3.0641166404842812e+16,
      "budget_used_percent": 30.641166404842814
    },
    {
      "type": "training",
      "description": "Training step 1290",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:40",
      "total_flops_so_far": 3.066491373033606e+16,
      "budget_used_percent": 30.664913730336057
    },
    {
      "type": "training",
      "description": "Training step 1291",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:41",
      "total_flops_so_far": 3.0688661055829308e+16,
      "budget_used_percent": 30.688661055829307
    },
    {
      "type": "training",
      "description": "Training step 1292",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:43",
      "total_flops_so_far": 3.0712408381322556e+16,
      "budget_used_percent": 30.712408381322554
    },
    {
      "type": "training",
      "description": "Training step 1293",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:44",
      "total_flops_so_far": 3.0736155706815804e+16,
      "budget_used_percent": 30.736155706815804
    },
    {
      "type": "training",
      "description": "Training step 1294",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:45",
      "total_flops_so_far": 3.0759903032309052e+16,
      "budget_used_percent": 30.75990303230905
    },
    {
      "type": "training",
      "description": "Training step 1295",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:47",
      "total_flops_so_far": 3.07836503578023e+16,
      "budget_used_percent": 30.7836503578023
    },
    {
      "type": "training",
      "description": "Training step 1296",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:48",
      "total_flops_so_far": 3.0807397683295548e+16,
      "budget_used_percent": 30.807397683295545
    },
    {
      "type": "training",
      "description": "Training step 1297",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:49",
      "total_flops_so_far": 3.0831145008788796e+16,
      "budget_used_percent": 30.831145008788795
    },
    {
      "type": "training",
      "description": "Training step 1298",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:50",
      "total_flops_so_far": 3.0854892334282044e+16,
      "budget_used_percent": 30.85489233428204
    },
    {
      "type": "training",
      "description": "Training step 1299",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:52",
      "total_flops_so_far": 3.0878639659775292e+16,
      "budget_used_percent": 30.878639659775292
    },
    {
      "type": "training",
      "description": "Training step 1300",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:53",
      "total_flops_so_far": 3.090238698526854e+16,
      "budget_used_percent": 30.90238698526854
    },
    {
      "type": "training",
      "description": "Training step 1301",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:54",
      "total_flops_so_far": 3.0926134310761788e+16,
      "budget_used_percent": 30.92613431076179
    },
    {
      "type": "training",
      "description": "Training step 1302",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:56",
      "total_flops_so_far": 3.0949881636255036e+16,
      "budget_used_percent": 30.949881636255032
    },
    {
      "type": "training",
      "description": "Training step 1303",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:57",
      "total_flops_so_far": 3.0973628961748284e+16,
      "budget_used_percent": 30.973628961748283
    },
    {
      "type": "training",
      "description": "Training step 1304",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:11:58",
      "total_flops_so_far": 3.0997376287241532e+16,
      "budget_used_percent": 30.99737628724153
    },
    {
      "type": "training",
      "description": "Training step 1305",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:00",
      "total_flops_so_far": 3.102112361273478e+16,
      "budget_used_percent": 31.02112361273478
    },
    {
      "type": "training",
      "description": "Training step 1306",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:01",
      "total_flops_so_far": 3.1044870938228028e+16,
      "budget_used_percent": 31.044870938228026
    },
    {
      "type": "training",
      "description": "Training step 1307",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:02",
      "total_flops_so_far": 3.1068618263721276e+16,
      "budget_used_percent": 31.068618263721277
    },
    {
      "type": "training",
      "description": "Training step 1308",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:04",
      "total_flops_so_far": 3.1092365589214524e+16,
      "budget_used_percent": 31.092365589214527
    },
    {
      "type": "training",
      "description": "Training step 1309",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:05",
      "total_flops_so_far": 3.1116112914707772e+16,
      "budget_used_percent": 31.11611291470777
    },
    {
      "type": "training",
      "description": "Training step 1310",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:06",
      "total_flops_so_far": 3.113986024020102e+16,
      "budget_used_percent": 31.139860240201024
    },
    {
      "type": "training",
      "description": "Training step 1311",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:07",
      "total_flops_so_far": 3.1163607565694268e+16,
      "budget_used_percent": 31.163607565694267
    },
    {
      "type": "training",
      "description": "Training step 1312",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:09",
      "total_flops_so_far": 3.1187354891187516e+16,
      "budget_used_percent": 31.187354891187518
    },
    {
      "type": "training",
      "description": "Training step 1313",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:10",
      "total_flops_so_far": 3.1211102216680764e+16,
      "budget_used_percent": 31.211102216680764
    },
    {
      "type": "training",
      "description": "Training step 1314",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:11",
      "total_flops_so_far": 3.1234849542174012e+16,
      "budget_used_percent": 31.234849542174015
    },
    {
      "type": "training",
      "description": "Training step 1315",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:13",
      "total_flops_so_far": 3.125859686766726e+16,
      "budget_used_percent": 31.25859686766726
    },
    {
      "type": "training",
      "description": "Training step 1316",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:14",
      "total_flops_so_far": 3.1282344193160508e+16,
      "budget_used_percent": 31.28234419316051
    },
    {
      "type": "training",
      "description": "Training step 1317",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:15",
      "total_flops_so_far": 3.1306091518653756e+16,
      "budget_used_percent": 31.306091518653755
    },
    {
      "type": "training",
      "description": "Training step 1318",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:17",
      "total_flops_so_far": 3.1329838844147004e+16,
      "budget_used_percent": 31.329838844147005
    },
    {
      "type": "training",
      "description": "Training step 1319",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:18",
      "total_flops_so_far": 3.1353586169640252e+16,
      "budget_used_percent": 31.353586169640252
    },
    {
      "type": "training",
      "description": "Training step 1320",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:18",
      "total_flops_so_far": 3.13773334951335e+16,
      "budget_used_percent": 31.377333495133502
    },
    {
      "type": "training",
      "description": "Training step 1321",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:20",
      "total_flops_so_far": 3.1401080820626748e+16,
      "budget_used_percent": 31.40108082062675
    },
    {
      "type": "training",
      "description": "Training step 1322",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:21",
      "total_flops_so_far": 3.1424828146119996e+16,
      "budget_used_percent": 31.42482814612
    },
    {
      "type": "training",
      "description": "Training step 1323",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:22",
      "total_flops_so_far": 3.1448575471613244e+16,
      "budget_used_percent": 31.448575471613243
    },
    {
      "type": "training",
      "description": "Training step 1324",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:24",
      "total_flops_so_far": 3.1472322797106492e+16,
      "budget_used_percent": 31.472322797106493
    },
    {
      "type": "training",
      "description": "Training step 1325",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:25",
      "total_flops_so_far": 3.149607012259974e+16,
      "budget_used_percent": 31.49607012259974
    },
    {
      "type": "training",
      "description": "Training step 1326",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:26",
      "total_flops_so_far": 3.1519817448092988e+16,
      "budget_used_percent": 31.51981744809299
    },
    {
      "type": "training",
      "description": "Training step 1327",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:27",
      "total_flops_so_far": 3.1543564773586236e+16,
      "budget_used_percent": 31.543564773586237
    },
    {
      "type": "training",
      "description": "Training step 1328",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:29",
      "total_flops_so_far": 3.1567312099079484e+16,
      "budget_used_percent": 31.567312099079487
    },
    {
      "type": "training",
      "description": "Training step 1329",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:30",
      "total_flops_so_far": 3.1591059424572732e+16,
      "budget_used_percent": 31.59105942457273
    },
    {
      "type": "training",
      "description": "Training step 1330",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:31",
      "total_flops_so_far": 3.161480675006598e+16,
      "budget_used_percent": 31.61480675006598
    },
    {
      "type": "training",
      "description": "Training step 1331",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:33",
      "total_flops_so_far": 3.1638554075559228e+16,
      "budget_used_percent": 31.638554075559227
    },
    {
      "type": "training",
      "description": "Training step 1332",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:34",
      "total_flops_so_far": 3.1662301401052476e+16,
      "budget_used_percent": 31.662301401052478
    },
    {
      "type": "training",
      "description": "Training step 1333",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:35",
      "total_flops_so_far": 3.1686048726545724e+16,
      "budget_used_percent": 31.686048726545724
    },
    {
      "type": "training",
      "description": "Training step 1334",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:37",
      "total_flops_so_far": 3.1709796052038972e+16,
      "budget_used_percent": 31.709796052038975
    },
    {
      "type": "training",
      "description": "Training step 1335",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:38",
      "total_flops_so_far": 3.173354337753222e+16,
      "budget_used_percent": 31.733543377532218
    },
    {
      "type": "training",
      "description": "Training step 1336",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:39",
      "total_flops_so_far": 3.1757290703025468e+16,
      "budget_used_percent": 31.757290703025472
    },
    {
      "type": "training",
      "description": "Training step 1337",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:41",
      "total_flops_so_far": 3.1781038028518716e+16,
      "budget_used_percent": 31.781038028518715
    },
    {
      "type": "training",
      "description": "Training step 1338",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:42",
      "total_flops_so_far": 3.1804785354011964e+16,
      "budget_used_percent": 31.804785354011965
    },
    {
      "type": "training",
      "description": "Training step 1339",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:43",
      "total_flops_so_far": 3.1828532679505212e+16,
      "budget_used_percent": 31.828532679505212
    },
    {
      "type": "training",
      "description": "Training step 1340",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:44",
      "total_flops_so_far": 3.185228000499846e+16,
      "budget_used_percent": 31.852280004998462
    },
    {
      "type": "training",
      "description": "Training step 1341",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:46",
      "total_flops_so_far": 3.1876027330491708e+16,
      "budget_used_percent": 31.87602733049171
    },
    {
      "type": "training",
      "description": "Training step 1342",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:47",
      "total_flops_so_far": 3.1899774655984956e+16,
      "budget_used_percent": 31.89977465598496
    },
    {
      "type": "training",
      "description": "Training step 1343",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:48",
      "total_flops_so_far": 3.1923521981478204e+16,
      "budget_used_percent": 31.923521981478203
    },
    {
      "type": "training",
      "description": "Training step 1344",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:50",
      "total_flops_so_far": 3.1947269306971452e+16,
      "budget_used_percent": 31.947269306971453
    },
    {
      "type": "training",
      "description": "Training step 1345",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:51",
      "total_flops_so_far": 3.19710166324647e+16,
      "budget_used_percent": 31.9710166324647
    },
    {
      "type": "training",
      "description": "Training step 1346",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:52",
      "total_flops_so_far": 3.1994763957957948e+16,
      "budget_used_percent": 31.99476395795795
    },
    {
      "type": "training",
      "description": "Training step 1347",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:54",
      "total_flops_so_far": 3.2018511283451196e+16,
      "budget_used_percent": 32.0185112834512
    },
    {
      "type": "training",
      "description": "Training step 1348",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:55",
      "total_flops_so_far": 3.2042258608944444e+16,
      "budget_used_percent": 32.042258608944444
    },
    {
      "type": "training",
      "description": "Training step 1349",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:56",
      "total_flops_so_far": 3.2066005934437692e+16,
      "budget_used_percent": 32.06600593443769
    },
    {
      "type": "training",
      "description": "Training step 1350",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:58",
      "total_flops_so_far": 3.208975325993094e+16,
      "budget_used_percent": 32.089753259930944
    },
    {
      "type": "training",
      "description": "Training step 1351",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:12:59",
      "total_flops_so_far": 3.2113500585424188e+16,
      "budget_used_percent": 32.113500585424184
    },
    {
      "type": "training",
      "description": "Training step 1352",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:00",
      "total_flops_so_far": 3.2137247910917436e+16,
      "budget_used_percent": 32.13724791091744
    },
    {
      "type": "training",
      "description": "Training step 1353",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:01",
      "total_flops_so_far": 3.2160995236410684e+16,
      "budget_used_percent": 32.160995236410685
    },
    {
      "type": "training",
      "description": "Training step 1354",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:03",
      "total_flops_so_far": 3.2184742561903932e+16,
      "budget_used_percent": 32.18474256190393
    },
    {
      "type": "training",
      "description": "Training step 1355",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:04",
      "total_flops_so_far": 3.220848988739718e+16,
      "budget_used_percent": 32.20848988739718
    },
    {
      "type": "training",
      "description": "Training step 1356",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:05",
      "total_flops_so_far": 3.2232237212890428e+16,
      "budget_used_percent": 32.23223721289043
    },
    {
      "type": "training",
      "description": "Training step 1357",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:07",
      "total_flops_so_far": 3.2255984538383676e+16,
      "budget_used_percent": 32.25598453838367
    },
    {
      "type": "training",
      "description": "Training step 1358",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:08",
      "total_flops_so_far": 3.2279731863876924e+16,
      "budget_used_percent": 32.279731863876926
    },
    {
      "type": "training",
      "description": "Training step 1359",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:09",
      "total_flops_so_far": 3.2303479189370172e+16,
      "budget_used_percent": 32.30347918937017
    },
    {
      "type": "training",
      "description": "Training step 1360",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:11",
      "total_flops_so_far": 3.232722651486342e+16,
      "budget_used_percent": 32.32722651486342
    },
    {
      "type": "training",
      "description": "Training step 1361",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:12",
      "total_flops_so_far": 3.2350973840356668e+16,
      "budget_used_percent": 32.350973840356666
    },
    {
      "type": "training",
      "description": "Training step 1362",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:13",
      "total_flops_so_far": 3.2374721165849916e+16,
      "budget_used_percent": 32.37472116584992
    },
    {
      "type": "training",
      "description": "Training step 1363",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:15",
      "total_flops_so_far": 3.2398468491343164e+16,
      "budget_used_percent": 32.398468491343166
    },
    {
      "type": "training",
      "description": "Training step 1364",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:16",
      "total_flops_so_far": 3.2422215816836412e+16,
      "budget_used_percent": 32.42221581683641
    },
    {
      "type": "training",
      "description": "Training step 1365",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:17",
      "total_flops_so_far": 3.244596314232966e+16,
      "budget_used_percent": 32.44596314232966
    },
    {
      "type": "training",
      "description": "Training step 1366",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:19",
      "total_flops_so_far": 3.2469710467822908e+16,
      "budget_used_percent": 32.46971046782291
    },
    {
      "type": "training",
      "description": "Training step 1367",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:20",
      "total_flops_so_far": 3.2493457793316156e+16,
      "budget_used_percent": 32.49345779331615
    },
    {
      "type": "training",
      "description": "Training step 1368",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:21",
      "total_flops_so_far": 3.2517205118809404e+16,
      "budget_used_percent": 32.51720511880941
    },
    {
      "type": "training",
      "description": "Training step 1369",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:22",
      "total_flops_so_far": 3.2540952444302652e+16,
      "budget_used_percent": 32.540952444302654
    },
    {
      "type": "training",
      "description": "Training step 1370",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:24",
      "total_flops_so_far": 3.25646997697959e+16,
      "budget_used_percent": 32.5646997697959
    },
    {
      "type": "training",
      "description": "Training step 1371",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:25",
      "total_flops_so_far": 3.2588447095289148e+16,
      "budget_used_percent": 32.58844709528915
    },
    {
      "type": "training",
      "description": "Training step 1372",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:26",
      "total_flops_so_far": 3.2612194420782396e+16,
      "budget_used_percent": 32.612194420782394
    },
    {
      "type": "training",
      "description": "Training step 1373",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:28",
      "total_flops_so_far": 3.2635941746275644e+16,
      "budget_used_percent": 32.63594174627564
    },
    {
      "type": "training",
      "description": "Training step 1374",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:29",
      "total_flops_so_far": 3.2659689071768892e+16,
      "budget_used_percent": 32.659689071768895
    },
    {
      "type": "training",
      "description": "Training step 1375",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:30",
      "total_flops_so_far": 3.268343639726214e+16,
      "budget_used_percent": 32.68343639726214
    },
    {
      "type": "training",
      "description": "Training step 1376",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:32",
      "total_flops_so_far": 3.2707183722755388e+16,
      "budget_used_percent": 32.70718372275539
    },
    {
      "type": "training",
      "description": "Training step 1377",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:33",
      "total_flops_so_far": 3.2730931048248636e+16,
      "budget_used_percent": 32.730931048248635
    },
    {
      "type": "training",
      "description": "Training step 1378",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:34",
      "total_flops_so_far": 3.2754678373741884e+16,
      "budget_used_percent": 32.75467837374188
    },
    {
      "type": "training",
      "description": "Training step 1379",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:36",
      "total_flops_so_far": 3.2778425699235132e+16,
      "budget_used_percent": 32.77842569923513
    },
    {
      "type": "training",
      "description": "Training step 1380",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:37",
      "total_flops_so_far": 3.280217302472838e+16,
      "budget_used_percent": 32.80217302472838
    },
    {
      "type": "training",
      "description": "Training step 1381",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:38",
      "total_flops_so_far": 3.2825920350221628e+16,
      "budget_used_percent": 32.82592035022163
    },
    {
      "type": "training",
      "description": "Training step 1382",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:39",
      "total_flops_so_far": 3.2849667675714876e+16,
      "budget_used_percent": 32.849667675714876
    },
    {
      "type": "training",
      "description": "Training step 1383",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:41",
      "total_flops_so_far": 3.2873415001208124e+16,
      "budget_used_percent": 32.87341500120812
    },
    {
      "type": "training",
      "description": "Training step 1384",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:42",
      "total_flops_so_far": 3.2897162326701372e+16,
      "budget_used_percent": 32.89716232670138
    },
    {
      "type": "training",
      "description": "Training step 1385",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:43",
      "total_flops_so_far": 3.292090965219462e+16,
      "budget_used_percent": 32.92090965219462
    },
    {
      "type": "training",
      "description": "Training step 1386",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:45",
      "total_flops_so_far": 3.2944656977687868e+16,
      "budget_used_percent": 32.94465697768787
    },
    {
      "type": "training",
      "description": "Training step 1387",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:46",
      "total_flops_so_far": 3.2968404303181116e+16,
      "budget_used_percent": 32.96840430318112
    },
    {
      "type": "training",
      "description": "Training step 1388",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:47",
      "total_flops_so_far": 3.2992151628674364e+16,
      "budget_used_percent": 32.992151628674364
    },
    {
      "type": "training",
      "description": "Training step 1389",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:49",
      "total_flops_so_far": 3.3015898954167612e+16,
      "budget_used_percent": 33.01589895416761
    },
    {
      "type": "training",
      "description": "Training step 1390",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:50",
      "total_flops_so_far": 3.303964627966086e+16,
      "budget_used_percent": 33.039646279660865
    },
    {
      "type": "training",
      "description": "Training step 1391",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:51",
      "total_flops_so_far": 3.3063393605154108e+16,
      "budget_used_percent": 33.063393605154104
    },
    {
      "type": "training",
      "description": "Training step 1392",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:53",
      "total_flops_so_far": 3.3087140930647356e+16,
      "budget_used_percent": 33.08714093064736
    },
    {
      "type": "training",
      "description": "Training step 1393",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:54",
      "total_flops_so_far": 3.3110888256140604e+16,
      "budget_used_percent": 33.110888256140605
    },
    {
      "type": "training",
      "description": "Training step 1394",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:55",
      "total_flops_so_far": 3.3134635581633852e+16,
      "budget_used_percent": 33.13463558163385
    },
    {
      "type": "training",
      "description": "Training step 1395",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:57",
      "total_flops_so_far": 3.31583829071271e+16,
      "budget_used_percent": 33.1583829071271
    },
    {
      "type": "training",
      "description": "Training step 1396",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:58",
      "total_flops_so_far": 3.3182130232620348e+16,
      "budget_used_percent": 33.18213023262035
    },
    {
      "type": "training",
      "description": "Training step 1397",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:13:59",
      "total_flops_so_far": 3.3205877558113596e+16,
      "budget_used_percent": 33.20587755811359
    },
    {
      "type": "training",
      "description": "Training step 1398",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:00",
      "total_flops_so_far": 3.3229624883606844e+16,
      "budget_used_percent": 33.229624883606846
    },
    {
      "type": "training",
      "description": "Training step 1399",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:02",
      "total_flops_so_far": 3.3253372209100092e+16,
      "budget_used_percent": 33.25337220910009
    },
    {
      "type": "training",
      "description": "Training step 1400",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:03",
      "total_flops_so_far": 3.327711953459334e+16,
      "budget_used_percent": 33.27711953459334
    },
    {
      "type": "training",
      "description": "Training step 1401",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:04",
      "total_flops_so_far": 3.3300866860086588e+16,
      "budget_used_percent": 33.300866860086586
    },
    {
      "type": "training",
      "description": "Training step 1402",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:06",
      "total_flops_so_far": 3.3324614185579836e+16,
      "budget_used_percent": 33.32461418557984
    },
    {
      "type": "training",
      "description": "Training step 1403",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:07",
      "total_flops_so_far": 3.3348361511073084e+16,
      "budget_used_percent": 33.34836151107308
    },
    {
      "type": "training",
      "description": "Training step 1404",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:08",
      "total_flops_so_far": 3.3372108836566332e+16,
      "budget_used_percent": 33.37210883656633
    },
    {
      "type": "training",
      "description": "Training step 1405",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:10",
      "total_flops_so_far": 3.339585616205958e+16,
      "budget_used_percent": 33.39585616205958
    },
    {
      "type": "training",
      "description": "Training step 1406",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:11",
      "total_flops_so_far": 3.3419603487552828e+16,
      "budget_used_percent": 33.41960348755283
    },
    {
      "type": "training",
      "description": "Training step 1407",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:12",
      "total_flops_so_far": 3.3443350813046076e+16,
      "budget_used_percent": 33.443350813046074
    },
    {
      "type": "training",
      "description": "Training step 1408",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:14",
      "total_flops_so_far": 3.3467098138539324e+16,
      "budget_used_percent": 33.46709813853933
    },
    {
      "type": "training",
      "description": "Training step 1409",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:15",
      "total_flops_so_far": 3.3490845464032572e+16,
      "budget_used_percent": 33.49084546403257
    },
    {
      "type": "training",
      "description": "Training step 1410",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:16",
      "total_flops_so_far": 3.351459278952582e+16,
      "budget_used_percent": 33.51459278952582
    },
    {
      "type": "training",
      "description": "Training step 1411",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:17",
      "total_flops_so_far": 3.3538340115019068e+16,
      "budget_used_percent": 33.53834011501907
    },
    {
      "type": "training",
      "description": "Training step 1412",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:19",
      "total_flops_so_far": 3.3562087440512316e+16,
      "budget_used_percent": 33.562087440512315
    },
    {
      "type": "training",
      "description": "Training step 1413",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:20",
      "total_flops_so_far": 3.3585834766005564e+16,
      "budget_used_percent": 33.58583476600556
    },
    {
      "type": "training",
      "description": "Training step 1414",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:21",
      "total_flops_so_far": 3.3609582091498812e+16,
      "budget_used_percent": 33.609582091498815
    },
    {
      "type": "training",
      "description": "Training step 1415",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:23",
      "total_flops_so_far": 3.363332941699206e+16,
      "budget_used_percent": 33.633329416992055
    },
    {
      "type": "training",
      "description": "Training step 1416",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:24",
      "total_flops_so_far": 3.3657076742485308e+16,
      "budget_used_percent": 33.65707674248531
    },
    {
      "type": "training",
      "description": "Training step 1417",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:25",
      "total_flops_so_far": 3.3680824067978556e+16,
      "budget_used_percent": 33.680824067978556
    },
    {
      "type": "training",
      "description": "Training step 1418",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:27",
      "total_flops_so_far": 3.3704571393471804e+16,
      "budget_used_percent": 33.7045713934718
    },
    {
      "type": "training",
      "description": "Training step 1419",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:28",
      "total_flops_so_far": 3.3728318718965052e+16,
      "budget_used_percent": 33.72831871896505
    },
    {
      "type": "training",
      "description": "Training step 1420",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:29",
      "total_flops_so_far": 3.37520660444583e+16,
      "budget_used_percent": 33.7520660444583
    },
    {
      "type": "training",
      "description": "Training step 1421",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:31",
      "total_flops_so_far": 3.3775813369951548e+16,
      "budget_used_percent": 33.77581336995155
    },
    {
      "type": "training",
      "description": "Training step 1422",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:32",
      "total_flops_so_far": 3.3799560695444796e+16,
      "budget_used_percent": 33.7995606954448
    },
    {
      "type": "training",
      "description": "Training step 1423",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:33",
      "total_flops_so_far": 3.3823308020938044e+16,
      "budget_used_percent": 33.82330802093804
    },
    {
      "type": "training",
      "description": "Training step 1424",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:35",
      "total_flops_so_far": 3.3847055346431292e+16,
      "budget_used_percent": 33.84705534643129
    },
    {
      "type": "training",
      "description": "Training step 1425",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:36",
      "total_flops_so_far": 3.387080267192454e+16,
      "budget_used_percent": 33.87080267192454
    },
    {
      "type": "training",
      "description": "Training step 1426",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:37",
      "total_flops_so_far": 3.3894549997417788e+16,
      "budget_used_percent": 33.89454999741779
    },
    {
      "type": "training",
      "description": "Training step 1427",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:38",
      "total_flops_so_far": 3.3918297322911036e+16,
      "budget_used_percent": 33.91829732291104
    },
    {
      "type": "training",
      "description": "Training step 1428",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:40",
      "total_flops_so_far": 3.3942044648404284e+16,
      "budget_used_percent": 33.942044648404284
    },
    {
      "type": "training",
      "description": "Training step 1429",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:41",
      "total_flops_so_far": 3.3965791973897532e+16,
      "budget_used_percent": 33.96579197389753
    },
    {
      "type": "training",
      "description": "Training step 1430",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:42",
      "total_flops_so_far": 3.398953929939078e+16,
      "budget_used_percent": 33.98953929939078
    },
    {
      "type": "training",
      "description": "Training step 1431",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:44",
      "total_flops_so_far": 3.4013286624884028e+16,
      "budget_used_percent": 34.013286624884024
    },
    {
      "type": "training",
      "description": "Training step 1432",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:45",
      "total_flops_so_far": 3.4037033950377276e+16,
      "budget_used_percent": 34.03703395037728
    },
    {
      "type": "training",
      "description": "Training step 1433",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:46",
      "total_flops_so_far": 3.4060781275870524e+16,
      "budget_used_percent": 34.060781275870525
    },
    {
      "type": "training",
      "description": "Training step 1434",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:48",
      "total_flops_so_far": 3.4084528601363772e+16,
      "budget_used_percent": 34.08452860136377
    },
    {
      "type": "training",
      "description": "Training step 1435",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:49",
      "total_flops_so_far": 3.410827592685702e+16,
      "budget_used_percent": 34.10827592685702
    },
    {
      "type": "training",
      "description": "Training step 1436",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:50",
      "total_flops_so_far": 3.4132023252350268e+16,
      "budget_used_percent": 34.132023252350265
    },
    {
      "type": "training",
      "description": "Training step 1437",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:52",
      "total_flops_so_far": 3.4155770577843516e+16,
      "budget_used_percent": 34.15577057784351
    },
    {
      "type": "training",
      "description": "Training step 1438",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:53",
      "total_flops_so_far": 3.4179517903336764e+16,
      "budget_used_percent": 34.179517903336766
    },
    {
      "type": "training",
      "description": "Training step 1439",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:54",
      "total_flops_so_far": 3.4203265228830012e+16,
      "budget_used_percent": 34.20326522883001
    },
    {
      "type": "training",
      "description": "Training step 1440",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:56",
      "total_flops_so_far": 3.422701255432326e+16,
      "budget_used_percent": 34.22701255432326
    },
    {
      "type": "training",
      "description": "Training step 1441",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:57",
      "total_flops_so_far": 3.4250759879816508e+16,
      "budget_used_percent": 34.250759879816506
    },
    {
      "type": "training",
      "description": "Training step 1442",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:58",
      "total_flops_so_far": 3.4274507205309756e+16,
      "budget_used_percent": 34.27450720530976
    },
    {
      "type": "training",
      "description": "Training step 1443",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:14:59",
      "total_flops_so_far": 3.4298254530803004e+16,
      "budget_used_percent": 34.298254530803
    },
    {
      "type": "training",
      "description": "Training step 1444",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:01",
      "total_flops_so_far": 3.4322001856296252e+16,
      "budget_used_percent": 34.322001856296254
    },
    {
      "type": "training",
      "description": "Training step 1445",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:02",
      "total_flops_so_far": 3.43457491817895e+16,
      "budget_used_percent": 34.3457491817895
    },
    {
      "type": "training",
      "description": "Training step 1446",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:03",
      "total_flops_so_far": 3.4369496507282748e+16,
      "budget_used_percent": 34.36949650728275
    },
    {
      "type": "training",
      "description": "Training step 1447",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:05",
      "total_flops_so_far": 3.4393243832775996e+16,
      "budget_used_percent": 34.393243832775994
    },
    {
      "type": "training",
      "description": "Training step 1448",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:06",
      "total_flops_so_far": 3.4416991158269244e+16,
      "budget_used_percent": 34.41699115826925
    },
    {
      "type": "training",
      "description": "Training step 1449",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:07",
      "total_flops_so_far": 3.4440738483762492e+16,
      "budget_used_percent": 34.44073848376249
    },
    {
      "type": "training",
      "description": "Training step 1450",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:09",
      "total_flops_so_far": 3.446448580925574e+16,
      "budget_used_percent": 34.46448580925574
    },
    {
      "type": "training",
      "description": "Training step 1451",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:10",
      "total_flops_so_far": 3.4488233134748988e+16,
      "budget_used_percent": 34.48823313474899
    },
    {
      "type": "training",
      "description": "Training step 1452",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:10",
      "total_flops_so_far": 3.4511980460242236e+16,
      "budget_used_percent": 34.511980460242235
    },
    {
      "type": "training",
      "description": "Training step 1453",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:12",
      "total_flops_so_far": 3.4535727785735484e+16,
      "budget_used_percent": 34.53572778573548
    },
    {
      "type": "training",
      "description": "Training step 1454",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:13",
      "total_flops_so_far": 3.4559475111228732e+16,
      "budget_used_percent": 34.559475111228735
    },
    {
      "type": "training",
      "description": "Training step 1455",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:14",
      "total_flops_so_far": 3.458322243672198e+16,
      "budget_used_percent": 34.583222436721975
    },
    {
      "type": "training",
      "description": "Training step 1456",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:16",
      "total_flops_so_far": 3.4606969762215228e+16,
      "budget_used_percent": 34.60696976221523
    },
    {
      "type": "training",
      "description": "Training step 1457",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:17",
      "total_flops_so_far": 3.4630717087708476e+16,
      "budget_used_percent": 34.630717087708476
    },
    {
      "type": "training",
      "description": "Training step 1458",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:18",
      "total_flops_so_far": 3.4654464413201724e+16,
      "budget_used_percent": 34.65446441320172
    },
    {
      "type": "training",
      "description": "Training step 1459",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:20",
      "total_flops_so_far": 3.4678211738694972e+16,
      "budget_used_percent": 34.67821173869497
    },
    {
      "type": "training",
      "description": "Training step 1460",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:21",
      "total_flops_so_far": 3.470195906418822e+16,
      "budget_used_percent": 34.70195906418822
    },
    {
      "type": "training",
      "description": "Training step 1461",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:22",
      "total_flops_so_far": 3.4725706389681468e+16,
      "budget_used_percent": 34.72570638968146
    },
    {
      "type": "training",
      "description": "Training step 1462",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:23",
      "total_flops_so_far": 3.4749453715174716e+16,
      "budget_used_percent": 34.74945371517472
    },
    {
      "type": "training",
      "description": "Training step 1463",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:25",
      "total_flops_so_far": 3.4773201040667964e+16,
      "budget_used_percent": 34.77320104066796
    },
    {
      "type": "training",
      "description": "Training step 1464",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:26",
      "total_flops_so_far": 3.4796948366161212e+16,
      "budget_used_percent": 34.79694836616121
    },
    {
      "type": "training",
      "description": "Training step 1465",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:27",
      "total_flops_so_far": 3.482069569165446e+16,
      "budget_used_percent": 34.82069569165446
    },
    {
      "type": "training",
      "description": "Training step 1466",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:29",
      "total_flops_so_far": 3.4844443017147708e+16,
      "budget_used_percent": 34.84444301714771
    },
    {
      "type": "training",
      "description": "Training step 1467",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:30",
      "total_flops_so_far": 3.4868190342640956e+16,
      "budget_used_percent": 34.86819034264095
    },
    {
      "type": "training",
      "description": "Training step 1468",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:31",
      "total_flops_so_far": 3.4891937668134204e+16,
      "budget_used_percent": 34.891937668134204
    },
    {
      "type": "training",
      "description": "Training step 1469",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:33",
      "total_flops_so_far": 3.4915684993627452e+16,
      "budget_used_percent": 34.91568499362745
    },
    {
      "type": "training",
      "description": "Training step 1470",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:34",
      "total_flops_so_far": 3.49394323191207e+16,
      "budget_used_percent": 34.9394323191207
    },
    {
      "type": "training",
      "description": "Training step 1471",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:35",
      "total_flops_so_far": 3.4963179644613948e+16,
      "budget_used_percent": 34.963179644613945
    },
    {
      "type": "training",
      "description": "Training step 1472",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:37",
      "total_flops_so_far": 3.4986926970107196e+16,
      "budget_used_percent": 34.9869269701072
    },
    {
      "type": "training",
      "description": "Training step 1473",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:38",
      "total_flops_so_far": 3.5010674295600444e+16,
      "budget_used_percent": 35.01067429560044
    },
    {
      "type": "training",
      "description": "Training step 1474",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:39",
      "total_flops_so_far": 3.5034421621093692e+16,
      "budget_used_percent": 35.03442162109369
    },
    {
      "type": "training",
      "description": "Training step 1475",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:40",
      "total_flops_so_far": 3.505816894658694e+16,
      "budget_used_percent": 35.05816894658694
    },
    {
      "type": "training",
      "description": "Training step 1476",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:42",
      "total_flops_so_far": 3.5081916272080188e+16,
      "budget_used_percent": 35.081916272080186
    },
    {
      "type": "training",
      "description": "Training step 1477",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:43",
      "total_flops_so_far": 3.5105663597573436e+16,
      "budget_used_percent": 35.10566359757343
    },
    {
      "type": "training",
      "description": "Training step 1478",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:44",
      "total_flops_so_far": 3.5129410923066684e+16,
      "budget_used_percent": 35.129410923066686
    },
    {
      "type": "training",
      "description": "Training step 1479",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:46",
      "total_flops_so_far": 3.5153158248559932e+16,
      "budget_used_percent": 35.15315824855993
    },
    {
      "type": "training",
      "description": "Training step 1480",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:47",
      "total_flops_so_far": 3.517690557405318e+16,
      "budget_used_percent": 35.17690557405318
    },
    {
      "type": "training",
      "description": "Training step 1481",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:48",
      "total_flops_so_far": 3.5200652899546428e+16,
      "budget_used_percent": 35.200652899546434
    },
    {
      "type": "training",
      "description": "Training step 1482",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:50",
      "total_flops_so_far": 3.5224400225039676e+16,
      "budget_used_percent": 35.22440022503967
    },
    {
      "type": "training",
      "description": "Training step 1483",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:51",
      "total_flops_so_far": 3.5248147550532924e+16,
      "budget_used_percent": 35.24814755053293
    },
    {
      "type": "training",
      "description": "Training step 1484",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:52",
      "total_flops_so_far": 3.5271894876026172e+16,
      "budget_used_percent": 35.271894876026174
    },
    {
      "type": "training",
      "description": "Training step 1485",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:54",
      "total_flops_so_far": 3.529564220151942e+16,
      "budget_used_percent": 35.29564220151942
    },
    {
      "type": "training",
      "description": "Training step 1486",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:55",
      "total_flops_so_far": 3.5319389527012668e+16,
      "budget_used_percent": 35.31938952701267
    },
    {
      "type": "training",
      "description": "Training step 1487",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:56",
      "total_flops_so_far": 3.5343136852505916e+16,
      "budget_used_percent": 35.34313685250592
    },
    {
      "type": "training",
      "description": "Training step 1488",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:58",
      "total_flops_so_far": 3.5366884177999164e+16,
      "budget_used_percent": 35.36688417799916
    },
    {
      "type": "training",
      "description": "Training step 1489",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:15:59",
      "total_flops_so_far": 3.5390631503492412e+16,
      "budget_used_percent": 35.390631503492415
    },
    {
      "type": "training",
      "description": "Training step 1490",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:00",
      "total_flops_so_far": 3.541437882898566e+16,
      "budget_used_percent": 35.41437882898566
    },
    {
      "type": "training",
      "description": "Training step 1491",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:02",
      "total_flops_so_far": 3.5438126154478908e+16,
      "budget_used_percent": 35.43812615447891
    },
    {
      "type": "training",
      "description": "Training step 1492",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:03",
      "total_flops_so_far": 3.5461873479972156e+16,
      "budget_used_percent": 35.461873479972155
    },
    {
      "type": "training",
      "description": "Training step 1493",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:04",
      "total_flops_so_far": 3.5485620805465404e+16,
      "budget_used_percent": 35.48562080546541
    },
    {
      "type": "training",
      "description": "Training step 1494",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:05",
      "total_flops_so_far": 3.5509368130958652e+16,
      "budget_used_percent": 35.50936813095865
    },
    {
      "type": "training",
      "description": "Training step 1495",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:07",
      "total_flops_so_far": 3.55331154564519e+16,
      "budget_used_percent": 35.5331154564519
    },
    {
      "type": "training",
      "description": "Training step 1496",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:08",
      "total_flops_so_far": 3.5556862781945148e+16,
      "budget_used_percent": 35.55686278194515
    },
    {
      "type": "training",
      "description": "Training step 1497",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:09",
      "total_flops_so_far": 3.5580610107438396e+16,
      "budget_used_percent": 35.580610107438396
    },
    {
      "type": "training",
      "description": "Training step 1498",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:11",
      "total_flops_so_far": 3.5604357432931644e+16,
      "budget_used_percent": 35.60435743293164
    },
    {
      "type": "training",
      "description": "Training step 1499",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:12",
      "total_flops_so_far": 3.5628104758424892e+16,
      "budget_used_percent": 35.6281047584249
    },
    {
      "type": "training",
      "description": "Training step 1500",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:13",
      "total_flops_so_far": 3.565185208391814e+16,
      "budget_used_percent": 35.651852083918136
    },
    {
      "type": "training",
      "description": "Training step 1501",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:15",
      "total_flops_so_far": 3.5675599409411388e+16,
      "budget_used_percent": 35.67559940941139
    },
    {
      "type": "training",
      "description": "Training step 1502",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:16",
      "total_flops_so_far": 3.5699346734904636e+16,
      "budget_used_percent": 35.69934673490464
    },
    {
      "type": "training",
      "description": "Training step 1503",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:17",
      "total_flops_so_far": 3.5723094060397884e+16,
      "budget_used_percent": 35.723094060397884
    },
    {
      "type": "training",
      "description": "Training step 1504",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:19",
      "total_flops_so_far": 3.5746841385891132e+16,
      "budget_used_percent": 35.74684138589113
    },
    {
      "type": "training",
      "description": "Training step 1505",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:20",
      "total_flops_so_far": 3.577058871138438e+16,
      "budget_used_percent": 35.770588711384384
    },
    {
      "type": "training",
      "description": "Training step 1506",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:21",
      "total_flops_so_far": 3.5794336036877628e+16,
      "budget_used_percent": 35.794336036877624
    },
    {
      "type": "training",
      "description": "Training step 1507",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:23",
      "total_flops_so_far": 3.5818083362370876e+16,
      "budget_used_percent": 35.81808336237088
    },
    {
      "type": "training",
      "description": "Training step 1508",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:24",
      "total_flops_so_far": 3.5841830687864124e+16,
      "budget_used_percent": 35.841830687864125
    },
    {
      "type": "training",
      "description": "Training step 1509",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:25",
      "total_flops_so_far": 3.5865578013357372e+16,
      "budget_used_percent": 35.86557801335737
    },
    {
      "type": "training",
      "description": "Training step 1510",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:26",
      "total_flops_so_far": 3.588932533885062e+16,
      "budget_used_percent": 35.88932533885062
    },
    {
      "type": "training",
      "description": "Training step 1511",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:28",
      "total_flops_so_far": 3.5913072664343868e+16,
      "budget_used_percent": 35.91307266434387
    },
    {
      "type": "training",
      "description": "Training step 1512",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:29",
      "total_flops_so_far": 3.5936819989837116e+16,
      "budget_used_percent": 35.93681998983712
    },
    {
      "type": "training",
      "description": "Training step 1513",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:30",
      "total_flops_so_far": 3.5960567315330364e+16,
      "budget_used_percent": 35.960567315330366
    },
    {
      "type": "training",
      "description": "Training step 1514",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:32",
      "total_flops_so_far": 3.5984314640823612e+16,
      "budget_used_percent": 35.98431464082361
    },
    {
      "type": "training",
      "description": "Training step 1515",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:33",
      "total_flops_so_far": 3.600806196631686e+16,
      "budget_used_percent": 36.00806196631686
    },
    {
      "type": "training",
      "description": "Training step 1516",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:34",
      "total_flops_so_far": 3.603180929181011e+16,
      "budget_used_percent": 36.03180929181011
    },
    {
      "type": "training",
      "description": "Training step 1517",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:36",
      "total_flops_so_far": 3.605555661730336e+16,
      "budget_used_percent": 36.05555661730336
    },
    {
      "type": "training",
      "description": "Training step 1518",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:37",
      "total_flops_so_far": 3.607930394279661e+16,
      "budget_used_percent": 36.07930394279661
    },
    {
      "type": "training",
      "description": "Training step 1519",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:38",
      "total_flops_so_far": 3.610305126828986e+16,
      "budget_used_percent": 36.10305126828985
    },
    {
      "type": "training",
      "description": "Training step 1520",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:40",
      "total_flops_so_far": 3.61267985937831e+16,
      "budget_used_percent": 36.12679859378311
    },
    {
      "type": "training",
      "description": "Training step 1521",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:41",
      "total_flops_so_far": 3.615054591927635e+16,
      "budget_used_percent": 36.15054591927635
    },
    {
      "type": "training",
      "description": "Training step 1522",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:42",
      "total_flops_so_far": 3.61742932447696e+16,
      "budget_used_percent": 36.1742932447696
    },
    {
      "type": "training",
      "description": "Training step 1523",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:44",
      "total_flops_so_far": 3.619804057026285e+16,
      "budget_used_percent": 36.19804057026285
    },
    {
      "type": "training",
      "description": "Training step 1524",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:45",
      "total_flops_so_far": 3.62217878957561e+16,
      "budget_used_percent": 36.221787895756094
    },
    {
      "type": "training",
      "description": "Training step 1525",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:46",
      "total_flops_so_far": 3.624553522124934e+16,
      "budget_used_percent": 36.24553522124934
    },
    {
      "type": "training",
      "description": "Training step 1526",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:47",
      "total_flops_so_far": 3.626928254674259e+16,
      "budget_used_percent": 36.269282546742595
    },
    {
      "type": "training",
      "description": "Training step 1527",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:49",
      "total_flops_so_far": 3.629302987223584e+16,
      "budget_used_percent": 36.293029872235834
    },
    {
      "type": "training",
      "description": "Training step 1528",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:50",
      "total_flops_so_far": 3.631677719772909e+16,
      "budget_used_percent": 36.31677719772909
    },
    {
      "type": "training",
      "description": "Training step 1529",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:51",
      "total_flops_so_far": 3.634052452322234e+16,
      "budget_used_percent": 36.340524523222335
    },
    {
      "type": "training",
      "description": "Training step 1530",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:53",
      "total_flops_so_far": 3.636427184871558e+16,
      "budget_used_percent": 36.36427184871558
    },
    {
      "type": "training",
      "description": "Training step 1531",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:54",
      "total_flops_so_far": 3.638801917420883e+16,
      "budget_used_percent": 36.38801917420883
    },
    {
      "type": "training",
      "description": "Training step 1532",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:55",
      "total_flops_so_far": 3.641176649970208e+16,
      "budget_used_percent": 36.41176649970208
    },
    {
      "type": "training",
      "description": "Training step 1533",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:57",
      "total_flops_so_far": 3.643551382519533e+16,
      "budget_used_percent": 36.43551382519533
    },
    {
      "type": "training",
      "description": "Training step 1534",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:58",
      "total_flops_so_far": 3.645926115068858e+16,
      "budget_used_percent": 36.459261150688576
    },
    {
      "type": "training",
      "description": "Training step 1535",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:16:59",
      "total_flops_so_far": 3.648300847618182e+16,
      "budget_used_percent": 36.48300847618182
    },
    {
      "type": "training",
      "description": "Training step 1536",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:01",
      "total_flops_so_far": 3.650675580167507e+16,
      "budget_used_percent": 36.50675580167507
    },
    {
      "type": "training",
      "description": "Training step 1537",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:02",
      "total_flops_so_far": 3.653050312716832e+16,
      "budget_used_percent": 36.530503127168316
    },
    {
      "type": "training",
      "description": "Training step 1538",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:03",
      "total_flops_so_far": 3.655425045266157e+16,
      "budget_used_percent": 36.55425045266157
    },
    {
      "type": "training",
      "description": "Training step 1539",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:05",
      "total_flops_so_far": 3.657799777815482e+16,
      "budget_used_percent": 36.57799777815482
    },
    {
      "type": "training",
      "description": "Training step 1540",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:06",
      "total_flops_so_far": 3.660174510364806e+16,
      "budget_used_percent": 36.601745103648064
    },
    {
      "type": "training",
      "description": "Training step 1541",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:07",
      "total_flops_so_far": 3.662549242914131e+16,
      "budget_used_percent": 36.62549242914131
    },
    {
      "type": "training",
      "description": "Training step 1542",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:09",
      "total_flops_so_far": 3.664923975463456e+16,
      "budget_used_percent": 36.64923975463456
    },
    {
      "type": "training",
      "description": "Training step 1543",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:10",
      "total_flops_so_far": 3.667298708012781e+16,
      "budget_used_percent": 36.672987080127804
    },
    {
      "type": "training",
      "description": "Training step 1544",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:11",
      "total_flops_so_far": 3.669673440562106e+16,
      "budget_used_percent": 36.69673440562106
    },
    {
      "type": "training",
      "description": "Training step 1545",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:12",
      "total_flops_so_far": 3.67204817311143e+16,
      "budget_used_percent": 36.720481731114305
    },
    {
      "type": "training",
      "description": "Training step 1546",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:14",
      "total_flops_so_far": 3.674422905660755e+16,
      "budget_used_percent": 36.74422905660755
    },
    {
      "type": "training",
      "description": "Training step 1547",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:15",
      "total_flops_so_far": 3.67679763821008e+16,
      "budget_used_percent": 36.7679763821008
    },
    {
      "type": "training",
      "description": "Training step 1548",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:16",
      "total_flops_so_far": 3.679172370759405e+16,
      "budget_used_percent": 36.791723707594045
    },
    {
      "type": "training",
      "description": "Training step 1549",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:18",
      "total_flops_so_far": 3.68154710330873e+16,
      "budget_used_percent": 36.81547103308729
    },
    {
      "type": "training",
      "description": "Training step 1550",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:19",
      "total_flops_so_far": 3.683921835858054e+16,
      "budget_used_percent": 36.839218358580545
    },
    {
      "type": "training",
      "description": "Training step 1551",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:20",
      "total_flops_so_far": 3.686296568407379e+16,
      "budget_used_percent": 36.86296568407379
    },
    {
      "type": "training",
      "description": "Training step 1552",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:22",
      "total_flops_so_far": 3.688671300956704e+16,
      "budget_used_percent": 36.88671300956704
    },
    {
      "type": "training",
      "description": "Training step 1553",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:23",
      "total_flops_so_far": 3.691046033506029e+16,
      "budget_used_percent": 36.910460335060286
    },
    {
      "type": "training",
      "description": "Training step 1554",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:24",
      "total_flops_so_far": 3.693420766055354e+16,
      "budget_used_percent": 36.93420766055354
    },
    {
      "type": "training",
      "description": "Training step 1555",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:26",
      "total_flops_so_far": 3.695795498604678e+16,
      "budget_used_percent": 36.95795498604678
    },
    {
      "type": "training",
      "description": "Training step 1556",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:27",
      "total_flops_so_far": 3.698170231154003e+16,
      "budget_used_percent": 36.98170231154003
    },
    {
      "type": "training",
      "description": "Training step 1557",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:28",
      "total_flops_so_far": 3.700544963703328e+16,
      "budget_used_percent": 37.00544963703328
    },
    {
      "type": "training",
      "description": "Training step 1558",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:30",
      "total_flops_so_far": 3.702919696252653e+16,
      "budget_used_percent": 37.02919696252653
    },
    {
      "type": "training",
      "description": "Training step 1559",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:31",
      "total_flops_so_far": 3.705294428801978e+16,
      "budget_used_percent": 37.05294428801977
    },
    {
      "type": "training",
      "description": "Training step 1560",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:32",
      "total_flops_so_far": 3.707669161351302e+16,
      "budget_used_percent": 37.07669161351303
    },
    {
      "type": "training",
      "description": "Training step 1561",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:33",
      "total_flops_so_far": 3.710043893900627e+16,
      "budget_used_percent": 37.10043893900627
    },
    {
      "type": "training",
      "description": "Training step 1562",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:35",
      "total_flops_so_far": 3.712418626449952e+16,
      "budget_used_percent": 37.12418626449952
    },
    {
      "type": "training",
      "description": "Training step 1563",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:36",
      "total_flops_so_far": 3.714793358999277e+16,
      "budget_used_percent": 37.14793358999277
    },
    {
      "type": "training",
      "description": "Training step 1564",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:38",
      "total_flops_so_far": 3.717168091548602e+16,
      "budget_used_percent": 37.171680915486014
    },
    {
      "type": "training",
      "description": "Training step 1565",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:39",
      "total_flops_so_far": 3.719542824097926e+16,
      "budget_used_percent": 37.19542824097926
    },
    {
      "type": "training",
      "description": "Training step 1566",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:40",
      "total_flops_so_far": 3.721917556647251e+16,
      "budget_used_percent": 37.219175566472515
    },
    {
      "type": "training",
      "description": "Training step 1567",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:42",
      "total_flops_so_far": 3.724292289196576e+16,
      "budget_used_percent": 37.242922891965755
    },
    {
      "type": "training",
      "description": "Training step 1568",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:43",
      "total_flops_so_far": 3.726667021745901e+16,
      "budget_used_percent": 37.26667021745901
    },
    {
      "type": "training",
      "description": "Training step 1569",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:44",
      "total_flops_so_far": 3.729041754295226e+16,
      "budget_used_percent": 37.290417542952255
    },
    {
      "type": "training",
      "description": "Training step 1570",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:45",
      "total_flops_so_far": 3.73141648684455e+16,
      "budget_used_percent": 37.3141648684455
    },
    {
      "type": "training",
      "description": "Training step 1571",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:47",
      "total_flops_so_far": 3.733791219393875e+16,
      "budget_used_percent": 37.33791219393875
    },
    {
      "type": "training",
      "description": "Training step 1572",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:48",
      "total_flops_so_far": 3.7361659519432e+16,
      "budget_used_percent": 37.361659519432
    },
    {
      "type": "training",
      "description": "Training step 1573",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:49",
      "total_flops_so_far": 3.738540684492525e+16,
      "budget_used_percent": 37.38540684492524
    },
    {
      "type": "training",
      "description": "Training step 1574",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:51",
      "total_flops_so_far": 3.74091541704185e+16,
      "budget_used_percent": 37.409154170418496
    },
    {
      "type": "training",
      "description": "Training step 1575",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:52",
      "total_flops_so_far": 3.743290149591174e+16,
      "budget_used_percent": 37.43290149591175
    },
    {
      "type": "training",
      "description": "Training step 1576",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:53",
      "total_flops_so_far": 3.745664882140499e+16,
      "budget_used_percent": 37.45664882140499
    },
    {
      "type": "training",
      "description": "Training step 1577",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:55",
      "total_flops_so_far": 3.748039614689824e+16,
      "budget_used_percent": 37.480396146898244
    },
    {
      "type": "training",
      "description": "Training step 1578",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:56",
      "total_flops_so_far": 3.750414347239149e+16,
      "budget_used_percent": 37.50414347239149
    },
    {
      "type": "training",
      "description": "Training step 1579",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:57",
      "total_flops_so_far": 3.752789079788474e+16,
      "budget_used_percent": 37.52789079788474
    },
    {
      "type": "training",
      "description": "Training step 1580",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:17:59",
      "total_flops_so_far": 3.755163812337798e+16,
      "budget_used_percent": 37.551638123377984
    },
    {
      "type": "training",
      "description": "Training step 1581",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:00",
      "total_flops_so_far": 3.757538544887123e+16,
      "budget_used_percent": 37.57538544887124
    },
    {
      "type": "training",
      "description": "Training step 1582",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:01",
      "total_flops_so_far": 3.759913277436448e+16,
      "budget_used_percent": 37.59913277436448
    },
    {
      "type": "training",
      "description": "Training step 1583",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:03",
      "total_flops_so_far": 3.762288009985773e+16,
      "budget_used_percent": 37.62288009985773
    },
    {
      "type": "training",
      "description": "Training step 1584",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:03",
      "total_flops_so_far": 3.764662742535098e+16,
      "budget_used_percent": 37.64662742535098
    },
    {
      "type": "training",
      "description": "Training step 1585",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:04",
      "total_flops_so_far": 3.767037475084422e+16,
      "budget_used_percent": 37.670374750844225
    },
    {
      "type": "training",
      "description": "Training step 1586",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:06",
      "total_flops_so_far": 3.769412207633747e+16,
      "budget_used_percent": 37.69412207633747
    },
    {
      "type": "training",
      "description": "Training step 1587",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:07",
      "total_flops_so_far": 3.771786940183072e+16,
      "budget_used_percent": 37.717869401830725
    },
    {
      "type": "training",
      "description": "Training step 1588",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:08",
      "total_flops_so_far": 3.774161672732397e+16,
      "budget_used_percent": 37.741616727323965
    },
    {
      "type": "training",
      "description": "Training step 1589",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:09",
      "total_flops_so_far": 3.776536405281722e+16,
      "budget_used_percent": 37.76536405281722
    },
    {
      "type": "training",
      "description": "Training step 1590",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:11",
      "total_flops_so_far": 3.778911137831046e+16,
      "budget_used_percent": 37.789111378310466
    },
    {
      "type": "training",
      "description": "Training step 1591",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:12",
      "total_flops_so_far": 3.781285870380371e+16,
      "budget_used_percent": 37.81285870380371
    },
    {
      "type": "training",
      "description": "Training step 1592",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:13",
      "total_flops_so_far": 3.783660602929696e+16,
      "budget_used_percent": 37.83660602929696
    },
    {
      "type": "training",
      "description": "Training step 1593",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:15",
      "total_flops_so_far": 3.786035335479021e+16,
      "budget_used_percent": 37.86035335479021
    },
    {
      "type": "training",
      "description": "Training step 1594",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:16",
      "total_flops_so_far": 3.788410068028346e+16,
      "budget_used_percent": 37.88410068028345
    },
    {
      "type": "training",
      "description": "Training step 1595",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:17",
      "total_flops_so_far": 3.79078480057767e+16,
      "budget_used_percent": 37.90784800577671
    },
    {
      "type": "training",
      "description": "Training step 1596",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:19",
      "total_flops_so_far": 3.793159533126995e+16,
      "budget_used_percent": 37.93159533126995
    },
    {
      "type": "training",
      "description": "Training step 1597",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:20",
      "total_flops_so_far": 3.79553426567632e+16,
      "budget_used_percent": 37.9553426567632
    },
    {
      "type": "training",
      "description": "Training step 1598",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:21",
      "total_flops_so_far": 3.797908998225645e+16,
      "budget_used_percent": 37.97908998225645
    },
    {
      "type": "training",
      "description": "Training step 1599",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:23",
      "total_flops_so_far": 3.80028373077497e+16,
      "budget_used_percent": 38.0028373077497
    },
    {
      "type": "training",
      "description": "Training step 1600",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:24",
      "total_flops_so_far": 3.802658463324294e+16,
      "budget_used_percent": 38.02658463324294
    },
    {
      "type": "training",
      "description": "Training step 1601",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:25",
      "total_flops_so_far": 3.805033195873619e+16,
      "budget_used_percent": 38.050331958736194
    },
    {
      "type": "training",
      "description": "Training step 1602",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:27",
      "total_flops_so_far": 3.807407928422944e+16,
      "budget_used_percent": 38.07407928422944
    },
    {
      "type": "training",
      "description": "Training step 1603",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:28",
      "total_flops_so_far": 3.809782660972269e+16,
      "budget_used_percent": 38.09782660972269
    },
    {
      "type": "training",
      "description": "Training step 1604",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:29",
      "total_flops_so_far": 3.812157393521594e+16,
      "budget_used_percent": 38.121573935215935
    },
    {
      "type": "training",
      "description": "Training step 1605",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:31",
      "total_flops_so_far": 3.814532126070918e+16,
      "budget_used_percent": 38.14532126070919
    },
    {
      "type": "training",
      "description": "Training step 1606",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:32",
      "total_flops_so_far": 3.816906858620243e+16,
      "budget_used_percent": 38.16906858620243
    },
    {
      "type": "training",
      "description": "Training step 1607",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:33",
      "total_flops_so_far": 3.819281591169568e+16,
      "budget_used_percent": 38.19281591169568
    },
    {
      "type": "training",
      "description": "Training step 1608",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:34",
      "total_flops_so_far": 3.821656323718893e+16,
      "budget_used_percent": 38.21656323718893
    },
    {
      "type": "training",
      "description": "Training step 1609",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:36",
      "total_flops_so_far": 3.824031056268218e+16,
      "budget_used_percent": 38.240310562682176
    },
    {
      "type": "training",
      "description": "Training step 1610",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:37",
      "total_flops_so_far": 3.826405788817542e+16,
      "budget_used_percent": 38.26405788817542
    },
    {
      "type": "training",
      "description": "Training step 1611",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:38",
      "total_flops_so_far": 3.828780521366867e+16,
      "budget_used_percent": 38.287805213668676
    },
    {
      "type": "training",
      "description": "Training step 1612",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:40",
      "total_flops_so_far": 3.831155253916192e+16,
      "budget_used_percent": 38.31155253916192
    },
    {
      "type": "training",
      "description": "Training step 1613",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:41",
      "total_flops_so_far": 3.833529986465517e+16,
      "budget_used_percent": 38.33529986465517
    },
    {
      "type": "training",
      "description": "Training step 1614",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:42",
      "total_flops_so_far": 3.835904719014842e+16,
      "budget_used_percent": 38.35904719014842
    },
    {
      "type": "training",
      "description": "Training step 1615",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:44",
      "total_flops_so_far": 3.838279451564166e+16,
      "budget_used_percent": 38.38279451564166
    },
    {
      "type": "training",
      "description": "Training step 1616",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:45",
      "total_flops_so_far": 3.840654184113491e+16,
      "budget_used_percent": 38.40654184113491
    },
    {
      "type": "training",
      "description": "Training step 1617",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:46",
      "total_flops_so_far": 3.843028916662816e+16,
      "budget_used_percent": 38.430289166628164
    },
    {
      "type": "training",
      "description": "Training step 1618",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:48",
      "total_flops_so_far": 3.845403649212141e+16,
      "budget_used_percent": 38.45403649212141
    },
    {
      "type": "training",
      "description": "Training step 1619",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:49",
      "total_flops_so_far": 3.847778381761466e+16,
      "budget_used_percent": 38.47778381761466
    },
    {
      "type": "training",
      "description": "Training step 1620",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:50",
      "total_flops_so_far": 3.85015311431079e+16,
      "budget_used_percent": 38.501531143107904
    },
    {
      "type": "training",
      "description": "Training step 1621",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:52",
      "total_flops_so_far": 3.852527846860115e+16,
      "budget_used_percent": 38.52527846860115
    },
    {
      "type": "training",
      "description": "Training step 1622",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:53",
      "total_flops_so_far": 3.85490257940944e+16,
      "budget_used_percent": 38.5490257940944
    },
    {
      "type": "training",
      "description": "Training step 1623",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:54",
      "total_flops_so_far": 3.857277311958765e+16,
      "budget_used_percent": 38.57277311958765
    },
    {
      "type": "training",
      "description": "Training step 1624",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:56",
      "total_flops_so_far": 3.85965204450809e+16,
      "budget_used_percent": 38.5965204450809
    },
    {
      "type": "training",
      "description": "Training step 1625",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:57",
      "total_flops_so_far": 3.862026777057414e+16,
      "budget_used_percent": 38.620267770574145
    },
    {
      "type": "training",
      "description": "Training step 1626",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:18:58",
      "total_flops_so_far": 3.864401509606739e+16,
      "budget_used_percent": 38.64401509606739
    },
    {
      "type": "training",
      "description": "Training step 1627",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:00",
      "total_flops_so_far": 3.866776242156064e+16,
      "budget_used_percent": 38.66776242156064
    },
    {
      "type": "training",
      "description": "Training step 1628",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:01",
      "total_flops_so_far": 3.869150974705389e+16,
      "budget_used_percent": 38.691509747053885
    },
    {
      "type": "training",
      "description": "Training step 1629",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:02",
      "total_flops_so_far": 3.871525707254714e+16,
      "budget_used_percent": 38.71525707254714
    },
    {
      "type": "training",
      "description": "Training step 1630",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:04",
      "total_flops_so_far": 3.873900439804038e+16,
      "budget_used_percent": 38.739004398040386
    },
    {
      "type": "training",
      "description": "Training step 1631",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:05",
      "total_flops_so_far": 3.876275172353363e+16,
      "budget_used_percent": 38.76275172353363
    },
    {
      "type": "training",
      "description": "Training step 1632",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:06",
      "total_flops_so_far": 3.878649904902688e+16,
      "budget_used_percent": 38.78649904902688
    },
    {
      "type": "training",
      "description": "Training step 1633",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:08",
      "total_flops_so_far": 3.881024637452013e+16,
      "budget_used_percent": 38.81024637452013
    },
    {
      "type": "training",
      "description": "Training step 1634",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:09",
      "total_flops_so_far": 3.883399370001338e+16,
      "budget_used_percent": 38.83399370001337
    },
    {
      "type": "training",
      "description": "Training step 1635",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:10",
      "total_flops_so_far": 3.885774102550662e+16,
      "budget_used_percent": 38.85774102550663
    },
    {
      "type": "training",
      "description": "Training step 1636",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:11",
      "total_flops_so_far": 3.888148835099987e+16,
      "budget_used_percent": 38.881488350999874
    },
    {
      "type": "training",
      "description": "Training step 1637",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:13",
      "total_flops_so_far": 3.890523567649312e+16,
      "budget_used_percent": 38.90523567649312
    },
    {
      "type": "training",
      "description": "Training step 1638",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:14",
      "total_flops_so_far": 3.892898300198637e+16,
      "budget_used_percent": 38.92898300198637
    },
    {
      "type": "training",
      "description": "Training step 1639",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:15",
      "total_flops_so_far": 3.895273032747962e+16,
      "budget_used_percent": 38.95273032747962
    },
    {
      "type": "training",
      "description": "Training step 1640",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:17",
      "total_flops_so_far": 3.897647765297286e+16,
      "budget_used_percent": 38.97647765297286
    },
    {
      "type": "training",
      "description": "Training step 1641",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:18",
      "total_flops_so_far": 3.900022497846611e+16,
      "budget_used_percent": 39.000224978466115
    },
    {
      "type": "training",
      "description": "Training step 1642",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:19",
      "total_flops_so_far": 3.902397230395936e+16,
      "budget_used_percent": 39.02397230395936
    },
    {
      "type": "training",
      "description": "Training step 1643",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:21",
      "total_flops_so_far": 3.904771962945261e+16,
      "budget_used_percent": 39.04771962945261
    },
    {
      "type": "training",
      "description": "Training step 1644",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:22",
      "total_flops_so_far": 3.907146695494586e+16,
      "budget_used_percent": 39.071466954945855
    },
    {
      "type": "training",
      "description": "Training step 1645",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:23",
      "total_flops_so_far": 3.90952142804391e+16,
      "budget_used_percent": 39.09521428043911
    },
    {
      "type": "training",
      "description": "Training step 1646",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:25",
      "total_flops_so_far": 3.911896160593235e+16,
      "budget_used_percent": 39.11896160593235
    },
    {
      "type": "training",
      "description": "Training step 1647",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:26",
      "total_flops_so_far": 3.91427089314256e+16,
      "budget_used_percent": 39.1427089314256
    },
    {
      "type": "training",
      "description": "Training step 1648",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:27",
      "total_flops_so_far": 3.916645625691885e+16,
      "budget_used_percent": 39.16645625691885
    },
    {
      "type": "training",
      "description": "Training step 1649",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:29",
      "total_flops_so_far": 3.91902035824121e+16,
      "budget_used_percent": 39.190203582412096
    },
    {
      "type": "training",
      "description": "Training step 1650",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:30",
      "total_flops_so_far": 3.921395090790534e+16,
      "budget_used_percent": 39.21395090790534
    },
    {
      "type": "training",
      "description": "Training step 1651",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:31",
      "total_flops_so_far": 3.923769823339859e+16,
      "budget_used_percent": 39.237698233398596
    },
    {
      "type": "training",
      "description": "Training step 1652",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:33",
      "total_flops_so_far": 3.926144555889184e+16,
      "budget_used_percent": 39.261445558891836
    },
    {
      "type": "training",
      "description": "Training step 1653",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:34",
      "total_flops_so_far": 3.928519288438509e+16,
      "budget_used_percent": 39.28519288438509
    },
    {
      "type": "training",
      "description": "Training step 1654",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:35",
      "total_flops_so_far": 3.930894020987834e+16,
      "budget_used_percent": 39.30894020987834
    },
    {
      "type": "training",
      "description": "Training step 1655",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:36",
      "total_flops_so_far": 3.933268753537158e+16,
      "budget_used_percent": 39.33268753537158
    },
    {
      "type": "training",
      "description": "Training step 1656",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:38",
      "total_flops_so_far": 3.935643486086483e+16,
      "budget_used_percent": 39.35643486086483
    },
    {
      "type": "training",
      "description": "Training step 1657",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:39",
      "total_flops_so_far": 3.938018218635808e+16,
      "budget_used_percent": 39.380182186358084
    },
    {
      "type": "training",
      "description": "Training step 1658",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:40",
      "total_flops_so_far": 3.940392951185133e+16,
      "budget_used_percent": 39.403929511851324
    },
    {
      "type": "training",
      "description": "Training step 1659",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:42",
      "total_flops_so_far": 3.942767683734458e+16,
      "budget_used_percent": 39.42767683734458
    },
    {
      "type": "training",
      "description": "Training step 1660",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:43",
      "total_flops_so_far": 3.945142416283782e+16,
      "budget_used_percent": 39.451424162837824
    },
    {
      "type": "training",
      "description": "Training step 1661",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:44",
      "total_flops_so_far": 3.947517148833107e+16,
      "budget_used_percent": 39.47517148833107
    },
    {
      "type": "training",
      "description": "Training step 1662",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:46",
      "total_flops_so_far": 3.949891881382432e+16,
      "budget_used_percent": 39.49891881382432
    },
    {
      "type": "training",
      "description": "Training step 1663",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:47",
      "total_flops_so_far": 3.952266613931757e+16,
      "budget_used_percent": 39.52266613931757
    },
    {
      "type": "training",
      "description": "Training step 1664",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:48",
      "total_flops_so_far": 3.954641346481082e+16,
      "budget_used_percent": 39.54641346481081
    },
    {
      "type": "training",
      "description": "Training step 1665",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:50",
      "total_flops_so_far": 3.957016079030406e+16,
      "budget_used_percent": 39.570160790304065
    },
    {
      "type": "training",
      "description": "Training step 1666",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:51",
      "total_flops_so_far": 3.959390811579731e+16,
      "budget_used_percent": 39.59390811579731
    },
    {
      "type": "training",
      "description": "Training step 1667",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:52",
      "total_flops_so_far": 3.961765544129056e+16,
      "budget_used_percent": 39.61765544129056
    },
    {
      "type": "training",
      "description": "Training step 1668",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:54",
      "total_flops_so_far": 3.964140276678381e+16,
      "budget_used_percent": 39.641402766783806
    },
    {
      "type": "training",
      "description": "Training step 1669",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:55",
      "total_flops_so_far": 3.966515009227706e+16,
      "budget_used_percent": 39.66515009227706
    },
    {
      "type": "training",
      "description": "Training step 1670",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:56",
      "total_flops_so_far": 3.96888974177703e+16,
      "budget_used_percent": 39.6888974177703
    },
    {
      "type": "training",
      "description": "Training step 1671",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:58",
      "total_flops_so_far": 3.971264474326355e+16,
      "budget_used_percent": 39.71264474326355
    },
    {
      "type": "training",
      "description": "Training step 1672",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:19:59",
      "total_flops_so_far": 3.97363920687568e+16,
      "budget_used_percent": 39.7363920687568
    },
    {
      "type": "training",
      "description": "Training step 1673",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:00",
      "total_flops_so_far": 3.976013939425005e+16,
      "budget_used_percent": 39.76013939425005
    },
    {
      "type": "training",
      "description": "Training step 1674",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:01",
      "total_flops_so_far": 3.97838867197433e+16,
      "budget_used_percent": 39.78388671974329
    },
    {
      "type": "training",
      "description": "Training step 1675",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:03",
      "total_flops_so_far": 3.980763404523654e+16,
      "budget_used_percent": 39.80763404523655
    },
    {
      "type": "training",
      "description": "Training step 1676",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:04",
      "total_flops_so_far": 3.983138137072979e+16,
      "budget_used_percent": 39.83138137072979
    },
    {
      "type": "training",
      "description": "Training step 1677",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:06",
      "total_flops_so_far": 3.985512869622304e+16,
      "budget_used_percent": 39.85512869622304
    },
    {
      "type": "training",
      "description": "Training step 1678",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:08",
      "total_flops_so_far": 3.987887602171629e+16,
      "budget_used_percent": 39.87887602171629
    },
    {
      "type": "training",
      "description": "Training step 1679",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:13",
      "total_flops_so_far": 3.990262334720954e+16,
      "budget_used_percent": 39.902623347209534
    },
    {
      "type": "training",
      "description": "Training step 1680",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:14",
      "total_flops_so_far": 3.992637067270278e+16,
      "budget_used_percent": 39.92637067270278
    },
    {
      "type": "training",
      "description": "Training step 1681",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:16",
      "total_flops_so_far": 3.995011799819603e+16,
      "budget_used_percent": 39.950117998196035
    },
    {
      "type": "training",
      "description": "Training step 1682",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:17",
      "total_flops_so_far": 3.997386532368928e+16,
      "budget_used_percent": 39.97386532368928
    },
    {
      "type": "training",
      "description": "Training step 1683",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:18",
      "total_flops_so_far": 3.999761264918253e+16,
      "budget_used_percent": 39.99761264918253
    },
    {
      "type": "training",
      "description": "Training step 1684",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:19",
      "total_flops_so_far": 4.002135997467578e+16,
      "budget_used_percent": 40.021359974675775
    },
    {
      "type": "training",
      "description": "Training step 1685",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:21",
      "total_flops_so_far": 4.004510730016902e+16,
      "budget_used_percent": 40.04510730016902
    },
    {
      "type": "training",
      "description": "Training step 1686",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:22",
      "total_flops_so_far": 4.006885462566227e+16,
      "budget_used_percent": 40.06885462566227
    },
    {
      "type": "training",
      "description": "Training step 1687",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:23",
      "total_flops_so_far": 4.009260195115552e+16,
      "budget_used_percent": 40.09260195115552
    },
    {
      "type": "training",
      "description": "Training step 1688",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:25",
      "total_flops_so_far": 4.011634927664877e+16,
      "budget_used_percent": 40.11634927664877
    },
    {
      "type": "training",
      "description": "Training step 1689",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:26",
      "total_flops_so_far": 4.014009660214202e+16,
      "budget_used_percent": 40.140096602142016
    },
    {
      "type": "training",
      "description": "Training step 1690",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:27",
      "total_flops_so_far": 4.016384392763526e+16,
      "budget_used_percent": 40.16384392763526
    },
    {
      "type": "training",
      "description": "Training step 1691",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:29",
      "total_flops_so_far": 4.018759125312851e+16,
      "budget_used_percent": 40.18759125312851
    },
    {
      "type": "training",
      "description": "Training step 1692",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:30",
      "total_flops_so_far": 4.021133857862176e+16,
      "budget_used_percent": 40.211338578621756
    },
    {
      "type": "training",
      "description": "Training step 1693",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:31",
      "total_flops_so_far": 4.023508590411501e+16,
      "budget_used_percent": 40.23508590411501
    },
    {
      "type": "training",
      "description": "Training step 1694",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:33",
      "total_flops_so_far": 4.025883322960826e+16,
      "budget_used_percent": 40.25883322960826
    },
    {
      "type": "training",
      "description": "Training step 1695",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:34",
      "total_flops_so_far": 4.02825805551015e+16,
      "budget_used_percent": 40.282580555101504
    },
    {
      "type": "training",
      "description": "Training step 1696",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:35",
      "total_flops_so_far": 4.030632788059475e+16,
      "budget_used_percent": 40.30632788059475
    },
    {
      "type": "training",
      "description": "Training step 1697",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:37",
      "total_flops_so_far": 4.0330075206088e+16,
      "budget_used_percent": 40.330075206088
    },
    {
      "type": "training",
      "description": "Training step 1698",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:38",
      "total_flops_so_far": 4.035382253158125e+16,
      "budget_used_percent": 40.353822531581244
    },
    {
      "type": "training",
      "description": "Training step 1699",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:39",
      "total_flops_so_far": 4.03775698570745e+16,
      "budget_used_percent": 40.3775698570745
    },
    {
      "type": "training",
      "description": "Training step 1700",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:41",
      "total_flops_so_far": 4.040131718256774e+16,
      "budget_used_percent": 40.401317182567745
    },
    {
      "type": "training",
      "description": "Training step 1701",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:42",
      "total_flops_so_far": 4.042506450806099e+16,
      "budget_used_percent": 40.42506450806099
    },
    {
      "type": "training",
      "description": "Training step 1702",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:43",
      "total_flops_so_far": 4.044881183355424e+16,
      "budget_used_percent": 40.44881183355424
    },
    {
      "type": "training",
      "description": "Training step 1703",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:44",
      "total_flops_so_far": 4.047255915904749e+16,
      "budget_used_percent": 40.47255915904749
    },
    {
      "type": "training",
      "description": "Training step 1704",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:46",
      "total_flops_so_far": 4.049630648454074e+16,
      "budget_used_percent": 40.49630648454073
    },
    {
      "type": "training",
      "description": "Training step 1705",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:47",
      "total_flops_so_far": 4.052005381003398e+16,
      "budget_used_percent": 40.520053810033986
    },
    {
      "type": "training",
      "description": "Training step 1706",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:48",
      "total_flops_so_far": 4.054380113552723e+16,
      "budget_used_percent": 40.54380113552723
    },
    {
      "type": "training",
      "description": "Training step 1707",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:50",
      "total_flops_so_far": 4.056754846102048e+16,
      "budget_used_percent": 40.56754846102048
    },
    {
      "type": "training",
      "description": "Training step 1708",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:51",
      "total_flops_so_far": 4.059129578651373e+16,
      "budget_used_percent": 40.591295786513726
    },
    {
      "type": "training",
      "description": "Training step 1709",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:52",
      "total_flops_so_far": 4.061504311200698e+16,
      "budget_used_percent": 40.61504311200698
    },
    {
      "type": "training",
      "description": "Training step 1710",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:54",
      "total_flops_so_far": 4.063879043750022e+16,
      "budget_used_percent": 40.63879043750022
    },
    {
      "type": "training",
      "description": "Training step 1711",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:55",
      "total_flops_so_far": 4.066253776299347e+16,
      "budget_used_percent": 40.66253776299347
    },
    {
      "type": "training",
      "description": "Training step 1712",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:56",
      "total_flops_so_far": 4.068628508848672e+16,
      "budget_used_percent": 40.68628508848672
    },
    {
      "type": "training",
      "description": "Training step 1713",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:58",
      "total_flops_so_far": 4.071003241397997e+16,
      "budget_used_percent": 40.71003241397997
    },
    {
      "type": "training",
      "description": "Training step 1714",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:20:59",
      "total_flops_so_far": 4.073377973947322e+16,
      "budget_used_percent": 40.73377973947321
    },
    {
      "type": "training",
      "description": "Training step 1715",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:00",
      "total_flops_so_far": 4.075752706496646e+16,
      "budget_used_percent": 40.75752706496647
    },
    {
      "type": "training",
      "description": "Training step 1716",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:01",
      "total_flops_so_far": 4.078127439045971e+16,
      "budget_used_percent": 40.78127439045971
    },
    {
      "type": "training",
      "description": "Training step 1717",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:02",
      "total_flops_so_far": 4.080502171595296e+16,
      "budget_used_percent": 40.80502171595296
    },
    {
      "type": "training",
      "description": "Training step 1718",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:03",
      "total_flops_so_far": 4.082876904144621e+16,
      "budget_used_percent": 40.82876904144621
    },
    {
      "type": "training",
      "description": "Training step 1719",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:05",
      "total_flops_so_far": 4.085251636693946e+16,
      "budget_used_percent": 40.852516366939454
    },
    {
      "type": "training",
      "description": "Training step 1720",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:06",
      "total_flops_so_far": 4.08762636924327e+16,
      "budget_used_percent": 40.8762636924327
    },
    {
      "type": "training",
      "description": "Training step 1721",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:07",
      "total_flops_so_far": 4.090001101792595e+16,
      "budget_used_percent": 40.900011017925955
    },
    {
      "type": "training",
      "description": "Training step 1722",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:09",
      "total_flops_so_far": 4.09237583434192e+16,
      "budget_used_percent": 40.923758343419195
    },
    {
      "type": "training",
      "description": "Training step 1723",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:10",
      "total_flops_so_far": 4.094750566891245e+16,
      "budget_used_percent": 40.94750566891245
    },
    {
      "type": "training",
      "description": "Training step 1724",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:11",
      "total_flops_so_far": 4.09712529944057e+16,
      "budget_used_percent": 40.971252994405695
    },
    {
      "type": "training",
      "description": "Training step 1725",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:13",
      "total_flops_so_far": 4.099500031989894e+16,
      "budget_used_percent": 40.99500031989894
    },
    {
      "type": "training",
      "description": "Training step 1726",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:14",
      "total_flops_so_far": 4.101874764539219e+16,
      "budget_used_percent": 41.01874764539219
    },
    {
      "type": "training",
      "description": "Training step 1727",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:15",
      "total_flops_so_far": 4.104249497088544e+16,
      "budget_used_percent": 41.04249497088544
    },
    {
      "type": "training",
      "description": "Training step 1728",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:17",
      "total_flops_so_far": 4.106624229637869e+16,
      "budget_used_percent": 41.06624229637868
    },
    {
      "type": "training",
      "description": "Training step 1729",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:18",
      "total_flops_so_far": 4.108998962187194e+16,
      "budget_used_percent": 41.089989621871936
    },
    {
      "type": "training",
      "description": "Training step 1730",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:19",
      "total_flops_so_far": 4.111373694736518e+16,
      "budget_used_percent": 41.11373694736518
    },
    {
      "type": "training",
      "description": "Training step 1731",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:20",
      "total_flops_so_far": 4.113748427285843e+16,
      "budget_used_percent": 41.13748427285843
    },
    {
      "type": "training",
      "description": "Training step 1732",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:22",
      "total_flops_so_far": 4.116123159835168e+16,
      "budget_used_percent": 41.16123159835168
    },
    {
      "type": "training",
      "description": "Training step 1733",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:23",
      "total_flops_so_far": 4.118497892384493e+16,
      "budget_used_percent": 41.18497892384493
    },
    {
      "type": "training",
      "description": "Training step 1734",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:24",
      "total_flops_so_far": 4.120872624933818e+16,
      "budget_used_percent": 41.20872624933817
    },
    {
      "type": "training",
      "description": "Training step 1735",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:26",
      "total_flops_so_far": 4.123247357483142e+16,
      "budget_used_percent": 41.232473574831424
    },
    {
      "type": "training",
      "description": "Training step 1736",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:27",
      "total_flops_so_far": 4.125622090032467e+16,
      "budget_used_percent": 41.25622090032467
    },
    {
      "type": "training",
      "description": "Training step 1737",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:28",
      "total_flops_so_far": 4.127996822581792e+16,
      "budget_used_percent": 41.27996822581792
    },
    {
      "type": "training",
      "description": "Training step 1738",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:30",
      "total_flops_so_far": 4.130371555131117e+16,
      "budget_used_percent": 41.303715551311164
    },
    {
      "type": "training",
      "description": "Training step 1739",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:31",
      "total_flops_so_far": 4.132746287680442e+16,
      "budget_used_percent": 41.32746287680442
    },
    {
      "type": "training",
      "description": "Training step 1740",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:32",
      "total_flops_so_far": 4.135121020229766e+16,
      "budget_used_percent": 41.351210202297665
    },
    {
      "type": "training",
      "description": "Training step 1741",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:34",
      "total_flops_so_far": 4.137495752779091e+16,
      "budget_used_percent": 41.37495752779091
    },
    {
      "type": "training",
      "description": "Training step 1742",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:35",
      "total_flops_so_far": 4.139870485328416e+16,
      "budget_used_percent": 41.39870485328416
    },
    {
      "type": "training",
      "description": "Training step 1743",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:36",
      "total_flops_so_far": 4.142245217877741e+16,
      "budget_used_percent": 41.422452178777405
    },
    {
      "type": "training",
      "description": "Training step 1744",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:38",
      "total_flops_so_far": 4.144619950427066e+16,
      "budget_used_percent": 41.44619950427065
    },
    {
      "type": "training",
      "description": "Training step 1745",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:39",
      "total_flops_so_far": 4.14699468297639e+16,
      "budget_used_percent": 41.469946829763906
    },
    {
      "type": "training",
      "description": "Training step 1746",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:40",
      "total_flops_so_far": 4.149369415525715e+16,
      "budget_used_percent": 41.49369415525715
    },
    {
      "type": "training",
      "description": "Training step 1747",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:42",
      "total_flops_so_far": 4.15174414807504e+16,
      "budget_used_percent": 41.5174414807504
    },
    {
      "type": "training",
      "description": "Training step 1748",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:43",
      "total_flops_so_far": 4.154118880624365e+16,
      "budget_used_percent": 41.54118880624365
    },
    {
      "type": "training",
      "description": "Training step 1749",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:44",
      "total_flops_so_far": 4.15649361317369e+16,
      "budget_used_percent": 41.56493613173689
    },
    {
      "type": "training",
      "description": "Training step 1750",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:46",
      "total_flops_so_far": 4.158868345723014e+16,
      "budget_used_percent": 41.58868345723015
    },
    {
      "type": "training",
      "description": "Training step 1751",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:47",
      "total_flops_so_far": 4.161243078272339e+16,
      "budget_used_percent": 41.61243078272339
    },
    {
      "type": "training",
      "description": "Training step 1752",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:48",
      "total_flops_so_far": 4.163617810821664e+16,
      "budget_used_percent": 41.63617810821664
    },
    {
      "type": "training",
      "description": "Training step 1753",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:49",
      "total_flops_so_far": 4.165992543370989e+16,
      "budget_used_percent": 41.65992543370989
    },
    {
      "type": "training",
      "description": "Training step 1754",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:51",
      "total_flops_so_far": 4.168367275920314e+16,
      "budget_used_percent": 41.68367275920314
    },
    {
      "type": "training",
      "description": "Training step 1755",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:52",
      "total_flops_so_far": 4.170742008469638e+16,
      "budget_used_percent": 41.70742008469638
    },
    {
      "type": "training",
      "description": "Training step 1756",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:53",
      "total_flops_so_far": 4.173116741018963e+16,
      "budget_used_percent": 41.731167410189634
    },
    {
      "type": "training",
      "description": "Training step 1757",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:55",
      "total_flops_so_far": 4.175491473568288e+16,
      "budget_used_percent": 41.75491473568288
    },
    {
      "type": "training",
      "description": "Training step 1758",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:56",
      "total_flops_so_far": 4.177866206117613e+16,
      "budget_used_percent": 41.77866206117613
    },
    {
      "type": "training",
      "description": "Training step 1759",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:57",
      "total_flops_so_far": 4.180240938666938e+16,
      "budget_used_percent": 41.802409386669375
    },
    {
      "type": "training",
      "description": "Training step 1760",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:21:59",
      "total_flops_so_far": 4.182615671216262e+16,
      "budget_used_percent": 41.82615671216263
    },
    {
      "type": "training",
      "description": "Training step 1761",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:00",
      "total_flops_so_far": 4.184990403765587e+16,
      "budget_used_percent": 41.849904037655875
    },
    {
      "type": "training",
      "description": "Training step 1762",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:01",
      "total_flops_so_far": 4.187365136314912e+16,
      "budget_used_percent": 41.87365136314912
    },
    {
      "type": "training",
      "description": "Training step 1763",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:03",
      "total_flops_so_far": 4.189739868864237e+16,
      "budget_used_percent": 41.89739868864237
    },
    {
      "type": "training",
      "description": "Training step 1764",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:04",
      "total_flops_so_far": 4.192114601413562e+16,
      "budget_used_percent": 41.921146014135616
    },
    {
      "type": "training",
      "description": "Training step 1765",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:05",
      "total_flops_so_far": 4.194489333962886e+16,
      "budget_used_percent": 41.94489333962886
    },
    {
      "type": "training",
      "description": "Training step 1766",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:07",
      "total_flops_so_far": 4.196864066512211e+16,
      "budget_used_percent": 41.968640665122116
    },
    {
      "type": "training",
      "description": "Training step 1767",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:08",
      "total_flops_so_far": 4.199238799061536e+16,
      "budget_used_percent": 41.99238799061536
    },
    {
      "type": "training",
      "description": "Training step 1768",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:09",
      "total_flops_so_far": 4.201613531610861e+16,
      "budget_used_percent": 42.01613531610861
    },
    {
      "type": "training",
      "description": "Training step 1769",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:11",
      "total_flops_so_far": 4.203988264160186e+16,
      "budget_used_percent": 42.03988264160186
    },
    {
      "type": "training",
      "description": "Training step 1770",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:12",
      "total_flops_so_far": 4.20636299670951e+16,
      "budget_used_percent": 42.0636299670951
    },
    {
      "type": "training",
      "description": "Training step 1771",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:13",
      "total_flops_so_far": 4.208737729258835e+16,
      "budget_used_percent": 42.08737729258835
    },
    {
      "type": "training",
      "description": "Training step 1772",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:15",
      "total_flops_so_far": 4.21111246180816e+16,
      "budget_used_percent": 42.111124618081604
    },
    {
      "type": "training",
      "description": "Training step 1773",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:16",
      "total_flops_so_far": 4.213487194357485e+16,
      "budget_used_percent": 42.13487194357485
    },
    {
      "type": "training",
      "description": "Training step 1774",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:17",
      "total_flops_so_far": 4.21586192690681e+16,
      "budget_used_percent": 42.1586192690681
    },
    {
      "type": "training",
      "description": "Training step 1775",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:19",
      "total_flops_so_far": 4.218236659456134e+16,
      "budget_used_percent": 42.182366594561344
    },
    {
      "type": "training",
      "description": "Training step 1776",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:20",
      "total_flops_so_far": 4.220611392005459e+16,
      "budget_used_percent": 42.20611392005459
    },
    {
      "type": "training",
      "description": "Training step 1777",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:21",
      "total_flops_so_far": 4.222986124554784e+16,
      "budget_used_percent": 42.22986124554784
    },
    {
      "type": "training",
      "description": "Training step 1778",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:22",
      "total_flops_so_far": 4.225360857104109e+16,
      "budget_used_percent": 42.25360857104109
    },
    {
      "type": "training",
      "description": "Training step 1779",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:24",
      "total_flops_so_far": 4.227735589653434e+16,
      "budget_used_percent": 42.27735589653434
    },
    {
      "type": "training",
      "description": "Training step 1780",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:25",
      "total_flops_so_far": 4.230110322202758e+16,
      "budget_used_percent": 42.301103222027585
    },
    {
      "type": "training",
      "description": "Training step 1781",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:26",
      "total_flops_so_far": 4.232485054752083e+16,
      "budget_used_percent": 42.32485054752083
    },
    {
      "type": "training",
      "description": "Training step 1782",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:28",
      "total_flops_so_far": 4.234859787301408e+16,
      "budget_used_percent": 42.348597873014086
    },
    {
      "type": "training",
      "description": "Training step 1783",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:29",
      "total_flops_so_far": 4.237234519850733e+16,
      "budget_used_percent": 42.372345198507325
    },
    {
      "type": "training",
      "description": "Training step 1784",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:31",
      "total_flops_so_far": 4.239609252400058e+16,
      "budget_used_percent": 42.39609252400058
    },
    {
      "type": "training",
      "description": "Training step 1785",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:32",
      "total_flops_so_far": 4.241983984949382e+16,
      "budget_used_percent": 42.419839849493826
    },
    {
      "type": "training",
      "description": "Training step 1786",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:33",
      "total_flops_so_far": 4.244358717498707e+16,
      "budget_used_percent": 42.44358717498707
    },
    {
      "type": "training",
      "description": "Training step 1787",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:35",
      "total_flops_so_far": 4.246733450048032e+16,
      "budget_used_percent": 42.46733450048032
    },
    {
      "type": "training",
      "description": "Training step 1788",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:36",
      "total_flops_so_far": 4.249108182597357e+16,
      "budget_used_percent": 42.49108182597357
    },
    {
      "type": "training",
      "description": "Training step 1789",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:37",
      "total_flops_so_far": 4.251482915146682e+16,
      "budget_used_percent": 42.51482915146681
    },
    {
      "type": "training",
      "description": "Training step 1790",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:38",
      "total_flops_so_far": 4.253857647696006e+16,
      "budget_used_percent": 42.53857647696007
    },
    {
      "type": "training",
      "description": "Training step 1791",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:40",
      "total_flops_so_far": 4.256232380245331e+16,
      "budget_used_percent": 42.562323802453314
    },
    {
      "type": "training",
      "description": "Training step 1792",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:41",
      "total_flops_so_far": 4.258607112794656e+16,
      "budget_used_percent": 42.58607112794656
    },
    {
      "type": "training",
      "description": "Training step 1793",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:42",
      "total_flops_so_far": 4.260981845343981e+16,
      "budget_used_percent": 42.60981845343981
    },
    {
      "type": "training",
      "description": "Training step 1794",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:44",
      "total_flops_so_far": 4.263356577893306e+16,
      "budget_used_percent": 42.63356577893306
    },
    {
      "type": "training",
      "description": "Training step 1795",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:45",
      "total_flops_so_far": 4.26573131044263e+16,
      "budget_used_percent": 42.6573131044263
    },
    {
      "type": "training",
      "description": "Training step 1796",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:46",
      "total_flops_so_far": 4.268106042991955e+16,
      "budget_used_percent": 42.681060429919555
    },
    {
      "type": "training",
      "description": "Training step 1797",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:48",
      "total_flops_so_far": 4.27048077554128e+16,
      "budget_used_percent": 42.7048077554128
    },
    {
      "type": "training",
      "description": "Training step 1798",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:49",
      "total_flops_so_far": 4.272855508090605e+16,
      "budget_used_percent": 42.72855508090605
    },
    {
      "type": "training",
      "description": "Training step 1799",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:50",
      "total_flops_so_far": 4.27523024063993e+16,
      "budget_used_percent": 42.752302406399295
    },
    {
      "type": "training",
      "description": "Training step 1800",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:52",
      "total_flops_so_far": 4.277604973189254e+16,
      "budget_used_percent": 42.77604973189255
    },
    {
      "type": "training",
      "description": "Training step 1801",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:53",
      "total_flops_so_far": 4.279979705738579e+16,
      "budget_used_percent": 42.79979705738579
    },
    {
      "type": "training",
      "description": "Training step 1802",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:54",
      "total_flops_so_far": 4.282354438287904e+16,
      "budget_used_percent": 42.82354438287904
    },
    {
      "type": "training",
      "description": "Training step 1803",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:56",
      "total_flops_so_far": 4.284729170837229e+16,
      "budget_used_percent": 42.84729170837229
    },
    {
      "type": "training",
      "description": "Training step 1804",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:57",
      "total_flops_so_far": 4.287103903386554e+16,
      "budget_used_percent": 42.871039033865536
    },
    {
      "type": "training",
      "description": "Training step 1805",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:22:58",
      "total_flops_so_far": 4.289478635935878e+16,
      "budget_used_percent": 42.89478635935878
    },
    {
      "type": "training",
      "description": "Training step 1806",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:00",
      "total_flops_so_far": 4.291853368485203e+16,
      "budget_used_percent": 42.91853368485204
    },
    {
      "type": "training",
      "description": "Training step 1807",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:01",
      "total_flops_so_far": 4.294228101034528e+16,
      "budget_used_percent": 42.942281010345276
    },
    {
      "type": "training",
      "description": "Training step 1808",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:02",
      "total_flops_so_far": 4.296602833583853e+16,
      "budget_used_percent": 42.96602833583853
    },
    {
      "type": "training",
      "description": "Training step 1809",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:04",
      "total_flops_so_far": 4.298977566133178e+16,
      "budget_used_percent": 42.98977566133178
    },
    {
      "type": "training",
      "description": "Training step 1810",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:05",
      "total_flops_so_far": 4.301352298682502e+16,
      "budget_used_percent": 43.01352298682502
    },
    {
      "type": "training",
      "description": "Training step 1811",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:06",
      "total_flops_so_far": 4.303727031231827e+16,
      "budget_used_percent": 43.03727031231827
    },
    {
      "type": "training",
      "description": "Training step 1812",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:08",
      "total_flops_so_far": 4.306101763781152e+16,
      "budget_used_percent": 43.061017637811524
    },
    {
      "type": "training",
      "description": "Training step 1813",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:09",
      "total_flops_so_far": 4.308476496330477e+16,
      "budget_used_percent": 43.084764963304764
    },
    {
      "type": "training",
      "description": "Training step 1814",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:10",
      "total_flops_so_far": 4.310851228879802e+16,
      "budget_used_percent": 43.10851228879802
    },
    {
      "type": "training",
      "description": "Training step 1815",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:11",
      "total_flops_so_far": 4.313225961429126e+16,
      "budget_used_percent": 43.132259614291264
    },
    {
      "type": "training",
      "description": "Training step 1816",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:13",
      "total_flops_so_far": 4.315600693978451e+16,
      "budget_used_percent": 43.15600693978451
    },
    {
      "type": "training",
      "description": "Training step 1817",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:14",
      "total_flops_so_far": 4.317975426527776e+16,
      "budget_used_percent": 43.17975426527776
    },
    {
      "type": "training",
      "description": "Training step 1818",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:15",
      "total_flops_so_far": 4.320350159077101e+16,
      "budget_used_percent": 43.20350159077101
    },
    {
      "type": "training",
      "description": "Training step 1819",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:17",
      "total_flops_so_far": 4.322724891626426e+16,
      "budget_used_percent": 43.22724891626425
    },
    {
      "type": "training",
      "description": "Training step 1820",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:18",
      "total_flops_so_far": 4.32509962417575e+16,
      "budget_used_percent": 43.250996241757505
    },
    {
      "type": "training",
      "description": "Training step 1821",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:19",
      "total_flops_so_far": 4.327474356725075e+16,
      "budget_used_percent": 43.27474356725075
    },
    {
      "type": "training",
      "description": "Training step 1822",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:21",
      "total_flops_so_far": 4.3298490892744e+16,
      "budget_used_percent": 43.298490892744
    },
    {
      "type": "training",
      "description": "Training step 1823",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:22",
      "total_flops_so_far": 4.332223821823725e+16,
      "budget_used_percent": 43.322238218237246
    },
    {
      "type": "training",
      "description": "Training step 1824",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:23",
      "total_flops_so_far": 4.33459855437305e+16,
      "budget_used_percent": 43.3459855437305
    },
    {
      "type": "training",
      "description": "Training step 1825",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:25",
      "total_flops_so_far": 4.336973286922374e+16,
      "budget_used_percent": 43.36973286922374
    },
    {
      "type": "training",
      "description": "Training step 1826",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:26",
      "total_flops_so_far": 4.339348019471699e+16,
      "budget_used_percent": 43.39348019471699
    },
    {
      "type": "training",
      "description": "Training step 1827",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:27",
      "total_flops_so_far": 4.341722752021024e+16,
      "budget_used_percent": 43.41722752021024
    },
    {
      "type": "training",
      "description": "Training step 1828",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:29",
      "total_flops_so_far": 4.344097484570349e+16,
      "budget_used_percent": 43.44097484570349
    },
    {
      "type": "training",
      "description": "Training step 1829",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:30",
      "total_flops_so_far": 4.346472217119674e+16,
      "budget_used_percent": 43.46472217119673
    },
    {
      "type": "training",
      "description": "Training step 1830",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:31",
      "total_flops_so_far": 4.348846949668998e+16,
      "budget_used_percent": 43.48846949668999
    },
    {
      "type": "training",
      "description": "Training step 1831",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:33",
      "total_flops_so_far": 4.351221682218323e+16,
      "budget_used_percent": 43.512216822183234
    },
    {
      "type": "training",
      "description": "Training step 1832",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:34",
      "total_flops_so_far": 4.353596414767648e+16,
      "budget_used_percent": 43.53596414767648
    },
    {
      "type": "training",
      "description": "Training step 1833",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:35",
      "total_flops_so_far": 4.355971147316973e+16,
      "budget_used_percent": 43.55971147316973
    },
    {
      "type": "training",
      "description": "Training step 1834",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:37",
      "total_flops_so_far": 4.358345879866298e+16,
      "budget_used_percent": 43.583458798662974
    },
    {
      "type": "training",
      "description": "Training step 1835",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:38",
      "total_flops_so_far": 4.360720612415622e+16,
      "budget_used_percent": 43.60720612415622
    },
    {
      "type": "training",
      "description": "Training step 1836",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:39",
      "total_flops_so_far": 4.363095344964947e+16,
      "budget_used_percent": 43.630953449649475
    },
    {
      "type": "training",
      "description": "Training step 1837",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:41",
      "total_flops_so_far": 4.365470077514272e+16,
      "budget_used_percent": 43.65470077514272
    },
    {
      "type": "training",
      "description": "Training step 1838",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:42",
      "total_flops_so_far": 4.367844810063597e+16,
      "budget_used_percent": 43.67844810063597
    },
    {
      "type": "training",
      "description": "Training step 1839",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:43",
      "total_flops_so_far": 4.370219542612922e+16,
      "budget_used_percent": 43.702195426129215
    },
    {
      "type": "training",
      "description": "Training step 1840",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:44",
      "total_flops_so_far": 4.372594275162246e+16,
      "budget_used_percent": 43.72594275162246
    },
    {
      "type": "training",
      "description": "Training step 1841",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:46",
      "total_flops_so_far": 4.374969007711571e+16,
      "budget_used_percent": 43.74969007711571
    },
    {
      "type": "training",
      "description": "Training step 1842",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:47",
      "total_flops_so_far": 4.377343740260896e+16,
      "budget_used_percent": 43.77343740260896
    },
    {
      "type": "training",
      "description": "Training step 1843",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:48",
      "total_flops_so_far": 4.379718472810221e+16,
      "budget_used_percent": 43.79718472810221
    },
    {
      "type": "training",
      "description": "Training step 1844",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:50",
      "total_flops_so_far": 4.382093205359546e+16,
      "budget_used_percent": 43.820932053595456
    },
    {
      "type": "training",
      "description": "Training step 1845",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:51",
      "total_flops_so_far": 4.38446793790887e+16,
      "budget_used_percent": 43.8446793790887
    },
    {
      "type": "training",
      "description": "Training step 1846",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:52",
      "total_flops_so_far": 4.386842670458195e+16,
      "budget_used_percent": 43.86842670458195
    },
    {
      "type": "training",
      "description": "Training step 1847",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:54",
      "total_flops_so_far": 4.38921740300752e+16,
      "budget_used_percent": 43.892174030075196
    },
    {
      "type": "training",
      "description": "Training step 1848",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:54",
      "total_flops_so_far": 4.391592135556845e+16,
      "budget_used_percent": 43.91592135556845
    },
    {
      "type": "training",
      "description": "Training step 1849",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:55",
      "total_flops_so_far": 4.39396686810617e+16,
      "budget_used_percent": 43.9396686810617
    },
    {
      "type": "training",
      "description": "Training step 1850",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:57",
      "total_flops_so_far": 4.396341600655494e+16,
      "budget_used_percent": 43.963416006554944
    },
    {
      "type": "training",
      "description": "Training step 1851",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:58",
      "total_flops_so_far": 4.398716333204819e+16,
      "budget_used_percent": 43.98716333204819
    },
    {
      "type": "training",
      "description": "Training step 1852",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:23:59",
      "total_flops_so_far": 4.401091065754144e+16,
      "budget_used_percent": 44.010910657541444
    },
    {
      "type": "training",
      "description": "Training step 1853",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:01",
      "total_flops_so_far": 4.403465798303469e+16,
      "budget_used_percent": 44.034657983034684
    },
    {
      "type": "training",
      "description": "Training step 1854",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:02",
      "total_flops_so_far": 4.405840530852794e+16,
      "budget_used_percent": 44.05840530852794
    },
    {
      "type": "training",
      "description": "Training step 1855",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:03",
      "total_flops_so_far": 4.408215263402118e+16,
      "budget_used_percent": 44.082152634021185
    },
    {
      "type": "training",
      "description": "Training step 1856",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:05",
      "total_flops_so_far": 4.410589995951443e+16,
      "budget_used_percent": 44.10589995951443
    },
    {
      "type": "training",
      "description": "Training step 1857",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:06",
      "total_flops_so_far": 4.412964728500768e+16,
      "budget_used_percent": 44.12964728500768
    },
    {
      "type": "training",
      "description": "Training step 1858",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:07",
      "total_flops_so_far": 4.415339461050093e+16,
      "budget_used_percent": 44.15339461050093
    },
    {
      "type": "training",
      "description": "Training step 1859",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:09",
      "total_flops_so_far": 4.417714193599418e+16,
      "budget_used_percent": 44.17714193599417
    },
    {
      "type": "training",
      "description": "Training step 1860",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:10",
      "total_flops_so_far": 4.420088926148742e+16,
      "budget_used_percent": 44.200889261487426
    },
    {
      "type": "training",
      "description": "Training step 1861",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:11",
      "total_flops_so_far": 4.422463658698067e+16,
      "budget_used_percent": 44.22463658698067
    },
    {
      "type": "training",
      "description": "Training step 1862",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:13",
      "total_flops_so_far": 4.424838391247392e+16,
      "budget_used_percent": 44.24838391247392
    },
    {
      "type": "training",
      "description": "Training step 1863",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:14",
      "total_flops_so_far": 4.427213123796717e+16,
      "budget_used_percent": 44.272131237967166
    },
    {
      "type": "training",
      "description": "Training step 1864",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:15",
      "total_flops_so_far": 4.429587856346042e+16,
      "budget_used_percent": 44.29587856346042
    },
    {
      "type": "training",
      "description": "Training step 1865",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:17",
      "total_flops_so_far": 4.431962588895366e+16,
      "budget_used_percent": 44.31962588895366
    },
    {
      "type": "training",
      "description": "Training step 1866",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:18",
      "total_flops_so_far": 4.434337321444691e+16,
      "budget_used_percent": 44.34337321444691
    },
    {
      "type": "training",
      "description": "Training step 1867",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:19",
      "total_flops_so_far": 4.436712053994016e+16,
      "budget_used_percent": 44.36712053994016
    },
    {
      "type": "training",
      "description": "Training step 1868",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:21",
      "total_flops_so_far": 4.439086786543341e+16,
      "budget_used_percent": 44.39086786543341
    },
    {
      "type": "training",
      "description": "Training step 1869",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:22",
      "total_flops_so_far": 4.441461519092666e+16,
      "budget_used_percent": 44.414615190926654
    },
    {
      "type": "training",
      "description": "Training step 1870",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:23",
      "total_flops_so_far": 4.44383625164199e+16,
      "budget_used_percent": 44.43836251641991
    },
    {
      "type": "training",
      "description": "Training step 1871",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:24",
      "total_flops_so_far": 4.446210984191315e+16,
      "budget_used_percent": 44.46210984191315
    },
    {
      "type": "training",
      "description": "Training step 1872",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:26",
      "total_flops_so_far": 4.44858571674064e+16,
      "budget_used_percent": 44.4858571674064
    },
    {
      "type": "training",
      "description": "Training step 1873",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:27",
      "total_flops_so_far": 4.450960449289965e+16,
      "budget_used_percent": 44.50960449289965
    },
    {
      "type": "training",
      "description": "Training step 1874",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:28",
      "total_flops_so_far": 4.45333518183929e+16,
      "budget_used_percent": 44.533351818392894
    },
    {
      "type": "training",
      "description": "Training step 1875",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:30",
      "total_flops_so_far": 4.455709914388614e+16,
      "budget_used_percent": 44.55709914388614
    },
    {
      "type": "training",
      "description": "Training step 1876",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:31",
      "total_flops_so_far": 4.458084646937939e+16,
      "budget_used_percent": 44.580846469379395
    },
    {
      "type": "training",
      "description": "Training step 1877",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:32",
      "total_flops_so_far": 4.460459379487264e+16,
      "budget_used_percent": 44.604593794872635
    },
    {
      "type": "training",
      "description": "Training step 1878",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:34",
      "total_flops_so_far": 4.462834112036589e+16,
      "budget_used_percent": 44.62834112036589
    },
    {
      "type": "training",
      "description": "Training step 1879",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:35",
      "total_flops_so_far": 4.465208844585914e+16,
      "budget_used_percent": 44.652088445859135
    },
    {
      "type": "training",
      "description": "Training step 1880",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:36",
      "total_flops_so_far": 4.467583577135238e+16,
      "budget_used_percent": 44.67583577135238
    },
    {
      "type": "training",
      "description": "Training step 1881",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:38",
      "total_flops_so_far": 4.469958309684563e+16,
      "budget_used_percent": 44.69958309684563
    },
    {
      "type": "training",
      "description": "Training step 1882",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:39",
      "total_flops_so_far": 4.472333042233888e+16,
      "budget_used_percent": 44.72333042233888
    },
    {
      "type": "training",
      "description": "Training step 1883",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:40",
      "total_flops_so_far": 4.474707774783213e+16,
      "budget_used_percent": 44.74707774783212
    },
    {
      "type": "training",
      "description": "Training step 1884",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:42",
      "total_flops_so_far": 4.477082507332538e+16,
      "budget_used_percent": 44.770825073325376
    },
    {
      "type": "training",
      "description": "Training step 1885",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:43",
      "total_flops_so_far": 4.479457239881862e+16,
      "budget_used_percent": 44.79457239881862
    },
    {
      "type": "training",
      "description": "Training step 1886",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:44",
      "total_flops_so_far": 4.481831972431187e+16,
      "budget_used_percent": 44.81831972431187
    },
    {
      "type": "training",
      "description": "Training step 1887",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:46",
      "total_flops_so_far": 4.484206704980512e+16,
      "budget_used_percent": 44.84206704980512
    },
    {
      "type": "training",
      "description": "Training step 1888",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:47",
      "total_flops_so_far": 4.486581437529837e+16,
      "budget_used_percent": 44.86581437529837
    },
    {
      "type": "training",
      "description": "Training step 1889",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:48",
      "total_flops_so_far": 4.488956170079162e+16,
      "budget_used_percent": 44.88956170079162
    },
    {
      "type": "training",
      "description": "Training step 1890",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:50",
      "total_flops_so_far": 4.491330902628486e+16,
      "budget_used_percent": 44.913309026284864
    },
    {
      "type": "training",
      "description": "Training step 1891",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:51",
      "total_flops_so_far": 4.493705635177811e+16,
      "budget_used_percent": 44.93705635177811
    },
    {
      "type": "training",
      "description": "Training step 1892",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:52",
      "total_flops_so_far": 4.496080367727136e+16,
      "budget_used_percent": 44.96080367727136
    },
    {
      "type": "training",
      "description": "Training step 1893",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:54",
      "total_flops_so_far": 4.498455100276461e+16,
      "budget_used_percent": 44.984551002764604
    },
    {
      "type": "training",
      "description": "Training step 1894",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:55",
      "total_flops_so_far": 4.500829832825786e+16,
      "budget_used_percent": 45.00829832825786
    },
    {
      "type": "training",
      "description": "Training step 1895",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:56",
      "total_flops_so_far": 4.50320456537511e+16,
      "budget_used_percent": 45.032045653751105
    },
    {
      "type": "training",
      "description": "Training step 1896",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:58",
      "total_flops_so_far": 4.505579297924435e+16,
      "budget_used_percent": 45.05579297924435
    },
    {
      "type": "training",
      "description": "Training step 1897",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:24:59",
      "total_flops_so_far": 4.50795403047376e+16,
      "budget_used_percent": 45.0795403047376
    },
    {
      "type": "training",
      "description": "Training step 1898",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:00",
      "total_flops_so_far": 4.510328763023085e+16,
      "budget_used_percent": 45.103287630230845
    },
    {
      "type": "training",
      "description": "Training step 1899",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:01",
      "total_flops_so_far": 4.51270349557241e+16,
      "budget_used_percent": 45.12703495572409
    },
    {
      "type": "training",
      "description": "Training step 1900",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:03",
      "total_flops_so_far": 4.515078228121734e+16,
      "budget_used_percent": 45.150782281217346
    },
    {
      "type": "training",
      "description": "Training step 1901",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:04",
      "total_flops_so_far": 4.517452960671059e+16,
      "budget_used_percent": 45.17452960671059
    },
    {
      "type": "training",
      "description": "Training step 1902",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:05",
      "total_flops_so_far": 4.519827693220384e+16,
      "budget_used_percent": 45.19827693220384
    },
    {
      "type": "training",
      "description": "Training step 1903",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:07",
      "total_flops_so_far": 4.522202425769709e+16,
      "budget_used_percent": 45.222024257697086
    },
    {
      "type": "training",
      "description": "Training step 1904",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:09",
      "total_flops_so_far": 4.524577158319034e+16,
      "budget_used_percent": 45.24577158319033
    },
    {
      "type": "training",
      "description": "Training step 1905",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:10",
      "total_flops_so_far": 4.526951890868358e+16,
      "budget_used_percent": 45.26951890868358
    },
    {
      "type": "training",
      "description": "Training step 1906",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:11",
      "total_flops_so_far": 4.529326623417683e+16,
      "budget_used_percent": 45.29326623417683
    },
    {
      "type": "training",
      "description": "Training step 1907",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:13",
      "total_flops_so_far": 4.531701355967008e+16,
      "budget_used_percent": 45.31701355967008
    },
    {
      "type": "training",
      "description": "Training step 1908",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:14",
      "total_flops_so_far": 4.534076088516333e+16,
      "budget_used_percent": 45.34076088516333
    },
    {
      "type": "training",
      "description": "Training step 1909",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:15",
      "total_flops_so_far": 4.536450821065658e+16,
      "budget_used_percent": 45.364508210656574
    },
    {
      "type": "training",
      "description": "Training step 1910",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:17",
      "total_flops_so_far": 4.538825553614982e+16,
      "budget_used_percent": 45.38825553614983
    },
    {
      "type": "training",
      "description": "Training step 1911",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:18",
      "total_flops_so_far": 4.541200286164307e+16,
      "budget_used_percent": 45.41200286164307
    },
    {
      "type": "training",
      "description": "Training step 1912",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:19",
      "total_flops_so_far": 4.543575018713632e+16,
      "budget_used_percent": 45.43575018713632
    },
    {
      "type": "training",
      "description": "Training step 1913",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:21",
      "total_flops_so_far": 4.545949751262957e+16,
      "budget_used_percent": 45.45949751262957
    },
    {
      "type": "training",
      "description": "Training step 1914",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:22",
      "total_flops_so_far": 4.548324483812282e+16,
      "budget_used_percent": 45.483244838122815
    },
    {
      "type": "training",
      "description": "Training step 1915",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:23",
      "total_flops_so_far": 4.550699216361606e+16,
      "budget_used_percent": 45.50699216361606
    },
    {
      "type": "training",
      "description": "Training step 1916",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:25",
      "total_flops_so_far": 4.553073948910931e+16,
      "budget_used_percent": 45.530739489109315
    },
    {
      "type": "training",
      "description": "Training step 1917",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:26",
      "total_flops_so_far": 4.555448681460256e+16,
      "budget_used_percent": 45.55448681460256
    },
    {
      "type": "training",
      "description": "Training step 1918",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:27",
      "total_flops_so_far": 4.557823414009581e+16,
      "budget_used_percent": 45.57823414009581
    },
    {
      "type": "training",
      "description": "Training step 1919",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:29",
      "total_flops_so_far": 4.560198146558906e+16,
      "budget_used_percent": 45.601981465589056
    },
    {
      "type": "training",
      "description": "Training step 1920",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:30",
      "total_flops_so_far": 4.56257287910823e+16,
      "budget_used_percent": 45.6257287910823
    },
    {
      "type": "training",
      "description": "Training step 1921",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:31",
      "total_flops_so_far": 4.564947611657555e+16,
      "budget_used_percent": 45.649476116575556
    },
    {
      "type": "training",
      "description": "Training step 1922",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:33",
      "total_flops_so_far": 4.56732234420688e+16,
      "budget_used_percent": 45.6732234420688
    },
    {
      "type": "training",
      "description": "Training step 1923",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:34",
      "total_flops_so_far": 4.569697076756205e+16,
      "budget_used_percent": 45.69697076756205
    },
    {
      "type": "training",
      "description": "Training step 1924",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:35",
      "total_flops_so_far": 4.57207180930553e+16,
      "budget_used_percent": 45.7207180930553
    },
    {
      "type": "training",
      "description": "Training step 1925",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:37",
      "total_flops_so_far": 4.574446541854854e+16,
      "budget_used_percent": 45.74446541854854
    },
    {
      "type": "training",
      "description": "Training step 1926",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:38",
      "total_flops_so_far": 4.576821274404179e+16,
      "budget_used_percent": 45.76821274404179
    },
    {
      "type": "training",
      "description": "Training step 1927",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:39",
      "total_flops_so_far": 4.579196006953504e+16,
      "budget_used_percent": 45.791960069535044
    },
    {
      "type": "training",
      "description": "Training step 1928",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:41",
      "total_flops_so_far": 4.581570739502829e+16,
      "budget_used_percent": 45.81570739502829
    },
    {
      "type": "training",
      "description": "Training step 1929",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:42",
      "total_flops_so_far": 4.583945472052154e+16,
      "budget_used_percent": 45.83945472052154
    },
    {
      "type": "training",
      "description": "Training step 1930",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:43",
      "total_flops_so_far": 4.586320204601478e+16,
      "budget_used_percent": 45.863202046014784
    },
    {
      "type": "training",
      "description": "Training step 1931",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:45",
      "total_flops_so_far": 4.588694937150803e+16,
      "budget_used_percent": 45.88694937150804
    },
    {
      "type": "training",
      "description": "Training step 1932",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:46",
      "total_flops_so_far": 4.591069669700128e+16,
      "budget_used_percent": 45.91069669700128
    },
    {
      "type": "training",
      "description": "Training step 1933",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:47",
      "total_flops_so_far": 4.593444402249453e+16,
      "budget_used_percent": 45.93444402249453
    },
    {
      "type": "training",
      "description": "Training step 1934",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:49",
      "total_flops_so_far": 4.595819134798778e+16,
      "budget_used_percent": 45.95819134798778
    },
    {
      "type": "training",
      "description": "Training step 1935",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:50",
      "total_flops_so_far": 4.598193867348102e+16,
      "budget_used_percent": 45.981938673481025
    },
    {
      "type": "training",
      "description": "Training step 1936",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:51",
      "total_flops_so_far": 4.600568599897427e+16,
      "budget_used_percent": 46.00568599897427
    },
    {
      "type": "training",
      "description": "Training step 1937",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:53",
      "total_flops_so_far": 4.602943332446752e+16,
      "budget_used_percent": 46.029433324467526
    },
    {
      "type": "training",
      "description": "Training step 1938",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:54",
      "total_flops_so_far": 4.605318064996077e+16,
      "budget_used_percent": 46.053180649960765
    },
    {
      "type": "training",
      "description": "Training step 1939",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:55",
      "total_flops_so_far": 4.607692797545402e+16,
      "budget_used_percent": 46.07692797545402
    },
    {
      "type": "training",
      "description": "Training step 1940",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:57",
      "total_flops_so_far": 4.610067530094726e+16,
      "budget_used_percent": 46.100675300947266
    },
    {
      "type": "training",
      "description": "Training step 1941",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:58",
      "total_flops_so_far": 4.612442262644051e+16,
      "budget_used_percent": 46.12442262644051
    },
    {
      "type": "training",
      "description": "Training step 1942",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:25:59",
      "total_flops_so_far": 4.614816995193376e+16,
      "budget_used_percent": 46.14816995193376
    },
    {
      "type": "training",
      "description": "Training step 1943",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:01",
      "total_flops_so_far": 4.617191727742701e+16,
      "budget_used_percent": 46.17191727742701
    },
    {
      "type": "training",
      "description": "Training step 1944",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:02",
      "total_flops_so_far": 4.619566460292026e+16,
      "budget_used_percent": 46.19566460292025
    },
    {
      "type": "training",
      "description": "Training step 1945",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:03",
      "total_flops_so_far": 4.62194119284135e+16,
      "budget_used_percent": 46.21941192841351
    },
    {
      "type": "training",
      "description": "Training step 1946",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:05",
      "total_flops_so_far": 4.624315925390675e+16,
      "budget_used_percent": 46.243159253906754
    },
    {
      "type": "training",
      "description": "Training step 1947",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:06",
      "total_flops_so_far": 4.62669065794e+16,
      "budget_used_percent": 46.2669065794
    },
    {
      "type": "training",
      "description": "Training step 1948",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:07",
      "total_flops_so_far": 4.629065390489325e+16,
      "budget_used_percent": 46.29065390489325
    },
    {
      "type": "training",
      "description": "Training step 1949",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:08",
      "total_flops_so_far": 4.63144012303865e+16,
      "budget_used_percent": 46.3144012303865
    },
    {
      "type": "training",
      "description": "Training step 1950",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:10",
      "total_flops_so_far": 4.633814855587974e+16,
      "budget_used_percent": 46.33814855587974
    },
    {
      "type": "training",
      "description": "Training step 1951",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:11",
      "total_flops_so_far": 4.636189588137299e+16,
      "budget_used_percent": 46.361895881372995
    },
    {
      "type": "training",
      "description": "Training step 1952",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:12",
      "total_flops_so_far": 4.638564320686624e+16,
      "budget_used_percent": 46.38564320686624
    },
    {
      "type": "training",
      "description": "Training step 1953",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:14",
      "total_flops_so_far": 4.640939053235949e+16,
      "budget_used_percent": 46.40939053235949
    },
    {
      "type": "training",
      "description": "Training step 1954",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:15",
      "total_flops_so_far": 4.643313785785274e+16,
      "budget_used_percent": 46.433137857852735
    },
    {
      "type": "training",
      "description": "Training step 1955",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:16",
      "total_flops_so_far": 4.645688518334598e+16,
      "budget_used_percent": 46.45688518334599
    },
    {
      "type": "training",
      "description": "Training step 1956",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:18",
      "total_flops_so_far": 4.648063250883923e+16,
      "budget_used_percent": 46.48063250883923
    },
    {
      "type": "training",
      "description": "Training step 1957",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:19",
      "total_flops_so_far": 4.650437983433248e+16,
      "budget_used_percent": 46.50437983433248
    },
    {
      "type": "training",
      "description": "Training step 1958",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:20",
      "total_flops_so_far": 4.652812715982573e+16,
      "budget_used_percent": 46.52812715982573
    },
    {
      "type": "training",
      "description": "Training step 1959",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:22",
      "total_flops_so_far": 4.655187448531898e+16,
      "budget_used_percent": 46.551874485318976
    },
    {
      "type": "training",
      "description": "Training step 1960",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:23",
      "total_flops_so_far": 4.657562181081222e+16,
      "budget_used_percent": 46.57562181081222
    },
    {
      "type": "training",
      "description": "Training step 1961",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:24",
      "total_flops_so_far": 4.659936913630547e+16,
      "budget_used_percent": 46.59936913630548
    },
    {
      "type": "training",
      "description": "Training step 1962",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:26",
      "total_flops_so_far": 4.662311646179872e+16,
      "budget_used_percent": 46.623116461798716
    },
    {
      "type": "training",
      "description": "Training step 1963",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:27",
      "total_flops_so_far": 4.664686378729197e+16,
      "budget_used_percent": 46.64686378729197
    },
    {
      "type": "training",
      "description": "Training step 1964",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:28",
      "total_flops_so_far": 4.667061111278522e+16,
      "budget_used_percent": 46.67061111278522
    },
    {
      "type": "training",
      "description": "Training step 1965",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:30",
      "total_flops_so_far": 4.669435843827846e+16,
      "budget_used_percent": 46.69435843827846
    },
    {
      "type": "training",
      "description": "Training step 1966",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:31",
      "total_flops_so_far": 4.671810576377171e+16,
      "budget_used_percent": 46.71810576377171
    },
    {
      "type": "training",
      "description": "Training step 1967",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:32",
      "total_flops_so_far": 4.674185308926496e+16,
      "budget_used_percent": 46.741853089264964
    },
    {
      "type": "training",
      "description": "Training step 1968",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:34",
      "total_flops_so_far": 4.676560041475821e+16,
      "budget_used_percent": 46.765600414758204
    },
    {
      "type": "training",
      "description": "Training step 1969",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:35",
      "total_flops_so_far": 4.678934774025146e+16,
      "budget_used_percent": 46.78934774025146
    },
    {
      "type": "training",
      "description": "Training step 1970",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:36",
      "total_flops_so_far": 4.68130950657447e+16,
      "budget_used_percent": 46.813095065744704
    },
    {
      "type": "training",
      "description": "Training step 1971",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:38",
      "total_flops_so_far": 4.683684239123795e+16,
      "budget_used_percent": 46.83684239123795
    },
    {
      "type": "training",
      "description": "Training step 1972",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:39",
      "total_flops_so_far": 4.68605897167312e+16,
      "budget_used_percent": 46.8605897167312
    },
    {
      "type": "training",
      "description": "Training step 1973",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:40",
      "total_flops_so_far": 4.688433704222445e+16,
      "budget_used_percent": 46.88433704222445
    },
    {
      "type": "training",
      "description": "Training step 1974",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:42",
      "total_flops_so_far": 4.69080843677177e+16,
      "budget_used_percent": 46.90808436771769
    },
    {
      "type": "training",
      "description": "Training step 1975",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:43",
      "total_flops_so_far": 4.693183169321094e+16,
      "budget_used_percent": 46.931831693210945
    },
    {
      "type": "training",
      "description": "Training step 1976",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:44",
      "total_flops_so_far": 4.695557901870419e+16,
      "budget_used_percent": 46.95557901870419
    },
    {
      "type": "training",
      "description": "Training step 1977",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:46",
      "total_flops_so_far": 4.697932634419744e+16,
      "budget_used_percent": 46.97932634419744
    },
    {
      "type": "training",
      "description": "Training step 1978",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:47",
      "total_flops_so_far": 4.700307366969069e+16,
      "budget_used_percent": 47.003073669690686
    },
    {
      "type": "training",
      "description": "Training step 1979",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:48",
      "total_flops_so_far": 4.702682099518394e+16,
      "budget_used_percent": 47.02682099518394
    },
    {
      "type": "training",
      "description": "Training step 1980",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:49",
      "total_flops_so_far": 4.705056832067718e+16,
      "budget_used_percent": 47.050568320677186
    },
    {
      "type": "training",
      "description": "Training step 1981",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:50",
      "total_flops_so_far": 4.707431564617043e+16,
      "budget_used_percent": 47.07431564617043
    },
    {
      "type": "training",
      "description": "Training step 1982",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:51",
      "total_flops_so_far": 4.709806297166368e+16,
      "budget_used_percent": 47.09806297166368
    },
    {
      "type": "training",
      "description": "Training step 1983",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:53",
      "total_flops_so_far": 4.712181029715693e+16,
      "budget_used_percent": 47.12181029715693
    },
    {
      "type": "training",
      "description": "Training step 1984",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:54",
      "total_flops_so_far": 4.714555762265018e+16,
      "budget_used_percent": 47.14555762265017
    },
    {
      "type": "training",
      "description": "Training step 1985",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:55",
      "total_flops_so_far": 4.716930494814342e+16,
      "budget_used_percent": 47.16930494814343
    },
    {
      "type": "training",
      "description": "Training step 1986",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:57",
      "total_flops_so_far": 4.719305227363667e+16,
      "budget_used_percent": 47.193052273636674
    },
    {
      "type": "training",
      "description": "Training step 1987",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:58",
      "total_flops_so_far": 4.721679959912992e+16,
      "budget_used_percent": 47.21679959912992
    },
    {
      "type": "training",
      "description": "Training step 1988",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:26:59",
      "total_flops_so_far": 4.724054692462317e+16,
      "budget_used_percent": 47.24054692462317
    },
    {
      "type": "training",
      "description": "Training step 1989",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:27:01",
      "total_flops_so_far": 4.726429425011642e+16,
      "budget_used_percent": 47.264294250116414
    },
    {
      "type": "training",
      "description": "Training step 1990",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:27:02",
      "total_flops_so_far": 4.728804157560966e+16,
      "budget_used_percent": 47.28804157560966
    },
    {
      "type": "training",
      "description": "Training step 1991",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:27:03",
      "total_flops_so_far": 4.731178890110291e+16,
      "budget_used_percent": 47.311788901102915
    },
    {
      "type": "training",
      "description": "Training step 1992",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:27:05",
      "total_flops_so_far": 4.733553622659616e+16,
      "budget_used_percent": 47.33553622659616
    },
    {
      "type": "training",
      "description": "Training step 1993",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:27:06",
      "total_flops_so_far": 4.735928355208941e+16,
      "budget_used_percent": 47.35928355208941
    },
    {
      "type": "training",
      "description": "Training step 1994",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:27:07",
      "total_flops_so_far": 4.738303087758266e+16,
      "budget_used_percent": 47.383030877582655
    },
    {
      "type": "training",
      "description": "Training step 1995",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:27:09",
      "total_flops_so_far": 4.74067782030759e+16,
      "budget_used_percent": 47.4067782030759
    },
    {
      "type": "training",
      "description": "Training step 1996",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:27:10",
      "total_flops_so_far": 4.743052552856915e+16,
      "budget_used_percent": 47.43052552856915
    },
    {
      "type": "training",
      "description": "Training step 1997",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:27:11",
      "total_flops_so_far": 4.74542728540624e+16,
      "budget_used_percent": 47.4542728540624
    },
    {
      "type": "training",
      "description": "Training step 1998",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:27:13",
      "total_flops_so_far": 4.747802017955565e+16,
      "budget_used_percent": 47.47802017955565
    },
    {
      "type": "training",
      "description": "Training step 1999",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:27:14",
      "total_flops_so_far": 4.75017675050489e+16,
      "budget_used_percent": 47.501767505048896
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710078789056.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:27:20",
      "total_flops_so_far": 4.750247758383795e+16,
      "budget_used_percent": 47.50247758383795
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 713780608688.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:27:25",
      "total_flops_so_far": 4.750319136444664e+16,
      "budget_used_percent": 47.50319136444664
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711929338680.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:27:30",
      "total_flops_so_far": 4.750390329378532e+16,
      "budget_used_percent": 47.50390329378532
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710078789056.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:27:35",
      "total_flops_so_far": 4.7504613372574376e+16,
      "budget_used_percent": 47.504613372574376
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712854883636.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:27:40",
      "total_flops_so_far": 4.750532622745802e+16,
      "budget_used_percent": 47.50532622745801
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 5",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710078789056.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:27:45",
      "total_flops_so_far": 4.750603630624707e+16,
      "budget_used_percent": 47.506036306247076
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 6",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711929338680.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:27:50",
      "total_flops_so_far": 4.750674823558575e+16,
      "budget_used_percent": 47.506748235585746
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 7",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711929338680.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:27:55",
      "total_flops_so_far": 4.750746016492443e+16,
      "budget_used_percent": 47.50746016492443
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 8",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711929338680.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:00",
      "total_flops_so_far": 4.750817209426311e+16,
      "budget_used_percent": 47.50817209426311
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 9",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711929338680.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:05",
      "total_flops_so_far": 4.750888402360179e+16,
      "budget_used_percent": 47.508884023601794
    },
    {
      "type": "training",
      "description": "Training step 2000",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:05",
      "total_flops_so_far": 4.753263134909504e+16,
      "budget_used_percent": 47.53263134909504
    },
    {
      "type": "training",
      "description": "Training step 2001",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:07",
      "total_flops_so_far": 4.755637867458829e+16,
      "budget_used_percent": 47.55637867458829
    },
    {
      "type": "training",
      "description": "Training step 2002",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:08",
      "total_flops_so_far": 4.758012600008154e+16,
      "budget_used_percent": 47.580126000081535
    },
    {
      "type": "training",
      "description": "Training step 2003",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:09",
      "total_flops_so_far": 4.760387332557478e+16,
      "budget_used_percent": 47.60387332557479
    },
    {
      "type": "training",
      "description": "Training step 2004",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:11",
      "total_flops_so_far": 4.762762065106803e+16,
      "budget_used_percent": 47.62762065106803
    },
    {
      "type": "training",
      "description": "Training step 2005",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:12",
      "total_flops_so_far": 4.765136797656128e+16,
      "budget_used_percent": 47.65136797656128
    },
    {
      "type": "training",
      "description": "Training step 2006",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:13",
      "total_flops_so_far": 4.767511530205453e+16,
      "budget_used_percent": 47.67511530205453
    },
    {
      "type": "training",
      "description": "Training step 2007",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:15",
      "total_flops_so_far": 4.769886262754778e+16,
      "budget_used_percent": 47.698862627547776
    },
    {
      "type": "training",
      "description": "Training step 2008",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:16",
      "total_flops_so_far": 4.772260995304102e+16,
      "budget_used_percent": 47.72260995304102
    },
    {
      "type": "training",
      "description": "Training step 2009",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:17",
      "total_flops_so_far": 4.774635727853427e+16,
      "budget_used_percent": 47.746357278534276
    },
    {
      "type": "training",
      "description": "Training step 2010",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:19",
      "total_flops_so_far": 4.777010460402752e+16,
      "budget_used_percent": 47.770104604027516
    },
    {
      "type": "training",
      "description": "Training step 2011",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:20",
      "total_flops_so_far": 4.779385192952077e+16,
      "budget_used_percent": 47.79385192952077
    },
    {
      "type": "training",
      "description": "Training step 2012",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:21",
      "total_flops_so_far": 4.781759925501402e+16,
      "budget_used_percent": 47.81759925501402
    },
    {
      "type": "training",
      "description": "Training step 2013",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:23",
      "total_flops_so_far": 4.784134658050726e+16,
      "budget_used_percent": 47.84134658050726
    },
    {
      "type": "training",
      "description": "Training step 2014",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:24",
      "total_flops_so_far": 4.786509390600051e+16,
      "budget_used_percent": 47.86509390600051
    },
    {
      "type": "training",
      "description": "Training step 2015",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:25",
      "total_flops_so_far": 4.788884123149376e+16,
      "budget_used_percent": 47.888841231493764
    },
    {
      "type": "training",
      "description": "Training step 2016",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:27",
      "total_flops_so_far": 4.791258855698701e+16,
      "budget_used_percent": 47.912588556987004
    },
    {
      "type": "training",
      "description": "Training step 2017",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:28",
      "total_flops_so_far": 4.793633588248026e+16,
      "budget_used_percent": 47.93633588248026
    },
    {
      "type": "training",
      "description": "Training step 2018",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:29",
      "total_flops_so_far": 4.79600832079735e+16,
      "budget_used_percent": 47.960083207973504
    },
    {
      "type": "training",
      "description": "Training step 2019",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:31",
      "total_flops_so_far": 4.798383053346675e+16,
      "budget_used_percent": 47.98383053346675
    },
    {
      "type": "training",
      "description": "Training step 2020",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:32",
      "total_flops_so_far": 4.800757785896e+16,
      "budget_used_percent": 48.00757785896
    },
    {
      "type": "training",
      "description": "Training step 2021",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:33",
      "total_flops_so_far": 4.803132518445325e+16,
      "budget_used_percent": 48.03132518445325
    },
    {
      "type": "training",
      "description": "Training step 2022",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:35",
      "total_flops_so_far": 4.80550725099465e+16,
      "budget_used_percent": 48.0550725099465
    },
    {
      "type": "training",
      "description": "Training step 2023",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:36",
      "total_flops_so_far": 4.807881983543974e+16,
      "budget_used_percent": 48.078819835439745
    },
    {
      "type": "training",
      "description": "Training step 2024",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:37",
      "total_flops_so_far": 4.810256716093299e+16,
      "budget_used_percent": 48.10256716093299
    },
    {
      "type": "training",
      "description": "Training step 2025",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:39",
      "total_flops_so_far": 4.812631448642624e+16,
      "budget_used_percent": 48.12631448642624
    },
    {
      "type": "training",
      "description": "Training step 2026",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:40",
      "total_flops_so_far": 4.815006181191949e+16,
      "budget_used_percent": 48.150061811919485
    },
    {
      "type": "training",
      "description": "Training step 2027",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:41",
      "total_flops_so_far": 4.817380913741274e+16,
      "budget_used_percent": 48.17380913741274
    },
    {
      "type": "training",
      "description": "Training step 2028",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:43",
      "total_flops_so_far": 4.819755646290598e+16,
      "budget_used_percent": 48.197556462905986
    },
    {
      "type": "training",
      "description": "Training step 2029",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:44",
      "total_flops_so_far": 4.822130378839923e+16,
      "budget_used_percent": 48.22130378839923
    },
    {
      "type": "training",
      "description": "Training step 2030",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:45",
      "total_flops_so_far": 4.824505111389248e+16,
      "budget_used_percent": 48.24505111389248
    },
    {
      "type": "training",
      "description": "Training step 2031",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:47",
      "total_flops_so_far": 4.826879843938573e+16,
      "budget_used_percent": 48.268798439385726
    },
    {
      "type": "training",
      "description": "Training step 2032",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:48",
      "total_flops_so_far": 4.829254576487898e+16,
      "budget_used_percent": 48.29254576487897
    },
    {
      "type": "training",
      "description": "Training step 2033",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:49",
      "total_flops_so_far": 4.831629309037222e+16,
      "budget_used_percent": 48.31629309037223
    },
    {
      "type": "training",
      "description": "Training step 2034",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:51",
      "total_flops_so_far": 4.834004041586547e+16,
      "budget_used_percent": 48.340040415865474
    },
    {
      "type": "training",
      "description": "Training step 2035",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:52",
      "total_flops_so_far": 4.836378774135872e+16,
      "budget_used_percent": 48.36378774135872
    },
    {
      "type": "training",
      "description": "Training step 2036",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:53",
      "total_flops_so_far": 4.838753506685197e+16,
      "budget_used_percent": 48.38753506685197
    },
    {
      "type": "training",
      "description": "Training step 2037",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:54",
      "total_flops_so_far": 4.841128239234522e+16,
      "budget_used_percent": 48.411282392345214
    },
    {
      "type": "training",
      "description": "Training step 2038",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:56",
      "total_flops_so_far": 4.843502971783846e+16,
      "budget_used_percent": 48.43502971783846
    },
    {
      "type": "training",
      "description": "Training step 2039",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:57",
      "total_flops_so_far": 4.845877704333171e+16,
      "budget_used_percent": 48.458777043331715
    },
    {
      "type": "training",
      "description": "Training step 2040",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:28:58",
      "total_flops_so_far": 4.848252436882496e+16,
      "budget_used_percent": 48.48252436882496
    },
    {
      "type": "training",
      "description": "Training step 2041",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:00",
      "total_flops_so_far": 4.850627169431821e+16,
      "budget_used_percent": 48.50627169431821
    },
    {
      "type": "training",
      "description": "Training step 2042",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:01",
      "total_flops_so_far": 4.853001901981146e+16,
      "budget_used_percent": 48.530019019811455
    },
    {
      "type": "training",
      "description": "Training step 2043",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:02",
      "total_flops_so_far": 4.85537663453047e+16,
      "budget_used_percent": 48.55376634530471
    },
    {
      "type": "training",
      "description": "Training step 2044",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:04",
      "total_flops_so_far": 4.857751367079795e+16,
      "budget_used_percent": 48.57751367079795
    },
    {
      "type": "training",
      "description": "Training step 2045",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:05",
      "total_flops_so_far": 4.86012609962912e+16,
      "budget_used_percent": 48.6012609962912
    },
    {
      "type": "training",
      "description": "Training step 2046",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:06",
      "total_flops_so_far": 4.862500832178445e+16,
      "budget_used_percent": 48.62500832178445
    },
    {
      "type": "training",
      "description": "Training step 2047",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:08",
      "total_flops_so_far": 4.86487556472777e+16,
      "budget_used_percent": 48.648755647277696
    },
    {
      "type": "training",
      "description": "Training step 2048",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:09",
      "total_flops_so_far": 4.867250297277094e+16,
      "budget_used_percent": 48.67250297277094
    },
    {
      "type": "training",
      "description": "Training step 2049",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:10",
      "total_flops_so_far": 4.869625029826419e+16,
      "budget_used_percent": 48.6962502982642
    },
    {
      "type": "training",
      "description": "Training step 2050",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:12",
      "total_flops_so_far": 4.871999762375744e+16,
      "budget_used_percent": 48.719997623757436
    },
    {
      "type": "training",
      "description": "Training step 2051",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:13",
      "total_flops_so_far": 4.874374494925069e+16,
      "budget_used_percent": 48.74374494925069
    },
    {
      "type": "training",
      "description": "Training step 2052",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:15",
      "total_flops_so_far": 4.876749227474394e+16,
      "budget_used_percent": 48.76749227474394
    },
    {
      "type": "training",
      "description": "Training step 2053",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:16",
      "total_flops_so_far": 4.879123960023718e+16,
      "budget_used_percent": 48.79123960023718
    },
    {
      "type": "training",
      "description": "Training step 2054",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:17",
      "total_flops_so_far": 4.881498692573043e+16,
      "budget_used_percent": 48.81498692573043
    },
    {
      "type": "training",
      "description": "Training step 2055",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:19",
      "total_flops_so_far": 4.883873425122368e+16,
      "budget_used_percent": 48.838734251223684
    },
    {
      "type": "training",
      "description": "Training step 2056",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:20",
      "total_flops_so_far": 4.886248157671693e+16,
      "budget_used_percent": 48.862481576716924
    },
    {
      "type": "training",
      "description": "Training step 2057",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:21",
      "total_flops_so_far": 4.888622890221018e+16,
      "budget_used_percent": 48.88622890221018
    },
    {
      "type": "training",
      "description": "Training step 2058",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:23",
      "total_flops_so_far": 4.890997622770342e+16,
      "budget_used_percent": 48.909976227703424
    },
    {
      "type": "training",
      "description": "Training step 2059",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:24",
      "total_flops_so_far": 4.893372355319667e+16,
      "budget_used_percent": 48.93372355319667
    },
    {
      "type": "training",
      "description": "Training step 2060",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:25",
      "total_flops_so_far": 4.895747087868992e+16,
      "budget_used_percent": 48.95747087868992
    },
    {
      "type": "training",
      "description": "Training step 2061",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:26",
      "total_flops_so_far": 4.898121820418317e+16,
      "budget_used_percent": 48.98121820418317
    },
    {
      "type": "training",
      "description": "Training step 2062",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:28",
      "total_flops_so_far": 4.900496552967642e+16,
      "budget_used_percent": 49.00496552967641
    },
    {
      "type": "training",
      "description": "Training step 2063",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:29",
      "total_flops_so_far": 4.902871285516966e+16,
      "budget_used_percent": 49.028712855169665
    },
    {
      "type": "training",
      "description": "Training step 2064",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:30",
      "total_flops_so_far": 4.905246018066291e+16,
      "budget_used_percent": 49.05246018066291
    },
    {
      "type": "training",
      "description": "Training step 2065",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:32",
      "total_flops_so_far": 4.907620750615616e+16,
      "budget_used_percent": 49.07620750615616
    },
    {
      "type": "training",
      "description": "Training step 2066",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:33",
      "total_flops_so_far": 4.909995483164941e+16,
      "budget_used_percent": 49.099954831649406
    },
    {
      "type": "training",
      "description": "Training step 2067",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:34",
      "total_flops_so_far": 4.912370215714266e+16,
      "budget_used_percent": 49.12370215714266
    },
    {
      "type": "training",
      "description": "Training step 2068",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:36",
      "total_flops_so_far": 4.91474494826359e+16,
      "budget_used_percent": 49.1474494826359
    },
    {
      "type": "training",
      "description": "Training step 2069",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:37",
      "total_flops_so_far": 4.917119680812915e+16,
      "budget_used_percent": 49.17119680812915
    },
    {
      "type": "training",
      "description": "Training step 2070",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:39",
      "total_flops_so_far": 4.91949441336224e+16,
      "budget_used_percent": 49.1949441336224
    },
    {
      "type": "training",
      "description": "Training step 2071",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:40",
      "total_flops_so_far": 4.921869145911565e+16,
      "budget_used_percent": 49.21869145911565
    },
    {
      "type": "training",
      "description": "Training step 2072",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:41",
      "total_flops_so_far": 4.92424387846089e+16,
      "budget_used_percent": 49.24243878460889
    },
    {
      "type": "training",
      "description": "Training step 2073",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:43",
      "total_flops_so_far": 4.926618611010214e+16,
      "budget_used_percent": 49.26618611010215
    },
    {
      "type": "training",
      "description": "Training step 2074",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:44",
      "total_flops_so_far": 4.928993343559539e+16,
      "budget_used_percent": 49.28993343559539
    },
    {
      "type": "training",
      "description": "Training step 2075",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:45",
      "total_flops_so_far": 4.931368076108864e+16,
      "budget_used_percent": 49.31368076108864
    },
    {
      "type": "training",
      "description": "Training step 2076",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:46",
      "total_flops_so_far": 4.933742808658189e+16,
      "budget_used_percent": 49.33742808658189
    },
    {
      "type": "training",
      "description": "Training step 2077",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:48",
      "total_flops_so_far": 4.936117541207514e+16,
      "budget_used_percent": 49.361175412075134
    },
    {
      "type": "training",
      "description": "Training step 2078",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:49",
      "total_flops_so_far": 4.938492273756838e+16,
      "budget_used_percent": 49.38492273756838
    },
    {
      "type": "training",
      "description": "Training step 2079",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:50",
      "total_flops_so_far": 4.940867006306163e+16,
      "budget_used_percent": 49.408670063061635
    },
    {
      "type": "training",
      "description": "Training step 2080",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:52",
      "total_flops_so_far": 4.943241738855488e+16,
      "budget_used_percent": 49.432417388554875
    },
    {
      "type": "training",
      "description": "Training step 2081",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:53",
      "total_flops_so_far": 4.945616471404813e+16,
      "budget_used_percent": 49.45616471404813
    },
    {
      "type": "training",
      "description": "Training step 2082",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:54",
      "total_flops_so_far": 4.947991203954138e+16,
      "budget_used_percent": 49.479912039541375
    },
    {
      "type": "training",
      "description": "Training step 2083",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:56",
      "total_flops_so_far": 4.950365936503462e+16,
      "budget_used_percent": 49.50365936503462
    },
    {
      "type": "training",
      "description": "Training step 2084",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:57",
      "total_flops_so_far": 4.952740669052787e+16,
      "budget_used_percent": 49.52740669052787
    },
    {
      "type": "training",
      "description": "Training step 2085",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:29:58",
      "total_flops_so_far": 4.955115401602112e+16,
      "budget_used_percent": 49.55115401602112
    },
    {
      "type": "training",
      "description": "Training step 2086",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:00",
      "total_flops_so_far": 4.957490134151437e+16,
      "budget_used_percent": 49.57490134151436
    },
    {
      "type": "training",
      "description": "Training step 2087",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:01",
      "total_flops_so_far": 4.959864866700762e+16,
      "budget_used_percent": 49.598648667007616
    },
    {
      "type": "training",
      "description": "Training step 2088",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:02",
      "total_flops_so_far": 4.962239599250086e+16,
      "budget_used_percent": 49.62239599250086
    },
    {
      "type": "training",
      "description": "Training step 2089",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:05",
      "total_flops_so_far": 4.964614331799411e+16,
      "budget_used_percent": 49.64614331799411
    },
    {
      "type": "training",
      "description": "Training step 2090",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:06",
      "total_flops_so_far": 4.966989064348736e+16,
      "budget_used_percent": 49.669890643487356
    },
    {
      "type": "training",
      "description": "Training step 2091",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:07",
      "total_flops_so_far": 4.969363796898061e+16,
      "budget_used_percent": 49.69363796898061
    },
    {
      "type": "training",
      "description": "Training step 2092",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:09",
      "total_flops_so_far": 4.971738529447386e+16,
      "budget_used_percent": 49.71738529447386
    },
    {
      "type": "training",
      "description": "Training step 2093",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:10",
      "total_flops_so_far": 4.97411326199671e+16,
      "budget_used_percent": 49.741132619967104
    },
    {
      "type": "training",
      "description": "Training step 2094",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:11",
      "total_flops_so_far": 4.976487994546035e+16,
      "budget_used_percent": 49.76487994546035
    },
    {
      "type": "training",
      "description": "Training step 2095",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:13",
      "total_flops_so_far": 4.97886272709536e+16,
      "budget_used_percent": 49.7886272709536
    },
    {
      "type": "training",
      "description": "Training step 2096",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:14",
      "total_flops_so_far": 4.981237459644685e+16,
      "budget_used_percent": 49.812374596446844
    },
    {
      "type": "training",
      "description": "Training step 2097",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:15",
      "total_flops_so_far": 4.98361219219401e+16,
      "budget_used_percent": 49.8361219219401
    },
    {
      "type": "training",
      "description": "Training step 2098",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:17",
      "total_flops_so_far": 4.985986924743334e+16,
      "budget_used_percent": 49.859869247433345
    },
    {
      "type": "training",
      "description": "Training step 2099",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:18",
      "total_flops_so_far": 4.988361657292659e+16,
      "budget_used_percent": 49.88361657292659
    },
    {
      "type": "training",
      "description": "Training step 2100",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:19",
      "total_flops_so_far": 4.990736389841984e+16,
      "budget_used_percent": 49.90736389841984
    },
    {
      "type": "training",
      "description": "Training step 2101",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:21",
      "total_flops_so_far": 4.993111122391309e+16,
      "budget_used_percent": 49.931111223913085
    },
    {
      "type": "training",
      "description": "Training step 2102",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:22",
      "total_flops_so_far": 4.995485854940634e+16,
      "budget_used_percent": 49.95485854940633
    },
    {
      "type": "training",
      "description": "Training step 2103",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:23",
      "total_flops_so_far": 4.997860587489958e+16,
      "budget_used_percent": 49.978605874899586
    },
    {
      "type": "training",
      "description": "Training step 2104",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:25",
      "total_flops_so_far": 5.000235320039283e+16,
      "budget_used_percent": 50.00235320039283
    },
    {
      "type": "training",
      "description": "Training step 2105",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:26",
      "total_flops_so_far": 5.002610052588608e+16,
      "budget_used_percent": 50.02610052588607
    },
    {
      "type": "training",
      "description": "Training step 2106",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:27",
      "total_flops_so_far": 5.004984785137933e+16,
      "budget_used_percent": 50.049847851379326
    },
    {
      "type": "training",
      "description": "Training step 2107",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:29",
      "total_flops_so_far": 5.007359517687258e+16,
      "budget_used_percent": 50.07359517687257
    },
    {
      "type": "training",
      "description": "Training step 2108",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:30",
      "total_flops_so_far": 5.009734250236582e+16,
      "budget_used_percent": 50.09734250236583
    },
    {
      "type": "training",
      "description": "Training step 2109",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:31",
      "total_flops_so_far": 5.012108982785907e+16,
      "budget_used_percent": 50.121089827859066
    },
    {
      "type": "training",
      "description": "Training step 2110",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:33",
      "total_flops_so_far": 5.014483715335232e+16,
      "budget_used_percent": 50.14483715335232
    },
    {
      "type": "training",
      "description": "Training step 2111",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:34",
      "total_flops_so_far": 5.016858447884557e+16,
      "budget_used_percent": 50.16858447884557
    },
    {
      "type": "training",
      "description": "Training step 2112",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:34",
      "total_flops_so_far": 5.019233180433882e+16,
      "budget_used_percent": 50.19233180433882
    },
    {
      "type": "training",
      "description": "Training step 2113",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:36",
      "total_flops_so_far": 5.021607912983206e+16,
      "budget_used_percent": 50.21607912983206
    },
    {
      "type": "training",
      "description": "Training step 2114",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:37",
      "total_flops_so_far": 5.023982645532531e+16,
      "budget_used_percent": 50.23982645532531
    },
    {
      "type": "training",
      "description": "Training step 2115",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:38",
      "total_flops_so_far": 5.026357378081856e+16,
      "budget_used_percent": 50.26357378081856
    },
    {
      "type": "training",
      "description": "Training step 2116",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:40",
      "total_flops_so_far": 5.028732110631181e+16,
      "budget_used_percent": 50.28732110631181
    },
    {
      "type": "training",
      "description": "Training step 2117",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:41",
      "total_flops_so_far": 5.031106843180506e+16,
      "budget_used_percent": 50.31106843180505
    },
    {
      "type": "training",
      "description": "Training step 2118",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:42",
      "total_flops_so_far": 5.03348157572983e+16,
      "budget_used_percent": 50.3348157572983
    },
    {
      "type": "training",
      "description": "Training step 2119",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:44",
      "total_flops_so_far": 5.035856308279155e+16,
      "budget_used_percent": 50.358563082791555
    },
    {
      "type": "training",
      "description": "Training step 2120",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:45",
      "total_flops_so_far": 5.03823104082848e+16,
      "budget_used_percent": 50.3823104082848
    },
    {
      "type": "training",
      "description": "Training step 2121",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:46",
      "total_flops_so_far": 5.040605773377805e+16,
      "budget_used_percent": 50.406057733778056
    },
    {
      "type": "training",
      "description": "Training step 2122",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:48",
      "total_flops_so_far": 5.04298050592713e+16,
      "budget_used_percent": 50.429805059271295
    },
    {
      "type": "training",
      "description": "Training step 2123",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:49",
      "total_flops_so_far": 5.045355238476454e+16,
      "budget_used_percent": 50.45355238476454
    },
    {
      "type": "training",
      "description": "Training step 2124",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:50",
      "total_flops_so_far": 5.047729971025779e+16,
      "budget_used_percent": 50.477299710257796
    },
    {
      "type": "training",
      "description": "Training step 2125",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:52",
      "total_flops_so_far": 5.050104703575104e+16,
      "budget_used_percent": 50.50104703575104
    },
    {
      "type": "training",
      "description": "Training step 2126",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:53",
      "total_flops_so_far": 5.052479436124429e+16,
      "budget_used_percent": 50.52479436124428
    },
    {
      "type": "training",
      "description": "Training step 2127",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:54",
      "total_flops_so_far": 5.054854168673754e+16,
      "budget_used_percent": 50.548541686737536
    },
    {
      "type": "training",
      "description": "Training step 2128",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:56",
      "total_flops_so_far": 5.057228901223078e+16,
      "budget_used_percent": 50.57228901223078
    },
    {
      "type": "training",
      "description": "Training step 2129",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:57",
      "total_flops_so_far": 5.059603633772403e+16,
      "budget_used_percent": 50.59603633772404
    },
    {
      "type": "training",
      "description": "Training step 2130",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:30:58",
      "total_flops_so_far": 5.061978366321728e+16,
      "budget_used_percent": 50.61978366321728
    },
    {
      "type": "training",
      "description": "Training step 2131",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:00",
      "total_flops_so_far": 5.064353098871053e+16,
      "budget_used_percent": 50.64353098871053
    },
    {
      "type": "training",
      "description": "Training step 2132",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:01",
      "total_flops_so_far": 5.066727831420378e+16,
      "budget_used_percent": 50.66727831420378
    },
    {
      "type": "training",
      "description": "Training step 2133",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:02",
      "total_flops_so_far": 5.069102563969702e+16,
      "budget_used_percent": 50.69102563969703
    },
    {
      "type": "training",
      "description": "Training step 2134",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:04",
      "total_flops_so_far": 5.071477296519027e+16,
      "budget_used_percent": 50.71477296519027
    },
    {
      "type": "training",
      "description": "Training step 2135",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:05",
      "total_flops_so_far": 5.073852029068352e+16,
      "budget_used_percent": 50.73852029068352
    },
    {
      "type": "training",
      "description": "Training step 2136",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:06",
      "total_flops_so_far": 5.076226761617677e+16,
      "budget_used_percent": 50.76226761617677
    },
    {
      "type": "training",
      "description": "Training step 2137",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:08",
      "total_flops_so_far": 5.078601494167002e+16,
      "budget_used_percent": 50.78601494167002
    },
    {
      "type": "training",
      "description": "Training step 2138",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:09",
      "total_flops_so_far": 5.080976226716326e+16,
      "budget_used_percent": 50.80976226716326
    },
    {
      "type": "training",
      "description": "Training step 2139",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:10",
      "total_flops_so_far": 5.083350959265651e+16,
      "budget_used_percent": 50.83350959265651
    },
    {
      "type": "training",
      "description": "Training step 2140",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:12",
      "total_flops_so_far": 5.085725691814976e+16,
      "budget_used_percent": 50.857256918149766
    },
    {
      "type": "training",
      "description": "Training step 2141",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:13",
      "total_flops_so_far": 5.088100424364301e+16,
      "budget_used_percent": 50.88100424364301
    },
    {
      "type": "training",
      "description": "Training step 2142",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:14",
      "total_flops_so_far": 5.090475156913626e+16,
      "budget_used_percent": 50.90475156913625
    },
    {
      "type": "training",
      "description": "Training step 2143",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:16",
      "total_flops_so_far": 5.09284988946295e+16,
      "budget_used_percent": 50.928498894629506
    },
    {
      "type": "training",
      "description": "Training step 2144",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:17",
      "total_flops_so_far": 5.095224622012275e+16,
      "budget_used_percent": 50.95224622012275
    },
    {
      "type": "training",
      "description": "Training step 2145",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:18",
      "total_flops_so_far": 5.0975993545616e+16,
      "budget_used_percent": 50.97599354561601
    },
    {
      "type": "training",
      "description": "Training step 2146",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:20",
      "total_flops_so_far": 5.099974087110925e+16,
      "budget_used_percent": 50.999740871109246
    },
    {
      "type": "training",
      "description": "Training step 2147",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:21",
      "total_flops_so_far": 5.10234881966025e+16,
      "budget_used_percent": 51.02348819660249
    },
    {
      "type": "training",
      "description": "Training step 2148",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:22",
      "total_flops_so_far": 5.104723552209574e+16,
      "budget_used_percent": 51.04723552209575
    },
    {
      "type": "training",
      "description": "Training step 2149",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:24",
      "total_flops_so_far": 5.107098284758899e+16,
      "budget_used_percent": 51.07098284758899
    },
    {
      "type": "training",
      "description": "Training step 2150",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:25",
      "total_flops_so_far": 5.109473017308224e+16,
      "budget_used_percent": 51.09473017308224
    },
    {
      "type": "training",
      "description": "Training step 2151",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:26",
      "total_flops_so_far": 5.111847749857549e+16,
      "budget_used_percent": 51.11847749857549
    },
    {
      "type": "training",
      "description": "Training step 2152",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:28",
      "total_flops_so_far": 5.114222482406874e+16,
      "budget_used_percent": 51.14222482406874
    },
    {
      "type": "training",
      "description": "Training step 2153",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:29",
      "total_flops_so_far": 5.116597214956198e+16,
      "budget_used_percent": 51.16597214956199
    },
    {
      "type": "training",
      "description": "Training step 2154",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:30",
      "total_flops_so_far": 5.118971947505523e+16,
      "budget_used_percent": 51.18971947505523
    },
    {
      "type": "training",
      "description": "Training step 2155",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:32",
      "total_flops_so_far": 5.121346680054848e+16,
      "budget_used_percent": 51.21346680054848
    },
    {
      "type": "training",
      "description": "Training step 2156",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:33",
      "total_flops_so_far": 5.123721412604173e+16,
      "budget_used_percent": 51.23721412604173
    },
    {
      "type": "training",
      "description": "Training step 2157",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:34",
      "total_flops_so_far": 5.126096145153498e+16,
      "budget_used_percent": 51.26096145153498
    },
    {
      "type": "training",
      "description": "Training step 2158",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:36",
      "total_flops_so_far": 5.128470877702822e+16,
      "budget_used_percent": 51.28470877702822
    },
    {
      "type": "training",
      "description": "Training step 2159",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:37",
      "total_flops_so_far": 5.130845610252147e+16,
      "budget_used_percent": 51.30845610252147
    },
    {
      "type": "training",
      "description": "Training step 2160",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:38",
      "total_flops_so_far": 5.133220342801472e+16,
      "budget_used_percent": 51.33220342801472
    },
    {
      "type": "training",
      "description": "Training step 2161",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:40",
      "total_flops_so_far": 5.135595075350797e+16,
      "budget_used_percent": 51.355950753507976
    },
    {
      "type": "training",
      "description": "Training step 2162",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:41",
      "total_flops_so_far": 5.137969807900122e+16,
      "budget_used_percent": 51.379698079001216
    },
    {
      "type": "training",
      "description": "Training step 2163",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:42",
      "total_flops_so_far": 5.140344540449446e+16,
      "budget_used_percent": 51.40344540449446
    },
    {
      "type": "training",
      "description": "Training step 2164",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:44",
      "total_flops_so_far": 5.142719272998771e+16,
      "budget_used_percent": 51.427192729987716
    },
    {
      "type": "training",
      "description": "Training step 2165",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:45",
      "total_flops_so_far": 5.145094005548096e+16,
      "budget_used_percent": 51.45094005548096
    },
    {
      "type": "training",
      "description": "Training step 2166",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:46",
      "total_flops_so_far": 5.147468738097421e+16,
      "budget_used_percent": 51.4746873809742
    },
    {
      "type": "training",
      "description": "Training step 2167",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:48",
      "total_flops_so_far": 5.149843470646746e+16,
      "budget_used_percent": 51.49843470646746
    },
    {
      "type": "training",
      "description": "Training step 2168",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:49",
      "total_flops_so_far": 5.15221820319607e+16,
      "budget_used_percent": 51.5221820319607
    },
    {
      "type": "training",
      "description": "Training step 2169",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:50",
      "total_flops_so_far": 5.154592935745395e+16,
      "budget_used_percent": 51.54592935745396
    },
    {
      "type": "training",
      "description": "Training step 2170",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:52",
      "total_flops_so_far": 5.15696766829472e+16,
      "budget_used_percent": 51.5696766829472
    },
    {
      "type": "training",
      "description": "Training step 2171",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:53",
      "total_flops_so_far": 5.159342400844045e+16,
      "budget_used_percent": 51.59342400844045
    },
    {
      "type": "training",
      "description": "Training step 2172",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:54",
      "total_flops_so_far": 5.16171713339337e+16,
      "budget_used_percent": 51.6171713339337
    },
    {
      "type": "training",
      "description": "Training step 2173",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:55",
      "total_flops_so_far": 5.164091865942694e+16,
      "budget_used_percent": 51.64091865942695
    },
    {
      "type": "training",
      "description": "Training step 2174",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:57",
      "total_flops_so_far": 5.166466598492019e+16,
      "budget_used_percent": 51.66466598492019
    },
    {
      "type": "training",
      "description": "Training step 2175",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:58",
      "total_flops_so_far": 5.168841331041344e+16,
      "budget_used_percent": 51.68841331041344
    },
    {
      "type": "training",
      "description": "Training step 2176",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:31:59",
      "total_flops_so_far": 5.171216063590669e+16,
      "budget_used_percent": 51.71216063590669
    },
    {
      "type": "training",
      "description": "Training step 2177",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:01",
      "total_flops_so_far": 5.173590796139994e+16,
      "budget_used_percent": 51.73590796139994
    },
    {
      "type": "training",
      "description": "Training step 2178",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:02",
      "total_flops_so_far": 5.175965528689318e+16,
      "budget_used_percent": 51.75965528689318
    },
    {
      "type": "training",
      "description": "Training step 2179",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:03",
      "total_flops_so_far": 5.178340261238643e+16,
      "budget_used_percent": 51.78340261238643
    },
    {
      "type": "training",
      "description": "Training step 2180",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:05",
      "total_flops_so_far": 5.180714993787968e+16,
      "budget_used_percent": 51.80714993787968
    },
    {
      "type": "training",
      "description": "Training step 2181",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:06",
      "total_flops_so_far": 5.183089726337293e+16,
      "budget_used_percent": 51.83089726337293
    },
    {
      "type": "training",
      "description": "Training step 2182",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:07",
      "total_flops_so_far": 5.185464458886618e+16,
      "budget_used_percent": 51.85464458886617
    },
    {
      "type": "training",
      "description": "Training step 2183",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:09",
      "total_flops_so_far": 5.187839191435942e+16,
      "budget_used_percent": 51.878391914359426
    },
    {
      "type": "training",
      "description": "Training step 2184",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:10",
      "total_flops_so_far": 5.190213923985267e+16,
      "budget_used_percent": 51.90213923985267
    },
    {
      "type": "training",
      "description": "Training step 2185",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:11",
      "total_flops_so_far": 5.192588656534592e+16,
      "budget_used_percent": 51.92588656534593
    },
    {
      "type": "training",
      "description": "Training step 2186",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:13",
      "total_flops_so_far": 5.194963389083917e+16,
      "budget_used_percent": 51.949633890839166
    },
    {
      "type": "training",
      "description": "Training step 2187",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:14",
      "total_flops_so_far": 5.197338121633242e+16,
      "budget_used_percent": 51.97338121633241
    },
    {
      "type": "training",
      "description": "Training step 2188",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:15",
      "total_flops_so_far": 5.199712854182566e+16,
      "budget_used_percent": 51.99712854182567
    },
    {
      "type": "training",
      "description": "Training step 2189",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:17",
      "total_flops_so_far": 5.202087586731891e+16,
      "budget_used_percent": 52.020875867318914
    },
    {
      "type": "training",
      "description": "Training step 2190",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:18",
      "total_flops_so_far": 5.204462319281216e+16,
      "budget_used_percent": 52.04462319281215
    },
    {
      "type": "training",
      "description": "Training step 2191",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:19",
      "total_flops_so_far": 5.206837051830541e+16,
      "budget_used_percent": 52.06837051830541
    },
    {
      "type": "training",
      "description": "Training step 2192",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:21",
      "total_flops_so_far": 5.209211784379866e+16,
      "budget_used_percent": 52.09211784379866
    },
    {
      "type": "training",
      "description": "Training step 2193",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:22",
      "total_flops_so_far": 5.21158651692919e+16,
      "budget_used_percent": 52.11586516929191
    },
    {
      "type": "training",
      "description": "Training step 2194",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:23",
      "total_flops_so_far": 5.213961249478515e+16,
      "budget_used_percent": 52.13961249478515
    },
    {
      "type": "training",
      "description": "Training step 2195",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:25",
      "total_flops_so_far": 5.21633598202784e+16,
      "budget_used_percent": 52.1633598202784
    },
    {
      "type": "training",
      "description": "Training step 2196",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:26",
      "total_flops_so_far": 5.218710714577165e+16,
      "budget_used_percent": 52.18710714577165
    },
    {
      "type": "training",
      "description": "Training step 2197",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:27",
      "total_flops_so_far": 5.22108544712649e+16,
      "budget_used_percent": 52.2108544712649
    },
    {
      "type": "training",
      "description": "Training step 2198",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:29",
      "total_flops_so_far": 5.223460179675814e+16,
      "budget_used_percent": 52.23460179675814
    },
    {
      "type": "training",
      "description": "Training step 2199",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:30",
      "total_flops_so_far": 5.225834912225139e+16,
      "budget_used_percent": 52.25834912225139
    },
    {
      "type": "training",
      "description": "Training step 2200",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:31",
      "total_flops_so_far": 5.228209644774464e+16,
      "budget_used_percent": 52.28209644774464
    },
    {
      "type": "training",
      "description": "Training step 2201",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:33",
      "total_flops_so_far": 5.230584377323789e+16,
      "budget_used_percent": 52.30584377323789
    },
    {
      "type": "training",
      "description": "Training step 2202",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:34",
      "total_flops_so_far": 5.232959109873114e+16,
      "budget_used_percent": 52.32959109873113
    },
    {
      "type": "training",
      "description": "Training step 2203",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:35",
      "total_flops_so_far": 5.235333842422438e+16,
      "budget_used_percent": 52.35333842422438
    },
    {
      "type": "training",
      "description": "Training step 2204",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:37",
      "total_flops_so_far": 5.237708574971763e+16,
      "budget_used_percent": 52.37708574971764
    },
    {
      "type": "training",
      "description": "Training step 2205",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:38",
      "total_flops_so_far": 5.240083307521088e+16,
      "budget_used_percent": 52.40083307521088
    },
    {
      "type": "training",
      "description": "Training step 2206",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:39",
      "total_flops_so_far": 5.242458040070413e+16,
      "budget_used_percent": 52.42458040070412
    },
    {
      "type": "training",
      "description": "Training step 2207",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:41",
      "total_flops_so_far": 5.244832772619738e+16,
      "budget_used_percent": 52.44832772619738
    },
    {
      "type": "training",
      "description": "Training step 2208",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:42",
      "total_flops_so_far": 5.247207505169062e+16,
      "budget_used_percent": 52.472075051690624
    },
    {
      "type": "training",
      "description": "Training step 2209",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:43",
      "total_flops_so_far": 5.249582237718387e+16,
      "budget_used_percent": 52.49582237718388
    },
    {
      "type": "training",
      "description": "Training step 2210",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:45",
      "total_flops_so_far": 5.251956970267712e+16,
      "budget_used_percent": 52.51956970267712
    },
    {
      "type": "training",
      "description": "Training step 2211",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:46",
      "total_flops_so_far": 5.254331702817037e+16,
      "budget_used_percent": 52.543317028170364
    },
    {
      "type": "training",
      "description": "Training step 2212",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:47",
      "total_flops_so_far": 5.256706435366362e+16,
      "budget_used_percent": 52.56706435366362
    },
    {
      "type": "training",
      "description": "Training step 2213",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:49",
      "total_flops_so_far": 5.259081167915686e+16,
      "budget_used_percent": 52.59081167915687
    },
    {
      "type": "training",
      "description": "Training step 2214",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:50",
      "total_flops_so_far": 5.261455900465011e+16,
      "budget_used_percent": 52.614559004650104
    },
    {
      "type": "training",
      "description": "Training step 2215",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:51",
      "total_flops_so_far": 5.263830633014336e+16,
      "budget_used_percent": 52.63830633014336
    },
    {
      "type": "training",
      "description": "Training step 2216",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:53",
      "total_flops_so_far": 5.266205365563661e+16,
      "budget_used_percent": 52.66205365563661
    },
    {
      "type": "training",
      "description": "Training step 2217",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:54",
      "total_flops_so_far": 5.268580098112986e+16,
      "budget_used_percent": 52.68580098112986
    },
    {
      "type": "training",
      "description": "Training step 2218",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:55",
      "total_flops_so_far": 5.27095483066231e+16,
      "budget_used_percent": 52.7095483066231
    },
    {
      "type": "training",
      "description": "Training step 2219",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:57",
      "total_flops_so_far": 5.273329563211635e+16,
      "budget_used_percent": 52.73329563211635
    },
    {
      "type": "training",
      "description": "Training step 2220",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:58",
      "total_flops_so_far": 5.27570429576096e+16,
      "budget_used_percent": 52.7570429576096
    },
    {
      "type": "training",
      "description": "Training step 2221",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:32:59",
      "total_flops_so_far": 5.278079028310285e+16,
      "budget_used_percent": 52.78079028310285
    },
    {
      "type": "training",
      "description": "Training step 2222",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:01",
      "total_flops_so_far": 5.28045376085961e+16,
      "budget_used_percent": 52.80453760859609
    },
    {
      "type": "training",
      "description": "Training step 2223",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:02",
      "total_flops_so_far": 5.282828493408934e+16,
      "budget_used_percent": 52.82828493408934
    },
    {
      "type": "training",
      "description": "Training step 2224",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:03",
      "total_flops_so_far": 5.285203225958259e+16,
      "budget_used_percent": 52.85203225958259
    },
    {
      "type": "training",
      "description": "Training step 2225",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:05",
      "total_flops_so_far": 5.287577958507584e+16,
      "budget_used_percent": 52.87577958507585
    },
    {
      "type": "training",
      "description": "Training step 2226",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:06",
      "total_flops_so_far": 5.289952691056909e+16,
      "budget_used_percent": 52.89952691056909
    },
    {
      "type": "training",
      "description": "Training step 2227",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:07",
      "total_flops_so_far": 5.292327423606234e+16,
      "budget_used_percent": 52.92327423606233
    },
    {
      "type": "training",
      "description": "Training step 2228",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:09",
      "total_flops_so_far": 5.294702156155558e+16,
      "budget_used_percent": 52.94702156155559
    },
    {
      "type": "training",
      "description": "Training step 2229",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:10",
      "total_flops_so_far": 5.297076888704883e+16,
      "budget_used_percent": 52.970768887048834
    },
    {
      "type": "training",
      "description": "Training step 2230",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:11",
      "total_flops_so_far": 5.299451621254208e+16,
      "budget_used_percent": 52.994516212542074
    },
    {
      "type": "training",
      "description": "Training step 2231",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:13",
      "total_flops_so_far": 5.301826353803533e+16,
      "budget_used_percent": 53.01826353803533
    },
    {
      "type": "training",
      "description": "Training step 2232",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:14",
      "total_flops_so_far": 5.304201086352858e+16,
      "budget_used_percent": 53.042010863528574
    },
    {
      "type": "training",
      "description": "Training step 2233",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:15",
      "total_flops_so_far": 5.306575818902182e+16,
      "budget_used_percent": 53.06575818902183
    },
    {
      "type": "training",
      "description": "Training step 2234",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:17",
      "total_flops_so_far": 5.308950551451507e+16,
      "budget_used_percent": 53.08950551451507
    },
    {
      "type": "training",
      "description": "Training step 2235",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:18",
      "total_flops_so_far": 5.311325284000832e+16,
      "budget_used_percent": 53.113252840008315
    },
    {
      "type": "training",
      "description": "Training step 2236",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:19",
      "total_flops_so_far": 5.313700016550157e+16,
      "budget_used_percent": 53.13700016550157
    },
    {
      "type": "training",
      "description": "Training step 2237",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:21",
      "total_flops_so_far": 5.316074749099482e+16,
      "budget_used_percent": 53.16074749099482
    },
    {
      "type": "training",
      "description": "Training step 2238",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:22",
      "total_flops_so_far": 5.318449481648806e+16,
      "budget_used_percent": 53.18449481648806
    },
    {
      "type": "training",
      "description": "Training step 2239",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:23",
      "total_flops_so_far": 5.320824214198131e+16,
      "budget_used_percent": 53.20824214198131
    },
    {
      "type": "training",
      "description": "Training step 2240",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:25",
      "total_flops_so_far": 5.323198946747456e+16,
      "budget_used_percent": 53.23198946747456
    },
    {
      "type": "training",
      "description": "Training step 2241",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:26",
      "total_flops_so_far": 5.325573679296781e+16,
      "budget_used_percent": 53.25573679296781
    },
    {
      "type": "training",
      "description": "Training step 2242",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:27",
      "total_flops_so_far": 5.327948411846106e+16,
      "budget_used_percent": 53.27948411846105
    },
    {
      "type": "training",
      "description": "Training step 2243",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:29",
      "total_flops_so_far": 5.33032314439543e+16,
      "budget_used_percent": 53.3032314439543
    },
    {
      "type": "training",
      "description": "Training step 2244",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:29",
      "total_flops_so_far": 5.332697876944755e+16,
      "budget_used_percent": 53.32697876944755
    },
    {
      "type": "training",
      "description": "Training step 2245",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:30",
      "total_flops_so_far": 5.33507260949408e+16,
      "budget_used_percent": 53.3507260949408
    },
    {
      "type": "training",
      "description": "Training step 2246",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:32",
      "total_flops_so_far": 5.337447342043405e+16,
      "budget_used_percent": 53.37447342043404
    },
    {
      "type": "training",
      "description": "Training step 2247",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:33",
      "total_flops_so_far": 5.33982207459273e+16,
      "budget_used_percent": 53.3982207459273
    },
    {
      "type": "training",
      "description": "Training step 2248",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:34",
      "total_flops_so_far": 5.342196807142054e+16,
      "budget_used_percent": 53.421968071420544
    },
    {
      "type": "training",
      "description": "Training step 2249",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:36",
      "total_flops_so_far": 5.344571539691379e+16,
      "budget_used_percent": 53.4457153969138
    },
    {
      "type": "training",
      "description": "Training step 2250",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:37",
      "total_flops_so_far": 5.346946272240704e+16,
      "budget_used_percent": 53.46946272240704
    },
    {
      "type": "training",
      "description": "Training step 2251",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:38",
      "total_flops_so_far": 5.349321004790029e+16,
      "budget_used_percent": 53.493210047900284
    },
    {
      "type": "training",
      "description": "Training step 2252",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:40",
      "total_flops_so_far": 5.351695737339354e+16,
      "budget_used_percent": 53.51695737339354
    },
    {
      "type": "training",
      "description": "Training step 2253",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:41",
      "total_flops_so_far": 5.354070469888678e+16,
      "budget_used_percent": 53.540704698886785
    },
    {
      "type": "training",
      "description": "Training step 2254",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:42",
      "total_flops_so_far": 5.356445202438003e+16,
      "budget_used_percent": 53.564452024380024
    },
    {
      "type": "training",
      "description": "Training step 2255",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:44",
      "total_flops_so_far": 5.358819934987328e+16,
      "budget_used_percent": 53.58819934987328
    },
    {
      "type": "training",
      "description": "Training step 2256",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:45",
      "total_flops_so_far": 5.361194667536653e+16,
      "budget_used_percent": 53.611946675366525
    },
    {
      "type": "training",
      "description": "Training step 2257",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:46",
      "total_flops_so_far": 5.363569400085978e+16,
      "budget_used_percent": 53.63569400085978
    },
    {
      "type": "training",
      "description": "Training step 2258",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:48",
      "total_flops_so_far": 5.365944132635302e+16,
      "budget_used_percent": 53.65944132635302
    },
    {
      "type": "training",
      "description": "Training step 2259",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:49",
      "total_flops_so_far": 5.368318865184627e+16,
      "budget_used_percent": 53.68318865184627
    },
    {
      "type": "training",
      "description": "Training step 2260",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:50",
      "total_flops_so_far": 5.370693597733952e+16,
      "budget_used_percent": 53.70693597733952
    },
    {
      "type": "training",
      "description": "Training step 2261",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:52",
      "total_flops_so_far": 5.373068330283277e+16,
      "budget_used_percent": 53.73068330283277
    },
    {
      "type": "training",
      "description": "Training step 2262",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:53",
      "total_flops_so_far": 5.375443062832602e+16,
      "budget_used_percent": 53.75443062832601
    },
    {
      "type": "training",
      "description": "Training step 2263",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:54",
      "total_flops_so_far": 5.377817795381926e+16,
      "budget_used_percent": 53.77817795381926
    },
    {
      "type": "training",
      "description": "Training step 2264",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:56",
      "total_flops_so_far": 5.380192527931251e+16,
      "budget_used_percent": 53.80192527931251
    },
    {
      "type": "training",
      "description": "Training step 2265",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:57",
      "total_flops_so_far": 5.382567260480576e+16,
      "budget_used_percent": 53.82567260480576
    },
    {
      "type": "training",
      "description": "Training step 2266",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:33:58",
      "total_flops_so_far": 5.384941993029901e+16,
      "budget_used_percent": 53.849419930299
    },
    {
      "type": "training",
      "description": "Training step 2267",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:00",
      "total_flops_so_far": 5.387316725579226e+16,
      "budget_used_percent": 53.873167255792254
    },
    {
      "type": "training",
      "description": "Training step 2268",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:01",
      "total_flops_so_far": 5.38969145812855e+16,
      "budget_used_percent": 53.89691458128551
    },
    {
      "type": "training",
      "description": "Training step 2269",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:02",
      "total_flops_so_far": 5.392066190677875e+16,
      "budget_used_percent": 53.920661906778754
    },
    {
      "type": "training",
      "description": "Training step 2270",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:04",
      "total_flops_so_far": 5.3944409232272e+16,
      "budget_used_percent": 53.944409232271994
    },
    {
      "type": "training",
      "description": "Training step 2271",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:05",
      "total_flops_so_far": 5.396815655776525e+16,
      "budget_used_percent": 53.96815655776525
    },
    {
      "type": "training",
      "description": "Training step 2272",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:06",
      "total_flops_so_far": 5.39919038832585e+16,
      "budget_used_percent": 53.991903883258495
    },
    {
      "type": "training",
      "description": "Training step 2273",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:08",
      "total_flops_so_far": 5.401565120875174e+16,
      "budget_used_percent": 54.01565120875175
    },
    {
      "type": "training",
      "description": "Training step 2274",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:09",
      "total_flops_so_far": 5.403939853424499e+16,
      "budget_used_percent": 54.03939853424499
    },
    {
      "type": "training",
      "description": "Training step 2275",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:10",
      "total_flops_so_far": 5.406314585973824e+16,
      "budget_used_percent": 54.063145859738235
    },
    {
      "type": "training",
      "description": "Training step 2276",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:11",
      "total_flops_so_far": 5.408689318523149e+16,
      "budget_used_percent": 54.08689318523149
    },
    {
      "type": "training",
      "description": "Training step 2277",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:13",
      "total_flops_so_far": 5.411064051072474e+16,
      "budget_used_percent": 54.110640510724735
    },
    {
      "type": "training",
      "description": "Training step 2278",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:14",
      "total_flops_so_far": 5.413438783621798e+16,
      "budget_used_percent": 54.13438783621798
    },
    {
      "type": "training",
      "description": "Training step 2279",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:15",
      "total_flops_so_far": 5.415813516171123e+16,
      "budget_used_percent": 54.15813516171123
    },
    {
      "type": "training",
      "description": "Training step 2280",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:17",
      "total_flops_so_far": 5.418188248720448e+16,
      "budget_used_percent": 54.18188248720448
    },
    {
      "type": "training",
      "description": "Training step 2281",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:18",
      "total_flops_so_far": 5.420562981269773e+16,
      "budget_used_percent": 54.20562981269773
    },
    {
      "type": "training",
      "description": "Training step 2282",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:19",
      "total_flops_so_far": 5.422937713819098e+16,
      "budget_used_percent": 54.22937713819097
    },
    {
      "type": "training",
      "description": "Training step 2283",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:21",
      "total_flops_so_far": 5.425312446368422e+16,
      "budget_used_percent": 54.25312446368422
    },
    {
      "type": "training",
      "description": "Training step 2284",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:22",
      "total_flops_so_far": 5.427687178917747e+16,
      "budget_used_percent": 54.27687178917747
    },
    {
      "type": "training",
      "description": "Training step 2285",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:23",
      "total_flops_so_far": 5.430061911467072e+16,
      "budget_used_percent": 54.300619114670724
    },
    {
      "type": "training",
      "description": "Training step 2286",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:25",
      "total_flops_so_far": 5.432436644016397e+16,
      "budget_used_percent": 54.32436644016396
    },
    {
      "type": "training",
      "description": "Training step 2287",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:26",
      "total_flops_so_far": 5.434811376565722e+16,
      "budget_used_percent": 54.34811376565721
    },
    {
      "type": "training",
      "description": "Training step 2288",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:27",
      "total_flops_so_far": 5.437186109115046e+16,
      "budget_used_percent": 54.371861091150464
    },
    {
      "type": "training",
      "description": "Training step 2289",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:29",
      "total_flops_so_far": 5.439560841664371e+16,
      "budget_used_percent": 54.39560841664372
    },
    {
      "type": "training",
      "description": "Training step 2290",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:30",
      "total_flops_so_far": 5.441935574213696e+16,
      "budget_used_percent": 54.41935574213696
    },
    {
      "type": "training",
      "description": "Training step 2291",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:31",
      "total_flops_so_far": 5.444310306763021e+16,
      "budget_used_percent": 54.443103067630204
    },
    {
      "type": "training",
      "description": "Training step 2292",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:33",
      "total_flops_so_far": 5.446685039312346e+16,
      "budget_used_percent": 54.46685039312346
    },
    {
      "type": "training",
      "description": "Training step 2293",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:34",
      "total_flops_so_far": 5.44905977186167e+16,
      "budget_used_percent": 54.490597718616705
    },
    {
      "type": "training",
      "description": "Training step 2294",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:35",
      "total_flops_so_far": 5.451434504410995e+16,
      "budget_used_percent": 54.51434504410996
    },
    {
      "type": "training",
      "description": "Training step 2295",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:37",
      "total_flops_so_far": 5.45380923696032e+16,
      "budget_used_percent": 54.5380923696032
    },
    {
      "type": "training",
      "description": "Training step 2296",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:38",
      "total_flops_so_far": 5.456183969509645e+16,
      "budget_used_percent": 54.561839695096445
    },
    {
      "type": "training",
      "description": "Training step 2297",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:39",
      "total_flops_so_far": 5.45855870205897e+16,
      "budget_used_percent": 54.5855870205897
    },
    {
      "type": "training",
      "description": "Training step 2298",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:41",
      "total_flops_so_far": 5.460933434608294e+16,
      "budget_used_percent": 54.609334346082946
    },
    {
      "type": "training",
      "description": "Training step 2299",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:42",
      "total_flops_so_far": 5.463308167157619e+16,
      "budget_used_percent": 54.63308167157619
    },
    {
      "type": "training",
      "description": "Training step 2300",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:44",
      "total_flops_so_far": 5.465682899706944e+16,
      "budget_used_percent": 54.65682899706944
    },
    {
      "type": "training",
      "description": "Training step 2301",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:45",
      "total_flops_so_far": 5.468057632256269e+16,
      "budget_used_percent": 54.68057632256269
    },
    {
      "type": "training",
      "description": "Training step 2302",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:46",
      "total_flops_so_far": 5.470432364805594e+16,
      "budget_used_percent": 54.70432364805594
    },
    {
      "type": "training",
      "description": "Training step 2303",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:48",
      "total_flops_so_far": 5.472807097354918e+16,
      "budget_used_percent": 54.72807097354918
    },
    {
      "type": "training",
      "description": "Training step 2304",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:49",
      "total_flops_so_far": 5.475181829904243e+16,
      "budget_used_percent": 54.751818299042434
    },
    {
      "type": "training",
      "description": "Training step 2305",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:50",
      "total_flops_so_far": 5.477556562453568e+16,
      "budget_used_percent": 54.77556562453568
    },
    {
      "type": "training",
      "description": "Training step 2306",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:52",
      "total_flops_so_far": 5.479931295002893e+16,
      "budget_used_percent": 54.799312950028934
    },
    {
      "type": "training",
      "description": "Training step 2307",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:53",
      "total_flops_so_far": 5.482306027552218e+16,
      "budget_used_percent": 54.823060275522174
    },
    {
      "type": "training",
      "description": "Training step 2308",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:54",
      "total_flops_so_far": 5.484680760101542e+16,
      "budget_used_percent": 54.84680760101542
    },
    {
      "type": "training",
      "description": "Training step 2309",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:56",
      "total_flops_so_far": 5.487055492650867e+16,
      "budget_used_percent": 54.870554926508674
    },
    {
      "type": "training",
      "description": "Training step 2310",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:57",
      "total_flops_so_far": 5.489430225200192e+16,
      "budget_used_percent": 54.89430225200193
    },
    {
      "type": "training",
      "description": "Training step 2311",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:34:58",
      "total_flops_so_far": 5.491804957749517e+16,
      "budget_used_percent": 54.91804957749517
    },
    {
      "type": "training",
      "description": "Training step 2312",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:00",
      "total_flops_so_far": 5.494179690298842e+16,
      "budget_used_percent": 54.941796902988415
    },
    {
      "type": "training",
      "description": "Training step 2313",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:01",
      "total_flops_so_far": 5.496554422848166e+16,
      "budget_used_percent": 54.96554422848167
    },
    {
      "type": "training",
      "description": "Training step 2314",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:02",
      "total_flops_so_far": 5.498929155397491e+16,
      "budget_used_percent": 54.989291553974915
    },
    {
      "type": "training",
      "description": "Training step 2315",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:04",
      "total_flops_so_far": 5.501303887946816e+16,
      "budget_used_percent": 55.013038879468155
    },
    {
      "type": "training",
      "description": "Training step 2316",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:05",
      "total_flops_so_far": 5.503678620496141e+16,
      "budget_used_percent": 55.03678620496141
    },
    {
      "type": "training",
      "description": "Training step 2317",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:06",
      "total_flops_so_far": 5.506053353045466e+16,
      "budget_used_percent": 55.060533530454656
    },
    {
      "type": "training",
      "description": "Training step 2318",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:08",
      "total_flops_so_far": 5.50842808559479e+16,
      "budget_used_percent": 55.08428085594791
    },
    {
      "type": "training",
      "description": "Training step 2319",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:09",
      "total_flops_so_far": 5.510802818144115e+16,
      "budget_used_percent": 55.10802818144115
    },
    {
      "type": "training",
      "description": "Training step 2320",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:10",
      "total_flops_so_far": 5.51317755069344e+16,
      "budget_used_percent": 55.1317755069344
    },
    {
      "type": "training",
      "description": "Training step 2321",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:12",
      "total_flops_so_far": 5.515552283242765e+16,
      "budget_used_percent": 55.15552283242765
    },
    {
      "type": "training",
      "description": "Training step 2322",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:15",
      "total_flops_so_far": 5.51792701579209e+16,
      "budget_used_percent": 55.179270157920904
    },
    {
      "type": "training",
      "description": "Training step 2323",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:16",
      "total_flops_so_far": 5.520301748341414e+16,
      "budget_used_percent": 55.20301748341414
    },
    {
      "type": "training",
      "description": "Training step 2324",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:17",
      "total_flops_so_far": 5.522676480890739e+16,
      "budget_used_percent": 55.22676480890739
    },
    {
      "type": "training",
      "description": "Training step 2325",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:19",
      "total_flops_so_far": 5.525051213440064e+16,
      "budget_used_percent": 55.250512134400644
    },
    {
      "type": "training",
      "description": "Training step 2326",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:20",
      "total_flops_so_far": 5.527425945989389e+16,
      "budget_used_percent": 55.27425945989389
    },
    {
      "type": "training",
      "description": "Training step 2327",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:21",
      "total_flops_so_far": 5.529800678538714e+16,
      "budget_used_percent": 55.29800678538713
    },
    {
      "type": "training",
      "description": "Training step 2328",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:23",
      "total_flops_so_far": 5.532175411088038e+16,
      "budget_used_percent": 55.321754110880384
    },
    {
      "type": "training",
      "description": "Training step 2329",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:24",
      "total_flops_so_far": 5.534550143637363e+16,
      "budget_used_percent": 55.34550143637363
    },
    {
      "type": "training",
      "description": "Training step 2330",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:25",
      "total_flops_so_far": 5.536924876186688e+16,
      "budget_used_percent": 55.369248761866885
    },
    {
      "type": "training",
      "description": "Training step 2331",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:27",
      "total_flops_so_far": 5.539299608736013e+16,
      "budget_used_percent": 55.392996087360125
    },
    {
      "type": "training",
      "description": "Training step 2332",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:28",
      "total_flops_so_far": 5.541674341285338e+16,
      "budget_used_percent": 55.41674341285338
    },
    {
      "type": "training",
      "description": "Training step 2333",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:29",
      "total_flops_so_far": 5.544049073834662e+16,
      "budget_used_percent": 55.440490738346625
    },
    {
      "type": "training",
      "description": "Training step 2334",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:31",
      "total_flops_so_far": 5.546423806383987e+16,
      "budget_used_percent": 55.46423806383988
    },
    {
      "type": "training",
      "description": "Training step 2335",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:32",
      "total_flops_so_far": 5.548798538933312e+16,
      "budget_used_percent": 55.48798538933312
    },
    {
      "type": "training",
      "description": "Training step 2336",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:33",
      "total_flops_so_far": 5.551173271482637e+16,
      "budget_used_percent": 55.511732714826366
    },
    {
      "type": "training",
      "description": "Training step 2337",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:35",
      "total_flops_so_far": 5.553548004031962e+16,
      "budget_used_percent": 55.53548004031962
    },
    {
      "type": "training",
      "description": "Training step 2338",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:36",
      "total_flops_so_far": 5.555922736581286e+16,
      "budget_used_percent": 55.559227365812866
    },
    {
      "type": "training",
      "description": "Training step 2339",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:37",
      "total_flops_so_far": 5.558297469130611e+16,
      "budget_used_percent": 55.582974691306106
    },
    {
      "type": "training",
      "description": "Training step 2340",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:39",
      "total_flops_so_far": 5.560672201679936e+16,
      "budget_used_percent": 55.60672201679936
    },
    {
      "type": "training",
      "description": "Training step 2341",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:40",
      "total_flops_so_far": 5.563046934229261e+16,
      "budget_used_percent": 55.63046934229261
    },
    {
      "type": "training",
      "description": "Training step 2342",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:41",
      "total_flops_so_far": 5.565421666778586e+16,
      "budget_used_percent": 55.65421666778586
    },
    {
      "type": "training",
      "description": "Training step 2343",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:43",
      "total_flops_so_far": 5.56779639932791e+16,
      "budget_used_percent": 55.6779639932791
    },
    {
      "type": "training",
      "description": "Training step 2344",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:44",
      "total_flops_so_far": 5.570171131877235e+16,
      "budget_used_percent": 55.701711318772354
    },
    {
      "type": "training",
      "description": "Training step 2345",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:45",
      "total_flops_so_far": 5.57254586442656e+16,
      "budget_used_percent": 55.7254586442656
    },
    {
      "type": "training",
      "description": "Training step 2346",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:47",
      "total_flops_so_far": 5.574920596975885e+16,
      "budget_used_percent": 55.749205969758854
    },
    {
      "type": "training",
      "description": "Training step 2347",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:48",
      "total_flops_so_far": 5.57729532952521e+16,
      "budget_used_percent": 55.772953295252094
    },
    {
      "type": "training",
      "description": "Training step 2348",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:49",
      "total_flops_so_far": 5.579670062074534e+16,
      "budget_used_percent": 55.79670062074534
    },
    {
      "type": "training",
      "description": "Training step 2349",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:51",
      "total_flops_so_far": 5.582044794623859e+16,
      "budget_used_percent": 55.820447946238595
    },
    {
      "type": "training",
      "description": "Training step 2350",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:52",
      "total_flops_so_far": 5.584419527173184e+16,
      "budget_used_percent": 55.84419527173184
    },
    {
      "type": "training",
      "description": "Training step 2351",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:53",
      "total_flops_so_far": 5.586794259722509e+16,
      "budget_used_percent": 55.86794259722508
    },
    {
      "type": "training",
      "description": "Training step 2352",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:55",
      "total_flops_so_far": 5.589168992271834e+16,
      "budget_used_percent": 55.891689922718335
    },
    {
      "type": "training",
      "description": "Training step 2353",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:56",
      "total_flops_so_far": 5.591543724821158e+16,
      "budget_used_percent": 55.91543724821159
    },
    {
      "type": "training",
      "description": "Training step 2354",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:57",
      "total_flops_so_far": 5.593918457370483e+16,
      "budget_used_percent": 55.939184573704836
    },
    {
      "type": "training",
      "description": "Training step 2355",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:35:59",
      "total_flops_so_far": 5.596293189919808e+16,
      "budget_used_percent": 55.962931899198075
    },
    {
      "type": "training",
      "description": "Training step 2356",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:00",
      "total_flops_so_far": 5.598667922469133e+16,
      "budget_used_percent": 55.98667922469133
    },
    {
      "type": "training",
      "description": "Training step 2357",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:01",
      "total_flops_so_far": 5.601042655018458e+16,
      "budget_used_percent": 56.010426550184576
    },
    {
      "type": "training",
      "description": "Training step 2358",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:03",
      "total_flops_so_far": 5.603417387567782e+16,
      "budget_used_percent": 56.03417387567783
    },
    {
      "type": "training",
      "description": "Training step 2359",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:04",
      "total_flops_so_far": 5.605792120117107e+16,
      "budget_used_percent": 56.05792120117107
    },
    {
      "type": "training",
      "description": "Training step 2360",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:05",
      "total_flops_so_far": 5.608166852666432e+16,
      "budget_used_percent": 56.081668526664316
    },
    {
      "type": "training",
      "description": "Training step 2361",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:07",
      "total_flops_so_far": 5.610541585215757e+16,
      "budget_used_percent": 56.10541585215757
    },
    {
      "type": "training",
      "description": "Training step 2362",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:08",
      "total_flops_so_far": 5.612916317765082e+16,
      "budget_used_percent": 56.129163177650824
    },
    {
      "type": "training",
      "description": "Training step 2363",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:09",
      "total_flops_so_far": 5.615291050314406e+16,
      "budget_used_percent": 56.15291050314406
    },
    {
      "type": "training",
      "description": "Training step 2364",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:11",
      "total_flops_so_far": 5.617665782863731e+16,
      "budget_used_percent": 56.17665782863731
    },
    {
      "type": "training",
      "description": "Training step 2365",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:12",
      "total_flops_so_far": 5.620040515413056e+16,
      "budget_used_percent": 56.200405154130564
    },
    {
      "type": "training",
      "description": "Training step 2366",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:13",
      "total_flops_so_far": 5.622415247962381e+16,
      "budget_used_percent": 56.22415247962381
    },
    {
      "type": "training",
      "description": "Training step 2367",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:15",
      "total_flops_so_far": 5.624789980511706e+16,
      "budget_used_percent": 56.24789980511705
    },
    {
      "type": "training",
      "description": "Training step 2368",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:16",
      "total_flops_so_far": 5.62716471306103e+16,
      "budget_used_percent": 56.271647130610305
    },
    {
      "type": "training",
      "description": "Training step 2369",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:17",
      "total_flops_so_far": 5.629539445610355e+16,
      "budget_used_percent": 56.29539445610355
    },
    {
      "type": "training",
      "description": "Training step 2370",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:19",
      "total_flops_so_far": 5.63191417815968e+16,
      "budget_used_percent": 56.319141781596805
    },
    {
      "type": "training",
      "description": "Training step 2371",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:20",
      "total_flops_so_far": 5.634288910709005e+16,
      "budget_used_percent": 56.342889107090045
    },
    {
      "type": "training",
      "description": "Training step 2372",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:21",
      "total_flops_so_far": 5.63666364325833e+16,
      "budget_used_percent": 56.36663643258329
    },
    {
      "type": "training",
      "description": "Training step 2373",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:23",
      "total_flops_so_far": 5.639038375807654e+16,
      "budget_used_percent": 56.390383758076545
    },
    {
      "type": "training",
      "description": "Training step 2374",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:24",
      "total_flops_so_far": 5.641413108356979e+16,
      "budget_used_percent": 56.4141310835698
    },
    {
      "type": "training",
      "description": "Training step 2375",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:25",
      "total_flops_so_far": 5.643787840906304e+16,
      "budget_used_percent": 56.43787840906304
    },
    {
      "type": "training",
      "description": "Training step 2376",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:26",
      "total_flops_so_far": 5.646162573455629e+16,
      "budget_used_percent": 56.461625734556286
    },
    {
      "type": "training",
      "description": "Training step 2377",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:27",
      "total_flops_so_far": 5.648537306004954e+16,
      "budget_used_percent": 56.48537306004954
    },
    {
      "type": "training",
      "description": "Training step 2378",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:28",
      "total_flops_so_far": 5.650912038554278e+16,
      "budget_used_percent": 56.509120385542786
    },
    {
      "type": "training",
      "description": "Training step 2379",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:30",
      "total_flops_so_far": 5.653286771103603e+16,
      "budget_used_percent": 56.532867711036026
    },
    {
      "type": "training",
      "description": "Training step 2380",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:31",
      "total_flops_so_far": 5.655661503652928e+16,
      "budget_used_percent": 56.55661503652928
    },
    {
      "type": "training",
      "description": "Training step 2381",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:32",
      "total_flops_so_far": 5.658036236202253e+16,
      "budget_used_percent": 56.58036236202253
    },
    {
      "type": "training",
      "description": "Training step 2382",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:34",
      "total_flops_so_far": 5.660410968751578e+16,
      "budget_used_percent": 56.60410968751578
    },
    {
      "type": "training",
      "description": "Training step 2383",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:35",
      "total_flops_so_far": 5.662785701300902e+16,
      "budget_used_percent": 56.62785701300902
    },
    {
      "type": "training",
      "description": "Training step 2384",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:36",
      "total_flops_so_far": 5.665160433850227e+16,
      "budget_used_percent": 56.65160433850227
    },
    {
      "type": "training",
      "description": "Training step 2385",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:38",
      "total_flops_so_far": 5.667535166399552e+16,
      "budget_used_percent": 56.67535166399552
    },
    {
      "type": "training",
      "description": "Training step 2386",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:39",
      "total_flops_so_far": 5.669909898948877e+16,
      "budget_used_percent": 56.699098989488775
    },
    {
      "type": "training",
      "description": "Training step 2387",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:40",
      "total_flops_so_far": 5.672284631498202e+16,
      "budget_used_percent": 56.722846314982014
    },
    {
      "type": "training",
      "description": "Training step 2388",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:42",
      "total_flops_so_far": 5.674659364047526e+16,
      "budget_used_percent": 56.74659364047526
    },
    {
      "type": "training",
      "description": "Training step 2389",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:43",
      "total_flops_so_far": 5.677034096596851e+16,
      "budget_used_percent": 56.770340965968515
    },
    {
      "type": "training",
      "description": "Training step 2390",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:44",
      "total_flops_so_far": 5.679408829146176e+16,
      "budget_used_percent": 56.79408829146176
    },
    {
      "type": "training",
      "description": "Training step 2391",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:46",
      "total_flops_so_far": 5.681783561695501e+16,
      "budget_used_percent": 56.817835616955
    },
    {
      "type": "training",
      "description": "Training step 2392",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:47",
      "total_flops_so_far": 5.684158294244826e+16,
      "budget_used_percent": 56.841582942448255
    },
    {
      "type": "training",
      "description": "Training step 2393",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:48",
      "total_flops_so_far": 5.68653302679415e+16,
      "budget_used_percent": 56.8653302679415
    },
    {
      "type": "training",
      "description": "Training step 2394",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:50",
      "total_flops_so_far": 5.688907759343475e+16,
      "budget_used_percent": 56.889077593434756
    },
    {
      "type": "training",
      "description": "Training step 2395",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:51",
      "total_flops_so_far": 5.6912824918928e+16,
      "budget_used_percent": 56.912824918927996
    },
    {
      "type": "training",
      "description": "Training step 2396",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:52",
      "total_flops_so_far": 5.693657224442125e+16,
      "budget_used_percent": 56.93657224442125
    },
    {
      "type": "training",
      "description": "Training step 2397",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:54",
      "total_flops_so_far": 5.69603195699145e+16,
      "budget_used_percent": 56.960319569914496
    },
    {
      "type": "training",
      "description": "Training step 2398",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:55",
      "total_flops_so_far": 5.698406689540774e+16,
      "budget_used_percent": 56.98406689540775
    },
    {
      "type": "training",
      "description": "Training step 2399",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:56",
      "total_flops_so_far": 5.700781422090099e+16,
      "budget_used_percent": 57.00781422090099
    },
    {
      "type": "training",
      "description": "Training step 2400",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:58",
      "total_flops_so_far": 5.703156154639424e+16,
      "budget_used_percent": 57.03156154639424
    },
    {
      "type": "training",
      "description": "Training step 2401",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:36:59",
      "total_flops_so_far": 5.705530887188749e+16,
      "budget_used_percent": 57.05530887188749
    },
    {
      "type": "training",
      "description": "Training step 2402",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:00",
      "total_flops_so_far": 5.707905619738074e+16,
      "budget_used_percent": 57.07905619738074
    },
    {
      "type": "training",
      "description": "Training step 2403",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:02",
      "total_flops_so_far": 5.710280352287398e+16,
      "budget_used_percent": 57.10280352287398
    },
    {
      "type": "training",
      "description": "Training step 2404",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:03",
      "total_flops_so_far": 5.712655084836723e+16,
      "budget_used_percent": 57.12655084836723
    },
    {
      "type": "training",
      "description": "Training step 2405",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:04",
      "total_flops_so_far": 5.715029817386048e+16,
      "budget_used_percent": 57.15029817386048
    },
    {
      "type": "training",
      "description": "Training step 2406",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:06",
      "total_flops_so_far": 5.717404549935373e+16,
      "budget_used_percent": 57.17404549935373
    },
    {
      "type": "training",
      "description": "Training step 2407",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:07",
      "total_flops_so_far": 5.719779282484698e+16,
      "budget_used_percent": 57.19779282484697
    },
    {
      "type": "training",
      "description": "Training step 2408",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:08",
      "total_flops_so_far": 5.722154015034022e+16,
      "budget_used_percent": 57.221540150340225
    },
    {
      "type": "training",
      "description": "Training step 2409",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:11",
      "total_flops_so_far": 5.724528747583347e+16,
      "budget_used_percent": 57.24528747583347
    },
    {
      "type": "training",
      "description": "Training step 2410",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:12",
      "total_flops_so_far": 5.726903480132672e+16,
      "budget_used_percent": 57.269034801326725
    },
    {
      "type": "training",
      "description": "Training step 2411",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:13",
      "total_flops_so_far": 5.729278212681997e+16,
      "budget_used_percent": 57.292782126819965
    },
    {
      "type": "training",
      "description": "Training step 2412",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:15",
      "total_flops_so_far": 5.731652945231322e+16,
      "budget_used_percent": 57.31652945231321
    },
    {
      "type": "training",
      "description": "Training step 2413",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:16",
      "total_flops_so_far": 5.734027677780646e+16,
      "budget_used_percent": 57.340276777806466
    },
    {
      "type": "training",
      "description": "Training step 2414",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:17",
      "total_flops_so_far": 5.736402410329971e+16,
      "budget_used_percent": 57.36402410329971
    },
    {
      "type": "training",
      "description": "Training step 2415",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:19",
      "total_flops_so_far": 5.738777142879296e+16,
      "budget_used_percent": 57.38777142879295
    },
    {
      "type": "training",
      "description": "Training step 2416",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:20",
      "total_flops_so_far": 5.741151875428621e+16,
      "budget_used_percent": 57.411518754286206
    },
    {
      "type": "training",
      "description": "Training step 2417",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:21",
      "total_flops_so_far": 5.743526607977946e+16,
      "budget_used_percent": 57.43526607977946
    },
    {
      "type": "training",
      "description": "Training step 2418",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:23",
      "total_flops_so_far": 5.74590134052727e+16,
      "budget_used_percent": 57.45901340527271
    },
    {
      "type": "training",
      "description": "Training step 2419",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:24",
      "total_flops_so_far": 5.748276073076595e+16,
      "budget_used_percent": 57.482760730765946
    },
    {
      "type": "training",
      "description": "Training step 2420",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:25",
      "total_flops_so_far": 5.75065080562592e+16,
      "budget_used_percent": 57.5065080562592
    },
    {
      "type": "training",
      "description": "Training step 2421",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:27",
      "total_flops_so_far": 5.753025538175245e+16,
      "budget_used_percent": 57.53025538175245
    },
    {
      "type": "training",
      "description": "Training step 2422",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:28",
      "total_flops_so_far": 5.75540027072457e+16,
      "budget_used_percent": 57.5540027072457
    },
    {
      "type": "training",
      "description": "Training step 2423",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:29",
      "total_flops_so_far": 5.757775003273894e+16,
      "budget_used_percent": 57.57775003273894
    },
    {
      "type": "training",
      "description": "Training step 2424",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:31",
      "total_flops_so_far": 5.760149735823219e+16,
      "budget_used_percent": 57.60149735823219
    },
    {
      "type": "training",
      "description": "Training step 2425",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:32",
      "total_flops_so_far": 5.762524468372544e+16,
      "budget_used_percent": 57.62524468372544
    },
    {
      "type": "training",
      "description": "Training step 2426",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:33",
      "total_flops_so_far": 5.764899200921869e+16,
      "budget_used_percent": 57.64899200921869
    },
    {
      "type": "training",
      "description": "Training step 2427",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:35",
      "total_flops_so_far": 5.767273933471194e+16,
      "budget_used_percent": 57.672739334711935
    },
    {
      "type": "training",
      "description": "Training step 2428",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:36",
      "total_flops_so_far": 5.769648666020518e+16,
      "budget_used_percent": 57.69648666020518
    },
    {
      "type": "training",
      "description": "Training step 2429",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:37",
      "total_flops_so_far": 5.772023398569843e+16,
      "budget_used_percent": 57.720233985698435
    },
    {
      "type": "training",
      "description": "Training step 2430",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:39",
      "total_flops_so_far": 5.774398131119168e+16,
      "budget_used_percent": 57.74398131119168
    },
    {
      "type": "training",
      "description": "Training step 2431",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:40",
      "total_flops_so_far": 5.776772863668493e+16,
      "budget_used_percent": 57.76772863668492
    },
    {
      "type": "training",
      "description": "Training step 2432",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:41",
      "total_flops_so_far": 5.779147596217818e+16,
      "budget_used_percent": 57.791475962178176
    },
    {
      "type": "training",
      "description": "Training step 2433",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:43",
      "total_flops_so_far": 5.781522328767142e+16,
      "budget_used_percent": 57.81522328767142
    },
    {
      "type": "training",
      "description": "Training step 2434",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:44",
      "total_flops_so_far": 5.783897061316467e+16,
      "budget_used_percent": 57.838970613164676
    },
    {
      "type": "training",
      "description": "Training step 2435",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:45",
      "total_flops_so_far": 5.786271793865792e+16,
      "budget_used_percent": 57.862717938657916
    },
    {
      "type": "training",
      "description": "Training step 2436",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:47",
      "total_flops_so_far": 5.788646526415117e+16,
      "budget_used_percent": 57.88646526415116
    },
    {
      "type": "training",
      "description": "Training step 2437",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:48",
      "total_flops_so_far": 5.791021258964442e+16,
      "budget_used_percent": 57.910212589644416
    },
    {
      "type": "training",
      "description": "Training step 2438",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:49",
      "total_flops_so_far": 5.793395991513766e+16,
      "budget_used_percent": 57.93395991513767
    },
    {
      "type": "training",
      "description": "Training step 2439",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:51",
      "total_flops_so_far": 5.795770724063091e+16,
      "budget_used_percent": 57.95770724063091
    },
    {
      "type": "training",
      "description": "Training step 2440",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:52",
      "total_flops_so_far": 5.798145456612416e+16,
      "budget_used_percent": 57.98145456612416
    },
    {
      "type": "training",
      "description": "Training step 2441",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:53",
      "total_flops_so_far": 5.800520189161741e+16,
      "budget_used_percent": 58.00520189161741
    },
    {
      "type": "training",
      "description": "Training step 2442",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:55",
      "total_flops_so_far": 5.802894921711066e+16,
      "budget_used_percent": 58.02894921711066
    },
    {
      "type": "training",
      "description": "Training step 2443",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:56",
      "total_flops_so_far": 5.80526965426039e+16,
      "budget_used_percent": 58.0526965426039
    },
    {
      "type": "training",
      "description": "Training step 2444",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:57",
      "total_flops_so_far": 5.807644386809715e+16,
      "budget_used_percent": 58.07644386809715
    },
    {
      "type": "training",
      "description": "Training step 2445",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:37:59",
      "total_flops_so_far": 5.81001911935904e+16,
      "budget_used_percent": 58.1001911935904
    },
    {
      "type": "training",
      "description": "Training step 2446",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:00",
      "total_flops_so_far": 5.812393851908365e+16,
      "budget_used_percent": 58.12393851908365
    },
    {
      "type": "training",
      "description": "Training step 2447",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:01",
      "total_flops_so_far": 5.81476858445769e+16,
      "budget_used_percent": 58.14768584457689
    },
    {
      "type": "training",
      "description": "Training step 2448",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:03",
      "total_flops_so_far": 5.817143317007014e+16,
      "budget_used_percent": 58.171433170070145
    },
    {
      "type": "training",
      "description": "Training step 2449",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:04",
      "total_flops_so_far": 5.819518049556339e+16,
      "budget_used_percent": 58.19518049556339
    },
    {
      "type": "training",
      "description": "Training step 2450",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:05",
      "total_flops_so_far": 5.821892782105664e+16,
      "budget_used_percent": 58.218927821056646
    },
    {
      "type": "training",
      "description": "Training step 2451",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:07",
      "total_flops_so_far": 5.824267514654989e+16,
      "budget_used_percent": 58.242675146549885
    },
    {
      "type": "training",
      "description": "Training step 2452",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:08",
      "total_flops_so_far": 5.826642247204314e+16,
      "budget_used_percent": 58.26642247204313
    },
    {
      "type": "training",
      "description": "Training step 2453",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:09",
      "total_flops_so_far": 5.829016979753638e+16,
      "budget_used_percent": 58.290169797536386
    },
    {
      "type": "training",
      "description": "Training step 2454",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:11",
      "total_flops_so_far": 5.831391712302963e+16,
      "budget_used_percent": 58.31391712302963
    },
    {
      "type": "training",
      "description": "Training step 2455",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:12",
      "total_flops_so_far": 5.833766444852288e+16,
      "budget_used_percent": 58.33766444852287
    },
    {
      "type": "training",
      "description": "Training step 2456",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:13",
      "total_flops_so_far": 5.836141177401613e+16,
      "budget_used_percent": 58.361411774016126
    },
    {
      "type": "training",
      "description": "Training step 2457",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:15",
      "total_flops_so_far": 5.838515909950938e+16,
      "budget_used_percent": 58.38515909950937
    },
    {
      "type": "training",
      "description": "Training step 2458",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:16",
      "total_flops_so_far": 5.840890642500262e+16,
      "budget_used_percent": 58.40890642500263
    },
    {
      "type": "training",
      "description": "Training step 2459",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:17",
      "total_flops_so_far": 5.843265375049587e+16,
      "budget_used_percent": 58.43265375049587
    },
    {
      "type": "training",
      "description": "Training step 2460",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:19",
      "total_flops_so_far": 5.845640107598912e+16,
      "budget_used_percent": 58.45640107598912
    },
    {
      "type": "training",
      "description": "Training step 2461",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:20",
      "total_flops_so_far": 5.848014840148237e+16,
      "budget_used_percent": 58.48014840148237
    },
    {
      "type": "training",
      "description": "Training step 2462",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:21",
      "total_flops_so_far": 5.850389572697562e+16,
      "budget_used_percent": 58.50389572697562
    },
    {
      "type": "training",
      "description": "Training step 2463",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:23",
      "total_flops_so_far": 5.852764305246886e+16,
      "budget_used_percent": 58.52764305246886
    },
    {
      "type": "training",
      "description": "Training step 2464",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:24",
      "total_flops_so_far": 5.855139037796211e+16,
      "budget_used_percent": 58.55139037796211
    },
    {
      "type": "training",
      "description": "Training step 2465",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:25",
      "total_flops_so_far": 5.857513770345536e+16,
      "budget_used_percent": 58.57513770345536
    },
    {
      "type": "training",
      "description": "Training step 2466",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:27",
      "total_flops_so_far": 5.859888502894861e+16,
      "budget_used_percent": 58.59888502894861
    },
    {
      "type": "training",
      "description": "Training step 2467",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:28",
      "total_flops_so_far": 5.862263235444186e+16,
      "budget_used_percent": 58.62263235444186
    },
    {
      "type": "training",
      "description": "Training step 2468",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:29",
      "total_flops_so_far": 5.86463796799351e+16,
      "budget_used_percent": 58.6463796799351
    },
    {
      "type": "training",
      "description": "Training step 2469",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:31",
      "total_flops_so_far": 5.867012700542835e+16,
      "budget_used_percent": 58.670127005428355
    },
    {
      "type": "training",
      "description": "Training step 2470",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:32",
      "total_flops_so_far": 5.86938743309216e+16,
      "budget_used_percent": 58.6938743309216
    },
    {
      "type": "training",
      "description": "Training step 2471",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:33",
      "total_flops_so_far": 5.871762165641485e+16,
      "budget_used_percent": 58.717621656414856
    },
    {
      "type": "training",
      "description": "Training step 2472",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:35",
      "total_flops_so_far": 5.87413689819081e+16,
      "budget_used_percent": 58.741368981908096
    },
    {
      "type": "training",
      "description": "Training step 2473",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:36",
      "total_flops_so_far": 5.876511630740134e+16,
      "budget_used_percent": 58.76511630740134
    },
    {
      "type": "training",
      "description": "Training step 2474",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:37",
      "total_flops_so_far": 5.878886363289459e+16,
      "budget_used_percent": 58.788863632894596
    },
    {
      "type": "training",
      "description": "Training step 2475",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:39",
      "total_flops_so_far": 5.881261095838784e+16,
      "budget_used_percent": 58.81261095838784
    },
    {
      "type": "training",
      "description": "Training step 2476",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:40",
      "total_flops_so_far": 5.883635828388109e+16,
      "budget_used_percent": 58.83635828388108
    },
    {
      "type": "training",
      "description": "Training step 2477",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:41",
      "total_flops_so_far": 5.886010560937434e+16,
      "budget_used_percent": 58.86010560937434
    },
    {
      "type": "training",
      "description": "Training step 2478",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:43",
      "total_flops_so_far": 5.888385293486758e+16,
      "budget_used_percent": 58.88385293486758
    },
    {
      "type": "training",
      "description": "Training step 2479",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:44",
      "total_flops_so_far": 5.890760026036083e+16,
      "budget_used_percent": 58.90760026036084
    },
    {
      "type": "training",
      "description": "Training step 2480",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:45",
      "total_flops_so_far": 5.893134758585408e+16,
      "budget_used_percent": 58.93134758585408
    },
    {
      "type": "training",
      "description": "Training step 2481",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:47",
      "total_flops_so_far": 5.895509491134733e+16,
      "budget_used_percent": 58.95509491134733
    },
    {
      "type": "training",
      "description": "Training step 2482",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:48",
      "total_flops_so_far": 5.897884223684058e+16,
      "budget_used_percent": 58.97884223684058
    },
    {
      "type": "training",
      "description": "Training step 2483",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:49",
      "total_flops_so_far": 5.900258956233382e+16,
      "budget_used_percent": 59.00258956233383
    },
    {
      "type": "training",
      "description": "Training step 2484",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:51",
      "total_flops_so_far": 5.902633688782707e+16,
      "budget_used_percent": 59.02633688782707
    },
    {
      "type": "training",
      "description": "Training step 2485",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:52",
      "total_flops_so_far": 5.905008421332032e+16,
      "budget_used_percent": 59.05008421332032
    },
    {
      "type": "training",
      "description": "Training step 2486",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:53",
      "total_flops_so_far": 5.907383153881357e+16,
      "budget_used_percent": 59.07383153881357
    },
    {
      "type": "training",
      "description": "Training step 2487",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:55",
      "total_flops_so_far": 5.909757886430682e+16,
      "budget_used_percent": 59.09757886430682
    },
    {
      "type": "training",
      "description": "Training step 2488",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:56",
      "total_flops_so_far": 5.912132618980006e+16,
      "budget_used_percent": 59.12132618980006
    },
    {
      "type": "training",
      "description": "Training step 2489",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:57",
      "total_flops_so_far": 5.914507351529331e+16,
      "budget_used_percent": 59.14507351529331
    },
    {
      "type": "training",
      "description": "Training step 2490",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:38:59",
      "total_flops_so_far": 5.916882084078656e+16,
      "budget_used_percent": 59.168820840786566
    },
    {
      "type": "training",
      "description": "Training step 2491",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:00",
      "total_flops_so_far": 5.919256816627981e+16,
      "budget_used_percent": 59.19256816627981
    },
    {
      "type": "training",
      "description": "Training step 2492",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:01",
      "total_flops_so_far": 5.921631549177306e+16,
      "budget_used_percent": 59.21631549177305
    },
    {
      "type": "training",
      "description": "Training step 2493",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:03",
      "total_flops_so_far": 5.92400628172663e+16,
      "budget_used_percent": 59.240062817266306
    },
    {
      "type": "training",
      "description": "Training step 2494",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:04",
      "total_flops_so_far": 5.926381014275955e+16,
      "budget_used_percent": 59.26381014275955
    },
    {
      "type": "training",
      "description": "Training step 2495",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:05",
      "total_flops_so_far": 5.92875574682528e+16,
      "budget_used_percent": 59.28755746825281
    },
    {
      "type": "training",
      "description": "Training step 2496",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:07",
      "total_flops_so_far": 5.931130479374605e+16,
      "budget_used_percent": 59.31130479374605
    },
    {
      "type": "training",
      "description": "Training step 2497",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:08",
      "total_flops_so_far": 5.93350521192393e+16,
      "budget_used_percent": 59.33505211923929
    },
    {
      "type": "training",
      "description": "Training step 2498",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:09",
      "total_flops_so_far": 5.935879944473254e+16,
      "budget_used_percent": 59.35879944473255
    },
    {
      "type": "training",
      "description": "Training step 2499",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:11",
      "total_flops_so_far": 5.938254677022579e+16,
      "budget_used_percent": 59.382546770225794
    },
    {
      "type": "training",
      "description": "Training step 2500",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:12",
      "total_flops_so_far": 5.940629409571904e+16,
      "budget_used_percent": 59.40629409571903
    },
    {
      "type": "training",
      "description": "Training step 2501",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:13",
      "total_flops_so_far": 5.943004142121229e+16,
      "budget_used_percent": 59.43004142121229
    },
    {
      "type": "training",
      "description": "Training step 2502",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:15",
      "total_flops_so_far": 5.945378874670554e+16,
      "budget_used_percent": 59.45378874670554
    },
    {
      "type": "training",
      "description": "Training step 2503",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:16",
      "total_flops_so_far": 5.947753607219878e+16,
      "budget_used_percent": 59.47753607219879
    },
    {
      "type": "training",
      "description": "Training step 2504",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:17",
      "total_flops_so_far": 5.950128339769203e+16,
      "budget_used_percent": 59.50128339769203
    },
    {
      "type": "training",
      "description": "Training step 2505",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:19",
      "total_flops_so_far": 5.952503072318528e+16,
      "budget_used_percent": 59.52503072318528
    },
    {
      "type": "training",
      "description": "Training step 2506",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:20",
      "total_flops_so_far": 5.954877804867853e+16,
      "budget_used_percent": 59.54877804867853
    },
    {
      "type": "training",
      "description": "Training step 2507",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:21",
      "total_flops_so_far": 5.957252537417178e+16,
      "budget_used_percent": 59.57252537417178
    },
    {
      "type": "training",
      "description": "Training step 2508",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:22",
      "total_flops_so_far": 5.959627269966502e+16,
      "budget_used_percent": 59.59627269966502
    },
    {
      "type": "training",
      "description": "Training step 2509",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:23",
      "total_flops_so_far": 5.962002002515827e+16,
      "budget_used_percent": 59.62002002515827
    },
    {
      "type": "training",
      "description": "Training step 2510",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:25",
      "total_flops_so_far": 5.964376735065152e+16,
      "budget_used_percent": 59.64376735065152
    },
    {
      "type": "training",
      "description": "Training step 2511",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:26",
      "total_flops_so_far": 5.966751467614477e+16,
      "budget_used_percent": 59.667514676144776
    },
    {
      "type": "training",
      "description": "Training step 2512",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:27",
      "total_flops_so_far": 5.969126200163802e+16,
      "budget_used_percent": 59.69126200163801
    },
    {
      "type": "training",
      "description": "Training step 2513",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:29",
      "total_flops_so_far": 5.971500932713126e+16,
      "budget_used_percent": 59.71500932713126
    },
    {
      "type": "training",
      "description": "Training step 2514",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:30",
      "total_flops_so_far": 5.973875665262451e+16,
      "budget_used_percent": 59.73875665262452
    },
    {
      "type": "training",
      "description": "Training step 2515",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:31",
      "total_flops_so_far": 5.976250397811776e+16,
      "budget_used_percent": 59.76250397811776
    },
    {
      "type": "training",
      "description": "Training step 2516",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:33",
      "total_flops_so_far": 5.978625130361101e+16,
      "budget_used_percent": 59.786251303611
    },
    {
      "type": "training",
      "description": "Training step 2517",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:34",
      "total_flops_so_far": 5.980999862910426e+16,
      "budget_used_percent": 59.80999862910426
    },
    {
      "type": "training",
      "description": "Training step 2518",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:35",
      "total_flops_so_far": 5.98337459545975e+16,
      "budget_used_percent": 59.833745954597504
    },
    {
      "type": "training",
      "description": "Training step 2519",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:37",
      "total_flops_so_far": 5.985749328009075e+16,
      "budget_used_percent": 59.85749328009076
    },
    {
      "type": "training",
      "description": "Training step 2520",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:38",
      "total_flops_so_far": 5.9881240605584e+16,
      "budget_used_percent": 59.881240605584
    },
    {
      "type": "training",
      "description": "Training step 2521",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:39",
      "total_flops_so_far": 5.990498793107725e+16,
      "budget_used_percent": 59.904987931077244
    },
    {
      "type": "training",
      "description": "Training step 2522",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:41",
      "total_flops_so_far": 5.99287352565705e+16,
      "budget_used_percent": 59.9287352565705
    },
    {
      "type": "training",
      "description": "Training step 2523",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:42",
      "total_flops_so_far": 5.995248258206374e+16,
      "budget_used_percent": 59.95248258206375
    },
    {
      "type": "training",
      "description": "Training step 2524",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:43",
      "total_flops_so_far": 5.997622990755699e+16,
      "budget_used_percent": 59.97622990755699
    },
    {
      "type": "training",
      "description": "Training step 2525",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:45",
      "total_flops_so_far": 5.999997723305024e+16,
      "budget_used_percent": 59.99997723305024
    },
    {
      "type": "training",
      "description": "Training step 2526",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:46",
      "total_flops_so_far": 6.002372455854349e+16,
      "budget_used_percent": 60.02372455854349
    },
    {
      "type": "training",
      "description": "Training step 2527",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:47",
      "total_flops_so_far": 6.004747188403674e+16,
      "budget_used_percent": 60.04747188403674
    },
    {
      "type": "training",
      "description": "Training step 2528",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:49",
      "total_flops_so_far": 6.007121920952998e+16,
      "budget_used_percent": 60.07121920952998
    },
    {
      "type": "training",
      "description": "Training step 2529",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:50",
      "total_flops_so_far": 6.009496653502323e+16,
      "budget_used_percent": 60.09496653502323
    },
    {
      "type": "training",
      "description": "Training step 2530",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:51",
      "total_flops_so_far": 6.011871386051648e+16,
      "budget_used_percent": 60.11871386051648
    },
    {
      "type": "training",
      "description": "Training step 2531",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:53",
      "total_flops_so_far": 6.014246118600973e+16,
      "budget_used_percent": 60.14246118600973
    },
    {
      "type": "training",
      "description": "Training step 2532",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:54",
      "total_flops_so_far": 6.016620851150298e+16,
      "budget_used_percent": 60.16620851150297
    },
    {
      "type": "training",
      "description": "Training step 2533",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:55",
      "total_flops_so_far": 6.018995583699622e+16,
      "budget_used_percent": 60.18995583699622
    },
    {
      "type": "training",
      "description": "Training step 2534",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:57",
      "total_flops_so_far": 6.021370316248947e+16,
      "budget_used_percent": 60.21370316248947
    },
    {
      "type": "training",
      "description": "Training step 2535",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:58",
      "total_flops_so_far": 6.023745048798272e+16,
      "budget_used_percent": 60.23745048798273
    },
    {
      "type": "training",
      "description": "Training step 2536",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:39:59",
      "total_flops_so_far": 6.026119781347597e+16,
      "budget_used_percent": 60.26119781347597
    },
    {
      "type": "training",
      "description": "Training step 2537",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:01",
      "total_flops_so_far": 6.028494513896922e+16,
      "budget_used_percent": 60.28494513896921
    },
    {
      "type": "training",
      "description": "Training step 2538",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:02",
      "total_flops_so_far": 6.030869246446246e+16,
      "budget_used_percent": 60.30869246446247
    },
    {
      "type": "training",
      "description": "Training step 2539",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:03",
      "total_flops_so_far": 6.033243978995571e+16,
      "budget_used_percent": 60.332439789955714
    },
    {
      "type": "training",
      "description": "Training step 2540",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:05",
      "total_flops_so_far": 6.035618711544896e+16,
      "budget_used_percent": 60.356187115448954
    },
    {
      "type": "training",
      "description": "Training step 2541",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:06",
      "total_flops_so_far": 6.037993444094221e+16,
      "budget_used_percent": 60.37993444094221
    },
    {
      "type": "training",
      "description": "Training step 2542",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:09",
      "total_flops_so_far": 6.040368176643546e+16,
      "budget_used_percent": 60.403681766435454
    },
    {
      "type": "training",
      "description": "Training step 2543",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:10",
      "total_flops_so_far": 6.04274290919287e+16,
      "budget_used_percent": 60.42742909192871
    },
    {
      "type": "training",
      "description": "Training step 2544",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:12",
      "total_flops_so_far": 6.045117641742195e+16,
      "budget_used_percent": 60.45117641742195
    },
    {
      "type": "training",
      "description": "Training step 2545",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:13",
      "total_flops_so_far": 6.04749237429152e+16,
      "budget_used_percent": 60.4749237429152
    },
    {
      "type": "training",
      "description": "Training step 2546",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:14",
      "total_flops_so_far": 6.049867106840845e+16,
      "budget_used_percent": 60.49867106840845
    },
    {
      "type": "training",
      "description": "Training step 2547",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:16",
      "total_flops_so_far": 6.05224183939017e+16,
      "budget_used_percent": 60.5224183939017
    },
    {
      "type": "training",
      "description": "Training step 2548",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:17",
      "total_flops_so_far": 6.054616571939494e+16,
      "budget_used_percent": 60.54616571939494
    },
    {
      "type": "training",
      "description": "Training step 2549",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:18",
      "total_flops_so_far": 6.056991304488819e+16,
      "budget_used_percent": 60.56991304488819
    },
    {
      "type": "training",
      "description": "Training step 2550",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:20",
      "total_flops_so_far": 6.059366037038144e+16,
      "budget_used_percent": 60.59366037038144
    },
    {
      "type": "training",
      "description": "Training step 2551",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:21",
      "total_flops_so_far": 6.061740769587469e+16,
      "budget_used_percent": 60.61740769587469
    },
    {
      "type": "training",
      "description": "Training step 2552",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:22",
      "total_flops_so_far": 6.064115502136794e+16,
      "budget_used_percent": 60.64115502136793
    },
    {
      "type": "training",
      "description": "Training step 2553",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:24",
      "total_flops_so_far": 6.066490234686118e+16,
      "budget_used_percent": 60.66490234686118
    },
    {
      "type": "training",
      "description": "Training step 2554",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:25",
      "total_flops_so_far": 6.068864967235443e+16,
      "budget_used_percent": 60.68864967235443
    },
    {
      "type": "training",
      "description": "Training step 2555",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:26",
      "total_flops_so_far": 6.071239699784768e+16,
      "budget_used_percent": 60.712396997847684
    },
    {
      "type": "training",
      "description": "Training step 2556",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:28",
      "total_flops_so_far": 6.073614432334093e+16,
      "budget_used_percent": 60.73614432334092
    },
    {
      "type": "training",
      "description": "Training step 2557",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:29",
      "total_flops_so_far": 6.075989164883418e+16,
      "budget_used_percent": 60.75989164883418
    },
    {
      "type": "training",
      "description": "Training step 2558",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:30",
      "total_flops_so_far": 6.078363897432742e+16,
      "budget_used_percent": 60.783638974327424
    },
    {
      "type": "training",
      "description": "Training step 2559",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:32",
      "total_flops_so_far": 6.080738629982067e+16,
      "budget_used_percent": 60.80738629982068
    },
    {
      "type": "training",
      "description": "Training step 2560",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:33",
      "total_flops_so_far": 6.083113362531392e+16,
      "budget_used_percent": 60.83113362531392
    },
    {
      "type": "training",
      "description": "Training step 2561",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:34",
      "total_flops_so_far": 6.085488095080717e+16,
      "budget_used_percent": 60.854880950807164
    },
    {
      "type": "training",
      "description": "Training step 2562",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:36",
      "total_flops_so_far": 6.087862827630042e+16,
      "budget_used_percent": 60.87862827630042
    },
    {
      "type": "training",
      "description": "Training step 2563",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:37",
      "total_flops_so_far": 6.090237560179366e+16,
      "budget_used_percent": 60.902375601793665
    },
    {
      "type": "training",
      "description": "Training step 2564",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:38",
      "total_flops_so_far": 6.092612292728691e+16,
      "budget_used_percent": 60.926122927286904
    },
    {
      "type": "training",
      "description": "Training step 2565",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:40",
      "total_flops_so_far": 6.094987025278016e+16,
      "budget_used_percent": 60.94987025278016
    },
    {
      "type": "training",
      "description": "Training step 2566",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:41",
      "total_flops_so_far": 6.097361757827341e+16,
      "budget_used_percent": 60.97361757827341
    },
    {
      "type": "training",
      "description": "Training step 2567",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:42",
      "total_flops_so_far": 6.099736490376666e+16,
      "budget_used_percent": 60.99736490376666
    },
    {
      "type": "training",
      "description": "Training step 2568",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:44",
      "total_flops_so_far": 6.10211122292599e+16,
      "budget_used_percent": 61.0211122292599
    },
    {
      "type": "training",
      "description": "Training step 2569",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:45",
      "total_flops_so_far": 6.104485955475315e+16,
      "budget_used_percent": 61.04485955475315
    },
    {
      "type": "training",
      "description": "Training step 2570",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:46",
      "total_flops_so_far": 6.10686068802464e+16,
      "budget_used_percent": 61.0686068802464
    },
    {
      "type": "training",
      "description": "Training step 2571",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:48",
      "total_flops_so_far": 6.109235420573965e+16,
      "budget_used_percent": 61.09235420573965
    },
    {
      "type": "training",
      "description": "Training step 2572",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:49",
      "total_flops_so_far": 6.11161015312329e+16,
      "budget_used_percent": 61.11610153123289
    },
    {
      "type": "training",
      "description": "Training step 2573",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:50",
      "total_flops_so_far": 6.113984885672614e+16,
      "budget_used_percent": 61.13984885672614
    },
    {
      "type": "training",
      "description": "Training step 2574",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:52",
      "total_flops_so_far": 6.116359618221939e+16,
      "budget_used_percent": 61.16359618221939
    },
    {
      "type": "training",
      "description": "Training step 2575",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:53",
      "total_flops_so_far": 6.118734350771264e+16,
      "budget_used_percent": 61.18734350771264
    },
    {
      "type": "training",
      "description": "Training step 2576",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:54",
      "total_flops_so_far": 6.121109083320589e+16,
      "budget_used_percent": 61.21109083320589
    },
    {
      "type": "training",
      "description": "Training step 2577",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:56",
      "total_flops_so_far": 6.123483815869914e+16,
      "budget_used_percent": 61.234838158699134
    },
    {
      "type": "training",
      "description": "Training step 2578",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:57",
      "total_flops_so_far": 6.125858548419238e+16,
      "budget_used_percent": 61.25858548419239
    },
    {
      "type": "training",
      "description": "Training step 2579",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:40:58",
      "total_flops_so_far": 6.128233280968563e+16,
      "budget_used_percent": 61.282332809685634
    },
    {
      "type": "training",
      "description": "Training step 2580",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:00",
      "total_flops_so_far": 6.130608013517888e+16,
      "budget_used_percent": 61.306080135178874
    },
    {
      "type": "training",
      "description": "Training step 2581",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:01",
      "total_flops_so_far": 6.132982746067213e+16,
      "budget_used_percent": 61.32982746067213
    },
    {
      "type": "training",
      "description": "Training step 2582",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:03",
      "total_flops_so_far": 6.135357478616538e+16,
      "budget_used_percent": 61.353574786165375
    },
    {
      "type": "training",
      "description": "Training step 2583",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:04",
      "total_flops_so_far": 6.137732211165862e+16,
      "budget_used_percent": 61.37732211165863
    },
    {
      "type": "training",
      "description": "Training step 2584",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:05",
      "total_flops_so_far": 6.140106943715187e+16,
      "budget_used_percent": 61.40106943715187
    },
    {
      "type": "training",
      "description": "Training step 2585",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:07",
      "total_flops_so_far": 6.142481676264512e+16,
      "budget_used_percent": 61.424816762645115
    },
    {
      "type": "training",
      "description": "Training step 2586",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:08",
      "total_flops_so_far": 6.144856408813837e+16,
      "budget_used_percent": 61.44856408813837
    },
    {
      "type": "training",
      "description": "Training step 2587",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:09",
      "total_flops_so_far": 6.147231141363162e+16,
      "budget_used_percent": 61.47231141363162
    },
    {
      "type": "training",
      "description": "Training step 2588",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:11",
      "total_flops_so_far": 6.149605873912486e+16,
      "budget_used_percent": 61.49605873912486
    },
    {
      "type": "training",
      "description": "Training step 2589",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:12",
      "total_flops_so_far": 6.151980606461811e+16,
      "budget_used_percent": 61.51980606461811
    },
    {
      "type": "training",
      "description": "Training step 2590",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:13",
      "total_flops_so_far": 6.154355339011136e+16,
      "budget_used_percent": 61.54355339011136
    },
    {
      "type": "training",
      "description": "Training step 2591",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:15",
      "total_flops_so_far": 6.156730071560461e+16,
      "budget_used_percent": 61.56730071560461
    },
    {
      "type": "training",
      "description": "Training step 2592",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:16",
      "total_flops_so_far": 6.159104804109786e+16,
      "budget_used_percent": 61.59104804109785
    },
    {
      "type": "training",
      "description": "Training step 2593",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:17",
      "total_flops_so_far": 6.16147953665911e+16,
      "budget_used_percent": 61.6147953665911
    },
    {
      "type": "training",
      "description": "Training step 2594",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:19",
      "total_flops_so_far": 6.163854269208435e+16,
      "budget_used_percent": 61.63854269208435
    },
    {
      "type": "training",
      "description": "Training step 2595",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:20",
      "total_flops_so_far": 6.16622900175776e+16,
      "budget_used_percent": 61.662290017577604
    },
    {
      "type": "training",
      "description": "Training step 2596",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:21",
      "total_flops_so_far": 6.168603734307085e+16,
      "budget_used_percent": 61.68603734307084
    },
    {
      "type": "training",
      "description": "Training step 2597",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:23",
      "total_flops_so_far": 6.17097846685641e+16,
      "budget_used_percent": 61.7097846685641
    },
    {
      "type": "training",
      "description": "Training step 2598",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:24",
      "total_flops_so_far": 6.173353199405734e+16,
      "budget_used_percent": 61.733531994057344
    },
    {
      "type": "training",
      "description": "Training step 2599",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:25",
      "total_flops_so_far": 6.175727931955059e+16,
      "budget_used_percent": 61.7572793195506
    },
    {
      "type": "training",
      "description": "Training step 2600",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:27",
      "total_flops_so_far": 6.178102664504384e+16,
      "budget_used_percent": 61.78102664504384
    },
    {
      "type": "training",
      "description": "Training step 2601",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:28",
      "total_flops_so_far": 6.180477397053709e+16,
      "budget_used_percent": 61.804773970537084
    },
    {
      "type": "training",
      "description": "Training step 2602",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:29",
      "total_flops_so_far": 6.182852129603034e+16,
      "budget_used_percent": 61.82852129603034
    },
    {
      "type": "training",
      "description": "Training step 2603",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:31",
      "total_flops_so_far": 6.185226862152358e+16,
      "budget_used_percent": 61.852268621523585
    },
    {
      "type": "training",
      "description": "Training step 2604",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:32",
      "total_flops_so_far": 6.187601594701683e+16,
      "budget_used_percent": 61.876015947016825
    },
    {
      "type": "training",
      "description": "Training step 2605",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:33",
      "total_flops_so_far": 6.189976327251008e+16,
      "budget_used_percent": 61.89976327251008
    },
    {
      "type": "training",
      "description": "Training step 2606",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:35",
      "total_flops_so_far": 6.192351059800333e+16,
      "budget_used_percent": 61.923510598003325
    },
    {
      "type": "training",
      "description": "Training step 2607",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:36",
      "total_flops_so_far": 6.194725792349658e+16,
      "budget_used_percent": 61.94725792349658
    },
    {
      "type": "training",
      "description": "Training step 2608",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:37",
      "total_flops_so_far": 6.197100524898982e+16,
      "budget_used_percent": 61.97100524898982
    },
    {
      "type": "training",
      "description": "Training step 2609",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:39",
      "total_flops_so_far": 6.199475257448307e+16,
      "budget_used_percent": 61.99475257448307
    },
    {
      "type": "training",
      "description": "Training step 2610",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:40",
      "total_flops_so_far": 6.201849989997632e+16,
      "budget_used_percent": 62.01849989997632
    },
    {
      "type": "training",
      "description": "Training step 2611",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:41",
      "total_flops_so_far": 6.204224722546957e+16,
      "budget_used_percent": 62.04224722546957
    },
    {
      "type": "training",
      "description": "Training step 2612",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:43",
      "total_flops_so_far": 6.206599455096282e+16,
      "budget_used_percent": 62.06599455096281
    },
    {
      "type": "training",
      "description": "Training step 2613",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:44",
      "total_flops_so_far": 6.208974187645606e+16,
      "budget_used_percent": 62.08974187645606
    },
    {
      "type": "training",
      "description": "Training step 2614",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:45",
      "total_flops_so_far": 6.211348920194931e+16,
      "budget_used_percent": 62.113489201949314
    },
    {
      "type": "training",
      "description": "Training step 2615",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:47",
      "total_flops_so_far": 6.213723652744256e+16,
      "budget_used_percent": 62.13723652744256
    },
    {
      "type": "training",
      "description": "Training step 2616",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:48",
      "total_flops_so_far": 6.216098385293581e+16,
      "budget_used_percent": 62.1609838529358
    },
    {
      "type": "training",
      "description": "Training step 2617",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:49",
      "total_flops_so_far": 6.218473117842906e+16,
      "budget_used_percent": 62.184731178429054
    },
    {
      "type": "training",
      "description": "Training step 2618",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:51",
      "total_flops_so_far": 6.22084785039223e+16,
      "budget_used_percent": 62.20847850392231
    },
    {
      "type": "training",
      "description": "Training step 2619",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:52",
      "total_flops_so_far": 6.223222582941555e+16,
      "budget_used_percent": 62.232225829415555
    },
    {
      "type": "training",
      "description": "Training step 2620",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:53",
      "total_flops_so_far": 6.22559731549088e+16,
      "budget_used_percent": 62.255973154908794
    },
    {
      "type": "training",
      "description": "Training step 2621",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:55",
      "total_flops_so_far": 6.227972048040205e+16,
      "budget_used_percent": 62.27972048040205
    },
    {
      "type": "training",
      "description": "Training step 2622",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:56",
      "total_flops_so_far": 6.23034678058953e+16,
      "budget_used_percent": 62.303467805895295
    },
    {
      "type": "training",
      "description": "Training step 2623",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:57",
      "total_flops_so_far": 6.232721513138854e+16,
      "budget_used_percent": 62.32721513138855
    },
    {
      "type": "training",
      "description": "Training step 2624",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:41:59",
      "total_flops_so_far": 6.235096245688179e+16,
      "budget_used_percent": 62.35096245688179
    },
    {
      "type": "training",
      "description": "Training step 2625",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:00",
      "total_flops_so_far": 6.237470978237504e+16,
      "budget_used_percent": 62.374709782375035
    },
    {
      "type": "training",
      "description": "Training step 2626",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:02",
      "total_flops_so_far": 6.239845710786829e+16,
      "budget_used_percent": 62.39845710786829
    },
    {
      "type": "training",
      "description": "Training step 2627",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:03",
      "total_flops_so_far": 6.242220443336154e+16,
      "budget_used_percent": 62.422204433361536
    },
    {
      "type": "training",
      "description": "Training step 2628",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:04",
      "total_flops_so_far": 6.244595175885478e+16,
      "budget_used_percent": 62.445951758854775
    },
    {
      "type": "training",
      "description": "Training step 2629",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:06",
      "total_flops_so_far": 6.246969908434803e+16,
      "budget_used_percent": 62.46969908434803
    },
    {
      "type": "training",
      "description": "Training step 2630",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:07",
      "total_flops_so_far": 6.249344640984128e+16,
      "budget_used_percent": 62.49344640984128
    },
    {
      "type": "training",
      "description": "Training step 2631",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:08",
      "total_flops_so_far": 6.251719373533453e+16,
      "budget_used_percent": 62.51719373533453
    },
    {
      "type": "training",
      "description": "Training step 2632",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:10",
      "total_flops_so_far": 6.254094106082778e+16,
      "budget_used_percent": 62.54094106082777
    },
    {
      "type": "training",
      "description": "Training step 2633",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:11",
      "total_flops_so_far": 6.256468838632102e+16,
      "budget_used_percent": 62.56468838632102
    },
    {
      "type": "training",
      "description": "Training step 2634",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:12",
      "total_flops_so_far": 6.258843571181427e+16,
      "budget_used_percent": 62.58843571181427
    },
    {
      "type": "training",
      "description": "Training step 2635",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:14",
      "total_flops_so_far": 6.261218303730752e+16,
      "budget_used_percent": 62.612183037307524
    },
    {
      "type": "training",
      "description": "Training step 2636",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:15",
      "total_flops_so_far": 6.263593036280077e+16,
      "budget_used_percent": 62.63593036280077
    },
    {
      "type": "training",
      "description": "Training step 2637",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:16",
      "total_flops_so_far": 6.265967768829402e+16,
      "budget_used_percent": 62.65967768829401
    },
    {
      "type": "training",
      "description": "Training step 2638",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:18",
      "total_flops_so_far": 6.268342501378726e+16,
      "budget_used_percent": 62.683425013787264
    },
    {
      "type": "training",
      "description": "Training step 2639",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:19",
      "total_flops_so_far": 6.270717233928051e+16,
      "budget_used_percent": 62.70717233928052
    },
    {
      "type": "training",
      "description": "Training step 2640",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:19",
      "total_flops_so_far": 6.273091966477376e+16,
      "budget_used_percent": 62.730919664773765
    },
    {
      "type": "training",
      "description": "Training step 2641",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:21",
      "total_flops_so_far": 6.275466699026701e+16,
      "budget_used_percent": 62.754666990267005
    },
    {
      "type": "training",
      "description": "Training step 2642",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:22",
      "total_flops_so_far": 6.277841431576026e+16,
      "budget_used_percent": 62.77841431576026
    },
    {
      "type": "training",
      "description": "Training step 2643",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:23",
      "total_flops_so_far": 6.28021616412535e+16,
      "budget_used_percent": 62.802161641253505
    },
    {
      "type": "training",
      "description": "Training step 2644",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:25",
      "total_flops_so_far": 6.282590896674675e+16,
      "budget_used_percent": 62.82590896674676
    },
    {
      "type": "training",
      "description": "Training step 2645",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:26",
      "total_flops_so_far": 6.284965629224e+16,
      "budget_used_percent": 62.84965629224
    },
    {
      "type": "training",
      "description": "Training step 2646",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:27",
      "total_flops_so_far": 6.287340361773325e+16,
      "budget_used_percent": 62.873403617733246
    },
    {
      "type": "training",
      "description": "Training step 2647",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:29",
      "total_flops_so_far": 6.28971509432265e+16,
      "budget_used_percent": 62.8971509432265
    },
    {
      "type": "training",
      "description": "Training step 2648",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:30",
      "total_flops_so_far": 6.292089826871974e+16,
      "budget_used_percent": 62.920898268719746
    },
    {
      "type": "training",
      "description": "Training step 2649",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:31",
      "total_flops_so_far": 6.294464559421299e+16,
      "budget_used_percent": 62.944645594212986
    },
    {
      "type": "training",
      "description": "Training step 2650",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:33",
      "total_flops_so_far": 6.296839291970624e+16,
      "budget_used_percent": 62.96839291970624
    },
    {
      "type": "training",
      "description": "Training step 2651",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:34",
      "total_flops_so_far": 6.299214024519949e+16,
      "budget_used_percent": 62.992140245199494
    },
    {
      "type": "training",
      "description": "Training step 2652",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:35",
      "total_flops_so_far": 6.301588757069274e+16,
      "budget_used_percent": 63.01588757069274
    },
    {
      "type": "training",
      "description": "Training step 2653",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:37",
      "total_flops_so_far": 6.303963489618598e+16,
      "budget_used_percent": 63.03963489618598
    },
    {
      "type": "training",
      "description": "Training step 2654",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:38",
      "total_flops_so_far": 6.306338222167923e+16,
      "budget_used_percent": 63.063382221679234
    },
    {
      "type": "training",
      "description": "Training step 2655",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:39",
      "total_flops_so_far": 6.308712954717248e+16,
      "budget_used_percent": 63.08712954717248
    },
    {
      "type": "training",
      "description": "Training step 2656",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:41",
      "total_flops_so_far": 6.311087687266573e+16,
      "budget_used_percent": 63.110876872665735
    },
    {
      "type": "training",
      "description": "Training step 2657",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:42",
      "total_flops_so_far": 6.313462419815898e+16,
      "budget_used_percent": 63.134624198158974
    },
    {
      "type": "training",
      "description": "Training step 2658",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:43",
      "total_flops_so_far": 6.315837152365222e+16,
      "budget_used_percent": 63.15837152365222
    },
    {
      "type": "training",
      "description": "Training step 2659",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:45",
      "total_flops_so_far": 6.318211884914547e+16,
      "budget_used_percent": 63.182118849145475
    },
    {
      "type": "training",
      "description": "Training step 2660",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:46",
      "total_flops_so_far": 6.320586617463872e+16,
      "budget_used_percent": 63.20586617463873
    },
    {
      "type": "training",
      "description": "Training step 2661",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:47",
      "total_flops_so_far": 6.322961350013197e+16,
      "budget_used_percent": 63.22961350013196
    },
    {
      "type": "training",
      "description": "Training step 2662",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:49",
      "total_flops_so_far": 6.325336082562522e+16,
      "budget_used_percent": 63.253360825625215
    },
    {
      "type": "training",
      "description": "Training step 2663",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:50",
      "total_flops_so_far": 6.327710815111846e+16,
      "budget_used_percent": 63.27710815111847
    },
    {
      "type": "training",
      "description": "Training step 2664",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:51",
      "total_flops_so_far": 6.330085547661171e+16,
      "budget_used_percent": 63.300855476611716
    },
    {
      "type": "training",
      "description": "Training step 2665",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:53",
      "total_flops_so_far": 6.332460280210496e+16,
      "budget_used_percent": 63.324602802104955
    },
    {
      "type": "training",
      "description": "Training step 2666",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:54",
      "total_flops_so_far": 6.334835012759821e+16,
      "budget_used_percent": 63.34835012759821
    },
    {
      "type": "training",
      "description": "Training step 2667",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:55",
      "total_flops_so_far": 6.337209745309146e+16,
      "budget_used_percent": 63.372097453091456
    },
    {
      "type": "training",
      "description": "Training step 2668",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:57",
      "total_flops_so_far": 6.33958447785847e+16,
      "budget_used_percent": 63.39584477858471
    },
    {
      "type": "training",
      "description": "Training step 2669",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:58",
      "total_flops_so_far": 6.341959210407795e+16,
      "budget_used_percent": 63.41959210407795
    },
    {
      "type": "training",
      "description": "Training step 2670",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:42:59",
      "total_flops_so_far": 6.34433394295712e+16,
      "budget_used_percent": 63.443339429571196
    },
    {
      "type": "training",
      "description": "Training step 2671",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:01",
      "total_flops_so_far": 6.346708675506445e+16,
      "budget_used_percent": 63.46708675506445
    },
    {
      "type": "training",
      "description": "Training step 2672",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:02",
      "total_flops_so_far": 6.34908340805577e+16,
      "budget_used_percent": 63.490834080557704
    },
    {
      "type": "training",
      "description": "Training step 2673",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:03",
      "total_flops_so_far": 6.351458140605094e+16,
      "budget_used_percent": 63.514581406050944
    },
    {
      "type": "training",
      "description": "Training step 2674",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:05",
      "total_flops_so_far": 6.353832873154419e+16,
      "budget_used_percent": 63.53832873154419
    },
    {
      "type": "training",
      "description": "Training step 2675",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:06",
      "total_flops_so_far": 6.356207605703744e+16,
      "budget_used_percent": 63.562076057037444
    },
    {
      "type": "training",
      "description": "Training step 2676",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:08",
      "total_flops_so_far": 6.358582338253069e+16,
      "budget_used_percent": 63.58582338253069
    },
    {
      "type": "training",
      "description": "Training step 2677",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:09",
      "total_flops_so_far": 6.360957070802394e+16,
      "budget_used_percent": 63.60957070802393
    },
    {
      "type": "training",
      "description": "Training step 2678",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:10",
      "total_flops_so_far": 6.363331803351718e+16,
      "budget_used_percent": 63.633318033517185
    },
    {
      "type": "training",
      "description": "Training step 2679",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:12",
      "total_flops_so_far": 6.365706535901043e+16,
      "budget_used_percent": 63.65706535901043
    },
    {
      "type": "training",
      "description": "Training step 2680",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:13",
      "total_flops_so_far": 6.368081268450368e+16,
      "budget_used_percent": 63.680812684503685
    },
    {
      "type": "training",
      "description": "Training step 2681",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:14",
      "total_flops_so_far": 6.370456000999693e+16,
      "budget_used_percent": 63.704560009996925
    },
    {
      "type": "training",
      "description": "Training step 2682",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:16",
      "total_flops_so_far": 6.372830733549018e+16,
      "budget_used_percent": 63.72830733549017
    },
    {
      "type": "training",
      "description": "Training step 2683",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:17",
      "total_flops_so_far": 6.375205466098342e+16,
      "budget_used_percent": 63.752054660983426
    },
    {
      "type": "training",
      "description": "Training step 2684",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:19",
      "total_flops_so_far": 6.377580198647667e+16,
      "budget_used_percent": 63.77580198647668
    },
    {
      "type": "training",
      "description": "Training step 2685",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:20",
      "total_flops_so_far": 6.379954931196992e+16,
      "budget_used_percent": 63.79954931196992
    },
    {
      "type": "training",
      "description": "Training step 2686",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:21",
      "total_flops_so_far": 6.382329663746317e+16,
      "budget_used_percent": 63.823296637463166
    },
    {
      "type": "training",
      "description": "Training step 2687",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:23",
      "total_flops_so_far": 6.384704396295642e+16,
      "budget_used_percent": 63.84704396295642
    },
    {
      "type": "training",
      "description": "Training step 2688",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:24",
      "total_flops_so_far": 6.387079128844966e+16,
      "budget_used_percent": 63.87079128844967
    },
    {
      "type": "training",
      "description": "Training step 2689",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:25",
      "total_flops_so_far": 6.389453861394291e+16,
      "budget_used_percent": 63.894538613942906
    },
    {
      "type": "training",
      "description": "Training step 2690",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:27",
      "total_flops_so_far": 6.391828593943616e+16,
      "budget_used_percent": 63.91828593943616
    },
    {
      "type": "training",
      "description": "Training step 2691",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:28",
      "total_flops_so_far": 6.394203326492941e+16,
      "budget_used_percent": 63.94203326492941
    },
    {
      "type": "training",
      "description": "Training step 2692",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:29",
      "total_flops_so_far": 6.396578059042266e+16,
      "budget_used_percent": 63.96578059042266
    },
    {
      "type": "training",
      "description": "Training step 2693",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:31",
      "total_flops_so_far": 6.39895279159159e+16,
      "budget_used_percent": 63.9895279159159
    },
    {
      "type": "training",
      "description": "Training step 2694",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:32",
      "total_flops_so_far": 6.401327524140915e+16,
      "budget_used_percent": 64.01327524140915
    },
    {
      "type": "training",
      "description": "Training step 2695",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:33",
      "total_flops_so_far": 6.40370225669024e+16,
      "budget_used_percent": 64.03702256690241
    },
    {
      "type": "training",
      "description": "Training step 2696",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:35",
      "total_flops_so_far": 6.406076989239565e+16,
      "budget_used_percent": 64.06076989239565
    },
    {
      "type": "training",
      "description": "Training step 2697",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:36",
      "total_flops_so_far": 6.40845172178889e+16,
      "budget_used_percent": 64.08451721788889
    },
    {
      "type": "training",
      "description": "Training step 2698",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:37",
      "total_flops_so_far": 6.410826454338214e+16,
      "budget_used_percent": 64.10826454338215
    },
    {
      "type": "training",
      "description": "Training step 2699",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:39",
      "total_flops_so_far": 6.413201186887539e+16,
      "budget_used_percent": 64.1320118688754
    },
    {
      "type": "training",
      "description": "Training step 2700",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:40",
      "total_flops_so_far": 6.415575919436864e+16,
      "budget_used_percent": 64.15575919436864
    },
    {
      "type": "training",
      "description": "Training step 2701",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:41",
      "total_flops_so_far": 6.417950651986189e+16,
      "budget_used_percent": 64.17950651986189
    },
    {
      "type": "training",
      "description": "Training step 2702",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:43",
      "total_flops_so_far": 6.420325384535514e+16,
      "budget_used_percent": 64.20325384535514
    },
    {
      "type": "training",
      "description": "Training step 2703",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:44",
      "total_flops_so_far": 6.422700117084838e+16,
      "budget_used_percent": 64.22700117084838
    },
    {
      "type": "training",
      "description": "Training step 2704",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:45",
      "total_flops_so_far": 6.425074849634163e+16,
      "budget_used_percent": 64.25074849634164
    },
    {
      "type": "training",
      "description": "Training step 2705",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:47",
      "total_flops_so_far": 6.427449582183488e+16,
      "budget_used_percent": 64.27449582183488
    },
    {
      "type": "training",
      "description": "Training step 2706",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:48",
      "total_flops_so_far": 6.429824314732813e+16,
      "budget_used_percent": 64.29824314732812
    },
    {
      "type": "training",
      "description": "Training step 2707",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:49",
      "total_flops_so_far": 6.432199047282138e+16,
      "budget_used_percent": 64.32199047282138
    },
    {
      "type": "training",
      "description": "Training step 2708",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:51",
      "total_flops_so_far": 6.434573779831462e+16,
      "budget_used_percent": 64.34573779831463
    },
    {
      "type": "training",
      "description": "Training step 2709",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:52",
      "total_flops_so_far": 6.436948512380787e+16,
      "budget_used_percent": 64.36948512380786
    },
    {
      "type": "training",
      "description": "Training step 2710",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:53",
      "total_flops_so_far": 6.439323244930112e+16,
      "budget_used_percent": 64.39323244930112
    },
    {
      "type": "training",
      "description": "Training step 2711",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:55",
      "total_flops_so_far": 6.441697977479437e+16,
      "budget_used_percent": 64.41697977479437
    },
    {
      "type": "training",
      "description": "Training step 2712",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:56",
      "total_flops_so_far": 6.444072710028762e+16,
      "budget_used_percent": 64.44072710028762
    },
    {
      "type": "training",
      "description": "Training step 2713",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:57",
      "total_flops_so_far": 6.446447442578086e+16,
      "budget_used_percent": 64.46447442578086
    },
    {
      "type": "training",
      "description": "Training step 2714",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:43:59",
      "total_flops_so_far": 6.448822175127411e+16,
      "budget_used_percent": 64.48822175127411
    },
    {
      "type": "training",
      "description": "Training step 2715",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:00",
      "total_flops_so_far": 6.451196907676736e+16,
      "budget_used_percent": 64.51196907676736
    },
    {
      "type": "training",
      "description": "Training step 2716",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:01",
      "total_flops_so_far": 6.453571640226061e+16,
      "budget_used_percent": 64.53571640226062
    },
    {
      "type": "training",
      "description": "Training step 2717",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:03",
      "total_flops_so_far": 6.455946372775386e+16,
      "budget_used_percent": 64.55946372775385
    },
    {
      "type": "training",
      "description": "Training step 2718",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:04",
      "total_flops_so_far": 6.45832110532471e+16,
      "budget_used_percent": 64.5832110532471
    },
    {
      "type": "training",
      "description": "Training step 2719",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:05",
      "total_flops_so_far": 6.460695837874035e+16,
      "budget_used_percent": 64.60695837874036
    },
    {
      "type": "training",
      "description": "Training step 2720",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:07",
      "total_flops_so_far": 6.46307057042336e+16,
      "budget_used_percent": 64.6307057042336
    },
    {
      "type": "training",
      "description": "Training step 2721",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:08",
      "total_flops_so_far": 6.465445302972685e+16,
      "budget_used_percent": 64.65445302972684
    },
    {
      "type": "training",
      "description": "Training step 2722",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:09",
      "total_flops_so_far": 6.46782003552201e+16,
      "budget_used_percent": 64.6782003552201
    },
    {
      "type": "training",
      "description": "Training step 2723",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:11",
      "total_flops_so_far": 6.470194768071334e+16,
      "budget_used_percent": 64.70194768071335
    },
    {
      "type": "training",
      "description": "Training step 2724",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:12",
      "total_flops_so_far": 6.472569500620659e+16,
      "budget_used_percent": 64.72569500620659
    },
    {
      "type": "training",
      "description": "Training step 2725",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:13",
      "total_flops_so_far": 6.474944233169984e+16,
      "budget_used_percent": 64.74944233169984
    },
    {
      "type": "training",
      "description": "Training step 2726",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:15",
      "total_flops_so_far": 6.477318965719309e+16,
      "budget_used_percent": 64.77318965719309
    },
    {
      "type": "training",
      "description": "Training step 2727",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:16",
      "total_flops_so_far": 6.479693698268634e+16,
      "budget_used_percent": 64.79693698268633
    },
    {
      "type": "training",
      "description": "Training step 2728",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:17",
      "total_flops_so_far": 6.482068430817958e+16,
      "budget_used_percent": 64.8206843081796
    },
    {
      "type": "training",
      "description": "Training step 2729",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:19",
      "total_flops_so_far": 6.484443163367283e+16,
      "budget_used_percent": 64.84443163367283
    },
    {
      "type": "training",
      "description": "Training step 2730",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:20",
      "total_flops_so_far": 6.486817895916608e+16,
      "budget_used_percent": 64.86817895916607
    },
    {
      "type": "training",
      "description": "Training step 2731",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:22",
      "total_flops_so_far": 6.489192628465933e+16,
      "budget_used_percent": 64.89192628465933
    },
    {
      "type": "training",
      "description": "Training step 2732",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:23",
      "total_flops_so_far": 6.491567361015258e+16,
      "budget_used_percent": 64.91567361015258
    },
    {
      "type": "training",
      "description": "Training step 2733",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:24",
      "total_flops_so_far": 6.493942093564582e+16,
      "budget_used_percent": 64.93942093564581
    },
    {
      "type": "training",
      "description": "Training step 2734",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:26",
      "total_flops_so_far": 6.496316826113907e+16,
      "budget_used_percent": 64.96316826113907
    },
    {
      "type": "training",
      "description": "Training step 2735",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:27",
      "total_flops_so_far": 6.498691558663232e+16,
      "budget_used_percent": 64.98691558663232
    },
    {
      "type": "training",
      "description": "Training step 2736",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:28",
      "total_flops_so_far": 6.501066291212557e+16,
      "budget_used_percent": 65.01066291212557
    },
    {
      "type": "training",
      "description": "Training step 2737",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:30",
      "total_flops_so_far": 6.503441023761882e+16,
      "budget_used_percent": 65.03441023761881
    },
    {
      "type": "training",
      "description": "Training step 2738",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:31",
      "total_flops_so_far": 6.505815756311206e+16,
      "budget_used_percent": 65.05815756311206
    },
    {
      "type": "training",
      "description": "Training step 2739",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:32",
      "total_flops_so_far": 6.508190488860531e+16,
      "budget_used_percent": 65.08190488860531
    },
    {
      "type": "training",
      "description": "Training step 2740",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:34",
      "total_flops_so_far": 6.510565221409856e+16,
      "budget_used_percent": 65.10565221409857
    },
    {
      "type": "training",
      "description": "Training step 2741",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:35",
      "total_flops_so_far": 6.512939953959181e+16,
      "budget_used_percent": 65.1293995395918
    },
    {
      "type": "training",
      "description": "Training step 2742",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:36",
      "total_flops_so_far": 6.515314686508506e+16,
      "budget_used_percent": 65.15314686508505
    },
    {
      "type": "training",
      "description": "Training step 2743",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:38",
      "total_flops_so_far": 6.51768941905783e+16,
      "budget_used_percent": 65.17689419057831
    },
    {
      "type": "training",
      "description": "Training step 2744",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:39",
      "total_flops_so_far": 6.520064151607155e+16,
      "budget_used_percent": 65.20064151607156
    },
    {
      "type": "training",
      "description": "Training step 2745",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:40",
      "total_flops_so_far": 6.52243888415648e+16,
      "budget_used_percent": 65.22438884156479
    },
    {
      "type": "training",
      "description": "Training step 2746",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:42",
      "total_flops_so_far": 6.524813616705805e+16,
      "budget_used_percent": 65.24813616705805
    },
    {
      "type": "training",
      "description": "Training step 2747",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:43",
      "total_flops_so_far": 6.52718834925513e+16,
      "budget_used_percent": 65.2718834925513
    },
    {
      "type": "training",
      "description": "Training step 2748",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:44",
      "total_flops_so_far": 6.529563081804454e+16,
      "budget_used_percent": 65.29563081804454
    },
    {
      "type": "training",
      "description": "Training step 2749",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:46",
      "total_flops_so_far": 6.531937814353779e+16,
      "budget_used_percent": 65.31937814353779
    },
    {
      "type": "training",
      "description": "Training step 2750",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:47",
      "total_flops_so_far": 6.534312546903104e+16,
      "budget_used_percent": 65.34312546903104
    },
    {
      "type": "training",
      "description": "Training step 2751",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:48",
      "total_flops_so_far": 6.536687279452429e+16,
      "budget_used_percent": 65.36687279452428
    },
    {
      "type": "training",
      "description": "Training step 2752",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:50",
      "total_flops_so_far": 6.539062012001754e+16,
      "budget_used_percent": 65.39062012001754
    },
    {
      "type": "training",
      "description": "Training step 2753",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:51",
      "total_flops_so_far": 6.541436744551078e+16,
      "budget_used_percent": 65.41436744551078
    },
    {
      "type": "training",
      "description": "Training step 2754",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:52",
      "total_flops_so_far": 6.543811477100403e+16,
      "budget_used_percent": 65.43811477100402
    },
    {
      "type": "training",
      "description": "Training step 2755",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:54",
      "total_flops_so_far": 6.546186209649728e+16,
      "budget_used_percent": 65.46186209649728
    },
    {
      "type": "training",
      "description": "Training step 2756",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:55",
      "total_flops_so_far": 6.548560942199053e+16,
      "budget_used_percent": 65.48560942199053
    },
    {
      "type": "training",
      "description": "Training step 2757",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:56",
      "total_flops_so_far": 6.550935674748378e+16,
      "budget_used_percent": 65.50935674748376
    },
    {
      "type": "training",
      "description": "Training step 2758",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:58",
      "total_flops_so_far": 6.553310407297702e+16,
      "budget_used_percent": 65.53310407297703
    },
    {
      "type": "training",
      "description": "Training step 2759",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:44:59",
      "total_flops_so_far": 6.555685139847027e+16,
      "budget_used_percent": 65.55685139847027
    },
    {
      "type": "training",
      "description": "Training step 2760",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:00",
      "total_flops_so_far": 6.558059872396352e+16,
      "budget_used_percent": 65.58059872396352
    },
    {
      "type": "training",
      "description": "Training step 2761",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:02",
      "total_flops_so_far": 6.560434604945677e+16,
      "budget_used_percent": 65.60434604945677
    },
    {
      "type": "training",
      "description": "Training step 2762",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:03",
      "total_flops_so_far": 6.562809337495002e+16,
      "budget_used_percent": 65.62809337495001
    },
    {
      "type": "training",
      "description": "Training step 2763",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:05",
      "total_flops_so_far": 6.565184070044326e+16,
      "budget_used_percent": 65.65184070044326
    },
    {
      "type": "training",
      "description": "Training step 2764",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:06",
      "total_flops_so_far": 6.567558802593651e+16,
      "budget_used_percent": 65.67558802593652
    },
    {
      "type": "training",
      "description": "Training step 2765",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:07",
      "total_flops_so_far": 6.569933535142976e+16,
      "budget_used_percent": 65.69933535142975
    },
    {
      "type": "training",
      "description": "Training step 2766",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:09",
      "total_flops_so_far": 6.572308267692301e+16,
      "budget_used_percent": 65.723082676923
    },
    {
      "type": "training",
      "description": "Training step 2767",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:10",
      "total_flops_so_far": 6.574683000241626e+16,
      "budget_used_percent": 65.74683000241626
    },
    {
      "type": "training",
      "description": "Training step 2768",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:11",
      "total_flops_so_far": 6.57705773279095e+16,
      "budget_used_percent": 65.7705773279095
    },
    {
      "type": "training",
      "description": "Training step 2769",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:13",
      "total_flops_so_far": 6.579432465340275e+16,
      "budget_used_percent": 65.79432465340275
    },
    {
      "type": "training",
      "description": "Training step 2770",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:14",
      "total_flops_so_far": 6.5818071978896e+16,
      "budget_used_percent": 65.818071978896
    },
    {
      "type": "training",
      "description": "Training step 2771",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:15",
      "total_flops_so_far": 6.584181930438925e+16,
      "budget_used_percent": 65.84181930438925
    },
    {
      "type": "training",
      "description": "Training step 2772",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:16",
      "total_flops_so_far": 6.58655666298825e+16,
      "budget_used_percent": 65.8655666298825
    },
    {
      "type": "training",
      "description": "Training step 2773",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:17",
      "total_flops_so_far": 6.588931395537574e+16,
      "budget_used_percent": 65.88931395537574
    },
    {
      "type": "training",
      "description": "Training step 2774",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:18",
      "total_flops_so_far": 6.591306128086899e+16,
      "budget_used_percent": 65.91306128086899
    },
    {
      "type": "training",
      "description": "Training step 2775",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:20",
      "total_flops_so_far": 6.593680860636224e+16,
      "budget_used_percent": 65.93680860636223
    },
    {
      "type": "training",
      "description": "Training step 2776",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:21",
      "total_flops_so_far": 6.596055593185549e+16,
      "budget_used_percent": 65.9605559318555
    },
    {
      "type": "training",
      "description": "Training step 2777",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:22",
      "total_flops_so_far": 6.598430325734874e+16,
      "budget_used_percent": 65.98430325734873
    },
    {
      "type": "training",
      "description": "Training step 2778",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:24",
      "total_flops_so_far": 6.600805058284198e+16,
      "budget_used_percent": 66.00805058284197
    },
    {
      "type": "training",
      "description": "Training step 2779",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:25",
      "total_flops_so_far": 6.603179790833523e+16,
      "budget_used_percent": 66.03179790833524
    },
    {
      "type": "training",
      "description": "Training step 2780",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:26",
      "total_flops_so_far": 6.605554523382848e+16,
      "budget_used_percent": 66.05554523382848
    },
    {
      "type": "training",
      "description": "Training step 2781",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:28",
      "total_flops_so_far": 6.607929255932173e+16,
      "budget_used_percent": 66.07929255932173
    },
    {
      "type": "training",
      "description": "Training step 2782",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:29",
      "total_flops_so_far": 6.610303988481498e+16,
      "budget_used_percent": 66.10303988481498
    },
    {
      "type": "training",
      "description": "Training step 2783",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:30",
      "total_flops_so_far": 6.612678721030822e+16,
      "budget_used_percent": 66.12678721030822
    },
    {
      "type": "training",
      "description": "Training step 2784",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:32",
      "total_flops_so_far": 6.615053453580147e+16,
      "budget_used_percent": 66.15053453580147
    },
    {
      "type": "training",
      "description": "Training step 2785",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:33",
      "total_flops_so_far": 6.617428186129472e+16,
      "budget_used_percent": 66.17428186129472
    },
    {
      "type": "training",
      "description": "Training step 2786",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:34",
      "total_flops_so_far": 6.619802918678797e+16,
      "budget_used_percent": 66.19802918678796
    },
    {
      "type": "training",
      "description": "Training step 2787",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:36",
      "total_flops_so_far": 6.622177651228122e+16,
      "budget_used_percent": 66.22177651228121
    },
    {
      "type": "training",
      "description": "Training step 2788",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:37",
      "total_flops_so_far": 6.624552383777446e+16,
      "budget_used_percent": 66.24552383777447
    },
    {
      "type": "training",
      "description": "Training step 2789",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:39",
      "total_flops_so_far": 6.626927116326771e+16,
      "budget_used_percent": 66.2692711632677
    },
    {
      "type": "training",
      "description": "Training step 2790",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:40",
      "total_flops_so_far": 6.629301848876096e+16,
      "budget_used_percent": 66.29301848876096
    },
    {
      "type": "training",
      "description": "Training step 2791",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:41",
      "total_flops_so_far": 6.631676581425421e+16,
      "budget_used_percent": 66.31676581425421
    },
    {
      "type": "training",
      "description": "Training step 2792",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:43",
      "total_flops_so_far": 6.634051313974746e+16,
      "budget_used_percent": 66.34051313974746
    },
    {
      "type": "training",
      "description": "Training step 2793",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:44",
      "total_flops_so_far": 6.63642604652407e+16,
      "budget_used_percent": 66.3642604652407
    },
    {
      "type": "training",
      "description": "Training step 2794",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:45",
      "total_flops_so_far": 6.638800779073395e+16,
      "budget_used_percent": 66.38800779073395
    },
    {
      "type": "training",
      "description": "Training step 2795",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:47",
      "total_flops_so_far": 6.64117551162272e+16,
      "budget_used_percent": 66.4117551162272
    },
    {
      "type": "training",
      "description": "Training step 2796",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:48",
      "total_flops_so_far": 6.643550244172045e+16,
      "budget_used_percent": 66.43550244172044
    },
    {
      "type": "training",
      "description": "Training step 2797",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:49",
      "total_flops_so_far": 6.64592497672137e+16,
      "budget_used_percent": 66.45924976721369
    },
    {
      "type": "training",
      "description": "Training step 2798",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:51",
      "total_flops_so_far": 6.648299709270694e+16,
      "budget_used_percent": 66.48299709270694
    },
    {
      "type": "training",
      "description": "Training step 2799",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:52",
      "total_flops_so_far": 6.650674441820019e+16,
      "budget_used_percent": 66.50674441820019
    },
    {
      "type": "training",
      "description": "Training step 2800",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:53",
      "total_flops_so_far": 6.653049174369344e+16,
      "budget_used_percent": 66.53049174369345
    },
    {
      "type": "training",
      "description": "Training step 2801",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:55",
      "total_flops_so_far": 6.655423906918669e+16,
      "budget_used_percent": 66.55423906918668
    },
    {
      "type": "training",
      "description": "Training step 2802",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:56",
      "total_flops_so_far": 6.657798639467994e+16,
      "budget_used_percent": 66.57798639467994
    },
    {
      "type": "training",
      "description": "Training step 2803",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:57",
      "total_flops_so_far": 6.660173372017318e+16,
      "budget_used_percent": 66.60173372017319
    },
    {
      "type": "training",
      "description": "Training step 2804",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:45:59",
      "total_flops_so_far": 6.662548104566643e+16,
      "budget_used_percent": 66.62548104566643
    },
    {
      "type": "training",
      "description": "Training step 2805",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:00",
      "total_flops_so_far": 6.664922837115968e+16,
      "budget_used_percent": 66.64922837115968
    },
    {
      "type": "training",
      "description": "Training step 2806",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:01",
      "total_flops_so_far": 6.667297569665293e+16,
      "budget_used_percent": 66.67297569665293
    },
    {
      "type": "training",
      "description": "Training step 2807",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:03",
      "total_flops_so_far": 6.669672302214618e+16,
      "budget_used_percent": 66.69672302214617
    },
    {
      "type": "training",
      "description": "Training step 2808",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:04",
      "total_flops_so_far": 6.672047034763942e+16,
      "budget_used_percent": 66.72047034763942
    },
    {
      "type": "training",
      "description": "Training step 2809",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:05",
      "total_flops_so_far": 6.674421767313267e+16,
      "budget_used_percent": 66.74421767313268
    },
    {
      "type": "training",
      "description": "Training step 2810",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:07",
      "total_flops_so_far": 6.676796499862592e+16,
      "budget_used_percent": 66.76796499862591
    },
    {
      "type": "training",
      "description": "Training step 2811",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:08",
      "total_flops_so_far": 6.679171232411917e+16,
      "budget_used_percent": 66.79171232411917
    },
    {
      "type": "training",
      "description": "Training step 2812",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:09",
      "total_flops_so_far": 6.681545964961242e+16,
      "budget_used_percent": 66.81545964961242
    },
    {
      "type": "training",
      "description": "Training step 2813",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:11",
      "total_flops_so_far": 6.683920697510566e+16,
      "budget_used_percent": 66.83920697510567
    },
    {
      "type": "training",
      "description": "Training step 2814",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:12",
      "total_flops_so_far": 6.686295430059891e+16,
      "budget_used_percent": 66.86295430059891
    },
    {
      "type": "training",
      "description": "Training step 2815",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:13",
      "total_flops_so_far": 6.688670162609216e+16,
      "budget_used_percent": 66.88670162609216
    },
    {
      "type": "training",
      "description": "Training step 2816",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:15",
      "total_flops_so_far": 6.691044895158541e+16,
      "budget_used_percent": 66.91044895158541
    },
    {
      "type": "training",
      "description": "Training step 2817",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:16",
      "total_flops_so_far": 6.693419627707866e+16,
      "budget_used_percent": 66.93419627707866
    },
    {
      "type": "training",
      "description": "Training step 2818",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:17",
      "total_flops_so_far": 6.69579436025719e+16,
      "budget_used_percent": 66.9579436025719
    },
    {
      "type": "training",
      "description": "Training step 2819",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:19",
      "total_flops_so_far": 6.698169092806515e+16,
      "budget_used_percent": 66.98169092806515
    },
    {
      "type": "training",
      "description": "Training step 2820",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:20",
      "total_flops_so_far": 6.70054382535584e+16,
      "budget_used_percent": 67.0054382535584
    },
    {
      "type": "training",
      "description": "Training step 2821",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:21",
      "total_flops_so_far": 6.702918557905165e+16,
      "budget_used_percent": 67.02918557905166
    },
    {
      "type": "training",
      "description": "Training step 2822",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:23",
      "total_flops_so_far": 6.70529329045449e+16,
      "budget_used_percent": 67.05293290454489
    },
    {
      "type": "training",
      "description": "Training step 2823",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:24",
      "total_flops_so_far": 6.707668023003814e+16,
      "budget_used_percent": 67.07668023003815
    },
    {
      "type": "training",
      "description": "Training step 2824",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:26",
      "total_flops_so_far": 6.710042755553139e+16,
      "budget_used_percent": 67.1004275555314
    },
    {
      "type": "training",
      "description": "Training step 2825",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:27",
      "total_flops_so_far": 6.712417488102464e+16,
      "budget_used_percent": 67.12417488102464
    },
    {
      "type": "training",
      "description": "Training step 2826",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:28",
      "total_flops_so_far": 6.714792220651789e+16,
      "budget_used_percent": 67.14792220651789
    },
    {
      "type": "training",
      "description": "Training step 2827",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:30",
      "total_flops_so_far": 6.717166953201114e+16,
      "budget_used_percent": 67.17166953201114
    },
    {
      "type": "training",
      "description": "Training step 2828",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:31",
      "total_flops_so_far": 6.719541685750438e+16,
      "budget_used_percent": 67.19541685750438
    },
    {
      "type": "training",
      "description": "Training step 2829",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:32",
      "total_flops_so_far": 6.721916418299763e+16,
      "budget_used_percent": 67.21916418299763
    },
    {
      "type": "training",
      "description": "Training step 2830",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:34",
      "total_flops_so_far": 6.724291150849088e+16,
      "budget_used_percent": 67.24291150849088
    },
    {
      "type": "training",
      "description": "Training step 2831",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:35",
      "total_flops_so_far": 6.726665883398413e+16,
      "budget_used_percent": 67.26665883398412
    },
    {
      "type": "training",
      "description": "Training step 2832",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:36",
      "total_flops_so_far": 6.729040615947738e+16,
      "budget_used_percent": 67.29040615947739
    },
    {
      "type": "training",
      "description": "Training step 2833",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:38",
      "total_flops_so_far": 6.731415348497062e+16,
      "budget_used_percent": 67.31415348497063
    },
    {
      "type": "training",
      "description": "Training step 2834",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:39",
      "total_flops_so_far": 6.733790081046387e+16,
      "budget_used_percent": 67.33790081046386
    },
    {
      "type": "training",
      "description": "Training step 2835",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:40",
      "total_flops_so_far": 6.736164813595712e+16,
      "budget_used_percent": 67.36164813595713
    },
    {
      "type": "training",
      "description": "Training step 2836",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:42",
      "total_flops_so_far": 6.738539546145037e+16,
      "budget_used_percent": 67.38539546145037
    },
    {
      "type": "training",
      "description": "Training step 2837",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:43",
      "total_flops_so_far": 6.740914278694362e+16,
      "budget_used_percent": 67.40914278694362
    },
    {
      "type": "training",
      "description": "Training step 2838",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:44",
      "total_flops_so_far": 6.743289011243686e+16,
      "budget_used_percent": 67.43289011243687
    },
    {
      "type": "training",
      "description": "Training step 2839",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:46",
      "total_flops_so_far": 6.745663743793011e+16,
      "budget_used_percent": 67.45663743793011
    },
    {
      "type": "training",
      "description": "Training step 2840",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:47",
      "total_flops_so_far": 6.748038476342336e+16,
      "budget_used_percent": 67.48038476342336
    },
    {
      "type": "training",
      "description": "Training step 2841",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:48",
      "total_flops_so_far": 6.750413208891661e+16,
      "budget_used_percent": 67.5041320889166
    },
    {
      "type": "training",
      "description": "Training step 2842",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:50",
      "total_flops_so_far": 6.752787941440986e+16,
      "budget_used_percent": 67.52787941440985
    },
    {
      "type": "training",
      "description": "Training step 2843",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:51",
      "total_flops_so_far": 6.75516267399031e+16,
      "budget_used_percent": 67.5516267399031
    },
    {
      "type": "training",
      "description": "Training step 2844",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:53",
      "total_flops_so_far": 6.757537406539635e+16,
      "budget_used_percent": 67.57537406539636
    },
    {
      "type": "training",
      "description": "Training step 2845",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:54",
      "total_flops_so_far": 6.75991213908896e+16,
      "budget_used_percent": 67.59912139088961
    },
    {
      "type": "training",
      "description": "Training step 2846",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:55",
      "total_flops_so_far": 6.762286871638285e+16,
      "budget_used_percent": 67.62286871638284
    },
    {
      "type": "training",
      "description": "Training step 2847",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:57",
      "total_flops_so_far": 6.76466160418761e+16,
      "budget_used_percent": 67.6466160418761
    },
    {
      "type": "training",
      "description": "Training step 2848",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:58",
      "total_flops_so_far": 6.767036336736934e+16,
      "budget_used_percent": 67.67036336736935
    },
    {
      "type": "training",
      "description": "Training step 2849",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:46:59",
      "total_flops_so_far": 6.769411069286259e+16,
      "budget_used_percent": 67.6941106928626
    },
    {
      "type": "training",
      "description": "Training step 2850",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:01",
      "total_flops_so_far": 6.771785801835584e+16,
      "budget_used_percent": 67.71785801835584
    },
    {
      "type": "training",
      "description": "Training step 2851",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:02",
      "total_flops_so_far": 6.774160534384909e+16,
      "budget_used_percent": 67.74160534384909
    },
    {
      "type": "training",
      "description": "Training step 2852",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:03",
      "total_flops_so_far": 6.776535266934234e+16,
      "budget_used_percent": 67.76535266934233
    },
    {
      "type": "training",
      "description": "Training step 2853",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:05",
      "total_flops_so_far": 6.778909999483558e+16,
      "budget_used_percent": 67.7890999948356
    },
    {
      "type": "training",
      "description": "Training step 2854",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:06",
      "total_flops_so_far": 6.781284732032883e+16,
      "budget_used_percent": 67.81284732032883
    },
    {
      "type": "training",
      "description": "Training step 2855",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:07",
      "total_flops_so_far": 6.783659464582208e+16,
      "budget_used_percent": 67.83659464582207
    },
    {
      "type": "training",
      "description": "Training step 2856",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:09",
      "total_flops_so_far": 6.786034197131533e+16,
      "budget_used_percent": 67.86034197131534
    },
    {
      "type": "training",
      "description": "Training step 2857",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:10",
      "total_flops_so_far": 6.788408929680858e+16,
      "budget_used_percent": 67.88408929680858
    },
    {
      "type": "training",
      "description": "Training step 2858",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:11",
      "total_flops_so_far": 6.790783662230182e+16,
      "budget_used_percent": 67.90783662230182
    },
    {
      "type": "training",
      "description": "Training step 2859",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:13",
      "total_flops_so_far": 6.793158394779507e+16,
      "budget_used_percent": 67.93158394779508
    },
    {
      "type": "training",
      "description": "Training step 2860",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:14",
      "total_flops_so_far": 6.795533127328832e+16,
      "budget_used_percent": 67.95533127328832
    },
    {
      "type": "training",
      "description": "Training step 2861",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:15",
      "total_flops_so_far": 6.797907859878157e+16,
      "budget_used_percent": 67.97907859878157
    },
    {
      "type": "training",
      "description": "Training step 2862",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:17",
      "total_flops_so_far": 6.800282592427482e+16,
      "budget_used_percent": 68.00282592427482
    },
    {
      "type": "training",
      "description": "Training step 2863",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:18",
      "total_flops_so_far": 6.802657324976806e+16,
      "budget_used_percent": 68.02657324976806
    },
    {
      "type": "training",
      "description": "Training step 2864",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:19",
      "total_flops_so_far": 6.805032057526131e+16,
      "budget_used_percent": 68.05032057526131
    },
    {
      "type": "training",
      "description": "Training step 2865",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:21",
      "total_flops_so_far": 6.807406790075456e+16,
      "budget_used_percent": 68.07406790075457
    },
    {
      "type": "training",
      "description": "Training step 2866",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:22",
      "total_flops_so_far": 6.809781522624781e+16,
      "budget_used_percent": 68.0978152262478
    },
    {
      "type": "training",
      "description": "Training step 2867",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:23",
      "total_flops_so_far": 6.812156255174106e+16,
      "budget_used_percent": 68.12156255174105
    },
    {
      "type": "training",
      "description": "Training step 2868",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:25",
      "total_flops_so_far": 6.81453098772343e+16,
      "budget_used_percent": 68.14530987723431
    },
    {
      "type": "training",
      "description": "Training step 2869",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:26",
      "total_flops_so_far": 6.816905720272755e+16,
      "budget_used_percent": 68.16905720272756
    },
    {
      "type": "training",
      "description": "Training step 2870",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:27",
      "total_flops_so_far": 6.81928045282208e+16,
      "budget_used_percent": 68.19280452822079
    },
    {
      "type": "training",
      "description": "Training step 2871",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:29",
      "total_flops_so_far": 6.821655185371405e+16,
      "budget_used_percent": 68.21655185371405
    },
    {
      "type": "training",
      "description": "Training step 2872",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:30",
      "total_flops_so_far": 6.82402991792073e+16,
      "budget_used_percent": 68.2402991792073
    },
    {
      "type": "training",
      "description": "Training step 2873",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:32",
      "total_flops_so_far": 6.826404650470054e+16,
      "budget_used_percent": 68.26404650470054
    },
    {
      "type": "training",
      "description": "Training step 2874",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:33",
      "total_flops_so_far": 6.828779383019379e+16,
      "budget_used_percent": 68.28779383019379
    },
    {
      "type": "training",
      "description": "Training step 2875",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:34",
      "total_flops_so_far": 6.831154115568704e+16,
      "budget_used_percent": 68.31154115568704
    },
    {
      "type": "training",
      "description": "Training step 2876",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:36",
      "total_flops_so_far": 6.833528848118029e+16,
      "budget_used_percent": 68.33528848118029
    },
    {
      "type": "training",
      "description": "Training step 2877",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:37",
      "total_flops_so_far": 6.835903580667354e+16,
      "budget_used_percent": 68.35903580667355
    },
    {
      "type": "training",
      "description": "Training step 2878",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:38",
      "total_flops_so_far": 6.838278313216678e+16,
      "budget_used_percent": 68.38278313216678
    },
    {
      "type": "training",
      "description": "Training step 2879",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:40",
      "total_flops_so_far": 6.840653045766003e+16,
      "budget_used_percent": 68.40653045766003
    },
    {
      "type": "training",
      "description": "Training step 2880",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:41",
      "total_flops_so_far": 6.843027778315328e+16,
      "budget_used_percent": 68.43027778315329
    },
    {
      "type": "training",
      "description": "Training step 2881",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:42",
      "total_flops_so_far": 6.845402510864653e+16,
      "budget_used_percent": 68.45402510864653
    },
    {
      "type": "training",
      "description": "Training step 2882",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:44",
      "total_flops_so_far": 6.847777243413978e+16,
      "budget_used_percent": 68.47777243413977
    },
    {
      "type": "training",
      "description": "Training step 2883",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:45",
      "total_flops_so_far": 6.850151975963302e+16,
      "budget_used_percent": 68.50151975963303
    },
    {
      "type": "training",
      "description": "Training step 2884",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:46",
      "total_flops_so_far": 6.852526708512627e+16,
      "budget_used_percent": 68.52526708512627
    },
    {
      "type": "training",
      "description": "Training step 2885",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:48",
      "total_flops_so_far": 6.854901441061952e+16,
      "budget_used_percent": 68.54901441061952
    },
    {
      "type": "training",
      "description": "Training step 2886",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:49",
      "total_flops_so_far": 6.857276173611277e+16,
      "budget_used_percent": 68.57276173611277
    },
    {
      "type": "training",
      "description": "Training step 2887",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:50",
      "total_flops_so_far": 6.859650906160602e+16,
      "budget_used_percent": 68.59650906160601
    },
    {
      "type": "training",
      "description": "Training step 2888",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:52",
      "total_flops_so_far": 6.862025638709926e+16,
      "budget_used_percent": 68.62025638709926
    },
    {
      "type": "training",
      "description": "Training step 2889",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:53",
      "total_flops_so_far": 6.864400371259251e+16,
      "budget_used_percent": 68.64400371259252
    },
    {
      "type": "training",
      "description": "Training step 2890",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:54",
      "total_flops_so_far": 6.866775103808576e+16,
      "budget_used_percent": 68.66775103808575
    },
    {
      "type": "training",
      "description": "Training step 2891",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:56",
      "total_flops_so_far": 6.869149836357901e+16,
      "budget_used_percent": 68.691498363579
    },
    {
      "type": "training",
      "description": "Training step 2892",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:57",
      "total_flops_so_far": 6.871524568907226e+16,
      "budget_used_percent": 68.71524568907226
    },
    {
      "type": "training",
      "description": "Training step 2893",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:47:59",
      "total_flops_so_far": 6.87389930145655e+16,
      "budget_used_percent": 68.73899301456551
    },
    {
      "type": "training",
      "description": "Training step 2894",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:00",
      "total_flops_so_far": 6.876274034005875e+16,
      "budget_used_percent": 68.76274034005874
    },
    {
      "type": "training",
      "description": "Training step 2895",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:01",
      "total_flops_so_far": 6.8786487665552e+16,
      "budget_used_percent": 68.786487665552
    },
    {
      "type": "training",
      "description": "Training step 2896",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:03",
      "total_flops_so_far": 6.881023499104525e+16,
      "budget_used_percent": 68.81023499104525
    },
    {
      "type": "training",
      "description": "Training step 2897",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:04",
      "total_flops_so_far": 6.88339823165385e+16,
      "budget_used_percent": 68.8339823165385
    },
    {
      "type": "training",
      "description": "Training step 2898",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:05",
      "total_flops_so_far": 6.885772964203174e+16,
      "budget_used_percent": 68.85772964203174
    },
    {
      "type": "training",
      "description": "Training step 2899",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:07",
      "total_flops_so_far": 6.888147696752499e+16,
      "budget_used_percent": 68.88147696752499
    },
    {
      "type": "training",
      "description": "Training step 2900",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:08",
      "total_flops_so_far": 6.890522429301824e+16,
      "budget_used_percent": 68.90522429301824
    },
    {
      "type": "training",
      "description": "Training step 2901",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:09",
      "total_flops_so_far": 6.892897161851149e+16,
      "budget_used_percent": 68.9289716185115
    },
    {
      "type": "training",
      "description": "Training step 2902",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:11",
      "total_flops_so_far": 6.895271894400474e+16,
      "budget_used_percent": 68.95271894400473
    },
    {
      "type": "training",
      "description": "Training step 2903",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:12",
      "total_flops_so_far": 6.897646626949798e+16,
      "budget_used_percent": 68.97646626949798
    },
    {
      "type": "training",
      "description": "Training step 2904",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:12",
      "total_flops_so_far": 6.900021359499123e+16,
      "budget_used_percent": 69.00021359499124
    },
    {
      "type": "training",
      "description": "Training step 2905",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:14",
      "total_flops_so_far": 6.902396092048448e+16,
      "budget_used_percent": 69.02396092048448
    },
    {
      "type": "training",
      "description": "Training step 2906",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:15",
      "total_flops_so_far": 6.904770824597773e+16,
      "budget_used_percent": 69.04770824597773
    },
    {
      "type": "training",
      "description": "Training step 2907",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:16",
      "total_flops_so_far": 6.907145557147098e+16,
      "budget_used_percent": 69.07145557147098
    },
    {
      "type": "training",
      "description": "Training step 2908",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:18",
      "total_flops_so_far": 6.909520289696422e+16,
      "budget_used_percent": 69.09520289696422
    },
    {
      "type": "training",
      "description": "Training step 2909",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:19",
      "total_flops_so_far": 6.911895022245747e+16,
      "budget_used_percent": 69.11895022245747
    },
    {
      "type": "training",
      "description": "Training step 2910",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:20",
      "total_flops_so_far": 6.914269754795072e+16,
      "budget_used_percent": 69.14269754795072
    },
    {
      "type": "training",
      "description": "Training step 2911",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:22",
      "total_flops_so_far": 6.916644487344397e+16,
      "budget_used_percent": 69.16644487344396
    },
    {
      "type": "training",
      "description": "Training step 2912",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:23",
      "total_flops_so_far": 6.919019219893722e+16,
      "budget_used_percent": 69.19019219893721
    },
    {
      "type": "training",
      "description": "Training step 2913",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:24",
      "total_flops_so_far": 6.921393952443046e+16,
      "budget_used_percent": 69.21393952443047
    },
    {
      "type": "training",
      "description": "Training step 2914",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:26",
      "total_flops_so_far": 6.923768684992371e+16,
      "budget_used_percent": 69.2376868499237
    },
    {
      "type": "training",
      "description": "Training step 2915",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:27",
      "total_flops_so_far": 6.926143417541696e+16,
      "budget_used_percent": 69.26143417541695
    },
    {
      "type": "training",
      "description": "Training step 2916",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:29",
      "total_flops_so_far": 6.928518150091021e+16,
      "budget_used_percent": 69.28518150091021
    },
    {
      "type": "training",
      "description": "Training step 2917",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:30",
      "total_flops_so_far": 6.930892882640346e+16,
      "budget_used_percent": 69.30892882640346
    },
    {
      "type": "training",
      "description": "Training step 2918",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:31",
      "total_flops_so_far": 6.93326761518967e+16,
      "budget_used_percent": 69.3326761518967
    },
    {
      "type": "training",
      "description": "Training step 2919",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:33",
      "total_flops_so_far": 6.935642347738995e+16,
      "budget_used_percent": 69.35642347738995
    },
    {
      "type": "training",
      "description": "Training step 2920",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:34",
      "total_flops_so_far": 6.93801708028832e+16,
      "budget_used_percent": 69.3801708028832
    },
    {
      "type": "training",
      "description": "Training step 2921",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:35",
      "total_flops_so_far": 6.940391812837645e+16,
      "budget_used_percent": 69.40391812837645
    },
    {
      "type": "training",
      "description": "Training step 2922",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:37",
      "total_flops_so_far": 6.94276654538697e+16,
      "budget_used_percent": 69.4276654538697
    },
    {
      "type": "training",
      "description": "Training step 2923",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:38",
      "total_flops_so_far": 6.945141277936294e+16,
      "budget_used_percent": 69.45141277936294
    },
    {
      "type": "training",
      "description": "Training step 2924",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:39",
      "total_flops_so_far": 6.947516010485619e+16,
      "budget_used_percent": 69.47516010485619
    },
    {
      "type": "training",
      "description": "Training step 2925",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:41",
      "total_flops_so_far": 6.949890743034944e+16,
      "budget_used_percent": 69.49890743034945
    },
    {
      "type": "training",
      "description": "Training step 2926",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:42",
      "total_flops_so_far": 6.952265475584269e+16,
      "budget_used_percent": 69.52265475584268
    },
    {
      "type": "training",
      "description": "Training step 2927",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:43",
      "total_flops_so_far": 6.954640208133594e+16,
      "budget_used_percent": 69.54640208133594
    },
    {
      "type": "training",
      "description": "Training step 2928",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:45",
      "total_flops_so_far": 6.957014940682918e+16,
      "budget_used_percent": 69.57014940682919
    },
    {
      "type": "training",
      "description": "Training step 2929",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:46",
      "total_flops_so_far": 6.959389673232243e+16,
      "budget_used_percent": 69.59389673232243
    },
    {
      "type": "training",
      "description": "Training step 2930",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:47",
      "total_flops_so_far": 6.961764405781568e+16,
      "budget_used_percent": 69.61764405781568
    },
    {
      "type": "training",
      "description": "Training step 2931",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:49",
      "total_flops_so_far": 6.964139138330893e+16,
      "budget_used_percent": 69.64139138330893
    },
    {
      "type": "training",
      "description": "Training step 2932",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:50",
      "total_flops_so_far": 6.966513870880218e+16,
      "budget_used_percent": 69.66513870880217
    },
    {
      "type": "training",
      "description": "Training step 2933",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:51",
      "total_flops_so_far": 6.968888603429542e+16,
      "budget_used_percent": 69.68888603429542
    },
    {
      "type": "training",
      "description": "Training step 2934",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:53",
      "total_flops_so_far": 6.971263335978867e+16,
      "budget_used_percent": 69.71263335978867
    },
    {
      "type": "training",
      "description": "Training step 2935",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:54",
      "total_flops_so_far": 6.973638068528192e+16,
      "budget_used_percent": 69.73638068528192
    },
    {
      "type": "training",
      "description": "Training step 2936",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:55",
      "total_flops_so_far": 6.976012801077517e+16,
      "budget_used_percent": 69.76012801077516
    },
    {
      "type": "training",
      "description": "Training step 2937",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:57",
      "total_flops_so_far": 6.978387533626842e+16,
      "budget_used_percent": 69.78387533626842
    },
    {
      "type": "training",
      "description": "Training step 2938",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:58",
      "total_flops_so_far": 6.980762266176166e+16,
      "budget_used_percent": 69.80762266176166
    },
    {
      "type": "training",
      "description": "Training step 2939",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:48:59",
      "total_flops_so_far": 6.983136998725491e+16,
      "budget_used_percent": 69.83136998725492
    },
    {
      "type": "training",
      "description": "Training step 2940",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:01",
      "total_flops_so_far": 6.985511731274816e+16,
      "budget_used_percent": 69.85511731274816
    },
    {
      "type": "training",
      "description": "Training step 2941",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:02",
      "total_flops_so_far": 6.987886463824141e+16,
      "budget_used_percent": 69.87886463824141
    },
    {
      "type": "training",
      "description": "Training step 2942",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:03",
      "total_flops_so_far": 6.990261196373466e+16,
      "budget_used_percent": 69.90261196373466
    },
    {
      "type": "training",
      "description": "Training step 2943",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:05",
      "total_flops_so_far": 6.99263592892279e+16,
      "budget_used_percent": 69.9263592892279
    },
    {
      "type": "training",
      "description": "Training step 2944",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:06",
      "total_flops_so_far": 6.995010661472115e+16,
      "budget_used_percent": 69.95010661472115
    },
    {
      "type": "training",
      "description": "Training step 2945",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:08",
      "total_flops_so_far": 6.99738539402144e+16,
      "budget_used_percent": 69.9738539402144
    },
    {
      "type": "training",
      "description": "Training step 2946",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:09",
      "total_flops_so_far": 6.999760126570765e+16,
      "budget_used_percent": 69.99760126570764
    },
    {
      "type": "training",
      "description": "Training step 2947",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:10",
      "total_flops_so_far": 7.00213485912009e+16,
      "budget_used_percent": 70.02134859120089
    },
    {
      "type": "training",
      "description": "Training step 2948",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:12",
      "total_flops_so_far": 7.004509591669414e+16,
      "budget_used_percent": 70.04509591669415
    },
    {
      "type": "training",
      "description": "Training step 2949",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:13",
      "total_flops_so_far": 7.006884324218739e+16,
      "budget_used_percent": 70.0688432421874
    },
    {
      "type": "training",
      "description": "Training step 2950",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:14",
      "total_flops_so_far": 7.009259056768064e+16,
      "budget_used_percent": 70.09259056768063
    },
    {
      "type": "training",
      "description": "Training step 2951",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:16",
      "total_flops_so_far": 7.011633789317389e+16,
      "budget_used_percent": 70.11633789317389
    },
    {
      "type": "training",
      "description": "Training step 2952",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:17",
      "total_flops_so_far": 7.014008521866714e+16,
      "budget_used_percent": 70.14008521866714
    },
    {
      "type": "training",
      "description": "Training step 2953",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:18",
      "total_flops_so_far": 7.016383254416038e+16,
      "budget_used_percent": 70.16383254416039
    },
    {
      "type": "training",
      "description": "Training step 2954",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:20",
      "total_flops_so_far": 7.018757986965363e+16,
      "budget_used_percent": 70.18757986965363
    },
    {
      "type": "training",
      "description": "Training step 2955",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:21",
      "total_flops_so_far": 7.021132719514688e+16,
      "budget_used_percent": 70.21132719514688
    },
    {
      "type": "training",
      "description": "Training step 2956",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:22",
      "total_flops_so_far": 7.023507452064013e+16,
      "budget_used_percent": 70.23507452064013
    },
    {
      "type": "training",
      "description": "Training step 2957",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:24",
      "total_flops_so_far": 7.025882184613338e+16,
      "budget_used_percent": 70.25882184613337
    },
    {
      "type": "training",
      "description": "Training step 2958",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:25",
      "total_flops_so_far": 7.028256917162662e+16,
      "budget_used_percent": 70.28256917162662
    },
    {
      "type": "training",
      "description": "Training step 2959",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:26",
      "total_flops_so_far": 7.030631649711987e+16,
      "budget_used_percent": 70.30631649711987
    },
    {
      "type": "training",
      "description": "Training step 2960",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:28",
      "total_flops_so_far": 7.033006382261312e+16,
      "budget_used_percent": 70.33006382261313
    },
    {
      "type": "training",
      "description": "Training step 2961",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:29",
      "total_flops_so_far": 7.035381114810637e+16,
      "budget_used_percent": 70.35381114810637
    },
    {
      "type": "training",
      "description": "Training step 2962",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:31",
      "total_flops_so_far": 7.037755847359962e+16,
      "budget_used_percent": 70.3775584735996
    },
    {
      "type": "training",
      "description": "Training step 2963",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:32",
      "total_flops_so_far": 7.040130579909286e+16,
      "budget_used_percent": 70.40130579909287
    },
    {
      "type": "training",
      "description": "Training step 2964",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:33",
      "total_flops_so_far": 7.042505312458611e+16,
      "budget_used_percent": 70.42505312458611
    },
    {
      "type": "training",
      "description": "Training step 2965",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:35",
      "total_flops_so_far": 7.044880045007936e+16,
      "budget_used_percent": 70.44880045007936
    },
    {
      "type": "training",
      "description": "Training step 2966",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:36",
      "total_flops_so_far": 7.047254777557261e+16,
      "budget_used_percent": 70.47254777557261
    },
    {
      "type": "training",
      "description": "Training step 2967",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:37",
      "total_flops_so_far": 7.049629510106586e+16,
      "budget_used_percent": 70.49629510106585
    },
    {
      "type": "training",
      "description": "Training step 2968",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:39",
      "total_flops_so_far": 7.05200424265591e+16,
      "budget_used_percent": 70.5200424265591
    },
    {
      "type": "training",
      "description": "Training step 2969",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:40",
      "total_flops_so_far": 7.054378975205235e+16,
      "budget_used_percent": 70.54378975205236
    },
    {
      "type": "training",
      "description": "Training step 2970",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:41",
      "total_flops_so_far": 7.05675370775456e+16,
      "budget_used_percent": 70.5675370775456
    },
    {
      "type": "training",
      "description": "Training step 2971",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:43",
      "total_flops_so_far": 7.059128440303885e+16,
      "budget_used_percent": 70.59128440303884
    },
    {
      "type": "training",
      "description": "Training step 2972",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:44",
      "total_flops_so_far": 7.06150317285321e+16,
      "budget_used_percent": 70.6150317285321
    },
    {
      "type": "training",
      "description": "Training step 2973",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:45",
      "total_flops_so_far": 7.063877905402534e+16,
      "budget_used_percent": 70.63877905402535
    },
    {
      "type": "training",
      "description": "Training step 2974",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:47",
      "total_flops_so_far": 7.066252637951859e+16,
      "budget_used_percent": 70.66252637951858
    },
    {
      "type": "training",
      "description": "Training step 2975",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:48",
      "total_flops_so_far": 7.068627370501184e+16,
      "budget_used_percent": 70.68627370501184
    },
    {
      "type": "training",
      "description": "Training step 2976",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:49",
      "total_flops_so_far": 7.071002103050509e+16,
      "budget_used_percent": 70.71002103050509
    },
    {
      "type": "training",
      "description": "Training step 2977",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:51",
      "total_flops_so_far": 7.073376835599834e+16,
      "budget_used_percent": 70.73376835599834
    },
    {
      "type": "training",
      "description": "Training step 2978",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:52",
      "total_flops_so_far": 7.075751568149158e+16,
      "budget_used_percent": 70.75751568149158
    },
    {
      "type": "training",
      "description": "Training step 2979",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:53",
      "total_flops_so_far": 7.078126300698483e+16,
      "budget_used_percent": 70.78126300698483
    },
    {
      "type": "training",
      "description": "Training step 2980",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:55",
      "total_flops_so_far": 7.080501033247808e+16,
      "budget_used_percent": 70.80501033247808
    },
    {
      "type": "training",
      "description": "Training step 2981",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:56",
      "total_flops_so_far": 7.082875765797133e+16,
      "budget_used_percent": 70.82875765797134
    },
    {
      "type": "training",
      "description": "Training step 2982",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:57",
      "total_flops_so_far": 7.085250498346458e+16,
      "budget_used_percent": 70.85250498346458
    },
    {
      "type": "training",
      "description": "Training step 2983",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:49:59",
      "total_flops_so_far": 7.087625230895782e+16,
      "budget_used_percent": 70.87625230895782
    },
    {
      "type": "training",
      "description": "Training step 2984",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:50:00",
      "total_flops_so_far": 7.089999963445107e+16,
      "budget_used_percent": 70.89999963445108
    },
    {
      "type": "training",
      "description": "Training step 2985",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:50:02",
      "total_flops_so_far": 7.092374695994432e+16,
      "budget_used_percent": 70.92374695994432
    },
    {
      "type": "training",
      "description": "Training step 2986",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:50:03",
      "total_flops_so_far": 7.094749428543757e+16,
      "budget_used_percent": 70.94749428543757
    },
    {
      "type": "training",
      "description": "Training step 2987",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:50:04",
      "total_flops_so_far": 7.097124161093082e+16,
      "budget_used_percent": 70.97124161093082
    },
    {
      "type": "training",
      "description": "Training step 2988",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:50:10",
      "total_flops_so_far": 7.099498893642406e+16,
      "budget_used_percent": 70.99498893642406
    },
    {
      "type": "training",
      "description": "Training step 2989",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:50:12",
      "total_flops_so_far": 7.101873626191731e+16,
      "budget_used_percent": 71.01873626191731
    },
    {
      "type": "training",
      "description": "Training step 2990",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:50:13",
      "total_flops_so_far": 7.104248358741056e+16,
      "budget_used_percent": 71.04248358741057
    },
    {
      "type": "training",
      "description": "Training step 2991",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:50:14",
      "total_flops_so_far": 7.106623091290381e+16,
      "budget_used_percent": 71.0662309129038
    },
    {
      "type": "training",
      "description": "Training step 2992",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:50:17",
      "total_flops_so_far": 7.108997823839706e+16,
      "budget_used_percent": 71.08997823839705
    },
    {
      "type": "training",
      "description": "Training step 2993",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:50:18",
      "total_flops_so_far": 7.11137255638903e+16,
      "budget_used_percent": 71.11372556389031
    },
    {
      "type": "training",
      "description": "Training step 2994",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:50:19",
      "total_flops_so_far": 7.113747288938355e+16,
      "budget_used_percent": 71.13747288938356
    },
    {
      "type": "training",
      "description": "Training step 2995",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:50:21",
      "total_flops_so_far": 7.11612202148768e+16,
      "budget_used_percent": 71.16122021487679
    },
    {
      "type": "training",
      "description": "Training step 2996",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:50:22",
      "total_flops_so_far": 7.118496754037005e+16,
      "budget_used_percent": 71.18496754037005
    },
    {
      "type": "training",
      "description": "Training step 2997",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:50:23",
      "total_flops_so_far": 7.12087148658633e+16,
      "budget_used_percent": 71.2087148658633
    },
    {
      "type": "training",
      "description": "Training step 2998",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:50:25",
      "total_flops_so_far": 7.123246219135654e+16,
      "budget_used_percent": 71.23246219135655
    },
    {
      "type": "training",
      "description": "Training step 2999",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:50:26",
      "total_flops_so_far": 7.125620951684979e+16,
      "budget_used_percent": 71.2562095168498
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710078789056.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:50:32",
      "total_flops_so_far": 7.125691959563885e+16,
      "budget_used_percent": 71.25691959563885
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 713780608688.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:50:37",
      "total_flops_so_far": 7.125763337624754e+16,
      "budget_used_percent": 71.25763337624753
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711929338680.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:50:42",
      "total_flops_so_far": 7.1258345305586216e+16,
      "budget_used_percent": 71.25834530558622
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710078789056.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:50:47",
      "total_flops_so_far": 7.125905538437527e+16,
      "budget_used_percent": 71.25905538437527
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712854883636.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:50:53",
      "total_flops_so_far": 7.125976823925891e+16,
      "budget_used_percent": 71.25976823925892
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 5",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710078789056.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:50:58",
      "total_flops_so_far": 7.126047831804797e+16,
      "budget_used_percent": 71.26047831804797
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 6",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711929338680.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:03",
      "total_flops_so_far": 7.126119024738665e+16,
      "budget_used_percent": 71.26119024738665
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 7",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711929338680.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:08",
      "total_flops_so_far": 7.126190217672533e+16,
      "budget_used_percent": 71.26190217672533
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 8",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711929338680.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:13",
      "total_flops_so_far": 7.126261410606401e+16,
      "budget_used_percent": 71.262614106064
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 9",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711929338680.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:18",
      "total_flops_so_far": 7.126332603540269e+16,
      "budget_used_percent": 71.26332603540268
    },
    {
      "type": "training",
      "description": "Training step 3000",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:18",
      "total_flops_so_far": 7.128707336089594e+16,
      "budget_used_percent": 71.28707336089593
    },
    {
      "type": "training",
      "description": "Training step 3001",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:19",
      "total_flops_so_far": 7.131082068638918e+16,
      "budget_used_percent": 71.31082068638919
    },
    {
      "type": "training",
      "description": "Training step 3002",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:21",
      "total_flops_so_far": 7.133456801188243e+16,
      "budget_used_percent": 71.33456801188242
    },
    {
      "type": "training",
      "description": "Training step 3003",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:22",
      "total_flops_so_far": 7.135831533737568e+16,
      "budget_used_percent": 71.35831533737567
    },
    {
      "type": "training",
      "description": "Training step 3004",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:23",
      "total_flops_so_far": 7.138206266286893e+16,
      "budget_used_percent": 71.38206266286893
    },
    {
      "type": "training",
      "description": "Training step 3005",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:25",
      "total_flops_so_far": 7.140580998836218e+16,
      "budget_used_percent": 71.40580998836218
    },
    {
      "type": "training",
      "description": "Training step 3006",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:26",
      "total_flops_so_far": 7.142955731385542e+16,
      "budget_used_percent": 71.42955731385541
    },
    {
      "type": "training",
      "description": "Training step 3007",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:28",
      "total_flops_so_far": 7.145330463934867e+16,
      "budget_used_percent": 71.45330463934867
    },
    {
      "type": "training",
      "description": "Training step 3008",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:29",
      "total_flops_so_far": 7.147705196484192e+16,
      "budget_used_percent": 71.47705196484192
    },
    {
      "type": "training",
      "description": "Training step 3009",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:30",
      "total_flops_so_far": 7.150079929033517e+16,
      "budget_used_percent": 71.50079929033517
    },
    {
      "type": "training",
      "description": "Training step 3010",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:32",
      "total_flops_so_far": 7.152454661582842e+16,
      "budget_used_percent": 71.52454661582843
    },
    {
      "type": "training",
      "description": "Training step 3011",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:33",
      "total_flops_so_far": 7.154829394132166e+16,
      "budget_used_percent": 71.54829394132166
    },
    {
      "type": "training",
      "description": "Training step 3012",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:34",
      "total_flops_so_far": 7.157204126681491e+16,
      "budget_used_percent": 71.5720412668149
    },
    {
      "type": "training",
      "description": "Training step 3013",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:36",
      "total_flops_so_far": 7.159578859230816e+16,
      "budget_used_percent": 71.59578859230817
    },
    {
      "type": "training",
      "description": "Training step 3014",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:37",
      "total_flops_so_far": 7.161953591780141e+16,
      "budget_used_percent": 71.61953591780141
    },
    {
      "type": "training",
      "description": "Training step 3015",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:38",
      "total_flops_so_far": 7.164328324329466e+16,
      "budget_used_percent": 71.64328324329465
    },
    {
      "type": "training",
      "description": "Training step 3016",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:40",
      "total_flops_so_far": 7.16670305687879e+16,
      "budget_used_percent": 71.66703056878791
    },
    {
      "type": "training",
      "description": "Training step 3017",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:41",
      "total_flops_so_far": 7.169077789428115e+16,
      "budget_used_percent": 71.69077789428115
    },
    {
      "type": "training",
      "description": "Training step 3018",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:42",
      "total_flops_so_far": 7.17145252197744e+16,
      "budget_used_percent": 71.7145252197744
    },
    {
      "type": "training",
      "description": "Training step 3019",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:44",
      "total_flops_so_far": 7.173827254526765e+16,
      "budget_used_percent": 71.73827254526765
    },
    {
      "type": "training",
      "description": "Training step 3020",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:45",
      "total_flops_so_far": 7.17620198707609e+16,
      "budget_used_percent": 71.7620198707609
    },
    {
      "type": "training",
      "description": "Training step 3021",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:46",
      "total_flops_so_far": 7.178576719625414e+16,
      "budget_used_percent": 71.78576719625414
    },
    {
      "type": "training",
      "description": "Training step 3022",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:48",
      "total_flops_so_far": 7.180951452174739e+16,
      "budget_used_percent": 71.8095145217474
    },
    {
      "type": "training",
      "description": "Training step 3023",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:49",
      "total_flops_so_far": 7.183326184724064e+16,
      "budget_used_percent": 71.83326184724064
    },
    {
      "type": "training",
      "description": "Training step 3024",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:51",
      "total_flops_so_far": 7.185700917273389e+16,
      "budget_used_percent": 71.85700917273388
    },
    {
      "type": "training",
      "description": "Training step 3025",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:52",
      "total_flops_so_far": 7.188075649822714e+16,
      "budget_used_percent": 71.88075649822714
    },
    {
      "type": "training",
      "description": "Training step 3026",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:53",
      "total_flops_so_far": 7.190450382372038e+16,
      "budget_used_percent": 71.90450382372039
    },
    {
      "type": "training",
      "description": "Training step 3027",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:55",
      "total_flops_so_far": 7.192825114921363e+16,
      "budget_used_percent": 71.92825114921362
    },
    {
      "type": "training",
      "description": "Training step 3028",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:56",
      "total_flops_so_far": 7.195199847470688e+16,
      "budget_used_percent": 71.95199847470688
    },
    {
      "type": "training",
      "description": "Training step 3029",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:57",
      "total_flops_so_far": 7.197574580020013e+16,
      "budget_used_percent": 71.97574580020013
    },
    {
      "type": "training",
      "description": "Training step 3030",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:51:59",
      "total_flops_so_far": 7.199949312569338e+16,
      "budget_used_percent": 71.99949312569338
    },
    {
      "type": "training",
      "description": "Training step 3031",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:00",
      "total_flops_so_far": 7.202324045118662e+16,
      "budget_used_percent": 72.02324045118662
    },
    {
      "type": "training",
      "description": "Training step 3032",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:01",
      "total_flops_so_far": 7.204698777667987e+16,
      "budget_used_percent": 72.04698777667987
    },
    {
      "type": "training",
      "description": "Training step 3033",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:03",
      "total_flops_so_far": 7.207073510217312e+16,
      "budget_used_percent": 72.07073510217312
    },
    {
      "type": "training",
      "description": "Training step 3034",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:04",
      "total_flops_so_far": 7.209448242766637e+16,
      "budget_used_percent": 72.09448242766638
    },
    {
      "type": "training",
      "description": "Training step 3035",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:05",
      "total_flops_so_far": 7.211822975315962e+16,
      "budget_used_percent": 72.11822975315961
    },
    {
      "type": "training",
      "description": "Training step 3036",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:06",
      "total_flops_so_far": 7.214197707865286e+16,
      "budget_used_percent": 72.14197707865286
    },
    {
      "type": "training",
      "description": "Training step 3037",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:07",
      "total_flops_so_far": 7.216572440414611e+16,
      "budget_used_percent": 72.16572440414612
    },
    {
      "type": "training",
      "description": "Training step 3038",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:09",
      "total_flops_so_far": 7.218947172963936e+16,
      "budget_used_percent": 72.18947172963937
    },
    {
      "type": "training",
      "description": "Training step 3039",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:10",
      "total_flops_so_far": 7.22132190551326e+16,
      "budget_used_percent": 72.21321905513261
    },
    {
      "type": "training",
      "description": "Training step 3040",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:11",
      "total_flops_so_far": 7.223696638062586e+16,
      "budget_used_percent": 72.23696638062586
    },
    {
      "type": "training",
      "description": "Training step 3041",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:13",
      "total_flops_so_far": 7.22607137061191e+16,
      "budget_used_percent": 72.2607137061191
    },
    {
      "type": "training",
      "description": "Training step 3042",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:14",
      "total_flops_so_far": 7.228446103161235e+16,
      "budget_used_percent": 72.28446103161235
    },
    {
      "type": "training",
      "description": "Training step 3043",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:15",
      "total_flops_so_far": 7.23082083571056e+16,
      "budget_used_percent": 72.3082083571056
    },
    {
      "type": "training",
      "description": "Training step 3044",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:17",
      "total_flops_so_far": 7.233195568259885e+16,
      "budget_used_percent": 72.33195568259885
    },
    {
      "type": "training",
      "description": "Training step 3045",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:18",
      "total_flops_so_far": 7.23557030080921e+16,
      "budget_used_percent": 72.35570300809209
    },
    {
      "type": "training",
      "description": "Training step 3046",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:19",
      "total_flops_so_far": 7.237945033358534e+16,
      "budget_used_percent": 72.37945033358535
    },
    {
      "type": "training",
      "description": "Training step 3047",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:21",
      "total_flops_so_far": 7.24031976590786e+16,
      "budget_used_percent": 72.40319765907859
    },
    {
      "type": "training",
      "description": "Training step 3048",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:22",
      "total_flops_so_far": 7.242694498457184e+16,
      "budget_used_percent": 72.42694498457183
    },
    {
      "type": "training",
      "description": "Training step 3049",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:23",
      "total_flops_so_far": 7.245069231006509e+16,
      "budget_used_percent": 72.4506923100651
    },
    {
      "type": "training",
      "description": "Training step 3050",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:25",
      "total_flops_so_far": 7.247443963555834e+16,
      "budget_used_percent": 72.47443963555834
    },
    {
      "type": "training",
      "description": "Training step 3051",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:26",
      "total_flops_so_far": 7.249818696105158e+16,
      "budget_used_percent": 72.49818696105159
    },
    {
      "type": "training",
      "description": "Training step 3052",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:27",
      "total_flops_so_far": 7.252193428654483e+16,
      "budget_used_percent": 72.52193428654483
    },
    {
      "type": "training",
      "description": "Training step 3053",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:29",
      "total_flops_so_far": 7.254568161203808e+16,
      "budget_used_percent": 72.54568161203808
    },
    {
      "type": "training",
      "description": "Training step 3054",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:30",
      "total_flops_so_far": 7.256942893753133e+16,
      "budget_used_percent": 72.56942893753133
    },
    {
      "type": "training",
      "description": "Training step 3055",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:31",
      "total_flops_so_far": 7.259317626302458e+16,
      "budget_used_percent": 72.59317626302457
    },
    {
      "type": "training",
      "description": "Training step 3056",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:33",
      "total_flops_so_far": 7.261692358851782e+16,
      "budget_used_percent": 72.61692358851782
    },
    {
      "type": "training",
      "description": "Training step 3057",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:34",
      "total_flops_so_far": 7.264067091401107e+16,
      "budget_used_percent": 72.64067091401107
    },
    {
      "type": "training",
      "description": "Training step 3058",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:35",
      "total_flops_so_far": 7.266441823950432e+16,
      "budget_used_percent": 72.66441823950433
    },
    {
      "type": "training",
      "description": "Training step 3059",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:37",
      "total_flops_so_far": 7.268816556499757e+16,
      "budget_used_percent": 72.68816556499756
    },
    {
      "type": "training",
      "description": "Training step 3060",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:38",
      "total_flops_so_far": 7.271191289049082e+16,
      "budget_used_percent": 72.71191289049082
    },
    {
      "type": "training",
      "description": "Training step 3061",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:40",
      "total_flops_so_far": 7.273566021598406e+16,
      "budget_used_percent": 72.73566021598407
    },
    {
      "type": "training",
      "description": "Training step 3062",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:41",
      "total_flops_so_far": 7.275940754147731e+16,
      "budget_used_percent": 72.75940754147732
    },
    {
      "type": "training",
      "description": "Training step 3063",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:42",
      "total_flops_so_far": 7.278315486697056e+16,
      "budget_used_percent": 72.78315486697056
    },
    {
      "type": "training",
      "description": "Training step 3064",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:44",
      "total_flops_so_far": 7.28069021924638e+16,
      "budget_used_percent": 72.80690219246381
    },
    {
      "type": "training",
      "description": "Training step 3065",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:45",
      "total_flops_so_far": 7.283064951795706e+16,
      "budget_used_percent": 72.83064951795706
    },
    {
      "type": "training",
      "description": "Training step 3066",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:46",
      "total_flops_so_far": 7.28543968434503e+16,
      "budget_used_percent": 72.8543968434503
    },
    {
      "type": "training",
      "description": "Training step 3067",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:48",
      "total_flops_so_far": 7.287814416894355e+16,
      "budget_used_percent": 72.87814416894355
    },
    {
      "type": "training",
      "description": "Training step 3068",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:49",
      "total_flops_so_far": 7.29018914944368e+16,
      "budget_used_percent": 72.9018914944368
    },
    {
      "type": "training",
      "description": "Training step 3069",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:50",
      "total_flops_so_far": 7.292563881993005e+16,
      "budget_used_percent": 72.92563881993004
    },
    {
      "type": "training",
      "description": "Training step 3070",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:52",
      "total_flops_so_far": 7.29493861454233e+16,
      "budget_used_percent": 72.9493861454233
    },
    {
      "type": "training",
      "description": "Training step 3071",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:53",
      "total_flops_so_far": 7.297313347091654e+16,
      "budget_used_percent": 72.97313347091654
    },
    {
      "type": "training",
      "description": "Training step 3072",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:54",
      "total_flops_so_far": 7.29968807964098e+16,
      "budget_used_percent": 72.9968807964098
    },
    {
      "type": "training",
      "description": "Training step 3073",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:56",
      "total_flops_so_far": 7.302062812190304e+16,
      "budget_used_percent": 73.02062812190304
    },
    {
      "type": "training",
      "description": "Training step 3074",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:57",
      "total_flops_so_far": 7.304437544739629e+16,
      "budget_used_percent": 73.04437544739629
    },
    {
      "type": "training",
      "description": "Training step 3075",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:52:58",
      "total_flops_so_far": 7.306812277288954e+16,
      "budget_used_percent": 73.06812277288954
    },
    {
      "type": "training",
      "description": "Training step 3076",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:00",
      "total_flops_so_far": 7.309187009838278e+16,
      "budget_used_percent": 73.09187009838278
    },
    {
      "type": "training",
      "description": "Training step 3077",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:01",
      "total_flops_so_far": 7.311561742387603e+16,
      "budget_used_percent": 73.11561742387603
    },
    {
      "type": "training",
      "description": "Training step 3078",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:03",
      "total_flops_so_far": 7.313936474936928e+16,
      "budget_used_percent": 73.13936474936928
    },
    {
      "type": "training",
      "description": "Training step 3079",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:04",
      "total_flops_so_far": 7.316311207486253e+16,
      "budget_used_percent": 73.16311207486252
    },
    {
      "type": "training",
      "description": "Training step 3080",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:05",
      "total_flops_so_far": 7.318685940035578e+16,
      "budget_used_percent": 73.18685940035577
    },
    {
      "type": "training",
      "description": "Training step 3081",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:07",
      "total_flops_so_far": 7.321060672584902e+16,
      "budget_used_percent": 73.21060672584903
    },
    {
      "type": "training",
      "description": "Training step 3082",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:08",
      "total_flops_so_far": 7.323435405134227e+16,
      "budget_used_percent": 73.23435405134228
    },
    {
      "type": "training",
      "description": "Training step 3083",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:09",
      "total_flops_so_far": 7.325810137683552e+16,
      "budget_used_percent": 73.25810137683551
    },
    {
      "type": "training",
      "description": "Training step 3084",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:11",
      "total_flops_so_far": 7.328184870232877e+16,
      "budget_used_percent": 73.28184870232877
    },
    {
      "type": "training",
      "description": "Training step 3085",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:12",
      "total_flops_so_far": 7.330559602782202e+16,
      "budget_used_percent": 73.30559602782202
    },
    {
      "type": "training",
      "description": "Training step 3086",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:13",
      "total_flops_so_far": 7.332934335331526e+16,
      "budget_used_percent": 73.32934335331527
    },
    {
      "type": "training",
      "description": "Training step 3087",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:15",
      "total_flops_so_far": 7.335309067880851e+16,
      "budget_used_percent": 73.35309067880851
    },
    {
      "type": "training",
      "description": "Training step 3088",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:16",
      "total_flops_so_far": 7.337683800430176e+16,
      "budget_used_percent": 73.37683800430176
    },
    {
      "type": "training",
      "description": "Training step 3089",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:18",
      "total_flops_so_far": 7.3400585329795e+16,
      "budget_used_percent": 73.400585329795
    },
    {
      "type": "training",
      "description": "Training step 3090",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:19",
      "total_flops_so_far": 7.342433265528826e+16,
      "budget_used_percent": 73.42433265528825
    },
    {
      "type": "training",
      "description": "Training step 3091",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:20",
      "total_flops_so_far": 7.34480799807815e+16,
      "budget_used_percent": 73.4480799807815
    },
    {
      "type": "training",
      "description": "Training step 3092",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:22",
      "total_flops_so_far": 7.347182730627475e+16,
      "budget_used_percent": 73.47182730627475
    },
    {
      "type": "training",
      "description": "Training step 3093",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:23",
      "total_flops_so_far": 7.3495574631768e+16,
      "budget_used_percent": 73.49557463176801
    },
    {
      "type": "training",
      "description": "Training step 3094",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:24",
      "total_flops_so_far": 7.351932195726125e+16,
      "budget_used_percent": 73.51932195726125
    },
    {
      "type": "training",
      "description": "Training step 3095",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:26",
      "total_flops_so_far": 7.35430692827545e+16,
      "budget_used_percent": 73.54306928275449
    },
    {
      "type": "training",
      "description": "Training step 3096",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:27",
      "total_flops_so_far": 7.356681660824774e+16,
      "budget_used_percent": 73.56681660824775
    },
    {
      "type": "training",
      "description": "Training step 3097",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:28",
      "total_flops_so_far": 7.3590563933741e+16,
      "budget_used_percent": 73.590563933741
    },
    {
      "type": "training",
      "description": "Training step 3098",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:30",
      "total_flops_so_far": 7.361431125923424e+16,
      "budget_used_percent": 73.61431125923424
    },
    {
      "type": "training",
      "description": "Training step 3099",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:31",
      "total_flops_so_far": 7.363805858472749e+16,
      "budget_used_percent": 73.63805858472749
    },
    {
      "type": "training",
      "description": "Training step 3100",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:32",
      "total_flops_so_far": 7.366180591022074e+16,
      "budget_used_percent": 73.66180591022074
    },
    {
      "type": "training",
      "description": "Training step 3101",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:34",
      "total_flops_so_far": 7.368555323571398e+16,
      "budget_used_percent": 73.68555323571398
    },
    {
      "type": "training",
      "description": "Training step 3102",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:35",
      "total_flops_so_far": 7.370930056120723e+16,
      "budget_used_percent": 73.70930056120724
    },
    {
      "type": "training",
      "description": "Training step 3103",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:36",
      "total_flops_so_far": 7.373304788670048e+16,
      "budget_used_percent": 73.73304788670048
    },
    {
      "type": "training",
      "description": "Training step 3104",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:38",
      "total_flops_so_far": 7.375679521219373e+16,
      "budget_used_percent": 73.75679521219372
    },
    {
      "type": "training",
      "description": "Training step 3105",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:39",
      "total_flops_so_far": 7.378054253768698e+16,
      "budget_used_percent": 73.78054253768698
    },
    {
      "type": "training",
      "description": "Training step 3106",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:40",
      "total_flops_so_far": 7.380428986318022e+16,
      "budget_used_percent": 73.80428986318023
    },
    {
      "type": "training",
      "description": "Training step 3107",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:42",
      "total_flops_so_far": 7.382803718867347e+16,
      "budget_used_percent": 73.82803718867346
    },
    {
      "type": "training",
      "description": "Training step 3108",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:43",
      "total_flops_so_far": 7.385178451416672e+16,
      "budget_used_percent": 73.85178451416672
    },
    {
      "type": "training",
      "description": "Training step 3109",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:45",
      "total_flops_so_far": 7.387553183965997e+16,
      "budget_used_percent": 73.87553183965997
    },
    {
      "type": "training",
      "description": "Training step 3110",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:46",
      "total_flops_so_far": 7.389927916515322e+16,
      "budget_used_percent": 73.89927916515322
    },
    {
      "type": "training",
      "description": "Training step 3111",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:47",
      "total_flops_so_far": 7.392302649064646e+16,
      "budget_used_percent": 73.92302649064646
    },
    {
      "type": "training",
      "description": "Training step 3112",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:49",
      "total_flops_so_far": 7.394677381613971e+16,
      "budget_used_percent": 73.94677381613971
    },
    {
      "type": "training",
      "description": "Training step 3113",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:50",
      "total_flops_so_far": 7.397052114163296e+16,
      "budget_used_percent": 73.97052114163296
    },
    {
      "type": "training",
      "description": "Training step 3114",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:51",
      "total_flops_so_far": 7.39942684671262e+16,
      "budget_used_percent": 73.99426846712622
    },
    {
      "type": "training",
      "description": "Training step 3115",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:53",
      "total_flops_so_far": 7.401801579261946e+16,
      "budget_used_percent": 74.01801579261945
    },
    {
      "type": "training",
      "description": "Training step 3116",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:54",
      "total_flops_so_far": 7.40417631181127e+16,
      "budget_used_percent": 74.0417631181127
    },
    {
      "type": "training",
      "description": "Training step 3117",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:55",
      "total_flops_so_far": 7.406551044360595e+16,
      "budget_used_percent": 74.06551044360596
    },
    {
      "type": "training",
      "description": "Training step 3118",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:57",
      "total_flops_so_far": 7.40892577690992e+16,
      "budget_used_percent": 74.0892577690992
    },
    {
      "type": "training",
      "description": "Training step 3119",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:58",
      "total_flops_so_far": 7.411300509459245e+16,
      "budget_used_percent": 74.11300509459244
    },
    {
      "type": "training",
      "description": "Training step 3120",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:53:59",
      "total_flops_so_far": 7.41367524200857e+16,
      "budget_used_percent": 74.1367524200857
    },
    {
      "type": "training",
      "description": "Training step 3121",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:01",
      "total_flops_so_far": 7.416049974557894e+16,
      "budget_used_percent": 74.16049974557895
    },
    {
      "type": "training",
      "description": "Training step 3122",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:02",
      "total_flops_so_far": 7.41842470710722e+16,
      "budget_used_percent": 74.18424707107219
    },
    {
      "type": "training",
      "description": "Training step 3123",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:03",
      "total_flops_so_far": 7.420799439656544e+16,
      "budget_used_percent": 74.20799439656544
    },
    {
      "type": "training",
      "description": "Training step 3124",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:05",
      "total_flops_so_far": 7.423174172205869e+16,
      "budget_used_percent": 74.23174172205869
    },
    {
      "type": "training",
      "description": "Training step 3125",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:06",
      "total_flops_so_far": 7.425548904755194e+16,
      "budget_used_percent": 74.25548904755193
    },
    {
      "type": "training",
      "description": "Training step 3126",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:08",
      "total_flops_so_far": 7.427923637304518e+16,
      "budget_used_percent": 74.2792363730452
    },
    {
      "type": "training",
      "description": "Training step 3127",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:09",
      "total_flops_so_far": 7.430298369853843e+16,
      "budget_used_percent": 74.30298369853843
    },
    {
      "type": "training",
      "description": "Training step 3128",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:10",
      "total_flops_so_far": 7.432673102403168e+16,
      "budget_used_percent": 74.32673102403167
    },
    {
      "type": "training",
      "description": "Training step 3129",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:12",
      "total_flops_so_far": 7.435047834952493e+16,
      "budget_used_percent": 74.35047834952493
    },
    {
      "type": "training",
      "description": "Training step 3130",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:13",
      "total_flops_so_far": 7.437422567501818e+16,
      "budget_used_percent": 74.37422567501818
    },
    {
      "type": "training",
      "description": "Training step 3131",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:14",
      "total_flops_so_far": 7.439797300051142e+16,
      "budget_used_percent": 74.39797300051141
    },
    {
      "type": "training",
      "description": "Training step 3132",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:16",
      "total_flops_so_far": 7.442172032600467e+16,
      "budget_used_percent": 74.42172032600467
    },
    {
      "type": "training",
      "description": "Training step 3133",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:17",
      "total_flops_so_far": 7.444546765149792e+16,
      "budget_used_percent": 74.44546765149792
    },
    {
      "type": "training",
      "description": "Training step 3134",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:18",
      "total_flops_so_far": 7.446921497699117e+16,
      "budget_used_percent": 74.46921497699117
    },
    {
      "type": "training",
      "description": "Training step 3135",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:20",
      "total_flops_so_far": 7.449296230248442e+16,
      "budget_used_percent": 74.49296230248441
    },
    {
      "type": "training",
      "description": "Training step 3136",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:21",
      "total_flops_so_far": 7.451670962797766e+16,
      "budget_used_percent": 74.51670962797766
    },
    {
      "type": "training",
      "description": "Training step 3137",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:22",
      "total_flops_so_far": 7.454045695347091e+16,
      "budget_used_percent": 74.54045695347091
    },
    {
      "type": "training",
      "description": "Training step 3138",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:24",
      "total_flops_so_far": 7.456420427896416e+16,
      "budget_used_percent": 74.56420427896417
    },
    {
      "type": "training",
      "description": "Training step 3139",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:25",
      "total_flops_so_far": 7.45879516044574e+16,
      "budget_used_percent": 74.5879516044574
    },
    {
      "type": "training",
      "description": "Training step 3140",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:27",
      "total_flops_so_far": 7.461169892995066e+16,
      "budget_used_percent": 74.61169892995065
    },
    {
      "type": "training",
      "description": "Training step 3141",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:28",
      "total_flops_so_far": 7.46354462554439e+16,
      "budget_used_percent": 74.63544625544391
    },
    {
      "type": "training",
      "description": "Training step 3142",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:29",
      "total_flops_so_far": 7.465919358093715e+16,
      "budget_used_percent": 74.65919358093716
    },
    {
      "type": "training",
      "description": "Training step 3143",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:31",
      "total_flops_so_far": 7.46829409064304e+16,
      "budget_used_percent": 74.68294090643039
    },
    {
      "type": "training",
      "description": "Training step 3144",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:32",
      "total_flops_so_far": 7.470668823192365e+16,
      "budget_used_percent": 74.70668823192365
    },
    {
      "type": "training",
      "description": "Training step 3145",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:33",
      "total_flops_so_far": 7.47304355574169e+16,
      "budget_used_percent": 74.7304355574169
    },
    {
      "type": "training",
      "description": "Training step 3146",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:35",
      "total_flops_so_far": 7.475418288291014e+16,
      "budget_used_percent": 74.75418288291014
    },
    {
      "type": "training",
      "description": "Training step 3147",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:36",
      "total_flops_so_far": 7.47779302084034e+16,
      "budget_used_percent": 74.77793020840339
    },
    {
      "type": "training",
      "description": "Training step 3148",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:37",
      "total_flops_so_far": 7.480167753389664e+16,
      "budget_used_percent": 74.80167753389664
    },
    {
      "type": "training",
      "description": "Training step 3149",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:39",
      "total_flops_so_far": 7.482542485938989e+16,
      "budget_used_percent": 74.82542485938988
    },
    {
      "type": "training",
      "description": "Training step 3150",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:40",
      "total_flops_so_far": 7.484917218488314e+16,
      "budget_used_percent": 74.84917218488314
    },
    {
      "type": "training",
      "description": "Training step 3151",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:41",
      "total_flops_so_far": 7.487291951037638e+16,
      "budget_used_percent": 74.87291951037638
    },
    {
      "type": "training",
      "description": "Training step 3152",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:43",
      "total_flops_so_far": 7.489666683586963e+16,
      "budget_used_percent": 74.89666683586962
    },
    {
      "type": "training",
      "description": "Training step 3153",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:44",
      "total_flops_so_far": 7.492041416136288e+16,
      "budget_used_percent": 74.92041416136288
    },
    {
      "type": "training",
      "description": "Training step 3154",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:45",
      "total_flops_so_far": 7.494416148685613e+16,
      "budget_used_percent": 74.94416148685613
    },
    {
      "type": "training",
      "description": "Training step 3155",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:47",
      "total_flops_so_far": 7.496790881234938e+16,
      "budget_used_percent": 74.96790881234936
    },
    {
      "type": "training",
      "description": "Training step 3156",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:48",
      "total_flops_so_far": 7.499165613784262e+16,
      "budget_used_percent": 74.99165613784263
    },
    {
      "type": "training",
      "description": "Training step 3157",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:50",
      "total_flops_so_far": 7.501540346333587e+16,
      "budget_used_percent": 75.01540346333587
    },
    {
      "type": "training",
      "description": "Training step 3158",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:51",
      "total_flops_so_far": 7.503915078882912e+16,
      "budget_used_percent": 75.03915078882912
    },
    {
      "type": "training",
      "description": "Training step 3159",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:52",
      "total_flops_so_far": 7.506289811432237e+16,
      "budget_used_percent": 75.06289811432237
    },
    {
      "type": "training",
      "description": "Training step 3160",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:54",
      "total_flops_so_far": 7.508664543981562e+16,
      "budget_used_percent": 75.08664543981561
    },
    {
      "type": "training",
      "description": "Training step 3161",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:55",
      "total_flops_so_far": 7.511039276530886e+16,
      "budget_used_percent": 75.11039276530886
    },
    {
      "type": "training",
      "description": "Training step 3162",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:56",
      "total_flops_so_far": 7.513414009080211e+16,
      "budget_used_percent": 75.13414009080212
    },
    {
      "type": "training",
      "description": "Training step 3163",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:58",
      "total_flops_so_far": 7.515788741629536e+16,
      "budget_used_percent": 75.15788741629535
    },
    {
      "type": "training",
      "description": "Training step 3164",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:54:59",
      "total_flops_so_far": 7.51816347417886e+16,
      "budget_used_percent": 75.1816347417886
    },
    {
      "type": "training",
      "description": "Training step 3165",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:00",
      "total_flops_so_far": 7.520538206728186e+16,
      "budget_used_percent": 75.20538206728186
    },
    {
      "type": "training",
      "description": "Training step 3166",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:02",
      "total_flops_so_far": 7.52291293927751e+16,
      "budget_used_percent": 75.2291293927751
    },
    {
      "type": "training",
      "description": "Training step 3167",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:03",
      "total_flops_so_far": 7.525287671826835e+16,
      "budget_used_percent": 75.25287671826834
    },
    {
      "type": "training",
      "description": "Training step 3168",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:03",
      "total_flops_so_far": 7.52766240437616e+16,
      "budget_used_percent": 75.2766240437616
    },
    {
      "type": "training",
      "description": "Training step 3169",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:05",
      "total_flops_so_far": 7.530037136925485e+16,
      "budget_used_percent": 75.30037136925485
    },
    {
      "type": "training",
      "description": "Training step 3170",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:06",
      "total_flops_so_far": 7.53241186947481e+16,
      "budget_used_percent": 75.3241186947481
    },
    {
      "type": "training",
      "description": "Training step 3171",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:07",
      "total_flops_so_far": 7.534786602024134e+16,
      "budget_used_percent": 75.34786602024134
    },
    {
      "type": "training",
      "description": "Training step 3172",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:09",
      "total_flops_so_far": 7.53716133457346e+16,
      "budget_used_percent": 75.37161334573459
    },
    {
      "type": "training",
      "description": "Training step 3173",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:10",
      "total_flops_so_far": 7.539536067122784e+16,
      "budget_used_percent": 75.39536067122783
    },
    {
      "type": "training",
      "description": "Training step 3174",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:12",
      "total_flops_so_far": 7.541910799672109e+16,
      "budget_used_percent": 75.4191079967211
    },
    {
      "type": "training",
      "description": "Training step 3175",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:13",
      "total_flops_so_far": 7.544285532221434e+16,
      "budget_used_percent": 75.44285532221433
    },
    {
      "type": "training",
      "description": "Training step 3176",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:14",
      "total_flops_so_far": 7.546660264770758e+16,
      "budget_used_percent": 75.46660264770757
    },
    {
      "type": "training",
      "description": "Training step 3177",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:16",
      "total_flops_so_far": 7.549034997320083e+16,
      "budget_used_percent": 75.49034997320084
    },
    {
      "type": "training",
      "description": "Training step 3178",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:17",
      "total_flops_so_far": 7.551409729869408e+16,
      "budget_used_percent": 75.51409729869408
    },
    {
      "type": "training",
      "description": "Training step 3179",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:18",
      "total_flops_so_far": 7.553784462418733e+16,
      "budget_used_percent": 75.53784462418733
    },
    {
      "type": "training",
      "description": "Training step 3180",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:20",
      "total_flops_so_far": 7.556159194968058e+16,
      "budget_used_percent": 75.56159194968058
    },
    {
      "type": "training",
      "description": "Training step 3181",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:21",
      "total_flops_so_far": 7.558533927517382e+16,
      "budget_used_percent": 75.58533927517382
    },
    {
      "type": "training",
      "description": "Training step 3182",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:22",
      "total_flops_so_far": 7.560908660066707e+16,
      "budget_used_percent": 75.60908660066707
    },
    {
      "type": "training",
      "description": "Training step 3183",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:24",
      "total_flops_so_far": 7.563283392616032e+16,
      "budget_used_percent": 75.63283392616033
    },
    {
      "type": "training",
      "description": "Training step 3184",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:25",
      "total_flops_so_far": 7.565658125165357e+16,
      "budget_used_percent": 75.65658125165356
    },
    {
      "type": "training",
      "description": "Training step 3185",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:27",
      "total_flops_so_far": 7.568032857714682e+16,
      "budget_used_percent": 75.68032857714681
    },
    {
      "type": "training",
      "description": "Training step 3186",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:28",
      "total_flops_so_far": 7.570407590264006e+16,
      "budget_used_percent": 75.70407590264007
    },
    {
      "type": "training",
      "description": "Training step 3187",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:29",
      "total_flops_so_far": 7.572782322813331e+16,
      "budget_used_percent": 75.72782322813332
    },
    {
      "type": "training",
      "description": "Training step 3188",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:31",
      "total_flops_so_far": 7.575157055362656e+16,
      "budget_used_percent": 75.75157055362655
    },
    {
      "type": "training",
      "description": "Training step 3189",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:32",
      "total_flops_so_far": 7.57753178791198e+16,
      "budget_used_percent": 75.77531787911981
    },
    {
      "type": "training",
      "description": "Training step 3190",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:34",
      "total_flops_so_far": 7.579906520461306e+16,
      "budget_used_percent": 75.79906520461306
    },
    {
      "type": "training",
      "description": "Training step 3191",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:35",
      "total_flops_so_far": 7.58228125301063e+16,
      "budget_used_percent": 75.8228125301063
    },
    {
      "type": "training",
      "description": "Training step 3192",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:36",
      "total_flops_so_far": 7.584655985559955e+16,
      "budget_used_percent": 75.84655985559955
    },
    {
      "type": "training",
      "description": "Training step 3193",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:38",
      "total_flops_so_far": 7.58703071810928e+16,
      "budget_used_percent": 75.8703071810928
    },
    {
      "type": "training",
      "description": "Training step 3194",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:39",
      "total_flops_so_far": 7.589405450658605e+16,
      "budget_used_percent": 75.89405450658604
    },
    {
      "type": "training",
      "description": "Training step 3195",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:40",
      "total_flops_so_far": 7.59178018320793e+16,
      "budget_used_percent": 75.9178018320793
    },
    {
      "type": "training",
      "description": "Training step 3196",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:42",
      "total_flops_so_far": 7.594154915757254e+16,
      "budget_used_percent": 75.94154915757254
    },
    {
      "type": "training",
      "description": "Training step 3197",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:43",
      "total_flops_so_far": 7.59652964830658e+16,
      "budget_used_percent": 75.96529648306579
    },
    {
      "type": "training",
      "description": "Training step 3198",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:44",
      "total_flops_so_far": 7.598904380855904e+16,
      "budget_used_percent": 75.98904380855905
    },
    {
      "type": "training",
      "description": "Training step 3199",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:46",
      "total_flops_so_far": 7.601279113405229e+16,
      "budget_used_percent": 76.01279113405229
    },
    {
      "type": "training",
      "description": "Training step 3200",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:47",
      "total_flops_so_far": 7.603653845954554e+16,
      "budget_used_percent": 76.03653845954554
    },
    {
      "type": "training",
      "description": "Training step 3201",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:48",
      "total_flops_so_far": 7.606028578503878e+16,
      "budget_used_percent": 76.06028578503879
    },
    {
      "type": "training",
      "description": "Training step 3202",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:50",
      "total_flops_so_far": 7.608403311053203e+16,
      "budget_used_percent": 76.08403311053203
    },
    {
      "type": "training",
      "description": "Training step 3203",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:51",
      "total_flops_so_far": 7.610778043602528e+16,
      "budget_used_percent": 76.10778043602528
    },
    {
      "type": "training",
      "description": "Training step 3204",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:52",
      "total_flops_so_far": 7.613152776151853e+16,
      "budget_used_percent": 76.13152776151853
    },
    {
      "type": "training",
      "description": "Training step 3205",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:54",
      "total_flops_so_far": 7.615527508701178e+16,
      "budget_used_percent": 76.15527508701177
    },
    {
      "type": "training",
      "description": "Training step 3206",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:55",
      "total_flops_so_far": 7.617902241250502e+16,
      "budget_used_percent": 76.17902241250502
    },
    {
      "type": "training",
      "description": "Training step 3207",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:56",
      "total_flops_so_far": 7.620276973799827e+16,
      "budget_used_percent": 76.20276973799828
    },
    {
      "type": "training",
      "description": "Training step 3208",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:58",
      "total_flops_so_far": 7.622651706349152e+16,
      "budget_used_percent": 76.22651706349151
    },
    {
      "type": "training",
      "description": "Training step 3209",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:55:59",
      "total_flops_so_far": 7.625026438898477e+16,
      "budget_used_percent": 76.25026438898476
    },
    {
      "type": "training",
      "description": "Training step 3210",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:01",
      "total_flops_so_far": 7.627401171447802e+16,
      "budget_used_percent": 76.27401171447802
    },
    {
      "type": "training",
      "description": "Training step 3211",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:02",
      "total_flops_so_far": 7.629775903997126e+16,
      "budget_used_percent": 76.29775903997127
    },
    {
      "type": "training",
      "description": "Training step 3212",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:03",
      "total_flops_so_far": 7.632150636546451e+16,
      "budget_used_percent": 76.32150636546451
    },
    {
      "type": "training",
      "description": "Training step 3213",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:05",
      "total_flops_so_far": 7.634525369095776e+16,
      "budget_used_percent": 76.34525369095776
    },
    {
      "type": "training",
      "description": "Training step 3214",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:06",
      "total_flops_so_far": 7.6369001016451e+16,
      "budget_used_percent": 76.36900101645101
    },
    {
      "type": "training",
      "description": "Training step 3215",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:07",
      "total_flops_so_far": 7.639274834194426e+16,
      "budget_used_percent": 76.39274834194426
    },
    {
      "type": "training",
      "description": "Training step 3216",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:09",
      "total_flops_so_far": 7.64164956674375e+16,
      "budget_used_percent": 76.4164956674375
    },
    {
      "type": "training",
      "description": "Training step 3217",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:10",
      "total_flops_so_far": 7.644024299293075e+16,
      "budget_used_percent": 76.44024299293075
    },
    {
      "type": "training",
      "description": "Training step 3218",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:11",
      "total_flops_so_far": 7.6463990318424e+16,
      "budget_used_percent": 76.463990318424
    },
    {
      "type": "training",
      "description": "Training step 3219",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:13",
      "total_flops_so_far": 7.648773764391725e+16,
      "budget_used_percent": 76.48773764391726
    },
    {
      "type": "training",
      "description": "Training step 3220",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:14",
      "total_flops_so_far": 7.65114849694105e+16,
      "budget_used_percent": 76.51148496941049
    },
    {
      "type": "training",
      "description": "Training step 3221",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:15",
      "total_flops_so_far": 7.653523229490374e+16,
      "budget_used_percent": 76.53523229490375
    },
    {
      "type": "training",
      "description": "Training step 3222",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:17",
      "total_flops_so_far": 7.6558979620397e+16,
      "budget_used_percent": 76.558979620397
    },
    {
      "type": "training",
      "description": "Training step 3223",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:18",
      "total_flops_so_far": 7.658272694589024e+16,
      "budget_used_percent": 76.58272694589024
    },
    {
      "type": "training",
      "description": "Training step 3224",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:19",
      "total_flops_so_far": 7.660647427138349e+16,
      "budget_used_percent": 76.60647427138349
    },
    {
      "type": "training",
      "description": "Training step 3225",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:21",
      "total_flops_so_far": 7.663022159687674e+16,
      "budget_used_percent": 76.63022159687674
    },
    {
      "type": "training",
      "description": "Training step 3226",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:22",
      "total_flops_so_far": 7.665396892236998e+16,
      "budget_used_percent": 76.65396892236998
    },
    {
      "type": "training",
      "description": "Training step 3227",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:24",
      "total_flops_so_far": 7.667771624786323e+16,
      "budget_used_percent": 76.67771624786323
    },
    {
      "type": "training",
      "description": "Training step 3228",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:25",
      "total_flops_so_far": 7.670146357335648e+16,
      "budget_used_percent": 76.70146357335648
    },
    {
      "type": "training",
      "description": "Training step 3229",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:26",
      "total_flops_so_far": 7.672521089884973e+16,
      "budget_used_percent": 76.72521089884972
    },
    {
      "type": "training",
      "description": "Training step 3230",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:28",
      "total_flops_so_far": 7.674895822434298e+16,
      "budget_used_percent": 76.74895822434297
    },
    {
      "type": "training",
      "description": "Training step 3231",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:29",
      "total_flops_so_far": 7.677270554983622e+16,
      "budget_used_percent": 76.77270554983623
    },
    {
      "type": "training",
      "description": "Training step 3232",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:30",
      "total_flops_so_far": 7.679645287532947e+16,
      "budget_used_percent": 76.79645287532946
    },
    {
      "type": "training",
      "description": "Training step 3233",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:32",
      "total_flops_so_far": 7.682020020082272e+16,
      "budget_used_percent": 76.82020020082273
    },
    {
      "type": "training",
      "description": "Training step 3234",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:33",
      "total_flops_so_far": 7.684394752631597e+16,
      "budget_used_percent": 76.84394752631597
    },
    {
      "type": "training",
      "description": "Training step 3235",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:34",
      "total_flops_so_far": 7.686769485180922e+16,
      "budget_used_percent": 76.86769485180922
    },
    {
      "type": "training",
      "description": "Training step 3236",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:36",
      "total_flops_so_far": 7.689144217730246e+16,
      "budget_used_percent": 76.89144217730247
    },
    {
      "type": "training",
      "description": "Training step 3237",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:37",
      "total_flops_so_far": 7.691518950279571e+16,
      "budget_used_percent": 76.91518950279571
    },
    {
      "type": "training",
      "description": "Training step 3238",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:39",
      "total_flops_so_far": 7.693893682828896e+16,
      "budget_used_percent": 76.93893682828896
    },
    {
      "type": "training",
      "description": "Training step 3239",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:40",
      "total_flops_so_far": 7.69626841537822e+16,
      "budget_used_percent": 76.9626841537822
    },
    {
      "type": "training",
      "description": "Training step 3240",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:41",
      "total_flops_so_far": 7.698643147927546e+16,
      "budget_used_percent": 76.98643147927545
    },
    {
      "type": "training",
      "description": "Training step 3241",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:43",
      "total_flops_so_far": 7.70101788047687e+16,
      "budget_used_percent": 77.0101788047687
    },
    {
      "type": "training",
      "description": "Training step 3242",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:44",
      "total_flops_so_far": 7.703392613026195e+16,
      "budget_used_percent": 77.03392613026196
    },
    {
      "type": "training",
      "description": "Training step 3243",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:45",
      "total_flops_so_far": 7.70576734557552e+16,
      "budget_used_percent": 77.05767345575521
    },
    {
      "type": "training",
      "description": "Training step 3244",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:47",
      "total_flops_so_far": 7.708142078124845e+16,
      "budget_used_percent": 77.08142078124844
    },
    {
      "type": "training",
      "description": "Training step 3245",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:48",
      "total_flops_so_far": 7.71051681067417e+16,
      "budget_used_percent": 77.1051681067417
    },
    {
      "type": "training",
      "description": "Training step 3246",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:49",
      "total_flops_so_far": 7.712891543223494e+16,
      "budget_used_percent": 77.12891543223495
    },
    {
      "type": "training",
      "description": "Training step 3247",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:51",
      "total_flops_so_far": 7.71526627577282e+16,
      "budget_used_percent": 77.1526627577282
    },
    {
      "type": "training",
      "description": "Training step 3248",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:52",
      "total_flops_so_far": 7.717641008322144e+16,
      "budget_used_percent": 77.17641008322144
    },
    {
      "type": "training",
      "description": "Training step 3249",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:53",
      "total_flops_so_far": 7.720015740871469e+16,
      "budget_used_percent": 77.20015740871469
    },
    {
      "type": "training",
      "description": "Training step 3250",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:55",
      "total_flops_so_far": 7.722390473420794e+16,
      "budget_used_percent": 77.22390473420793
    },
    {
      "type": "training",
      "description": "Training step 3251",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:56",
      "total_flops_so_far": 7.724765205970118e+16,
      "budget_used_percent": 77.24765205970118
    },
    {
      "type": "training",
      "description": "Training step 3252",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:57",
      "total_flops_so_far": 7.727139938519443e+16,
      "budget_used_percent": 77.27139938519443
    },
    {
      "type": "training",
      "description": "Training step 3253",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:56:59",
      "total_flops_so_far": 7.729514671068768e+16,
      "budget_used_percent": 77.29514671068767
    },
    {
      "type": "training",
      "description": "Training step 3254",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:00",
      "total_flops_so_far": 7.731889403618093e+16,
      "budget_used_percent": 77.31889403618094
    },
    {
      "type": "training",
      "description": "Training step 3255",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:02",
      "total_flops_so_far": 7.734264136167418e+16,
      "budget_used_percent": 77.34264136167418
    },
    {
      "type": "training",
      "description": "Training step 3256",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:03",
      "total_flops_so_far": 7.736638868716742e+16,
      "budget_used_percent": 77.36638868716742
    },
    {
      "type": "training",
      "description": "Training step 3257",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:04",
      "total_flops_so_far": 7.739013601266067e+16,
      "budget_used_percent": 77.39013601266068
    },
    {
      "type": "training",
      "description": "Training step 3258",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:06",
      "total_flops_so_far": 7.741388333815392e+16,
      "budget_used_percent": 77.41388333815392
    },
    {
      "type": "training",
      "description": "Training step 3259",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:07",
      "total_flops_so_far": 7.743763066364717e+16,
      "budget_used_percent": 77.43763066364717
    },
    {
      "type": "training",
      "description": "Training step 3260",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:08",
      "total_flops_so_far": 7.746137798914042e+16,
      "budget_used_percent": 77.46137798914042
    },
    {
      "type": "training",
      "description": "Training step 3261",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:10",
      "total_flops_so_far": 7.748512531463366e+16,
      "budget_used_percent": 77.48512531463366
    },
    {
      "type": "training",
      "description": "Training step 3262",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:11",
      "total_flops_so_far": 7.750887264012691e+16,
      "budget_used_percent": 77.50887264012691
    },
    {
      "type": "training",
      "description": "Training step 3263",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:12",
      "total_flops_so_far": 7.753261996562016e+16,
      "budget_used_percent": 77.53261996562017
    },
    {
      "type": "training",
      "description": "Training step 3264",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:14",
      "total_flops_so_far": 7.75563672911134e+16,
      "budget_used_percent": 77.5563672911134
    },
    {
      "type": "training",
      "description": "Training step 3265",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:15",
      "total_flops_so_far": 7.758011461660666e+16,
      "budget_used_percent": 77.58011461660665
    },
    {
      "type": "training",
      "description": "Training step 3266",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:16",
      "total_flops_so_far": 7.76038619420999e+16,
      "budget_used_percent": 77.60386194209991
    },
    {
      "type": "training",
      "description": "Training step 3267",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:18",
      "total_flops_so_far": 7.762760926759315e+16,
      "budget_used_percent": 77.62760926759316
    },
    {
      "type": "training",
      "description": "Training step 3268",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:19",
      "total_flops_so_far": 7.76513565930864e+16,
      "budget_used_percent": 77.65135659308639
    },
    {
      "type": "training",
      "description": "Training step 3269",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:20",
      "total_flops_so_far": 7.767510391857965e+16,
      "budget_used_percent": 77.67510391857965
    },
    {
      "type": "training",
      "description": "Training step 3270",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:22",
      "total_flops_so_far": 7.76988512440729e+16,
      "budget_used_percent": 77.6988512440729
    },
    {
      "type": "training",
      "description": "Training step 3271",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:23",
      "total_flops_so_far": 7.772259856956614e+16,
      "budget_used_percent": 77.72259856956614
    },
    {
      "type": "training",
      "description": "Training step 3272",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:25",
      "total_flops_so_far": 7.77463458950594e+16,
      "budget_used_percent": 77.74634589505939
    },
    {
      "type": "training",
      "description": "Training step 3273",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:26",
      "total_flops_so_far": 7.777009322055264e+16,
      "budget_used_percent": 77.77009322055264
    },
    {
      "type": "training",
      "description": "Training step 3274",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:27",
      "total_flops_so_far": 7.779384054604589e+16,
      "budget_used_percent": 77.79384054604589
    },
    {
      "type": "training",
      "description": "Training step 3275",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:29",
      "total_flops_so_far": 7.781758787153914e+16,
      "budget_used_percent": 77.81758787153915
    },
    {
      "type": "training",
      "description": "Training step 3276",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:30",
      "total_flops_so_far": 7.784133519703238e+16,
      "budget_used_percent": 77.84133519703238
    },
    {
      "type": "training",
      "description": "Training step 3277",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:31",
      "total_flops_so_far": 7.786508252252563e+16,
      "budget_used_percent": 77.86508252252563
    },
    {
      "type": "training",
      "description": "Training step 3278",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:33",
      "total_flops_so_far": 7.788882984801888e+16,
      "budget_used_percent": 77.88882984801889
    },
    {
      "type": "training",
      "description": "Training step 3279",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:34",
      "total_flops_so_far": 7.791257717351213e+16,
      "budget_used_percent": 77.91257717351213
    },
    {
      "type": "training",
      "description": "Training step 3280",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:36",
      "total_flops_so_far": 7.793632449900538e+16,
      "budget_used_percent": 77.93632449900537
    },
    {
      "type": "training",
      "description": "Training step 3281",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:37",
      "total_flops_so_far": 7.796007182449862e+16,
      "budget_used_percent": 77.96007182449863
    },
    {
      "type": "training",
      "description": "Training step 3282",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:38",
      "total_flops_so_far": 7.798381914999187e+16,
      "budget_used_percent": 77.98381914999187
    },
    {
      "type": "training",
      "description": "Training step 3283",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:40",
      "total_flops_so_far": 7.800756647548512e+16,
      "budget_used_percent": 78.00756647548512
    },
    {
      "type": "training",
      "description": "Training step 3284",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:41",
      "total_flops_so_far": 7.803131380097837e+16,
      "budget_used_percent": 78.03131380097837
    },
    {
      "type": "training",
      "description": "Training step 3285",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:42",
      "total_flops_so_far": 7.805506112647162e+16,
      "budget_used_percent": 78.05506112647161
    },
    {
      "type": "training",
      "description": "Training step 3286",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:44",
      "total_flops_so_far": 7.807880845196486e+16,
      "budget_used_percent": 78.07880845196486
    },
    {
      "type": "training",
      "description": "Training step 3287",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:45",
      "total_flops_so_far": 7.810255577745811e+16,
      "budget_used_percent": 78.10255577745812
    },
    {
      "type": "training",
      "description": "Training step 3288",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:46",
      "total_flops_so_far": 7.812630310295136e+16,
      "budget_used_percent": 78.12630310295135
    },
    {
      "type": "training",
      "description": "Training step 3289",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:48",
      "total_flops_so_far": 7.81500504284446e+16,
      "budget_used_percent": 78.1500504284446
    },
    {
      "type": "training",
      "description": "Training step 3290",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:49",
      "total_flops_so_far": 7.817379775393786e+16,
      "budget_used_percent": 78.17379775393786
    },
    {
      "type": "training",
      "description": "Training step 3291",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:50",
      "total_flops_so_far": 7.81975450794311e+16,
      "budget_used_percent": 78.19754507943111
    },
    {
      "type": "training",
      "description": "Training step 3292",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:52",
      "total_flops_so_far": 7.822129240492435e+16,
      "budget_used_percent": 78.22129240492434
    },
    {
      "type": "training",
      "description": "Training step 3293",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:53",
      "total_flops_so_far": 7.82450397304176e+16,
      "budget_used_percent": 78.2450397304176
    },
    {
      "type": "training",
      "description": "Training step 3294",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:55",
      "total_flops_so_far": 7.826878705591085e+16,
      "budget_used_percent": 78.26878705591085
    },
    {
      "type": "training",
      "description": "Training step 3295",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:56",
      "total_flops_so_far": 7.82925343814041e+16,
      "budget_used_percent": 78.2925343814041
    },
    {
      "type": "training",
      "description": "Training step 3296",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:57",
      "total_flops_so_far": 7.831628170689734e+16,
      "budget_used_percent": 78.31628170689734
    },
    {
      "type": "training",
      "description": "Training step 3297",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:57:59",
      "total_flops_so_far": 7.83400290323906e+16,
      "budget_used_percent": 78.34002903239059
    },
    {
      "type": "training",
      "description": "Training step 3298",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:00",
      "total_flops_so_far": 7.836377635788384e+16,
      "budget_used_percent": 78.36377635788384
    },
    {
      "type": "training",
      "description": "Training step 3299",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:01",
      "total_flops_so_far": 7.838752368337709e+16,
      "budget_used_percent": 78.3875236833771
    },
    {
      "type": "training",
      "description": "Training step 3300",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:02",
      "total_flops_so_far": 7.841127100887034e+16,
      "budget_used_percent": 78.41127100887033
    },
    {
      "type": "training",
      "description": "Training step 3301",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:03",
      "total_flops_so_far": 7.843501833436358e+16,
      "budget_used_percent": 78.43501833436358
    },
    {
      "type": "training",
      "description": "Training step 3302",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:04",
      "total_flops_so_far": 7.845876565985683e+16,
      "budget_used_percent": 78.45876565985684
    },
    {
      "type": "training",
      "description": "Training step 3303",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:06",
      "total_flops_so_far": 7.848251298535008e+16,
      "budget_used_percent": 78.48251298535008
    },
    {
      "type": "training",
      "description": "Training step 3304",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:07",
      "total_flops_so_far": 7.850626031084333e+16,
      "budget_used_percent": 78.50626031084332
    },
    {
      "type": "training",
      "description": "Training step 3305",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:08",
      "total_flops_so_far": 7.853000763633658e+16,
      "budget_used_percent": 78.53000763633658
    },
    {
      "type": "training",
      "description": "Training step 3306",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:10",
      "total_flops_so_far": 7.855375496182982e+16,
      "budget_used_percent": 78.55375496182982
    },
    {
      "type": "training",
      "description": "Training step 3307",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:11",
      "total_flops_so_far": 7.857750228732307e+16,
      "budget_used_percent": 78.57750228732307
    },
    {
      "type": "training",
      "description": "Training step 3308",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:13",
      "total_flops_so_far": 7.860124961281632e+16,
      "budget_used_percent": 78.60124961281632
    },
    {
      "type": "training",
      "description": "Training step 3309",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:14",
      "total_flops_so_far": 7.862499693830957e+16,
      "budget_used_percent": 78.62499693830956
    },
    {
      "type": "training",
      "description": "Training step 3310",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:15",
      "total_flops_so_far": 7.864874426380282e+16,
      "budget_used_percent": 78.64874426380281
    },
    {
      "type": "training",
      "description": "Training step 3311",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:17",
      "total_flops_so_far": 7.867249158929606e+16,
      "budget_used_percent": 78.67249158929607
    },
    {
      "type": "training",
      "description": "Training step 3312",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:18",
      "total_flops_so_far": 7.869623891478931e+16,
      "budget_used_percent": 78.6962389147893
    },
    {
      "type": "training",
      "description": "Training step 3313",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:19",
      "total_flops_so_far": 7.871998624028256e+16,
      "budget_used_percent": 78.71998624028255
    },
    {
      "type": "training",
      "description": "Training step 3314",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:21",
      "total_flops_so_far": 7.87437335657758e+16,
      "budget_used_percent": 78.74373356577581
    },
    {
      "type": "training",
      "description": "Training step 3315",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:22",
      "total_flops_so_far": 7.876748089126906e+16,
      "budget_used_percent": 78.76748089126906
    },
    {
      "type": "training",
      "description": "Training step 3316",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:23",
      "total_flops_so_far": 7.87912282167623e+16,
      "budget_used_percent": 78.7912282167623
    },
    {
      "type": "training",
      "description": "Training step 3317",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:25",
      "total_flops_so_far": 7.881497554225555e+16,
      "budget_used_percent": 78.81497554225555
    },
    {
      "type": "training",
      "description": "Training step 3318",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:26",
      "total_flops_so_far": 7.88387228677488e+16,
      "budget_used_percent": 78.8387228677488
    },
    {
      "type": "training",
      "description": "Training step 3319",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:27",
      "total_flops_so_far": 7.886247019324205e+16,
      "budget_used_percent": 78.86247019324205
    },
    {
      "type": "training",
      "description": "Training step 3320",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:29",
      "total_flops_so_far": 7.88862175187353e+16,
      "budget_used_percent": 78.8862175187353
    },
    {
      "type": "training",
      "description": "Training step 3321",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:30",
      "total_flops_so_far": 7.890996484422854e+16,
      "budget_used_percent": 78.90996484422854
    },
    {
      "type": "training",
      "description": "Training step 3322",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:31",
      "total_flops_so_far": 7.89337121697218e+16,
      "budget_used_percent": 78.93371216972179
    },
    {
      "type": "training",
      "description": "Training step 3323",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:33",
      "total_flops_so_far": 7.895745949521504e+16,
      "budget_used_percent": 78.95745949521505
    },
    {
      "type": "training",
      "description": "Training step 3324",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:34",
      "total_flops_so_far": 7.898120682070829e+16,
      "budget_used_percent": 78.98120682070828
    },
    {
      "type": "training",
      "description": "Training step 3325",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:36",
      "total_flops_so_far": 7.900495414620154e+16,
      "budget_used_percent": 79.00495414620153
    },
    {
      "type": "training",
      "description": "Training step 3326",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:37",
      "total_flops_so_far": 7.902870147169478e+16,
      "budget_used_percent": 79.02870147169479
    },
    {
      "type": "training",
      "description": "Training step 3327",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:38",
      "total_flops_so_far": 7.905244879718803e+16,
      "budget_used_percent": 79.05244879718803
    },
    {
      "type": "training",
      "description": "Training step 3328",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:40",
      "total_flops_so_far": 7.907619612268128e+16,
      "budget_used_percent": 79.07619612268128
    },
    {
      "type": "training",
      "description": "Training step 3329",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:41",
      "total_flops_so_far": 7.909994344817453e+16,
      "budget_used_percent": 79.09994344817453
    },
    {
      "type": "training",
      "description": "Training step 3330",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:42",
      "total_flops_so_far": 7.912369077366778e+16,
      "budget_used_percent": 79.12369077366778
    },
    {
      "type": "training",
      "description": "Training step 3331",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:44",
      "total_flops_so_far": 7.914743809916102e+16,
      "budget_used_percent": 79.14743809916102
    },
    {
      "type": "training",
      "description": "Training step 3332",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:45",
      "total_flops_so_far": 7.917118542465427e+16,
      "budget_used_percent": 79.17118542465427
    },
    {
      "type": "training",
      "description": "Training step 3333",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:46",
      "total_flops_so_far": 7.919493275014752e+16,
      "budget_used_percent": 79.19493275014752
    },
    {
      "type": "training",
      "description": "Training step 3334",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:48",
      "total_flops_so_far": 7.921868007564077e+16,
      "budget_used_percent": 79.21868007564076
    },
    {
      "type": "training",
      "description": "Training step 3335",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:49",
      "total_flops_so_far": 7.924242740113402e+16,
      "budget_used_percent": 79.24242740113402
    },
    {
      "type": "training",
      "description": "Training step 3336",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:50",
      "total_flops_so_far": 7.926617472662726e+16,
      "budget_used_percent": 79.26617472662726
    },
    {
      "type": "training",
      "description": "Training step 3337",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:52",
      "total_flops_so_far": 7.928992205212051e+16,
      "budget_used_percent": 79.28992205212052
    },
    {
      "type": "training",
      "description": "Training step 3338",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:53",
      "total_flops_so_far": 7.931366937761376e+16,
      "budget_used_percent": 79.31366937761376
    },
    {
      "type": "training",
      "description": "Training step 3339",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:55",
      "total_flops_so_far": 7.9337416703107e+16,
      "budget_used_percent": 79.33741670310701
    },
    {
      "type": "training",
      "description": "Training step 3340",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:56",
      "total_flops_so_far": 7.936116402860026e+16,
      "budget_used_percent": 79.36116402860026
    },
    {
      "type": "training",
      "description": "Training step 3341",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:57",
      "total_flops_so_far": 7.93849113540935e+16,
      "budget_used_percent": 79.3849113540935
    },
    {
      "type": "training",
      "description": "Training step 3342",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:58:59",
      "total_flops_so_far": 7.940865867958675e+16,
      "budget_used_percent": 79.40865867958675
    },
    {
      "type": "training",
      "description": "Training step 3343",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:00",
      "total_flops_so_far": 7.943240600508e+16,
      "budget_used_percent": 79.43240600508
    },
    {
      "type": "training",
      "description": "Training step 3344",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:01",
      "total_flops_so_far": 7.945615333057325e+16,
      "budget_used_percent": 79.45615333057324
    },
    {
      "type": "training",
      "description": "Training step 3345",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:03",
      "total_flops_so_far": 7.94799006560665e+16,
      "budget_used_percent": 79.47990065606649
    },
    {
      "type": "training",
      "description": "Training step 3346",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:04",
      "total_flops_so_far": 7.950364798155974e+16,
      "budget_used_percent": 79.50364798155974
    },
    {
      "type": "training",
      "description": "Training step 3347",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:05",
      "total_flops_so_far": 7.9527395307053e+16,
      "budget_used_percent": 79.527395307053
    },
    {
      "type": "training",
      "description": "Training step 3348",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:07",
      "total_flops_so_far": 7.955114263254624e+16,
      "budget_used_percent": 79.55114263254623
    },
    {
      "type": "training",
      "description": "Training step 3349",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:08",
      "total_flops_so_far": 7.957488995803949e+16,
      "budget_used_percent": 79.57488995803949
    },
    {
      "type": "training",
      "description": "Training step 3350",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:10",
      "total_flops_so_far": 7.959863728353274e+16,
      "budget_used_percent": 79.59863728353274
    },
    {
      "type": "training",
      "description": "Training step 3351",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:11",
      "total_flops_so_far": 7.962238460902598e+16,
      "budget_used_percent": 79.62238460902599
    },
    {
      "type": "training",
      "description": "Training step 3352",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:12",
      "total_flops_so_far": 7.964613193451923e+16,
      "budget_used_percent": 79.64613193451923
    },
    {
      "type": "training",
      "description": "Training step 3353",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:14",
      "total_flops_so_far": 7.966987926001248e+16,
      "budget_used_percent": 79.66987926001248
    },
    {
      "type": "training",
      "description": "Training step 3354",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:15",
      "total_flops_so_far": 7.969362658550573e+16,
      "budget_used_percent": 79.69362658550573
    },
    {
      "type": "training",
      "description": "Training step 3355",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:16",
      "total_flops_so_far": 7.971737391099898e+16,
      "budget_used_percent": 79.71737391099897
    },
    {
      "type": "training",
      "description": "Training step 3356",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:18",
      "total_flops_so_far": 7.974112123649222e+16,
      "budget_used_percent": 79.74112123649223
    },
    {
      "type": "training",
      "description": "Training step 3357",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:19",
      "total_flops_so_far": 7.976486856198547e+16,
      "budget_used_percent": 79.76486856198547
    },
    {
      "type": "training",
      "description": "Training step 3358",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:20",
      "total_flops_so_far": 7.978861588747872e+16,
      "budget_used_percent": 79.78861588747873
    },
    {
      "type": "training",
      "description": "Training step 3359",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:22",
      "total_flops_so_far": 7.981236321297197e+16,
      "budget_used_percent": 79.81236321297197
    },
    {
      "type": "training",
      "description": "Training step 3360",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:23",
      "total_flops_so_far": 7.983611053846522e+16,
      "budget_used_percent": 79.83611053846522
    },
    {
      "type": "training",
      "description": "Training step 3361",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:25",
      "total_flops_so_far": 7.985985786395846e+16,
      "budget_used_percent": 79.85985786395847
    },
    {
      "type": "training",
      "description": "Training step 3362",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:26",
      "total_flops_so_far": 7.988360518945171e+16,
      "budget_used_percent": 79.88360518945171
    },
    {
      "type": "training",
      "description": "Training step 3363",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:27",
      "total_flops_so_far": 7.990735251494496e+16,
      "budget_used_percent": 79.90735251494496
    },
    {
      "type": "training",
      "description": "Training step 3364",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:29",
      "total_flops_so_far": 7.99310998404382e+16,
      "budget_used_percent": 79.93109984043821
    },
    {
      "type": "training",
      "description": "Training step 3365",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:30",
      "total_flops_so_far": 7.995484716593146e+16,
      "budget_used_percent": 79.95484716593145
    },
    {
      "type": "training",
      "description": "Training step 3366",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:31",
      "total_flops_so_far": 7.99785944914247e+16,
      "budget_used_percent": 79.9785944914247
    },
    {
      "type": "training",
      "description": "Training step 3367",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:33",
      "total_flops_so_far": 8.000234181691795e+16,
      "budget_used_percent": 80.00234181691795
    },
    {
      "type": "training",
      "description": "Training step 3368",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:34",
      "total_flops_so_far": 8.00260891424112e+16,
      "budget_used_percent": 80.02608914241121
    },
    {
      "type": "training",
      "description": "Training step 3369",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:36",
      "total_flops_so_far": 8.004983646790445e+16,
      "budget_used_percent": 80.04983646790444
    },
    {
      "type": "training",
      "description": "Training step 3370",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:37",
      "total_flops_so_far": 8.00735837933977e+16,
      "budget_used_percent": 80.0735837933977
    },
    {
      "type": "training",
      "description": "Training step 3371",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:38",
      "total_flops_so_far": 8.009733111889094e+16,
      "budget_used_percent": 80.09733111889095
    },
    {
      "type": "training",
      "description": "Training step 3372",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:40",
      "total_flops_so_far": 8.01210784443842e+16,
      "budget_used_percent": 80.1210784443842
    },
    {
      "type": "training",
      "description": "Training step 3373",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:41",
      "total_flops_so_far": 8.014482576987744e+16,
      "budget_used_percent": 80.14482576987744
    },
    {
      "type": "training",
      "description": "Training step 3374",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:42",
      "total_flops_so_far": 8.016857309537069e+16,
      "budget_used_percent": 80.16857309537069
    },
    {
      "type": "training",
      "description": "Training step 3375",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:44",
      "total_flops_so_far": 8.019232042086394e+16,
      "budget_used_percent": 80.19232042086394
    },
    {
      "type": "training",
      "description": "Training step 3376",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:45",
      "total_flops_so_far": 8.021606774635718e+16,
      "budget_used_percent": 80.21606774635718
    },
    {
      "type": "training",
      "description": "Training step 3377",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:46",
      "total_flops_so_far": 8.023981507185043e+16,
      "budget_used_percent": 80.23981507185043
    },
    {
      "type": "training",
      "description": "Training step 3378",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:48",
      "total_flops_so_far": 8.026356239734368e+16,
      "budget_used_percent": 80.26356239734368
    },
    {
      "type": "training",
      "description": "Training step 3379",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:49",
      "total_flops_so_far": 8.028730972283693e+16,
      "budget_used_percent": 80.28730972283694
    },
    {
      "type": "training",
      "description": "Training step 3380",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:50",
      "total_flops_so_far": 8.031105704833018e+16,
      "budget_used_percent": 80.31105704833018
    },
    {
      "type": "training",
      "description": "Training step 3381",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:52",
      "total_flops_so_far": 8.033480437382342e+16,
      "budget_used_percent": 80.33480437382342
    },
    {
      "type": "training",
      "description": "Training step 3382",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:53",
      "total_flops_so_far": 8.035855169931667e+16,
      "budget_used_percent": 80.35855169931668
    },
    {
      "type": "training",
      "description": "Training step 3383",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:55",
      "total_flops_so_far": 8.038229902480992e+16,
      "budget_used_percent": 80.38229902480992
    },
    {
      "type": "training",
      "description": "Training step 3384",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:56",
      "total_flops_so_far": 8.040604635030317e+16,
      "budget_used_percent": 80.40604635030317
    },
    {
      "type": "training",
      "description": "Training step 3385",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:57",
      "total_flops_so_far": 8.042979367579642e+16,
      "budget_used_percent": 80.42979367579642
    },
    {
      "type": "training",
      "description": "Training step 3386",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 06:59:59",
      "total_flops_so_far": 8.045354100128966e+16,
      "budget_used_percent": 80.45354100128966
    },
    {
      "type": "training",
      "description": "Training step 3387",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:00",
      "total_flops_so_far": 8.047728832678291e+16,
      "budget_used_percent": 80.47728832678291
    },
    {
      "type": "training",
      "description": "Training step 3388",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:01",
      "total_flops_so_far": 8.050103565227616e+16,
      "budget_used_percent": 80.50103565227616
    },
    {
      "type": "training",
      "description": "Training step 3389",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:03",
      "total_flops_so_far": 8.05247829777694e+16,
      "budget_used_percent": 80.5247829777694
    },
    {
      "type": "training",
      "description": "Training step 3390",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:05",
      "total_flops_so_far": 8.054853030326266e+16,
      "budget_used_percent": 80.54853030326265
    },
    {
      "type": "training",
      "description": "Training step 3391",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:06",
      "total_flops_so_far": 8.05722776287559e+16,
      "budget_used_percent": 80.57227762875591
    },
    {
      "type": "training",
      "description": "Training step 3392",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:07",
      "total_flops_so_far": 8.059602495424915e+16,
      "budget_used_percent": 80.59602495424916
    },
    {
      "type": "training",
      "description": "Training step 3393",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:09",
      "total_flops_so_far": 8.06197722797424e+16,
      "budget_used_percent": 80.61977227974239
    },
    {
      "type": "training",
      "description": "Training step 3394",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:11",
      "total_flops_so_far": 8.064351960523565e+16,
      "budget_used_percent": 80.64351960523565
    },
    {
      "type": "training",
      "description": "Training step 3395",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:12",
      "total_flops_so_far": 8.06672669307289e+16,
      "budget_used_percent": 80.6672669307289
    },
    {
      "type": "training",
      "description": "Training step 3396",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:15",
      "total_flops_so_far": 8.069101425622214e+16,
      "budget_used_percent": 80.69101425622215
    },
    {
      "type": "training",
      "description": "Training step 3397",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:17",
      "total_flops_so_far": 8.07147615817154e+16,
      "budget_used_percent": 80.7147615817154
    },
    {
      "type": "training",
      "description": "Training step 3398",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:18",
      "total_flops_so_far": 8.073850890720864e+16,
      "budget_used_percent": 80.73850890720864
    },
    {
      "type": "training",
      "description": "Training step 3399",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:19",
      "total_flops_so_far": 8.076225623270189e+16,
      "budget_used_percent": 80.76225623270189
    },
    {
      "type": "training",
      "description": "Training step 3400",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:21",
      "total_flops_so_far": 8.078600355819514e+16,
      "budget_used_percent": 80.78600355819515
    },
    {
      "type": "training",
      "description": "Training step 3401",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:22",
      "total_flops_so_far": 8.080975088368838e+16,
      "budget_used_percent": 80.80975088368838
    },
    {
      "type": "training",
      "description": "Training step 3402",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:23",
      "total_flops_so_far": 8.083349820918163e+16,
      "budget_used_percent": 80.83349820918163
    },
    {
      "type": "training",
      "description": "Training step 3403",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:25",
      "total_flops_so_far": 8.085724553467488e+16,
      "budget_used_percent": 80.85724553467489
    },
    {
      "type": "training",
      "description": "Training step 3404",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:26",
      "total_flops_so_far": 8.088099286016813e+16,
      "budget_used_percent": 80.88099286016813
    },
    {
      "type": "training",
      "description": "Training step 3405",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:27",
      "total_flops_so_far": 8.090474018566138e+16,
      "budget_used_percent": 80.90474018566137
    },
    {
      "type": "training",
      "description": "Training step 3406",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:29",
      "total_flops_so_far": 8.092848751115462e+16,
      "budget_used_percent": 80.92848751115463
    },
    {
      "type": "training",
      "description": "Training step 3407",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:30",
      "total_flops_so_far": 8.095223483664787e+16,
      "budget_used_percent": 80.95223483664788
    },
    {
      "type": "training",
      "description": "Training step 3408",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:31",
      "total_flops_so_far": 8.097598216214112e+16,
      "budget_used_percent": 80.97598216214112
    },
    {
      "type": "training",
      "description": "Training step 3409",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:33",
      "total_flops_so_far": 8.099972948763437e+16,
      "budget_used_percent": 80.99972948763437
    },
    {
      "type": "training",
      "description": "Training step 3410",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:34",
      "total_flops_so_far": 8.102347681312762e+16,
      "budget_used_percent": 81.02347681312762
    },
    {
      "type": "training",
      "description": "Training step 3411",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:35",
      "total_flops_so_far": 8.104722413862086e+16,
      "budget_used_percent": 81.04722413862086
    },
    {
      "type": "training",
      "description": "Training step 3412",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:37",
      "total_flops_so_far": 8.107097146411411e+16,
      "budget_used_percent": 81.07097146411412
    },
    {
      "type": "training",
      "description": "Training step 3413",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:38",
      "total_flops_so_far": 8.109471878960736e+16,
      "budget_used_percent": 81.09471878960736
    },
    {
      "type": "training",
      "description": "Training step 3414",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:40",
      "total_flops_so_far": 8.11184661151006e+16,
      "budget_used_percent": 81.1184661151006
    },
    {
      "type": "training",
      "description": "Training step 3415",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:41",
      "total_flops_so_far": 8.114221344059386e+16,
      "budget_used_percent": 81.14221344059386
    },
    {
      "type": "training",
      "description": "Training step 3416",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:42",
      "total_flops_so_far": 8.11659607660871e+16,
      "budget_used_percent": 81.16596076608711
    },
    {
      "type": "training",
      "description": "Training step 3417",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:44",
      "total_flops_so_far": 8.118970809158035e+16,
      "budget_used_percent": 81.18970809158034
    },
    {
      "type": "training",
      "description": "Training step 3418",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:45",
      "total_flops_so_far": 8.12134554170736e+16,
      "budget_used_percent": 81.2134554170736
    },
    {
      "type": "training",
      "description": "Training step 3419",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:46",
      "total_flops_so_far": 8.123720274256685e+16,
      "budget_used_percent": 81.23720274256685
    },
    {
      "type": "training",
      "description": "Training step 3420",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:48",
      "total_flops_so_far": 8.12609500680601e+16,
      "budget_used_percent": 81.2609500680601
    },
    {
      "type": "training",
      "description": "Training step 3421",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:49",
      "total_flops_so_far": 8.128469739355334e+16,
      "budget_used_percent": 81.28469739355334
    },
    {
      "type": "training",
      "description": "Training step 3422",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:51",
      "total_flops_so_far": 8.13084447190466e+16,
      "budget_used_percent": 81.30844471904659
    },
    {
      "type": "training",
      "description": "Training step 3423",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:52",
      "total_flops_so_far": 8.133219204453984e+16,
      "budget_used_percent": 81.33219204453984
    },
    {
      "type": "training",
      "description": "Training step 3424",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:53",
      "total_flops_so_far": 8.135593937003309e+16,
      "budget_used_percent": 81.3559393700331
    },
    {
      "type": "training",
      "description": "Training step 3425",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:55",
      "total_flops_so_far": 8.137968669552634e+16,
      "budget_used_percent": 81.37968669552633
    },
    {
      "type": "training",
      "description": "Training step 3426",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:56",
      "total_flops_so_far": 8.140343402101958e+16,
      "budget_used_percent": 81.40343402101958
    },
    {
      "type": "training",
      "description": "Training step 3427",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:57",
      "total_flops_so_far": 8.142718134651283e+16,
      "budget_used_percent": 81.42718134651284
    },
    {
      "type": "training",
      "description": "Training step 3428",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:00:59",
      "total_flops_so_far": 8.145092867200608e+16,
      "budget_used_percent": 81.45092867200609
    },
    {
      "type": "training",
      "description": "Training step 3429",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:00",
      "total_flops_so_far": 8.147467599749933e+16,
      "budget_used_percent": 81.47467599749932
    },
    {
      "type": "training",
      "description": "Training step 3430",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:02",
      "total_flops_so_far": 8.149842332299258e+16,
      "budget_used_percent": 81.49842332299258
    },
    {
      "type": "training",
      "description": "Training step 3431",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:03",
      "total_flops_so_far": 8.152217064848582e+16,
      "budget_used_percent": 81.52217064848583
    },
    {
      "type": "training",
      "description": "Training step 3432",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:03",
      "total_flops_so_far": 8.154591797397907e+16,
      "budget_used_percent": 81.54591797397907
    },
    {
      "type": "training",
      "description": "Training step 3433",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:05",
      "total_flops_so_far": 8.156966529947232e+16,
      "budget_used_percent": 81.56966529947232
    },
    {
      "type": "training",
      "description": "Training step 3434",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:06",
      "total_flops_so_far": 8.159341262496557e+16,
      "budget_used_percent": 81.59341262496557
    },
    {
      "type": "training",
      "description": "Training step 3435",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:07",
      "total_flops_so_far": 8.161715995045882e+16,
      "budget_used_percent": 81.61715995045881
    },
    {
      "type": "training",
      "description": "Training step 3436",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:09",
      "total_flops_so_far": 8.164090727595206e+16,
      "budget_used_percent": 81.64090727595207
    },
    {
      "type": "training",
      "description": "Training step 3437",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:10",
      "total_flops_so_far": 8.166465460144531e+16,
      "budget_used_percent": 81.6646546014453
    },
    {
      "type": "training",
      "description": "Training step 3438",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:11",
      "total_flops_so_far": 8.168840192693856e+16,
      "budget_used_percent": 81.68840192693855
    },
    {
      "type": "training",
      "description": "Training step 3439",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:13",
      "total_flops_so_far": 8.17121492524318e+16,
      "budget_used_percent": 81.71214925243181
    },
    {
      "type": "training",
      "description": "Training step 3440",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:14",
      "total_flops_so_far": 8.173589657792506e+16,
      "budget_used_percent": 81.73589657792506
    },
    {
      "type": "training",
      "description": "Training step 3441",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:16",
      "total_flops_so_far": 8.17596439034183e+16,
      "budget_used_percent": 81.7596439034183
    },
    {
      "type": "training",
      "description": "Training step 3442",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:17",
      "total_flops_so_far": 8.178339122891155e+16,
      "budget_used_percent": 81.78339122891155
    },
    {
      "type": "training",
      "description": "Training step 3443",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:18",
      "total_flops_so_far": 8.18071385544048e+16,
      "budget_used_percent": 81.8071385544048
    },
    {
      "type": "training",
      "description": "Training step 3444",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:20",
      "total_flops_so_far": 8.183088587989805e+16,
      "budget_used_percent": 81.83088587989805
    },
    {
      "type": "training",
      "description": "Training step 3445",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:21",
      "total_flops_so_far": 8.18546332053913e+16,
      "budget_used_percent": 81.8546332053913
    },
    {
      "type": "training",
      "description": "Training step 3446",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:22",
      "total_flops_so_far": 8.187838053088454e+16,
      "budget_used_percent": 81.87838053088454
    },
    {
      "type": "training",
      "description": "Training step 3447",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:24",
      "total_flops_so_far": 8.19021278563778e+16,
      "budget_used_percent": 81.90212785637779
    },
    {
      "type": "training",
      "description": "Training step 3448",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:25",
      "total_flops_so_far": 8.192587518187104e+16,
      "budget_used_percent": 81.92587518187105
    },
    {
      "type": "training",
      "description": "Training step 3449",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:26",
      "total_flops_so_far": 8.194962250736429e+16,
      "budget_used_percent": 81.94962250736428
    },
    {
      "type": "training",
      "description": "Training step 3450",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:28",
      "total_flops_so_far": 8.197336983285754e+16,
      "budget_used_percent": 81.97336983285753
    },
    {
      "type": "training",
      "description": "Training step 3451",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:29",
      "total_flops_so_far": 8.199711715835078e+16,
      "budget_used_percent": 81.99711715835079
    },
    {
      "type": "training",
      "description": "Training step 3452",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:30",
      "total_flops_so_far": 8.202086448384403e+16,
      "budget_used_percent": 82.02086448384404
    },
    {
      "type": "training",
      "description": "Training step 3453",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:32",
      "total_flops_so_far": 8.204461180933728e+16,
      "budget_used_percent": 82.04461180933727
    },
    {
      "type": "training",
      "description": "Training step 3454",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:33",
      "total_flops_so_far": 8.206835913483053e+16,
      "budget_used_percent": 82.06835913483053
    },
    {
      "type": "training",
      "description": "Training step 3455",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:34",
      "total_flops_so_far": 8.209210646032378e+16,
      "budget_used_percent": 82.09210646032378
    },
    {
      "type": "training",
      "description": "Training step 3456",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:36",
      "total_flops_so_far": 8.211585378581702e+16,
      "budget_used_percent": 82.11585378581702
    },
    {
      "type": "training",
      "description": "Training step 3457",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:37",
      "total_flops_so_far": 8.213960111131027e+16,
      "budget_used_percent": 82.13960111131027
    },
    {
      "type": "training",
      "description": "Training step 3458",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:39",
      "total_flops_so_far": 8.216334843680352e+16,
      "budget_used_percent": 82.16334843680352
    },
    {
      "type": "training",
      "description": "Training step 3459",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:40",
      "total_flops_so_far": 8.218709576229677e+16,
      "budget_used_percent": 82.18709576229676
    },
    {
      "type": "training",
      "description": "Training step 3460",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:41",
      "total_flops_so_far": 8.221084308779002e+16,
      "budget_used_percent": 82.21084308779002
    },
    {
      "type": "training",
      "description": "Training step 3461",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:43",
      "total_flops_so_far": 8.223459041328326e+16,
      "budget_used_percent": 82.23459041328326
    },
    {
      "type": "training",
      "description": "Training step 3462",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:44",
      "total_flops_so_far": 8.225833773877651e+16,
      "budget_used_percent": 82.2583377387765
    },
    {
      "type": "training",
      "description": "Training step 3463",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:45",
      "total_flops_so_far": 8.228208506426976e+16,
      "budget_used_percent": 82.28208506426976
    },
    {
      "type": "training",
      "description": "Training step 3464",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:47",
      "total_flops_so_far": 8.2305832389763e+16,
      "budget_used_percent": 82.30583238976301
    },
    {
      "type": "training",
      "description": "Training step 3465",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:48",
      "total_flops_so_far": 8.232957971525626e+16,
      "budget_used_percent": 82.32957971525624
    },
    {
      "type": "training",
      "description": "Training step 3466",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:49",
      "total_flops_so_far": 8.23533270407495e+16,
      "budget_used_percent": 82.3533270407495
    },
    {
      "type": "training",
      "description": "Training step 3467",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:51",
      "total_flops_so_far": 8.237707436624275e+16,
      "budget_used_percent": 82.37707436624275
    },
    {
      "type": "training",
      "description": "Training step 3468",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:52",
      "total_flops_so_far": 8.2400821691736e+16,
      "budget_used_percent": 82.400821691736
    },
    {
      "type": "training",
      "description": "Training step 3469",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:53",
      "total_flops_so_far": 8.242456901722925e+16,
      "budget_used_percent": 82.42456901722925
    },
    {
      "type": "training",
      "description": "Training step 3470",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:55",
      "total_flops_so_far": 8.24483163427225e+16,
      "budget_used_percent": 82.44831634272249
    },
    {
      "type": "training",
      "description": "Training step 3471",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:56",
      "total_flops_so_far": 8.247206366821574e+16,
      "budget_used_percent": 82.47206366821574
    },
    {
      "type": "training",
      "description": "Training step 3472",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:58",
      "total_flops_so_far": 8.2495810993709e+16,
      "budget_used_percent": 82.495810993709
    },
    {
      "type": "training",
      "description": "Training step 3473",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:01:59",
      "total_flops_so_far": 8.251955831920224e+16,
      "budget_used_percent": 82.51955831920223
    },
    {
      "type": "training",
      "description": "Training step 3474",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:00",
      "total_flops_so_far": 8.254330564469549e+16,
      "budget_used_percent": 82.54330564469548
    },
    {
      "type": "training",
      "description": "Training step 3475",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:02",
      "total_flops_so_far": 8.256705297018874e+16,
      "budget_used_percent": 82.56705297018874
    },
    {
      "type": "training",
      "description": "Training step 3476",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:03",
      "total_flops_so_far": 8.259080029568198e+16,
      "budget_used_percent": 82.59080029568199
    },
    {
      "type": "training",
      "description": "Training step 3477",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:04",
      "total_flops_so_far": 8.261454762117523e+16,
      "budget_used_percent": 82.61454762117523
    },
    {
      "type": "training",
      "description": "Training step 3478",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:06",
      "total_flops_so_far": 8.263829494666848e+16,
      "budget_used_percent": 82.63829494666848
    },
    {
      "type": "training",
      "description": "Training step 3479",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:08",
      "total_flops_so_far": 8.266204227216173e+16,
      "budget_used_percent": 82.66204227216173
    },
    {
      "type": "training",
      "description": "Training step 3480",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:09",
      "total_flops_so_far": 8.268578959765498e+16,
      "budget_used_percent": 82.68578959765497
    },
    {
      "type": "training",
      "description": "Training step 3481",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:11",
      "total_flops_so_far": 8.270953692314822e+16,
      "budget_used_percent": 82.70953692314822
    },
    {
      "type": "training",
      "description": "Training step 3482",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:12",
      "total_flops_so_far": 8.273328424864147e+16,
      "budget_used_percent": 82.73328424864147
    },
    {
      "type": "training",
      "description": "Training step 3483",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:13",
      "total_flops_so_far": 8.275703157413472e+16,
      "budget_used_percent": 82.75703157413471
    },
    {
      "type": "training",
      "description": "Training step 3484",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:15",
      "total_flops_so_far": 8.278077889962797e+16,
      "budget_used_percent": 82.78077889962798
    },
    {
      "type": "training",
      "description": "Training step 3485",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:16",
      "total_flops_so_far": 8.280452622512122e+16,
      "budget_used_percent": 82.80452622512121
    },
    {
      "type": "training",
      "description": "Training step 3486",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:17",
      "total_flops_so_far": 8.282827355061446e+16,
      "budget_used_percent": 82.82827355061445
    },
    {
      "type": "training",
      "description": "Training step 3487",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:19",
      "total_flops_so_far": 8.285202087610771e+16,
      "budget_used_percent": 82.85202087610772
    },
    {
      "type": "training",
      "description": "Training step 3488",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:20",
      "total_flops_so_far": 8.287576820160096e+16,
      "budget_used_percent": 82.87576820160096
    },
    {
      "type": "training",
      "description": "Training step 3489",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:22",
      "total_flops_so_far": 8.28995155270942e+16,
      "budget_used_percent": 82.89951552709421
    },
    {
      "type": "training",
      "description": "Training step 3490",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:23",
      "total_flops_so_far": 8.292326285258746e+16,
      "budget_used_percent": 82.92326285258746
    },
    {
      "type": "training",
      "description": "Training step 3491",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:24",
      "total_flops_so_far": 8.29470101780807e+16,
      "budget_used_percent": 82.9470101780807
    },
    {
      "type": "training",
      "description": "Training step 3492",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:26",
      "total_flops_so_far": 8.297075750357395e+16,
      "budget_used_percent": 82.97075750357395
    },
    {
      "type": "training",
      "description": "Training step 3493",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:27",
      "total_flops_so_far": 8.29945048290672e+16,
      "budget_used_percent": 82.9945048290672
    },
    {
      "type": "training",
      "description": "Training step 3494",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:28",
      "total_flops_so_far": 8.301825215456045e+16,
      "budget_used_percent": 83.01825215456044
    },
    {
      "type": "training",
      "description": "Training step 3495",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:30",
      "total_flops_so_far": 8.30419994800537e+16,
      "budget_used_percent": 83.04199948005369
    },
    {
      "type": "training",
      "description": "Training step 3496",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:31",
      "total_flops_so_far": 8.306574680554694e+16,
      "budget_used_percent": 83.06574680554695
    },
    {
      "type": "training",
      "description": "Training step 3497",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:32",
      "total_flops_so_far": 8.30894941310402e+16,
      "budget_used_percent": 83.08949413104018
    },
    {
      "type": "training",
      "description": "Training step 3498",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:34",
      "total_flops_so_far": 8.311324145653344e+16,
      "budget_used_percent": 83.11324145653344
    },
    {
      "type": "training",
      "description": "Training step 3499",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:35",
      "total_flops_so_far": 8.313698878202669e+16,
      "budget_used_percent": 83.13698878202669
    },
    {
      "type": "training",
      "description": "Training step 3500",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:36",
      "total_flops_so_far": 8.316073610751994e+16,
      "budget_used_percent": 83.16073610751994
    },
    {
      "type": "training",
      "description": "Training step 3501",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:38",
      "total_flops_so_far": 8.318448343301318e+16,
      "budget_used_percent": 83.18448343301318
    },
    {
      "type": "training",
      "description": "Training step 3502",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:39",
      "total_flops_so_far": 8.320823075850643e+16,
      "budget_used_percent": 83.20823075850643
    },
    {
      "type": "training",
      "description": "Training step 3503",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:41",
      "total_flops_so_far": 8.323197808399968e+16,
      "budget_used_percent": 83.23197808399968
    },
    {
      "type": "training",
      "description": "Training step 3504",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:42",
      "total_flops_so_far": 8.325572540949293e+16,
      "budget_used_percent": 83.25572540949292
    },
    {
      "type": "training",
      "description": "Training step 3505",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:43",
      "total_flops_so_far": 8.327947273498618e+16,
      "budget_used_percent": 83.27947273498617
    },
    {
      "type": "training",
      "description": "Training step 3506",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:45",
      "total_flops_so_far": 8.330322006047942e+16,
      "budget_used_percent": 83.30322006047942
    },
    {
      "type": "training",
      "description": "Training step 3507",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:46",
      "total_flops_so_far": 8.332696738597267e+16,
      "budget_used_percent": 83.32696738597267
    },
    {
      "type": "training",
      "description": "Training step 3508",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:47",
      "total_flops_so_far": 8.335071471146592e+16,
      "budget_used_percent": 83.35071471146593
    },
    {
      "type": "training",
      "description": "Training step 3509",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:49",
      "total_flops_so_far": 8.337446203695917e+16,
      "budget_used_percent": 83.37446203695916
    },
    {
      "type": "training",
      "description": "Training step 3510",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:50",
      "total_flops_so_far": 8.339820936245242e+16,
      "budget_used_percent": 83.39820936245242
    },
    {
      "type": "training",
      "description": "Training step 3511",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:52",
      "total_flops_so_far": 8.342195668794566e+16,
      "budget_used_percent": 83.42195668794567
    },
    {
      "type": "training",
      "description": "Training step 3512",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:53",
      "total_flops_so_far": 8.344570401343891e+16,
      "budget_used_percent": 83.44570401343891
    },
    {
      "type": "training",
      "description": "Training step 3513",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:54",
      "total_flops_so_far": 8.346945133893216e+16,
      "budget_used_percent": 83.46945133893216
    },
    {
      "type": "training",
      "description": "Training step 3514",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:56",
      "total_flops_so_far": 8.34931986644254e+16,
      "budget_used_percent": 83.4931986644254
    },
    {
      "type": "training",
      "description": "Training step 3515",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:57",
      "total_flops_so_far": 8.351694598991866e+16,
      "budget_used_percent": 83.51694598991865
    },
    {
      "type": "training",
      "description": "Training step 3516",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:02:58",
      "total_flops_so_far": 8.35406933154119e+16,
      "budget_used_percent": 83.5406933154119
    },
    {
      "type": "training",
      "description": "Training step 3517",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:00",
      "total_flops_so_far": 8.356444064090515e+16,
      "budget_used_percent": 83.56444064090515
    },
    {
      "type": "training",
      "description": "Training step 3518",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:01",
      "total_flops_so_far": 8.35881879663984e+16,
      "budget_used_percent": 83.5881879663984
    },
    {
      "type": "training",
      "description": "Training step 3519",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:02",
      "total_flops_so_far": 8.361193529189165e+16,
      "budget_used_percent": 83.61193529189165
    },
    {
      "type": "training",
      "description": "Training step 3520",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:04",
      "total_flops_so_far": 8.36356826173849e+16,
      "budget_used_percent": 83.6356826173849
    },
    {
      "type": "training",
      "description": "Training step 3521",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:05",
      "total_flops_so_far": 8.365942994287814e+16,
      "budget_used_percent": 83.65942994287813
    },
    {
      "type": "training",
      "description": "Training step 3522",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:07",
      "total_flops_so_far": 8.36831772683714e+16,
      "budget_used_percent": 83.6831772683714
    },
    {
      "type": "training",
      "description": "Training step 3523",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:08",
      "total_flops_so_far": 8.370692459386464e+16,
      "budget_used_percent": 83.70692459386464
    },
    {
      "type": "training",
      "description": "Training step 3524",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:09",
      "total_flops_so_far": 8.373067191935789e+16,
      "budget_used_percent": 83.73067191935789
    },
    {
      "type": "training",
      "description": "Training step 3525",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:11",
      "total_flops_so_far": 8.375441924485114e+16,
      "budget_used_percent": 83.75441924485114
    },
    {
      "type": "training",
      "description": "Training step 3526",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:12",
      "total_flops_so_far": 8.377816657034438e+16,
      "budget_used_percent": 83.77816657034438
    },
    {
      "type": "training",
      "description": "Training step 3527",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:13",
      "total_flops_so_far": 8.380191389583763e+16,
      "budget_used_percent": 83.80191389583763
    },
    {
      "type": "training",
      "description": "Training step 3528",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:15",
      "total_flops_so_far": 8.382566122133088e+16,
      "budget_used_percent": 83.82566122133088
    },
    {
      "type": "training",
      "description": "Training step 3529",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:16",
      "total_flops_so_far": 8.384940854682413e+16,
      "budget_used_percent": 83.84940854682414
    },
    {
      "type": "training",
      "description": "Training step 3530",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:18",
      "total_flops_so_far": 8.387315587231738e+16,
      "budget_used_percent": 83.87315587231737
    },
    {
      "type": "training",
      "description": "Training step 3531",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:19",
      "total_flops_so_far": 8.389690319781062e+16,
      "budget_used_percent": 83.89690319781063
    },
    {
      "type": "training",
      "description": "Training step 3532",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:20",
      "total_flops_so_far": 8.392065052330387e+16,
      "budget_used_percent": 83.92065052330388
    },
    {
      "type": "training",
      "description": "Training step 3533",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:22",
      "total_flops_so_far": 8.394439784879712e+16,
      "budget_used_percent": 83.94439784879712
    },
    {
      "type": "training",
      "description": "Training step 3534",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:23",
      "total_flops_so_far": 8.396814517429037e+16,
      "budget_used_percent": 83.96814517429037
    },
    {
      "type": "training",
      "description": "Training step 3535",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:24",
      "total_flops_so_far": 8.399189249978362e+16,
      "budget_used_percent": 83.99189249978362
    },
    {
      "type": "training",
      "description": "Training step 3536",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:26",
      "total_flops_so_far": 8.401563982527686e+16,
      "budget_used_percent": 84.01563982527686
    },
    {
      "type": "training",
      "description": "Training step 3537",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:27",
      "total_flops_so_far": 8.403938715077011e+16,
      "budget_used_percent": 84.03938715077011
    },
    {
      "type": "training",
      "description": "Training step 3538",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:29",
      "total_flops_so_far": 8.406313447626336e+16,
      "budget_used_percent": 84.06313447626336
    },
    {
      "type": "training",
      "description": "Training step 3539",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:30",
      "total_flops_so_far": 8.40868818017566e+16,
      "budget_used_percent": 84.0868818017566
    },
    {
      "type": "training",
      "description": "Training step 3540",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:31",
      "total_flops_so_far": 8.411062912724986e+16,
      "budget_used_percent": 84.11062912724987
    },
    {
      "type": "training",
      "description": "Training step 3541",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:33",
      "total_flops_so_far": 8.41343764527431e+16,
      "budget_used_percent": 84.13437645274311
    },
    {
      "type": "training",
      "description": "Training step 3542",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:34",
      "total_flops_so_far": 8.415812377823635e+16,
      "budget_used_percent": 84.15812377823634
    },
    {
      "type": "training",
      "description": "Training step 3543",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:35",
      "total_flops_so_far": 8.41818711037296e+16,
      "budget_used_percent": 84.1818711037296
    },
    {
      "type": "training",
      "description": "Training step 3544",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:37",
      "total_flops_so_far": 8.420561842922285e+16,
      "budget_used_percent": 84.20561842922285
    },
    {
      "type": "training",
      "description": "Training step 3545",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:38",
      "total_flops_so_far": 8.42293657547161e+16,
      "budget_used_percent": 84.2293657547161
    },
    {
      "type": "training",
      "description": "Training step 3546",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:39",
      "total_flops_so_far": 8.425311308020934e+16,
      "budget_used_percent": 84.25311308020935
    },
    {
      "type": "training",
      "description": "Training step 3547",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:41",
      "total_flops_so_far": 8.42768604057026e+16,
      "budget_used_percent": 84.27686040570259
    },
    {
      "type": "training",
      "description": "Training step 3548",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:42",
      "total_flops_so_far": 8.430060773119584e+16,
      "budget_used_percent": 84.30060773119584
    },
    {
      "type": "training",
      "description": "Training step 3549",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:44",
      "total_flops_so_far": 8.432435505668909e+16,
      "budget_used_percent": 84.32435505668909
    },
    {
      "type": "training",
      "description": "Training step 3550",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:45",
      "total_flops_so_far": 8.434810238218234e+16,
      "budget_used_percent": 84.34810238218233
    },
    {
      "type": "training",
      "description": "Training step 3551",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:46",
      "total_flops_so_far": 8.437184970767558e+16,
      "budget_used_percent": 84.37184970767558
    },
    {
      "type": "training",
      "description": "Training step 3552",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:48",
      "total_flops_so_far": 8.439559703316883e+16,
      "budget_used_percent": 84.39559703316884
    },
    {
      "type": "training",
      "description": "Training step 3553",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:49",
      "total_flops_so_far": 8.441934435866208e+16,
      "budget_used_percent": 84.41934435866209
    },
    {
      "type": "training",
      "description": "Training step 3554",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:50",
      "total_flops_so_far": 8.444309168415533e+16,
      "budget_used_percent": 84.44309168415532
    },
    {
      "type": "training",
      "description": "Training step 3555",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:52",
      "total_flops_so_far": 8.446683900964858e+16,
      "budget_used_percent": 84.46683900964858
    },
    {
      "type": "training",
      "description": "Training step 3556",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:53",
      "total_flops_so_far": 8.449058633514182e+16,
      "budget_used_percent": 84.49058633514183
    },
    {
      "type": "training",
      "description": "Training step 3557",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:54",
      "total_flops_so_far": 8.451433366063507e+16,
      "budget_used_percent": 84.51433366063507
    },
    {
      "type": "training",
      "description": "Training step 3558",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:56",
      "total_flops_so_far": 8.453808098612832e+16,
      "budget_used_percent": 84.53808098612832
    },
    {
      "type": "training",
      "description": "Training step 3559",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:57",
      "total_flops_so_far": 8.456182831162157e+16,
      "budget_used_percent": 84.56182831162157
    },
    {
      "type": "training",
      "description": "Training step 3560",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:03:58",
      "total_flops_so_far": 8.458557563711482e+16,
      "budget_used_percent": 84.58557563711481
    },
    {
      "type": "training",
      "description": "Training step 3561",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:00",
      "total_flops_so_far": 8.460932296260806e+16,
      "budget_used_percent": 84.60932296260808
    },
    {
      "type": "training",
      "description": "Training step 3562",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:01",
      "total_flops_so_far": 8.463307028810131e+16,
      "budget_used_percent": 84.63307028810131
    },
    {
      "type": "training",
      "description": "Training step 3563",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:03",
      "total_flops_so_far": 8.465681761359456e+16,
      "budget_used_percent": 84.65681761359455
    },
    {
      "type": "training",
      "description": "Training step 3564",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:03",
      "total_flops_so_far": 8.46805649390878e+16,
      "budget_used_percent": 84.68056493908782
    },
    {
      "type": "training",
      "description": "Training step 3565",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:04",
      "total_flops_so_far": 8.470431226458106e+16,
      "budget_used_percent": 84.70431226458106
    },
    {
      "type": "training",
      "description": "Training step 3566",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:06",
      "total_flops_so_far": 8.47280595900743e+16,
      "budget_used_percent": 84.7280595900743
    },
    {
      "type": "training",
      "description": "Training step 3567",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:07",
      "total_flops_so_far": 8.475180691556755e+16,
      "budget_used_percent": 84.75180691556756
    },
    {
      "type": "training",
      "description": "Training step 3568",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:09",
      "total_flops_so_far": 8.47755542410608e+16,
      "budget_used_percent": 84.7755542410608
    },
    {
      "type": "training",
      "description": "Training step 3569",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:10",
      "total_flops_so_far": 8.479930156655405e+16,
      "budget_used_percent": 84.79930156655405
    },
    {
      "type": "training",
      "description": "Training step 3570",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:11",
      "total_flops_so_far": 8.48230488920473e+16,
      "budget_used_percent": 84.8230488920473
    },
    {
      "type": "training",
      "description": "Training step 3571",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:13",
      "total_flops_so_far": 8.484679621754054e+16,
      "budget_used_percent": 84.84679621754054
    },
    {
      "type": "training",
      "description": "Training step 3572",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:14",
      "total_flops_so_far": 8.48705435430338e+16,
      "budget_used_percent": 84.87054354303379
    },
    {
      "type": "training",
      "description": "Training step 3573",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:15",
      "total_flops_so_far": 8.489429086852704e+16,
      "budget_used_percent": 84.89429086852705
    },
    {
      "type": "training",
      "description": "Training step 3574",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:17",
      "total_flops_so_far": 8.491803819402029e+16,
      "budget_used_percent": 84.91803819402028
    },
    {
      "type": "training",
      "description": "Training step 3575",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:18",
      "total_flops_so_far": 8.494178551951354e+16,
      "budget_used_percent": 84.94178551951353
    },
    {
      "type": "training",
      "description": "Training step 3576",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:19",
      "total_flops_so_far": 8.496553284500678e+16,
      "budget_used_percent": 84.96553284500679
    },
    {
      "type": "training",
      "description": "Training step 3577",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:21",
      "total_flops_so_far": 8.498928017050003e+16,
      "budget_used_percent": 84.98928017050004
    },
    {
      "type": "training",
      "description": "Training step 3578",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:22",
      "total_flops_so_far": 8.501302749599328e+16,
      "budget_used_percent": 85.01302749599327
    },
    {
      "type": "training",
      "description": "Training step 3579",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:24",
      "total_flops_so_far": 8.503677482148653e+16,
      "budget_used_percent": 85.03677482148653
    },
    {
      "type": "training",
      "description": "Training step 3580",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:25",
      "total_flops_so_far": 8.506052214697978e+16,
      "budget_used_percent": 85.06052214697978
    },
    {
      "type": "training",
      "description": "Training step 3581",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:26",
      "total_flops_so_far": 8.508426947247302e+16,
      "budget_used_percent": 85.08426947247303
    },
    {
      "type": "training",
      "description": "Training step 3582",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:28",
      "total_flops_so_far": 8.510801679796627e+16,
      "budget_used_percent": 85.10801679796627
    },
    {
      "type": "training",
      "description": "Training step 3583",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:29",
      "total_flops_so_far": 8.513176412345952e+16,
      "budget_used_percent": 85.13176412345952
    },
    {
      "type": "training",
      "description": "Training step 3584",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:30",
      "total_flops_so_far": 8.515551144895277e+16,
      "budget_used_percent": 85.15551144895277
    },
    {
      "type": "training",
      "description": "Training step 3585",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:32",
      "total_flops_so_far": 8.517925877444602e+16,
      "budget_used_percent": 85.17925877444603
    },
    {
      "type": "training",
      "description": "Training step 3586",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:33",
      "total_flops_so_far": 8.520300609993926e+16,
      "budget_used_percent": 85.20300609993926
    },
    {
      "type": "training",
      "description": "Training step 3587",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:34",
      "total_flops_so_far": 8.522675342543251e+16,
      "budget_used_percent": 85.2267534254325
    },
    {
      "type": "training",
      "description": "Training step 3588",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:36",
      "total_flops_so_far": 8.525050075092576e+16,
      "budget_used_percent": 85.25050075092577
    },
    {
      "type": "training",
      "description": "Training step 3589",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:37",
      "total_flops_so_far": 8.5274248076419e+16,
      "budget_used_percent": 85.27424807641901
    },
    {
      "type": "training",
      "description": "Training step 3590",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:38",
      "total_flops_so_far": 8.529799540191226e+16,
      "budget_used_percent": 85.29799540191225
    },
    {
      "type": "training",
      "description": "Training step 3591",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:40",
      "total_flops_so_far": 8.53217427274055e+16,
      "budget_used_percent": 85.3217427274055
    },
    {
      "type": "training",
      "description": "Training step 3592",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:41",
      "total_flops_so_far": 8.534549005289875e+16,
      "budget_used_percent": 85.34549005289875
    },
    {
      "type": "training",
      "description": "Training step 3593",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:43",
      "total_flops_so_far": 8.5369237378392e+16,
      "budget_used_percent": 85.369237378392
    },
    {
      "type": "training",
      "description": "Training step 3594",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:44",
      "total_flops_so_far": 8.539298470388525e+16,
      "budget_used_percent": 85.39298470388525
    },
    {
      "type": "training",
      "description": "Training step 3595",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:45",
      "total_flops_so_far": 8.54167320293785e+16,
      "budget_used_percent": 85.4167320293785
    },
    {
      "type": "training",
      "description": "Training step 3596",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:47",
      "total_flops_so_far": 8.544047935487174e+16,
      "budget_used_percent": 85.44047935487174
    },
    {
      "type": "training",
      "description": "Training step 3597",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:48",
      "total_flops_so_far": 8.5464226680365e+16,
      "budget_used_percent": 85.464226680365
    },
    {
      "type": "training",
      "description": "Training step 3598",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:49",
      "total_flops_so_far": 8.548797400585824e+16,
      "budget_used_percent": 85.48797400585823
    },
    {
      "type": "training",
      "description": "Training step 3599",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:51",
      "total_flops_so_far": 8.551172133135149e+16,
      "budget_used_percent": 85.51172133135148
    },
    {
      "type": "training",
      "description": "Training step 3600",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:52",
      "total_flops_so_far": 8.553546865684474e+16,
      "budget_used_percent": 85.53546865684474
    },
    {
      "type": "training",
      "description": "Training step 3601",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:53",
      "total_flops_so_far": 8.555921598233798e+16,
      "budget_used_percent": 85.55921598233799
    },
    {
      "type": "training",
      "description": "Training step 3602",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:55",
      "total_flops_so_far": 8.558296330783123e+16,
      "budget_used_percent": 85.58296330783122
    },
    {
      "type": "training",
      "description": "Training step 3603",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:56",
      "total_flops_so_far": 8.560671063332448e+16,
      "budget_used_percent": 85.60671063332448
    },
    {
      "type": "training",
      "description": "Training step 3604",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:58",
      "total_flops_so_far": 8.563045795881773e+16,
      "budget_used_percent": 85.63045795881773
    },
    {
      "type": "training",
      "description": "Training step 3605",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:04:59",
      "total_flops_so_far": 8.565420528431098e+16,
      "budget_used_percent": 85.65420528431098
    },
    {
      "type": "training",
      "description": "Training step 3606",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:00",
      "total_flops_so_far": 8.567795260980422e+16,
      "budget_used_percent": 85.67795260980422
    },
    {
      "type": "training",
      "description": "Training step 3607",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:02",
      "total_flops_so_far": 8.570169993529747e+16,
      "budget_used_percent": 85.70169993529747
    },
    {
      "type": "training",
      "description": "Training step 3608",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:03",
      "total_flops_so_far": 8.572544726079072e+16,
      "budget_used_percent": 85.72544726079072
    },
    {
      "type": "training",
      "description": "Training step 3609",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:04",
      "total_flops_so_far": 8.574919458628397e+16,
      "budget_used_percent": 85.74919458628398
    },
    {
      "type": "training",
      "description": "Training step 3610",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:06",
      "total_flops_so_far": 8.577294191177722e+16,
      "budget_used_percent": 85.77294191177721
    },
    {
      "type": "training",
      "description": "Training step 3611",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:07",
      "total_flops_so_far": 8.579668923727046e+16,
      "budget_used_percent": 85.79668923727046
    },
    {
      "type": "training",
      "description": "Training step 3612",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:09",
      "total_flops_so_far": 8.582043656276371e+16,
      "budget_used_percent": 85.82043656276372
    },
    {
      "type": "training",
      "description": "Training step 3613",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:10",
      "total_flops_so_far": 8.584418388825696e+16,
      "budget_used_percent": 85.84418388825696
    },
    {
      "type": "training",
      "description": "Training step 3614",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:11",
      "total_flops_so_far": 8.58679312137502e+16,
      "budget_used_percent": 85.86793121375021
    },
    {
      "type": "training",
      "description": "Training step 3615",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:13",
      "total_flops_so_far": 8.589167853924346e+16,
      "budget_used_percent": 85.89167853924346
    },
    {
      "type": "training",
      "description": "Training step 3616",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:14",
      "total_flops_so_far": 8.59154258647367e+16,
      "budget_used_percent": 85.9154258647367
    },
    {
      "type": "training",
      "description": "Training step 3617",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:15",
      "total_flops_so_far": 8.593917319022995e+16,
      "budget_used_percent": 85.93917319022995
    },
    {
      "type": "training",
      "description": "Training step 3618",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:17",
      "total_flops_so_far": 8.59629205157232e+16,
      "budget_used_percent": 85.9629205157232
    },
    {
      "type": "training",
      "description": "Training step 3619",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:18",
      "total_flops_so_far": 8.598666784121645e+16,
      "budget_used_percent": 85.98666784121644
    },
    {
      "type": "training",
      "description": "Training step 3620",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:19",
      "total_flops_so_far": 8.60104151667097e+16,
      "budget_used_percent": 86.01041516670969
    },
    {
      "type": "training",
      "description": "Training step 3621",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:21",
      "total_flops_so_far": 8.603416249220294e+16,
      "budget_used_percent": 86.03416249220295
    },
    {
      "type": "training",
      "description": "Training step 3622",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:22",
      "total_flops_so_far": 8.60579098176962e+16,
      "budget_used_percent": 86.05790981769618
    },
    {
      "type": "training",
      "description": "Training step 3623",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:24",
      "total_flops_so_far": 8.608165714318944e+16,
      "budget_used_percent": 86.08165714318943
    },
    {
      "type": "training",
      "description": "Training step 3624",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:25",
      "total_flops_so_far": 8.610540446868269e+16,
      "budget_used_percent": 86.10540446868269
    },
    {
      "type": "training",
      "description": "Training step 3625",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:26",
      "total_flops_so_far": 8.612915179417594e+16,
      "budget_used_percent": 86.12915179417594
    },
    {
      "type": "training",
      "description": "Training step 3626",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:28",
      "total_flops_so_far": 8.615289911966918e+16,
      "budget_used_percent": 86.15289911966919
    },
    {
      "type": "training",
      "description": "Training step 3627",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:29",
      "total_flops_so_far": 8.617664644516243e+16,
      "budget_used_percent": 86.17664644516243
    },
    {
      "type": "training",
      "description": "Training step 3628",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:30",
      "total_flops_so_far": 8.620039377065568e+16,
      "budget_used_percent": 86.20039377065568
    },
    {
      "type": "training",
      "description": "Training step 3629",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:32",
      "total_flops_so_far": 8.622414109614893e+16,
      "budget_used_percent": 86.22414109614893
    },
    {
      "type": "training",
      "description": "Training step 3630",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:33",
      "total_flops_so_far": 8.624788842164218e+16,
      "budget_used_percent": 86.24788842164217
    },
    {
      "type": "training",
      "description": "Training step 3631",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:34",
      "total_flops_so_far": 8.627163574713542e+16,
      "budget_used_percent": 86.27163574713542
    },
    {
      "type": "training",
      "description": "Training step 3632",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:36",
      "total_flops_so_far": 8.629538307262867e+16,
      "budget_used_percent": 86.29538307262867
    },
    {
      "type": "training",
      "description": "Training step 3633",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:37",
      "total_flops_so_far": 8.631913039812192e+16,
      "budget_used_percent": 86.31913039812193
    },
    {
      "type": "training",
      "description": "Training step 3634",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:38",
      "total_flops_so_far": 8.634287772361517e+16,
      "budget_used_percent": 86.34287772361516
    },
    {
      "type": "training",
      "description": "Training step 3635",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:40",
      "total_flops_so_far": 8.636662504910842e+16,
      "budget_used_percent": 86.36662504910842
    },
    {
      "type": "training",
      "description": "Training step 3636",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:41",
      "total_flops_so_far": 8.639037237460166e+16,
      "budget_used_percent": 86.39037237460167
    },
    {
      "type": "training",
      "description": "Training step 3637",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:43",
      "total_flops_so_far": 8.641411970009491e+16,
      "budget_used_percent": 86.41411970009491
    },
    {
      "type": "training",
      "description": "Training step 3638",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:44",
      "total_flops_so_far": 8.643786702558816e+16,
      "budget_used_percent": 86.43786702558816
    },
    {
      "type": "training",
      "description": "Training step 3639",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:45",
      "total_flops_so_far": 8.64616143510814e+16,
      "budget_used_percent": 86.46161435108141
    },
    {
      "type": "training",
      "description": "Training step 3640",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:47",
      "total_flops_so_far": 8.648536167657466e+16,
      "budget_used_percent": 86.48536167657466
    },
    {
      "type": "training",
      "description": "Training step 3641",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:48",
      "total_flops_so_far": 8.65091090020679e+16,
      "budget_used_percent": 86.5091090020679
    },
    {
      "type": "training",
      "description": "Training step 3642",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:49",
      "total_flops_so_far": 8.653285632756115e+16,
      "budget_used_percent": 86.53285632756115
    },
    {
      "type": "training",
      "description": "Training step 3643",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:51",
      "total_flops_so_far": 8.65566036530544e+16,
      "budget_used_percent": 86.5566036530544
    },
    {
      "type": "training",
      "description": "Training step 3644",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:52",
      "total_flops_so_far": 8.658035097854765e+16,
      "budget_used_percent": 86.58035097854764
    },
    {
      "type": "training",
      "description": "Training step 3645",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:53",
      "total_flops_so_far": 8.66040983040409e+16,
      "budget_used_percent": 86.6040983040409
    },
    {
      "type": "training",
      "description": "Training step 3646",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:55",
      "total_flops_so_far": 8.662784562953414e+16,
      "budget_used_percent": 86.62784562953414
    },
    {
      "type": "training",
      "description": "Training step 3647",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:56",
      "total_flops_so_far": 8.66515929550274e+16,
      "budget_used_percent": 86.6515929550274
    },
    {
      "type": "training",
      "description": "Training step 3648",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:58",
      "total_flops_so_far": 8.667534028052064e+16,
      "budget_used_percent": 86.67534028052064
    },
    {
      "type": "training",
      "description": "Training step 3649",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:05:59",
      "total_flops_so_far": 8.669908760601389e+16,
      "budget_used_percent": 86.69908760601389
    },
    {
      "type": "training",
      "description": "Training step 3650",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:00",
      "total_flops_so_far": 8.672283493150714e+16,
      "budget_used_percent": 86.72283493150714
    },
    {
      "type": "training",
      "description": "Training step 3651",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:02",
      "total_flops_so_far": 8.674658225700038e+16,
      "budget_used_percent": 86.74658225700038
    },
    {
      "type": "training",
      "description": "Training step 3652",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:03",
      "total_flops_so_far": 8.677032958249363e+16,
      "budget_used_percent": 86.77032958249363
    },
    {
      "type": "training",
      "description": "Training step 3653",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:05",
      "total_flops_so_far": 8.679407690798688e+16,
      "budget_used_percent": 86.79407690798688
    },
    {
      "type": "training",
      "description": "Training step 3654",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:06",
      "total_flops_so_far": 8.681782423348013e+16,
      "budget_used_percent": 86.81782423348012
    },
    {
      "type": "training",
      "description": "Training step 3655",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:07",
      "total_flops_so_far": 8.684157155897338e+16,
      "budget_used_percent": 86.84157155897337
    },
    {
      "type": "training",
      "description": "Training step 3656",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:09",
      "total_flops_so_far": 8.686531888446662e+16,
      "budget_used_percent": 86.86531888446663
    },
    {
      "type": "training",
      "description": "Training step 3657",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:10",
      "total_flops_so_far": 8.688906620995987e+16,
      "budget_used_percent": 86.88906620995988
    },
    {
      "type": "training",
      "description": "Training step 3658",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:11",
      "total_flops_so_far": 8.691281353545312e+16,
      "budget_used_percent": 86.91281353545311
    },
    {
      "type": "training",
      "description": "Training step 3659",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:13",
      "total_flops_so_far": 8.693656086094637e+16,
      "budget_used_percent": 86.93656086094637
    },
    {
      "type": "training",
      "description": "Training step 3660",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:14",
      "total_flops_so_far": 8.696030818643962e+16,
      "budget_used_percent": 86.96030818643962
    },
    {
      "type": "training",
      "description": "Training step 3661",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:15",
      "total_flops_so_far": 8.698405551193286e+16,
      "budget_used_percent": 86.98405551193287
    },
    {
      "type": "training",
      "description": "Training step 3662",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:17",
      "total_flops_so_far": 8.700780283742611e+16,
      "budget_used_percent": 87.00780283742611
    },
    {
      "type": "training",
      "description": "Training step 3663",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:18",
      "total_flops_so_far": 8.703155016291936e+16,
      "budget_used_percent": 87.03155016291936
    },
    {
      "type": "training",
      "description": "Training step 3664",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:19",
      "total_flops_so_far": 8.70552974884126e+16,
      "budget_used_percent": 87.0552974884126
    },
    {
      "type": "training",
      "description": "Training step 3665",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:21",
      "total_flops_so_far": 8.707904481390586e+16,
      "budget_used_percent": 87.07904481390585
    },
    {
      "type": "training",
      "description": "Training step 3666",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:22",
      "total_flops_so_far": 8.71027921393991e+16,
      "budget_used_percent": 87.1027921393991
    },
    {
      "type": "training",
      "description": "Training step 3667",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:24",
      "total_flops_so_far": 8.712653946489235e+16,
      "budget_used_percent": 87.12653946489235
    },
    {
      "type": "training",
      "description": "Training step 3668",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:25",
      "total_flops_so_far": 8.71502867903856e+16,
      "budget_used_percent": 87.1502867903856
    },
    {
      "type": "training",
      "description": "Training step 3669",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:26",
      "total_flops_so_far": 8.717403411587885e+16,
      "budget_used_percent": 87.17403411587885
    },
    {
      "type": "training",
      "description": "Training step 3670",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:28",
      "total_flops_so_far": 8.71977814413721e+16,
      "budget_used_percent": 87.19778144137209
    },
    {
      "type": "training",
      "description": "Training step 3671",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:29",
      "total_flops_so_far": 8.722152876686534e+16,
      "budget_used_percent": 87.22152876686535
    },
    {
      "type": "training",
      "description": "Training step 3672",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:30",
      "total_flops_so_far": 8.72452760923586e+16,
      "budget_used_percent": 87.2452760923586
    },
    {
      "type": "training",
      "description": "Training step 3673",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:32",
      "total_flops_so_far": 8.726902341785184e+16,
      "budget_used_percent": 87.26902341785184
    },
    {
      "type": "training",
      "description": "Training step 3674",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:33",
      "total_flops_so_far": 8.729277074334509e+16,
      "budget_used_percent": 87.29277074334509
    },
    {
      "type": "training",
      "description": "Training step 3675",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:34",
      "total_flops_so_far": 8.731651806883834e+16,
      "budget_used_percent": 87.31651806883833
    },
    {
      "type": "training",
      "description": "Training step 3676",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:36",
      "total_flops_so_far": 8.734026539433158e+16,
      "budget_used_percent": 87.34026539433158
    },
    {
      "type": "training",
      "description": "Training step 3677",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:37",
      "total_flops_so_far": 8.736401271982483e+16,
      "budget_used_percent": 87.36401271982484
    },
    {
      "type": "training",
      "description": "Training step 3678",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:39",
      "total_flops_so_far": 8.738776004531808e+16,
      "budget_used_percent": 87.38776004531807
    },
    {
      "type": "training",
      "description": "Training step 3679",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:40",
      "total_flops_so_far": 8.741150737081133e+16,
      "budget_used_percent": 87.41150737081132
    },
    {
      "type": "training",
      "description": "Training step 3680",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:41",
      "total_flops_so_far": 8.743525469630458e+16,
      "budget_used_percent": 87.43525469630458
    },
    {
      "type": "training",
      "description": "Training step 3681",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:43",
      "total_flops_so_far": 8.745900202179782e+16,
      "budget_used_percent": 87.45900202179783
    },
    {
      "type": "training",
      "description": "Training step 3682",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:44",
      "total_flops_so_far": 8.748274934729107e+16,
      "budget_used_percent": 87.48274934729106
    },
    {
      "type": "training",
      "description": "Training step 3683",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:45",
      "total_flops_so_far": 8.750649667278432e+16,
      "budget_used_percent": 87.50649667278432
    },
    {
      "type": "training",
      "description": "Training step 3684",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:47",
      "total_flops_so_far": 8.753024399827757e+16,
      "budget_used_percent": 87.53024399827757
    },
    {
      "type": "training",
      "description": "Training step 3685",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:48",
      "total_flops_so_far": 8.755399132377082e+16,
      "budget_used_percent": 87.55399132377082
    },
    {
      "type": "training",
      "description": "Training step 3686",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:49",
      "total_flops_so_far": 8.757773864926406e+16,
      "budget_used_percent": 87.57773864926406
    },
    {
      "type": "training",
      "description": "Training step 3687",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:51",
      "total_flops_so_far": 8.760148597475731e+16,
      "budget_used_percent": 87.60148597475731
    },
    {
      "type": "training",
      "description": "Training step 3688",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:52",
      "total_flops_so_far": 8.762523330025056e+16,
      "budget_used_percent": 87.62523330025056
    },
    {
      "type": "training",
      "description": "Training step 3689",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:54",
      "total_flops_so_far": 8.76489806257438e+16,
      "budget_used_percent": 87.64898062574382
    },
    {
      "type": "training",
      "description": "Training step 3690",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:55",
      "total_flops_so_far": 8.767272795123706e+16,
      "budget_used_percent": 87.67272795123705
    },
    {
      "type": "training",
      "description": "Training step 3691",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:56",
      "total_flops_so_far": 8.76964752767303e+16,
      "budget_used_percent": 87.6964752767303
    },
    {
      "type": "training",
      "description": "Training step 3692",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:58",
      "total_flops_so_far": 8.772022260222355e+16,
      "budget_used_percent": 87.72022260222356
    },
    {
      "type": "training",
      "description": "Training step 3693",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:06:59",
      "total_flops_so_far": 8.77439699277168e+16,
      "budget_used_percent": 87.7439699277168
    },
    {
      "type": "training",
      "description": "Training step 3694",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:01",
      "total_flops_so_far": 8.776771725321005e+16,
      "budget_used_percent": 87.76771725321004
    },
    {
      "type": "training",
      "description": "Training step 3695",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:02",
      "total_flops_so_far": 8.77914645787033e+16,
      "budget_used_percent": 87.7914645787033
    },
    {
      "type": "training",
      "description": "Training step 3696",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:03",
      "total_flops_so_far": 8.781521190419654e+16,
      "budget_used_percent": 87.81521190419654
    },
    {
      "type": "training",
      "description": "Training step 3697",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:04",
      "total_flops_so_far": 8.78389592296898e+16,
      "budget_used_percent": 87.83895922968979
    },
    {
      "type": "training",
      "description": "Training step 3698",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:06",
      "total_flops_so_far": 8.786270655518304e+16,
      "budget_used_percent": 87.86270655518305
    },
    {
      "type": "training",
      "description": "Training step 3699",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:07",
      "total_flops_so_far": 8.788645388067629e+16,
      "budget_used_percent": 87.88645388067629
    },
    {
      "type": "training",
      "description": "Training step 3700",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:08",
      "total_flops_so_far": 8.791020120616954e+16,
      "budget_used_percent": 87.91020120616953
    },
    {
      "type": "training",
      "description": "Training step 3701",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:10",
      "total_flops_so_far": 8.793394853166278e+16,
      "budget_used_percent": 87.9339485316628
    },
    {
      "type": "training",
      "description": "Training step 3702",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:11",
      "total_flops_so_far": 8.795769585715603e+16,
      "budget_used_percent": 87.95769585715604
    },
    {
      "type": "training",
      "description": "Training step 3703",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:12",
      "total_flops_so_far": 8.798144318264928e+16,
      "budget_used_percent": 87.98144318264927
    },
    {
      "type": "training",
      "description": "Training step 3704",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:14",
      "total_flops_so_far": 8.800519050814253e+16,
      "budget_used_percent": 88.00519050814253
    },
    {
      "type": "training",
      "description": "Training step 3705",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:15",
      "total_flops_so_far": 8.802893783363578e+16,
      "budget_used_percent": 88.02893783363578
    },
    {
      "type": "training",
      "description": "Training step 3706",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:16",
      "total_flops_so_far": 8.805268515912902e+16,
      "budget_used_percent": 88.05268515912903
    },
    {
      "type": "training",
      "description": "Training step 3707",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:18",
      "total_flops_so_far": 8.807643248462227e+16,
      "budget_used_percent": 88.07643248462227
    },
    {
      "type": "training",
      "description": "Training step 3708",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:19",
      "total_flops_so_far": 8.810017981011552e+16,
      "budget_used_percent": 88.10017981011552
    },
    {
      "type": "training",
      "description": "Training step 3709",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:21",
      "total_flops_so_far": 8.812392713560877e+16,
      "budget_used_percent": 88.12392713560877
    },
    {
      "type": "training",
      "description": "Training step 3710",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:22",
      "total_flops_so_far": 8.814767446110202e+16,
      "budget_used_percent": 88.14767446110203
    },
    {
      "type": "training",
      "description": "Training step 3711",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:23",
      "total_flops_so_far": 8.817142178659526e+16,
      "budget_used_percent": 88.17142178659526
    },
    {
      "type": "training",
      "description": "Training step 3712",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:25",
      "total_flops_so_far": 8.819516911208851e+16,
      "budget_used_percent": 88.19516911208851
    },
    {
      "type": "training",
      "description": "Training step 3713",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:26",
      "total_flops_so_far": 8.821891643758176e+16,
      "budget_used_percent": 88.21891643758177
    },
    {
      "type": "training",
      "description": "Training step 3714",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:27",
      "total_flops_so_far": 8.8242663763075e+16,
      "budget_used_percent": 88.24266376307502
    },
    {
      "type": "training",
      "description": "Training step 3715",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:29",
      "total_flops_so_far": 8.826641108856826e+16,
      "budget_used_percent": 88.26641108856825
    },
    {
      "type": "training",
      "description": "Training step 3716",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:30",
      "total_flops_so_far": 8.82901584140615e+16,
      "budget_used_percent": 88.29015841406151
    },
    {
      "type": "training",
      "description": "Training step 3717",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:31",
      "total_flops_so_far": 8.831390573955475e+16,
      "budget_used_percent": 88.31390573955476
    },
    {
      "type": "training",
      "description": "Training step 3718",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:33",
      "total_flops_so_far": 8.8337653065048e+16,
      "budget_used_percent": 88.337653065048
    },
    {
      "type": "training",
      "description": "Training step 3719",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:34",
      "total_flops_so_far": 8.836140039054125e+16,
      "budget_used_percent": 88.36140039054125
    },
    {
      "type": "training",
      "description": "Training step 3720",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:36",
      "total_flops_so_far": 8.83851477160345e+16,
      "budget_used_percent": 88.3851477160345
    },
    {
      "type": "training",
      "description": "Training step 3721",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:37",
      "total_flops_so_far": 8.840889504152774e+16,
      "budget_used_percent": 88.40889504152774
    },
    {
      "type": "training",
      "description": "Training step 3722",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:38",
      "total_flops_so_far": 8.8432642367021e+16,
      "budget_used_percent": 88.432642367021
    },
    {
      "type": "training",
      "description": "Training step 3723",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:40",
      "total_flops_so_far": 8.845638969251424e+16,
      "budget_used_percent": 88.45638969251424
    },
    {
      "type": "training",
      "description": "Training step 3724",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:41",
      "total_flops_so_far": 8.848013701800749e+16,
      "budget_used_percent": 88.48013701800748
    },
    {
      "type": "training",
      "description": "Training step 3725",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:42",
      "total_flops_so_far": 8.850388434350074e+16,
      "budget_used_percent": 88.50388434350074
    },
    {
      "type": "training",
      "description": "Training step 3726",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:44",
      "total_flops_so_far": 8.852763166899398e+16,
      "budget_used_percent": 88.52763166899399
    },
    {
      "type": "training",
      "description": "Training step 3727",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:45",
      "total_flops_so_far": 8.855137899448723e+16,
      "budget_used_percent": 88.55137899448722
    },
    {
      "type": "training",
      "description": "Training step 3728",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:46",
      "total_flops_so_far": 8.857512631998048e+16,
      "budget_used_percent": 88.57512631998048
    },
    {
      "type": "training",
      "description": "Training step 3729",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:48",
      "total_flops_so_far": 8.859887364547373e+16,
      "budget_used_percent": 88.59887364547373
    },
    {
      "type": "training",
      "description": "Training step 3730",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:49",
      "total_flops_so_far": 8.862262097096698e+16,
      "budget_used_percent": 88.62262097096698
    },
    {
      "type": "training",
      "description": "Training step 3731",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:51",
      "total_flops_so_far": 8.864636829646022e+16,
      "budget_used_percent": 88.64636829646022
    },
    {
      "type": "training",
      "description": "Training step 3732",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:52",
      "total_flops_so_far": 8.867011562195347e+16,
      "budget_used_percent": 88.67011562195347
    },
    {
      "type": "training",
      "description": "Training step 3733",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:53",
      "total_flops_so_far": 8.869386294744672e+16,
      "budget_used_percent": 88.69386294744672
    },
    {
      "type": "training",
      "description": "Training step 3734",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:55",
      "total_flops_so_far": 8.871761027293997e+16,
      "budget_used_percent": 88.71761027293998
    },
    {
      "type": "training",
      "description": "Training step 3735",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:56",
      "total_flops_so_far": 8.874135759843322e+16,
      "budget_used_percent": 88.74135759843321
    },
    {
      "type": "training",
      "description": "Training step 3736",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:57",
      "total_flops_so_far": 8.876510492392646e+16,
      "budget_used_percent": 88.76510492392646
    },
    {
      "type": "training",
      "description": "Training step 3737",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:07:59",
      "total_flops_so_far": 8.878885224941971e+16,
      "budget_used_percent": 88.78885224941972
    },
    {
      "type": "training",
      "description": "Training step 3738",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:00",
      "total_flops_so_far": 8.881259957491296e+16,
      "budget_used_percent": 88.81259957491297
    },
    {
      "type": "training",
      "description": "Training step 3739",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:02",
      "total_flops_so_far": 8.88363469004062e+16,
      "budget_used_percent": 88.8363469004062
    },
    {
      "type": "training",
      "description": "Training step 3740",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:03",
      "total_flops_so_far": 8.886009422589946e+16,
      "budget_used_percent": 88.86009422589946
    },
    {
      "type": "training",
      "description": "Training step 3741",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:04",
      "total_flops_so_far": 8.88838415513927e+16,
      "budget_used_percent": 88.8838415513927
    },
    {
      "type": "training",
      "description": "Training step 3742",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:06",
      "total_flops_so_far": 8.890758887688595e+16,
      "budget_used_percent": 88.90758887688595
    },
    {
      "type": "training",
      "description": "Training step 3743",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:07",
      "total_flops_so_far": 8.89313362023792e+16,
      "budget_used_percent": 88.9313362023792
    },
    {
      "type": "training",
      "description": "Training step 3744",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:08",
      "total_flops_so_far": 8.895508352787245e+16,
      "budget_used_percent": 88.95508352787245
    },
    {
      "type": "training",
      "description": "Training step 3745",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:10",
      "total_flops_so_far": 8.89788308533657e+16,
      "budget_used_percent": 88.9788308533657
    },
    {
      "type": "training",
      "description": "Training step 3746",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:11",
      "total_flops_so_far": 8.900257817885894e+16,
      "budget_used_percent": 89.00257817885895
    },
    {
      "type": "training",
      "description": "Training step 3747",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:13",
      "total_flops_so_far": 8.90263255043522e+16,
      "budget_used_percent": 89.02632550435219
    },
    {
      "type": "training",
      "description": "Training step 3748",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:14",
      "total_flops_so_far": 8.905007282984544e+16,
      "budget_used_percent": 89.05007282984543
    },
    {
      "type": "training",
      "description": "Training step 3749",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:15",
      "total_flops_so_far": 8.907382015533869e+16,
      "budget_used_percent": 89.0738201553387
    },
    {
      "type": "training",
      "description": "Training step 3750",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:17",
      "total_flops_so_far": 8.909756748083194e+16,
      "budget_used_percent": 89.09756748083194
    },
    {
      "type": "training",
      "description": "Training step 3751",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:18",
      "total_flops_so_far": 8.912131480632518e+16,
      "budget_used_percent": 89.12131480632517
    },
    {
      "type": "training",
      "description": "Training step 3752",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:19",
      "total_flops_so_far": 8.914506213181843e+16,
      "budget_used_percent": 89.14506213181843
    },
    {
      "type": "training",
      "description": "Training step 3753",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:21",
      "total_flops_so_far": 8.916880945731168e+16,
      "budget_used_percent": 89.16880945731168
    },
    {
      "type": "training",
      "description": "Training step 3754",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:22",
      "total_flops_so_far": 8.919255678280493e+16,
      "budget_used_percent": 89.19255678280493
    },
    {
      "type": "training",
      "description": "Training step 3755",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:23",
      "total_flops_so_far": 8.921630410829818e+16,
      "budget_used_percent": 89.21630410829817
    },
    {
      "type": "training",
      "description": "Training step 3756",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:25",
      "total_flops_so_far": 8.924005143379142e+16,
      "budget_used_percent": 89.24005143379142
    },
    {
      "type": "training",
      "description": "Training step 3757",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:26",
      "total_flops_so_far": 8.926379875928467e+16,
      "budget_used_percent": 89.26379875928467
    },
    {
      "type": "training",
      "description": "Training step 3758",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:27",
      "total_flops_so_far": 8.928754608477792e+16,
      "budget_used_percent": 89.28754608477793
    },
    {
      "type": "training",
      "description": "Training step 3759",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:29",
      "total_flops_so_far": 8.931129341027117e+16,
      "budget_used_percent": 89.31129341027116
    },
    {
      "type": "training",
      "description": "Training step 3760",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:30",
      "total_flops_so_far": 8.933504073576442e+16,
      "budget_used_percent": 89.33504073576441
    },
    {
      "type": "training",
      "description": "Training step 3761",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:32",
      "total_flops_so_far": 8.935878806125766e+16,
      "budget_used_percent": 89.35878806125767
    },
    {
      "type": "training",
      "description": "Training step 3762",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:33",
      "total_flops_so_far": 8.938253538675091e+16,
      "budget_used_percent": 89.38253538675092
    },
    {
      "type": "training",
      "description": "Training step 3763",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:34",
      "total_flops_so_far": 8.940628271224416e+16,
      "budget_used_percent": 89.40628271224415
    },
    {
      "type": "training",
      "description": "Training step 3764",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:36",
      "total_flops_so_far": 8.94300300377374e+16,
      "budget_used_percent": 89.43003003773741
    },
    {
      "type": "training",
      "description": "Training step 3765",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:37",
      "total_flops_so_far": 8.945377736323066e+16,
      "budget_used_percent": 89.45377736323066
    },
    {
      "type": "training",
      "description": "Training step 3766",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:38",
      "total_flops_so_far": 8.94775246887239e+16,
      "budget_used_percent": 89.4775246887239
    },
    {
      "type": "training",
      "description": "Training step 3767",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:40",
      "total_flops_so_far": 8.950127201421715e+16,
      "budget_used_percent": 89.50127201421715
    },
    {
      "type": "training",
      "description": "Training step 3768",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:41",
      "total_flops_so_far": 8.95250193397104e+16,
      "budget_used_percent": 89.5250193397104
    },
    {
      "type": "training",
      "description": "Training step 3769",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:42",
      "total_flops_so_far": 8.954876666520365e+16,
      "budget_used_percent": 89.54876666520364
    },
    {
      "type": "training",
      "description": "Training step 3770",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:44",
      "total_flops_so_far": 8.95725139906969e+16,
      "budget_used_percent": 89.5725139906969
    },
    {
      "type": "training",
      "description": "Training step 3771",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:45",
      "total_flops_so_far": 8.959626131619014e+16,
      "budget_used_percent": 89.59626131619014
    },
    {
      "type": "training",
      "description": "Training step 3772",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:47",
      "total_flops_so_far": 8.96200086416834e+16,
      "budget_used_percent": 89.62000864168338
    },
    {
      "type": "training",
      "description": "Training step 3773",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:48",
      "total_flops_so_far": 8.964375596717664e+16,
      "budget_used_percent": 89.64375596717665
    },
    {
      "type": "training",
      "description": "Training step 3774",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:49",
      "total_flops_so_far": 8.966750329266989e+16,
      "budget_used_percent": 89.66750329266989
    },
    {
      "type": "training",
      "description": "Training step 3775",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:51",
      "total_flops_so_far": 8.969125061816314e+16,
      "budget_used_percent": 89.69125061816314
    },
    {
      "type": "training",
      "description": "Training step 3776",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:52",
      "total_flops_so_far": 8.971499794365638e+16,
      "budget_used_percent": 89.71499794365639
    },
    {
      "type": "training",
      "description": "Training step 3777",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:54",
      "total_flops_so_far": 8.973874526914963e+16,
      "budget_used_percent": 89.73874526914963
    },
    {
      "type": "training",
      "description": "Training step 3778",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:55",
      "total_flops_so_far": 8.976249259464288e+16,
      "budget_used_percent": 89.76249259464288
    },
    {
      "type": "training",
      "description": "Training step 3779",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:56",
      "total_flops_so_far": 8.978623992013613e+16,
      "budget_used_percent": 89.78623992013613
    },
    {
      "type": "training",
      "description": "Training step 3780",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:58",
      "total_flops_so_far": 8.980998724562938e+16,
      "budget_used_percent": 89.80998724562937
    },
    {
      "type": "training",
      "description": "Training step 3781",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:08:59",
      "total_flops_so_far": 8.983373457112262e+16,
      "budget_used_percent": 89.83373457112262
    },
    {
      "type": "training",
      "description": "Training step 3782",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:09:00",
      "total_flops_so_far": 8.985748189661587e+16,
      "budget_used_percent": 89.85748189661588
    },
    {
      "type": "training",
      "description": "Training step 3783",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:09:02",
      "total_flops_so_far": 8.988122922210912e+16,
      "budget_used_percent": 89.88122922210911
    },
    {
      "type": "training",
      "description": "Training step 3784",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:09:03",
      "total_flops_so_far": 8.990497654760237e+16,
      "budget_used_percent": 89.90497654760236
    },
    {
      "type": "training",
      "description": "Training step 3785",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:09:04",
      "total_flops_so_far": 8.992872387309562e+16,
      "budget_used_percent": 89.92872387309562
    },
    {
      "type": "training",
      "description": "Training step 3786",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:09:06",
      "total_flops_so_far": 8.995247119858886e+16,
      "budget_used_percent": 89.95247119858887
    },
    {
      "type": "training",
      "description": "Training step 3787",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:09:07",
      "total_flops_so_far": 8.997621852408211e+16,
      "budget_used_percent": 89.97621852408211
    },
    {
      "type": "training",
      "description": "Training step 3788",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:09:09",
      "total_flops_so_far": 8.999996584957536e+16,
      "budget_used_percent": 89.99996584957536
    },
    {
      "type": "training",
      "description": "Training step 3789",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7915775164416.0,
      "backward_flops": 15831550328832.0,
      "flops": 23747325493248.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:09:10",
      "total_flops_so_far": 9.00237131750686e+16,
      "budget_used_percent": 90.02371317506861
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710078789056.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:09:16",
      "total_flops_so_far": 9.002442325385766e+16,
      "budget_used_percent": 90.02442325385766
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 713780608688.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:09:21",
      "total_flops_so_far": 9.002513703446635e+16,
      "budget_used_percent": 90.02513703446635
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711929338680.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:09:27",
      "total_flops_so_far": 9.002584896380502e+16,
      "budget_used_percent": 90.02584896380502
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710078789056.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:09:32",
      "total_flops_so_far": 9.002655904259408e+16,
      "budget_used_percent": 90.02655904259407
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712854883636.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:09:37",
      "total_flops_so_far": 9.002727189747771e+16,
      "budget_used_percent": 90.02727189747772
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 5",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710078789056.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:09:42",
      "total_flops_so_far": 9.002798197626677e+16,
      "budget_used_percent": 90.02798197626677
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 6",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711929338680.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:09:47",
      "total_flops_so_far": 9.002869390560544e+16,
      "budget_used_percent": 90.02869390560544
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 7",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711929338680.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:09:52",
      "total_flops_so_far": 9.002940583494413e+16,
      "budget_used_percent": 90.02940583494413
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 8",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711929338680.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:09:57",
      "total_flops_so_far": 9.003011776428282e+16,
      "budget_used_percent": 90.0301177642828
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 9",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711929338680.0,
      "lora_r": 4,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:02",
      "total_flops_so_far": 9.00308296936215e+16,
      "budget_used_percent": 90.03082969362151
    }
  ],
  "total_flops": 9.00308296936215e+16,
  "budget_used_percent": 90.03082969362151
}