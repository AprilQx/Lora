{
  "experiment_name": "lr1e-05_rank2",
  "model_config": {
    "hidden_size": 896,
    "num_attention_heads": 14,
    "num_hidden_layers": 24,
    "intermediate_size": 4864,
    "head_dim": 64,
    "vocab_size": 151936,
    "lora_r": 2,
    "lora_target_modules": [
      "q_proj",
      "v_proj"
    ]
  },
  "max_budget": 1e+17,
  "start_time": "2025-03-19 19:31:29",
  "operations": [
    {
      "type": "training",
      "description": "Training step 0",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:31:47",
      "total_flops_so_far": 23738872135680.0,
      "budget_used_percent": 0.02373887213568
    },
    {
      "type": "training",
      "description": "Training step 1",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:31:50",
      "total_flops_so_far": 47477744271360.0,
      "budget_used_percent": 0.04747774427136
    },
    {
      "type": "training",
      "description": "Training step 2",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:31:51",
      "total_flops_so_far": 71216616407040.0,
      "budget_used_percent": 0.07121661640704001
    },
    {
      "type": "training",
      "description": "Training step 3",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:31:52",
      "total_flops_so_far": 94955488542720.0,
      "budget_used_percent": 0.09495548854272
    },
    {
      "type": "training",
      "description": "Training step 4",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:31:54",
      "total_flops_so_far": 118694360678400.0,
      "budget_used_percent": 0.1186943606784
    },
    {
      "type": "training",
      "description": "Training step 5",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:31:55",
      "total_flops_so_far": 142433232814080.0,
      "budget_used_percent": 0.14243323281408002
    },
    {
      "type": "training",
      "description": "Training step 6",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:31:56",
      "total_flops_so_far": 166172104949760.0,
      "budget_used_percent": 0.16617210494975998
    },
    {
      "type": "training",
      "description": "Training step 7",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:31:57",
      "total_flops_so_far": 189910977085440.0,
      "budget_used_percent": 0.18991097708544
    },
    {
      "type": "training",
      "description": "Training step 8",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:31:59",
      "total_flops_so_far": 213649849221120.0,
      "budget_used_percent": 0.21364984922112
    },
    {
      "type": "training",
      "description": "Training step 9",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:00",
      "total_flops_so_far": 237388721356800.0,
      "budget_used_percent": 0.2373887213568
    },
    {
      "type": "training",
      "description": "Training step 10",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:01",
      "total_flops_so_far": 261127593492480.0,
      "budget_used_percent": 0.26112759349248
    },
    {
      "type": "training",
      "description": "Training step 11",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:03",
      "total_flops_so_far": 284866465628160.0,
      "budget_used_percent": 0.28486646562816004
    },
    {
      "type": "training",
      "description": "Training step 12",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:04",
      "total_flops_so_far": 308605337763840.0,
      "budget_used_percent": 0.30860533776384
    },
    {
      "type": "training",
      "description": "Training step 13",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:05",
      "total_flops_so_far": 332344209899520.0,
      "budget_used_percent": 0.33234420989951996
    },
    {
      "type": "training",
      "description": "Training step 14",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:06",
      "total_flops_so_far": 356083082035200.0,
      "budget_used_percent": 0.3560830820352
    },
    {
      "type": "training",
      "description": "Training step 15",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:08",
      "total_flops_so_far": 379821954170880.0,
      "budget_used_percent": 0.37982195417088
    },
    {
      "type": "training",
      "description": "Training step 16",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:09",
      "total_flops_so_far": 403560826306560.0,
      "budget_used_percent": 0.40356082630656
    },
    {
      "type": "training",
      "description": "Training step 17",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:10",
      "total_flops_so_far": 427299698442240.0,
      "budget_used_percent": 0.42729969844224
    },
    {
      "type": "training",
      "description": "Training step 18",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:12",
      "total_flops_so_far": 451038570577920.0,
      "budget_used_percent": 0.45103857057792
    },
    {
      "type": "training",
      "description": "Training step 19",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:13",
      "total_flops_so_far": 474777442713600.0,
      "budget_used_percent": 0.4747774427136
    },
    {
      "type": "training",
      "description": "Training step 20",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:14",
      "total_flops_so_far": 498516314849280.0,
      "budget_used_percent": 0.49851631484927994
    },
    {
      "type": "training",
      "description": "Training step 21",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:15",
      "total_flops_so_far": 522255186984960.0,
      "budget_used_percent": 0.52225518698496
    },
    {
      "type": "training",
      "description": "Training step 22",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:17",
      "total_flops_so_far": 545994059120640.0,
      "budget_used_percent": 0.54599405912064
    },
    {
      "type": "training",
      "description": "Training step 23",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:18",
      "total_flops_so_far": 569732931256320.0,
      "budget_used_percent": 0.5697329312563201
    },
    {
      "type": "training",
      "description": "Training step 24",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:19",
      "total_flops_so_far": 593471803392000.0,
      "budget_used_percent": 0.593471803392
    },
    {
      "type": "training",
      "description": "Training step 25",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:20",
      "total_flops_so_far": 617210675527680.0,
      "budget_used_percent": 0.61721067552768
    },
    {
      "type": "training",
      "description": "Training step 26",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:22",
      "total_flops_so_far": 640949547663360.0,
      "budget_used_percent": 0.64094954766336
    },
    {
      "type": "training",
      "description": "Training step 27",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:23",
      "total_flops_so_far": 664688419799040.0,
      "budget_used_percent": 0.6646884197990399
    },
    {
      "type": "training",
      "description": "Training step 28",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:24",
      "total_flops_so_far": 688427291934720.0,
      "budget_used_percent": 0.6884272919347201
    },
    {
      "type": "training",
      "description": "Training step 29",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:26",
      "total_flops_so_far": 712166164070400.0,
      "budget_used_percent": 0.7121661640704
    },
    {
      "type": "training",
      "description": "Training step 30",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:27",
      "total_flops_so_far": 735905036206080.0,
      "budget_used_percent": 0.73590503620608
    },
    {
      "type": "training",
      "description": "Training step 31",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:28",
      "total_flops_so_far": 759643908341760.0,
      "budget_used_percent": 0.75964390834176
    },
    {
      "type": "training",
      "description": "Training step 32",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:29",
      "total_flops_so_far": 783382780477440.0,
      "budget_used_percent": 0.7833827804774399
    },
    {
      "type": "training",
      "description": "Training step 33",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:31",
      "total_flops_so_far": 807121652613120.0,
      "budget_used_percent": 0.80712165261312
    },
    {
      "type": "training",
      "description": "Training step 34",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:32",
      "total_flops_so_far": 830860524748800.0,
      "budget_used_percent": 0.8308605247488001
    },
    {
      "type": "training",
      "description": "Training step 35",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:33",
      "total_flops_so_far": 854599396884480.0,
      "budget_used_percent": 0.85459939688448
    },
    {
      "type": "training",
      "description": "Training step 36",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:35",
      "total_flops_so_far": 878338269020160.0,
      "budget_used_percent": 0.87833826902016
    },
    {
      "type": "training",
      "description": "Training step 37",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:36",
      "total_flops_so_far": 902077141155840.0,
      "budget_used_percent": 0.90207714115584
    },
    {
      "type": "training",
      "description": "Training step 38",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:37",
      "total_flops_so_far": 925816013291520.0,
      "budget_used_percent": 0.92581601329152
    },
    {
      "type": "training",
      "description": "Training step 39",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:38",
      "total_flops_so_far": 949554885427200.0,
      "budget_used_percent": 0.9495548854272
    },
    {
      "type": "training",
      "description": "Training step 40",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:40",
      "total_flops_so_far": 973293757562880.0,
      "budget_used_percent": 0.9732937575628801
    },
    {
      "type": "training",
      "description": "Training step 41",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:41",
      "total_flops_so_far": 997032629698560.0,
      "budget_used_percent": 0.9970326296985599
    },
    {
      "type": "training",
      "description": "Training step 42",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:42",
      "total_flops_so_far": 1020771501834240.0,
      "budget_used_percent": 1.02077150183424
    },
    {
      "type": "training",
      "description": "Training step 43",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:44",
      "total_flops_so_far": 1044510373969920.0,
      "budget_used_percent": 1.04451037396992
    },
    {
      "type": "training",
      "description": "Training step 44",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:45",
      "total_flops_so_far": 1068249246105600.0,
      "budget_used_percent": 1.0682492461056001
    },
    {
      "type": "training",
      "description": "Training step 45",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:46",
      "total_flops_so_far": 1091988118241280.0,
      "budget_used_percent": 1.09198811824128
    },
    {
      "type": "training",
      "description": "Training step 46",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:47",
      "total_flops_so_far": 1115726990376960.0,
      "budget_used_percent": 1.11572699037696
    },
    {
      "type": "training",
      "description": "Training step 47",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:49",
      "total_flops_so_far": 1139465862512640.0,
      "budget_used_percent": 1.1394658625126401
    },
    {
      "type": "training",
      "description": "Training step 48",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:50",
      "total_flops_so_far": 1163204734648320.0,
      "budget_used_percent": 1.1632047346483199
    },
    {
      "type": "training",
      "description": "Training step 49",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:51",
      "total_flops_so_far": 1186943606784000.0,
      "budget_used_percent": 1.186943606784
    },
    {
      "type": "training",
      "description": "Training step 50",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:52",
      "total_flops_so_far": 1210682478919680.0,
      "budget_used_percent": 1.21068247891968
    },
    {
      "type": "training",
      "description": "Training step 51",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:54",
      "total_flops_so_far": 1234421351055360.0,
      "budget_used_percent": 1.23442135105536
    },
    {
      "type": "training",
      "description": "Training step 52",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:55",
      "total_flops_so_far": 1258160223191040.0,
      "budget_used_percent": 1.25816022319104
    },
    {
      "type": "training",
      "description": "Training step 53",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:56",
      "total_flops_so_far": 1281899095326720.0,
      "budget_used_percent": 1.28189909532672
    },
    {
      "type": "training",
      "description": "Training step 54",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:58",
      "total_flops_so_far": 1305637967462400.0,
      "budget_used_percent": 1.3056379674624001
    },
    {
      "type": "training",
      "description": "Training step 55",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:32:59",
      "total_flops_so_far": 1329376839598080.0,
      "budget_used_percent": 1.3293768395980798
    },
    {
      "type": "training",
      "description": "Training step 56",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:00",
      "total_flops_so_far": 1353115711733760.0,
      "budget_used_percent": 1.35311571173376
    },
    {
      "type": "training",
      "description": "Training step 57",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:01",
      "total_flops_so_far": 1376854583869440.0,
      "budget_used_percent": 1.3768545838694402
    },
    {
      "type": "training",
      "description": "Training step 58",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:03",
      "total_flops_so_far": 1400593456005120.0,
      "budget_used_percent": 1.4005934560051199
    },
    {
      "type": "training",
      "description": "Training step 59",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:04",
      "total_flops_so_far": 1424332328140800.0,
      "budget_used_percent": 1.4243323281408
    },
    {
      "type": "training",
      "description": "Training step 60",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:05",
      "total_flops_so_far": 1448071200276480.0,
      "budget_used_percent": 1.44807120027648
    },
    {
      "type": "training",
      "description": "Training step 61",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:07",
      "total_flops_so_far": 1471810072412160.0,
      "budget_used_percent": 1.47181007241216
    },
    {
      "type": "training",
      "description": "Training step 62",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:08",
      "total_flops_so_far": 1495548944547840.0,
      "budget_used_percent": 1.49554894454784
    },
    {
      "type": "training",
      "description": "Training step 63",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:09",
      "total_flops_so_far": 1519287816683520.0,
      "budget_used_percent": 1.51928781668352
    },
    {
      "type": "training",
      "description": "Training step 64",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:10",
      "total_flops_so_far": 1543026688819200.0,
      "budget_used_percent": 1.5430266888192001
    },
    {
      "type": "training",
      "description": "Training step 65",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:12",
      "total_flops_so_far": 1566765560954880.0,
      "budget_used_percent": 1.5667655609548798
    },
    {
      "type": "training",
      "description": "Training step 66",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:13",
      "total_flops_so_far": 1590504433090560.0,
      "budget_used_percent": 1.5905044330905602
    },
    {
      "type": "training",
      "description": "Training step 67",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:14",
      "total_flops_so_far": 1614243305226240.0,
      "budget_used_percent": 1.61424330522624
    },
    {
      "type": "training",
      "description": "Training step 68",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:16",
      "total_flops_so_far": 1637982177361920.0,
      "budget_used_percent": 1.6379821773619199
    },
    {
      "type": "training",
      "description": "Training step 69",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:17",
      "total_flops_so_far": 1661721049497600.0,
      "budget_used_percent": 1.6617210494976002
    },
    {
      "type": "training",
      "description": "Training step 70",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:18",
      "total_flops_so_far": 1685459921633280.0,
      "budget_used_percent": 1.68545992163328
    },
    {
      "type": "training",
      "description": "Training step 71",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:19",
      "total_flops_so_far": 1709198793768960.0,
      "budget_used_percent": 1.70919879376896
    },
    {
      "type": "training",
      "description": "Training step 72",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:21",
      "total_flops_so_far": 1732937665904640.0,
      "budget_used_percent": 1.73293766590464
    },
    {
      "type": "training",
      "description": "Training step 73",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:22",
      "total_flops_so_far": 1756676538040320.0,
      "budget_used_percent": 1.75667653804032
    },
    {
      "type": "training",
      "description": "Training step 74",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:23",
      "total_flops_so_far": 1780415410176000.0,
      "budget_used_percent": 1.780415410176
    },
    {
      "type": "training",
      "description": "Training step 75",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:25",
      "total_flops_so_far": 1804154282311680.0,
      "budget_used_percent": 1.80415428231168
    },
    {
      "type": "training",
      "description": "Training step 76",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:26",
      "total_flops_so_far": 1827893154447360.0,
      "budget_used_percent": 1.82789315444736
    },
    {
      "type": "training",
      "description": "Training step 77",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:27",
      "total_flops_so_far": 1851632026583040.0,
      "budget_used_percent": 1.85163202658304
    },
    {
      "type": "training",
      "description": "Training step 78",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:28",
      "total_flops_so_far": 1875370898718720.0,
      "budget_used_percent": 1.87537089871872
    },
    {
      "type": "training",
      "description": "Training step 79",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:30",
      "total_flops_so_far": 1899109770854400.0,
      "budget_used_percent": 1.8991097708544
    },
    {
      "type": "training",
      "description": "Training step 80",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:31",
      "total_flops_so_far": 1922848642990080.0,
      "budget_used_percent": 1.92284864299008
    },
    {
      "type": "training",
      "description": "Training step 81",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:32",
      "total_flops_so_far": 1946587515125760.0,
      "budget_used_percent": 1.9465875151257601
    },
    {
      "type": "training",
      "description": "Training step 82",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:33",
      "total_flops_so_far": 1970326387261440.0,
      "budget_used_percent": 1.97032638726144
    },
    {
      "type": "training",
      "description": "Training step 83",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:35",
      "total_flops_so_far": 1994065259397120.0,
      "budget_used_percent": 1.9940652593971198
    },
    {
      "type": "training",
      "description": "Training step 84",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:36",
      "total_flops_so_far": 2017804131532800.0,
      "budget_used_percent": 2.0178041315328
    },
    {
      "type": "training",
      "description": "Training step 85",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:37",
      "total_flops_so_far": 2041543003668480.0,
      "budget_used_percent": 2.04154300366848
    },
    {
      "type": "training",
      "description": "Training step 86",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:39",
      "total_flops_so_far": 2065281875804160.0,
      "budget_used_percent": 2.06528187580416
    },
    {
      "type": "training",
      "description": "Training step 87",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:40",
      "total_flops_so_far": 2089020747939840.0,
      "budget_used_percent": 2.08902074793984
    },
    {
      "type": "training",
      "description": "Training step 88",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:41",
      "total_flops_so_far": 2112759620075520.0,
      "budget_used_percent": 2.11275962007552
    },
    {
      "type": "training",
      "description": "Training step 89",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:42",
      "total_flops_so_far": 2136498492211200.0,
      "budget_used_percent": 2.1364984922112003
    },
    {
      "type": "training",
      "description": "Training step 90",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:44",
      "total_flops_so_far": 2160237364346880.0,
      "budget_used_percent": 2.16023736434688
    },
    {
      "type": "training",
      "description": "Training step 91",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:45",
      "total_flops_so_far": 2183976236482560.0,
      "budget_used_percent": 2.18397623648256
    },
    {
      "type": "training",
      "description": "Training step 92",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:46",
      "total_flops_so_far": 2207715108618240.0,
      "budget_used_percent": 2.20771510861824
    },
    {
      "type": "training",
      "description": "Training step 93",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:48",
      "total_flops_so_far": 2231453980753920.0,
      "budget_used_percent": 2.23145398075392
    },
    {
      "type": "training",
      "description": "Training step 94",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:49",
      "total_flops_so_far": 2255192852889600.0,
      "budget_used_percent": 2.2551928528896
    },
    {
      "type": "training",
      "description": "Training step 95",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:50",
      "total_flops_so_far": 2278931725025280.0,
      "budget_used_percent": 2.2789317250252803
    },
    {
      "type": "training",
      "description": "Training step 96",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:51",
      "total_flops_so_far": 2302670597160960.0,
      "budget_used_percent": 2.30267059716096
    },
    {
      "type": "training",
      "description": "Training step 97",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:53",
      "total_flops_so_far": 2326409469296640.0,
      "budget_used_percent": 2.3264094692966397
    },
    {
      "type": "training",
      "description": "Training step 98",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:54",
      "total_flops_so_far": 2350148341432320.0,
      "budget_used_percent": 2.35014834143232
    },
    {
      "type": "training",
      "description": "Training step 99",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:55",
      "total_flops_so_far": 2373887213568000.0,
      "budget_used_percent": 2.373887213568
    },
    {
      "type": "training",
      "description": "Training step 100",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:57",
      "total_flops_so_far": 2397626085703680.0,
      "budget_used_percent": 2.39762608570368
    },
    {
      "type": "training",
      "description": "Training step 101",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:58",
      "total_flops_so_far": 2421364957839360.0,
      "budget_used_percent": 2.42136495783936
    },
    {
      "type": "training",
      "description": "Training step 102",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:33:59",
      "total_flops_so_far": 2445103829975040.0,
      "budget_used_percent": 2.44510382997504
    },
    {
      "type": "training",
      "description": "Training step 103",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:00",
      "total_flops_so_far": 2468842702110720.0,
      "budget_used_percent": 2.46884270211072
    },
    {
      "type": "training",
      "description": "Training step 104",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:02",
      "total_flops_so_far": 2492581574246400.0,
      "budget_used_percent": 2.4925815742464
    },
    {
      "type": "training",
      "description": "Training step 105",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:03",
      "total_flops_so_far": 2516320446382080.0,
      "budget_used_percent": 2.51632044638208
    },
    {
      "type": "training",
      "description": "Training step 106",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:04",
      "total_flops_so_far": 2540059318517760.0,
      "budget_used_percent": 2.54005931851776
    },
    {
      "type": "training",
      "description": "Training step 107",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:06",
      "total_flops_so_far": 2563798190653440.0,
      "budget_used_percent": 2.56379819065344
    },
    {
      "type": "training",
      "description": "Training step 108",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:07",
      "total_flops_so_far": 2587537062789120.0,
      "budget_used_percent": 2.58753706278912
    },
    {
      "type": "training",
      "description": "Training step 109",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:08",
      "total_flops_so_far": 2611275934924800.0,
      "budget_used_percent": 2.6112759349248003
    },
    {
      "type": "training",
      "description": "Training step 110",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:09",
      "total_flops_so_far": 2635014807060480.0,
      "budget_used_percent": 2.63501480706048
    },
    {
      "type": "training",
      "description": "Training step 111",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:11",
      "total_flops_so_far": 2658753679196160.0,
      "budget_used_percent": 2.6587536791961597
    },
    {
      "type": "training",
      "description": "Training step 112",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:12",
      "total_flops_so_far": 2682492551331840.0,
      "budget_used_percent": 2.68249255133184
    },
    {
      "type": "training",
      "description": "Training step 113",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:13",
      "total_flops_so_far": 2706231423467520.0,
      "budget_used_percent": 2.70623142346752
    },
    {
      "type": "training",
      "description": "Training step 114",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:14",
      "total_flops_so_far": 2729970295603200.0,
      "budget_used_percent": 2.7299702956032
    },
    {
      "type": "training",
      "description": "Training step 115",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:16",
      "total_flops_so_far": 2753709167738880.0,
      "budget_used_percent": 2.7537091677388803
    },
    {
      "type": "training",
      "description": "Training step 116",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:17",
      "total_flops_so_far": 2777448039874560.0,
      "budget_used_percent": 2.77744803987456
    },
    {
      "type": "training",
      "description": "Training step 117",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:18",
      "total_flops_so_far": 2801186912010240.0,
      "budget_used_percent": 2.8011869120102397
    },
    {
      "type": "training",
      "description": "Training step 118",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:20",
      "total_flops_so_far": 2824925784145920.0,
      "budget_used_percent": 2.82492578414592
    },
    {
      "type": "training",
      "description": "Training step 119",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:21",
      "total_flops_so_far": 2848664656281600.0,
      "budget_used_percent": 2.8486646562816
    },
    {
      "type": "training",
      "description": "Training step 120",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:22",
      "total_flops_so_far": 2872403528417280.0,
      "budget_used_percent": 2.87240352841728
    },
    {
      "type": "training",
      "description": "Training step 121",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:23",
      "total_flops_so_far": 2896142400552960.0,
      "budget_used_percent": 2.89614240055296
    },
    {
      "type": "training",
      "description": "Training step 122",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:25",
      "total_flops_so_far": 2919881272688640.0,
      "budget_used_percent": 2.91988127268864
    },
    {
      "type": "training",
      "description": "Training step 123",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:26",
      "total_flops_so_far": 2943620144824320.0,
      "budget_used_percent": 2.94362014482432
    },
    {
      "type": "training",
      "description": "Training step 124",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:27",
      "total_flops_so_far": 2967359016960000.0,
      "budget_used_percent": 2.96735901696
    },
    {
      "type": "training",
      "description": "Training step 125",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:29",
      "total_flops_so_far": 2991097889095680.0,
      "budget_used_percent": 2.99109788909568
    },
    {
      "type": "training",
      "description": "Training step 126",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:30",
      "total_flops_so_far": 3014836761231360.0,
      "budget_used_percent": 3.01483676123136
    },
    {
      "type": "training",
      "description": "Training step 127",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:31",
      "total_flops_so_far": 3038575633367040.0,
      "budget_used_percent": 3.03857563336704
    },
    {
      "type": "training",
      "description": "Training step 128",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:32",
      "total_flops_so_far": 3062314505502720.0,
      "budget_used_percent": 3.06231450550272
    },
    {
      "type": "training",
      "description": "Training step 129",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:34",
      "total_flops_so_far": 3086053377638400.0,
      "budget_used_percent": 3.0860533776384003
    },
    {
      "type": "training",
      "description": "Training step 130",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:35",
      "total_flops_so_far": 3109792249774080.0,
      "budget_used_percent": 3.10979224977408
    },
    {
      "type": "training",
      "description": "Training step 131",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:36",
      "total_flops_so_far": 3133531121909760.0,
      "budget_used_percent": 3.1335311219097597
    },
    {
      "type": "training",
      "description": "Training step 132",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:37",
      "total_flops_so_far": 3157269994045440.0,
      "budget_used_percent": 3.1572699940454396
    },
    {
      "type": "training",
      "description": "Training step 133",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:38",
      "total_flops_so_far": 3181008866181120.0,
      "budget_used_percent": 3.1810088661811204
    },
    {
      "type": "training",
      "description": "Training step 134",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:39",
      "total_flops_so_far": 3204747738316800.0,
      "budget_used_percent": 3.2047477383168004
    },
    {
      "type": "training",
      "description": "Training step 135",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:40",
      "total_flops_so_far": 3228486610452480.0,
      "budget_used_percent": 3.22848661045248
    },
    {
      "type": "training",
      "description": "Training step 136",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:42",
      "total_flops_so_far": 3252225482588160.0,
      "budget_used_percent": 3.25222548258816
    },
    {
      "type": "training",
      "description": "Training step 137",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:43",
      "total_flops_so_far": 3275964354723840.0,
      "budget_used_percent": 3.2759643547238397
    },
    {
      "type": "training",
      "description": "Training step 138",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:44",
      "total_flops_so_far": 3299703226859520.0,
      "budget_used_percent": 3.2997032268595197
    },
    {
      "type": "training",
      "description": "Training step 139",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:46",
      "total_flops_so_far": 3323442098995200.0,
      "budget_used_percent": 3.3234420989952005
    },
    {
      "type": "training",
      "description": "Training step 140",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:47",
      "total_flops_so_far": 3347180971130880.0,
      "budget_used_percent": 3.34718097113088
    },
    {
      "type": "training",
      "description": "Training step 141",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:48",
      "total_flops_so_far": 3370919843266560.0,
      "budget_used_percent": 3.37091984326656
    },
    {
      "type": "training",
      "description": "Training step 142",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:49",
      "total_flops_so_far": 3394658715402240.0,
      "budget_used_percent": 3.39465871540224
    },
    {
      "type": "training",
      "description": "Training step 143",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:51",
      "total_flops_so_far": 3418397587537920.0,
      "budget_used_percent": 3.41839758753792
    },
    {
      "type": "training",
      "description": "Training step 144",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:52",
      "total_flops_so_far": 3442136459673600.0,
      "budget_used_percent": 3.4421364596735997
    },
    {
      "type": "training",
      "description": "Training step 145",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:53",
      "total_flops_so_far": 3465875331809280.0,
      "budget_used_percent": 3.46587533180928
    },
    {
      "type": "training",
      "description": "Training step 146",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:55",
      "total_flops_so_far": 3489614203944960.0,
      "budget_used_percent": 3.48961420394496
    },
    {
      "type": "training",
      "description": "Training step 147",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:56",
      "total_flops_so_far": 3513353076080640.0,
      "budget_used_percent": 3.51335307608064
    },
    {
      "type": "training",
      "description": "Training step 148",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:57",
      "total_flops_so_far": 3537091948216320.0,
      "budget_used_percent": 3.53709194821632
    },
    {
      "type": "training",
      "description": "Training step 149",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:34:58",
      "total_flops_so_far": 3560830820352000.0,
      "budget_used_percent": 3.560830820352
    },
    {
      "type": "training",
      "description": "Training step 150",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:00",
      "total_flops_so_far": 3584569692487680.0,
      "budget_used_percent": 3.58456969248768
    },
    {
      "type": "training",
      "description": "Training step 151",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:01",
      "total_flops_so_far": 3608308564623360.0,
      "budget_used_percent": 3.60830856462336
    },
    {
      "type": "training",
      "description": "Training step 152",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:02",
      "total_flops_so_far": 3632047436759040.0,
      "budget_used_percent": 3.63204743675904
    },
    {
      "type": "training",
      "description": "Training step 153",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:04",
      "total_flops_so_far": 3655786308894720.0,
      "budget_used_percent": 3.65578630889472
    },
    {
      "type": "training",
      "description": "Training step 154",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:05",
      "total_flops_so_far": 3679525181030400.0,
      "budget_used_percent": 3.6795251810304
    },
    {
      "type": "training",
      "description": "Training step 155",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:06",
      "total_flops_so_far": 3703264053166080.0,
      "budget_used_percent": 3.70326405316608
    },
    {
      "type": "training",
      "description": "Training step 156",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:07",
      "total_flops_so_far": 3727002925301760.0,
      "budget_used_percent": 3.7270029253017602
    },
    {
      "type": "training",
      "description": "Training step 157",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:09",
      "total_flops_so_far": 3750741797437440.0,
      "budget_used_percent": 3.75074179743744
    },
    {
      "type": "training",
      "description": "Training step 158",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:10",
      "total_flops_so_far": 3774480669573120.0,
      "budget_used_percent": 3.77448066957312
    },
    {
      "type": "training",
      "description": "Training step 159",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:11",
      "total_flops_so_far": 3798219541708800.0,
      "budget_used_percent": 3.7982195417088
    },
    {
      "type": "training",
      "description": "Training step 160",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:13",
      "total_flops_so_far": 3821958413844480.0,
      "budget_used_percent": 3.82195841384448
    },
    {
      "type": "training",
      "description": "Training step 161",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:14",
      "total_flops_so_far": 3845697285980160.0,
      "budget_used_percent": 3.84569728598016
    },
    {
      "type": "training",
      "description": "Training step 162",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:15",
      "total_flops_so_far": 3869436158115840.0,
      "budget_used_percent": 3.8694361581158403
    },
    {
      "type": "training",
      "description": "Training step 163",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:17",
      "total_flops_so_far": 3893175030251520.0,
      "budget_used_percent": 3.8931750302515202
    },
    {
      "type": "training",
      "description": "Training step 164",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:18",
      "total_flops_so_far": 3916913902387200.0,
      "budget_used_percent": 3.9169139023872
    },
    {
      "type": "training",
      "description": "Training step 165",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:19",
      "total_flops_so_far": 3940652774522880.0,
      "budget_used_percent": 3.94065277452288
    },
    {
      "type": "training",
      "description": "Training step 166",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:20",
      "total_flops_so_far": 3964391646658560.0,
      "budget_used_percent": 3.9643916466585596
    },
    {
      "type": "training",
      "description": "Training step 167",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:22",
      "total_flops_so_far": 3988130518794240.0,
      "budget_used_percent": 3.9881305187942395
    },
    {
      "type": "training",
      "description": "Training step 168",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:23",
      "total_flops_so_far": 4011869390929920.0,
      "budget_used_percent": 4.01186939092992
    },
    {
      "type": "training",
      "description": "Training step 169",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:24",
      "total_flops_so_far": 4035608263065600.0,
      "budget_used_percent": 4.0356082630656
    },
    {
      "type": "training",
      "description": "Training step 170",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:26",
      "total_flops_so_far": 4059347135201280.0,
      "budget_used_percent": 4.05934713520128
    },
    {
      "type": "training",
      "description": "Training step 171",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:27",
      "total_flops_so_far": 4083086007336960.0,
      "budget_used_percent": 4.08308600733696
    },
    {
      "type": "training",
      "description": "Training step 172",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:28",
      "total_flops_so_far": 4106824879472640.0,
      "budget_used_percent": 4.10682487947264
    },
    {
      "type": "training",
      "description": "Training step 173",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:29",
      "total_flops_so_far": 4130563751608320.0,
      "budget_used_percent": 4.13056375160832
    },
    {
      "type": "training",
      "description": "Training step 174",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:31",
      "total_flops_so_far": 4154302623744000.0,
      "budget_used_percent": 4.154302623744
    },
    {
      "type": "training",
      "description": "Training step 175",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:32",
      "total_flops_so_far": 4178041495879680.0,
      "budget_used_percent": 4.17804149587968
    },
    {
      "type": "training",
      "description": "Training step 176",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:33",
      "total_flops_so_far": 4201780368015360.0,
      "budget_used_percent": 4.20178036801536
    },
    {
      "type": "training",
      "description": "Training step 177",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:34",
      "total_flops_so_far": 4225519240151040.0,
      "budget_used_percent": 4.22551924015104
    },
    {
      "type": "training",
      "description": "Training step 178",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:36",
      "total_flops_so_far": 4249258112286720.0,
      "budget_used_percent": 4.24925811228672
    },
    {
      "type": "training",
      "description": "Training step 179",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:37",
      "total_flops_so_far": 4272996984422400.0,
      "budget_used_percent": 4.2729969844224005
    },
    {
      "type": "training",
      "description": "Training step 180",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:38",
      "total_flops_so_far": 4296735856558080.0,
      "budget_used_percent": 4.29673585655808
    },
    {
      "type": "training",
      "description": "Training step 181",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:40",
      "total_flops_so_far": 4320474728693760.0,
      "budget_used_percent": 4.32047472869376
    },
    {
      "type": "training",
      "description": "Training step 182",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:41",
      "total_flops_so_far": 4344213600829440.0,
      "budget_used_percent": 4.34421360082944
    },
    {
      "type": "training",
      "description": "Training step 183",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:42",
      "total_flops_so_far": 4367952472965120.0,
      "budget_used_percent": 4.36795247296512
    },
    {
      "type": "training",
      "description": "Training step 184",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:43",
      "total_flops_so_far": 4391691345100800.0,
      "budget_used_percent": 4.391691345100799
    },
    {
      "type": "training",
      "description": "Training step 185",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:45",
      "total_flops_so_far": 4415430217236480.0,
      "budget_used_percent": 4.41543021723648
    },
    {
      "type": "training",
      "description": "Training step 186",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:46",
      "total_flops_so_far": 4439169089372160.0,
      "budget_used_percent": 4.43916908937216
    },
    {
      "type": "training",
      "description": "Training step 187",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:47",
      "total_flops_so_far": 4462907961507840.0,
      "budget_used_percent": 4.46290796150784
    },
    {
      "type": "training",
      "description": "Training step 188",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:49",
      "total_flops_so_far": 4486646833643520.0,
      "budget_used_percent": 4.48664683364352
    },
    {
      "type": "training",
      "description": "Training step 189",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:50",
      "total_flops_so_far": 4510385705779200.0,
      "budget_used_percent": 4.5103857057792
    },
    {
      "type": "training",
      "description": "Training step 190",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:51",
      "total_flops_so_far": 4534124577914880.0,
      "budget_used_percent": 4.534124577914881
    },
    {
      "type": "training",
      "description": "Training step 191",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:52",
      "total_flops_so_far": 4557863450050560.0,
      "budget_used_percent": 4.557863450050561
    },
    {
      "type": "training",
      "description": "Training step 192",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:54",
      "total_flops_so_far": 4581602322186240.0,
      "budget_used_percent": 4.5816023221862405
    },
    {
      "type": "training",
      "description": "Training step 193",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:55",
      "total_flops_so_far": 4605341194321920.0,
      "budget_used_percent": 4.60534119432192
    },
    {
      "type": "training",
      "description": "Training step 194",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:56",
      "total_flops_so_far": 4629080066457600.0,
      "budget_used_percent": 4.6290800664575995
    },
    {
      "type": "training",
      "description": "Training step 195",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:58",
      "total_flops_so_far": 4652818938593280.0,
      "budget_used_percent": 4.652818938593279
    },
    {
      "type": "training",
      "description": "Training step 196",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:35:59",
      "total_flops_so_far": 4676557810728960.0,
      "budget_used_percent": 4.67655781072896
    },
    {
      "type": "training",
      "description": "Training step 197",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:00",
      "total_flops_so_far": 4700296682864640.0,
      "budget_used_percent": 4.70029668286464
    },
    {
      "type": "training",
      "description": "Training step 198",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:01",
      "total_flops_so_far": 4724035555000320.0,
      "budget_used_percent": 4.72403555500032
    },
    {
      "type": "training",
      "description": "Training step 199",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:03",
      "total_flops_so_far": 4747774427136000.0,
      "budget_used_percent": 4.747774427136
    },
    {
      "type": "training",
      "description": "Training step 200",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:04",
      "total_flops_so_far": 4771513299271680.0,
      "budget_used_percent": 4.77151329927168
    },
    {
      "type": "training",
      "description": "Training step 201",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:05",
      "total_flops_so_far": 4795252171407360.0,
      "budget_used_percent": 4.79525217140736
    },
    {
      "type": "training",
      "description": "Training step 202",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:07",
      "total_flops_so_far": 4818991043543040.0,
      "budget_used_percent": 4.81899104354304
    },
    {
      "type": "training",
      "description": "Training step 203",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:08",
      "total_flops_so_far": 4842729915678720.0,
      "budget_used_percent": 4.84272991567872
    },
    {
      "type": "training",
      "description": "Training step 204",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:09",
      "total_flops_so_far": 4866468787814400.0,
      "budget_used_percent": 4.8664687878144
    },
    {
      "type": "training",
      "description": "Training step 205",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:10",
      "total_flops_so_far": 4890207659950080.0,
      "budget_used_percent": 4.89020765995008
    },
    {
      "type": "training",
      "description": "Training step 206",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:12",
      "total_flops_so_far": 4913946532085760.0,
      "budget_used_percent": 4.91394653208576
    },
    {
      "type": "training",
      "description": "Training step 207",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:13",
      "total_flops_so_far": 4937685404221440.0,
      "budget_used_percent": 4.93768540422144
    },
    {
      "type": "training",
      "description": "Training step 208",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:14",
      "total_flops_so_far": 4961424276357120.0,
      "budget_used_percent": 4.96142427635712
    },
    {
      "type": "training",
      "description": "Training step 209",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:16",
      "total_flops_so_far": 4985163148492800.0,
      "budget_used_percent": 4.9851631484928
    },
    {
      "type": "training",
      "description": "Training step 210",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:17",
      "total_flops_so_far": 5008902020628480.0,
      "budget_used_percent": 5.00890202062848
    },
    {
      "type": "training",
      "description": "Training step 211",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:18",
      "total_flops_so_far": 5032640892764160.0,
      "budget_used_percent": 5.03264089276416
    },
    {
      "type": "training",
      "description": "Training step 212",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:19",
      "total_flops_so_far": 5056379764899840.0,
      "budget_used_percent": 5.05637976489984
    },
    {
      "type": "training",
      "description": "Training step 213",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:21",
      "total_flops_so_far": 5080118637035520.0,
      "budget_used_percent": 5.08011863703552
    },
    {
      "type": "training",
      "description": "Training step 214",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:22",
      "total_flops_so_far": 5103857509171200.0,
      "budget_used_percent": 5.1038575091712
    },
    {
      "type": "training",
      "description": "Training step 215",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:23",
      "total_flops_so_far": 5127596381306880.0,
      "budget_used_percent": 5.12759638130688
    },
    {
      "type": "training",
      "description": "Training step 216",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:25",
      "total_flops_so_far": 5151335253442560.0,
      "budget_used_percent": 5.15133525344256
    },
    {
      "type": "training",
      "description": "Training step 217",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:26",
      "total_flops_so_far": 5175074125578240.0,
      "budget_used_percent": 5.17507412557824
    },
    {
      "type": "training",
      "description": "Training step 218",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:27",
      "total_flops_so_far": 5198812997713920.0,
      "budget_used_percent": 5.19881299771392
    },
    {
      "type": "training",
      "description": "Training step 219",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:28",
      "total_flops_so_far": 5222551869849600.0,
      "budget_used_percent": 5.2225518698496005
    },
    {
      "type": "training",
      "description": "Training step 220",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:30",
      "total_flops_so_far": 5246290741985280.0,
      "budget_used_percent": 5.2462907419852804
    },
    {
      "type": "training",
      "description": "Training step 221",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:31",
      "total_flops_so_far": 5270029614120960.0,
      "budget_used_percent": 5.27002961412096
    },
    {
      "type": "training",
      "description": "Training step 222",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:32",
      "total_flops_so_far": 5293768486256640.0,
      "budget_used_percent": 5.29376848625664
    },
    {
      "type": "training",
      "description": "Training step 223",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:34",
      "total_flops_so_far": 5317507358392320.0,
      "budget_used_percent": 5.317507358392319
    },
    {
      "type": "training",
      "description": "Training step 224",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:35",
      "total_flops_so_far": 5341246230528000.0,
      "budget_used_percent": 5.341246230527999
    },
    {
      "type": "training",
      "description": "Training step 225",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:36",
      "total_flops_so_far": 5364985102663680.0,
      "budget_used_percent": 5.36498510266368
    },
    {
      "type": "training",
      "description": "Training step 226",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:37",
      "total_flops_so_far": 5388723974799360.0,
      "budget_used_percent": 5.38872397479936
    },
    {
      "type": "training",
      "description": "Training step 227",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:39",
      "total_flops_so_far": 5412462846935040.0,
      "budget_used_percent": 5.41246284693504
    },
    {
      "type": "training",
      "description": "Training step 228",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:40",
      "total_flops_so_far": 5436201719070720.0,
      "budget_used_percent": 5.43620171907072
    },
    {
      "type": "training",
      "description": "Training step 229",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:41",
      "total_flops_so_far": 5459940591206400.0,
      "budget_used_percent": 5.4599405912064
    },
    {
      "type": "training",
      "description": "Training step 230",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:43",
      "total_flops_so_far": 5483679463342080.0,
      "budget_used_percent": 5.483679463342081
    },
    {
      "type": "training",
      "description": "Training step 231",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:44",
      "total_flops_so_far": 5507418335477760.0,
      "budget_used_percent": 5.507418335477761
    },
    {
      "type": "training",
      "description": "Training step 232",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:45",
      "total_flops_so_far": 5531157207613440.0,
      "budget_used_percent": 5.53115720761344
    },
    {
      "type": "training",
      "description": "Training step 233",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:46",
      "total_flops_so_far": 5554896079749120.0,
      "budget_used_percent": 5.55489607974912
    },
    {
      "type": "training",
      "description": "Training step 234",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:48",
      "total_flops_so_far": 5578634951884800.0,
      "budget_used_percent": 5.5786349518847995
    },
    {
      "type": "training",
      "description": "Training step 235",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:49",
      "total_flops_so_far": 5602373824020480.0,
      "budget_used_percent": 5.6023738240204795
    },
    {
      "type": "training",
      "description": "Training step 236",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:50",
      "total_flops_so_far": 5626112696156160.0,
      "budget_used_percent": 5.62611269615616
    },
    {
      "type": "training",
      "description": "Training step 237",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:52",
      "total_flops_so_far": 5649851568291840.0,
      "budget_used_percent": 5.64985156829184
    },
    {
      "type": "training",
      "description": "Training step 238",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:53",
      "total_flops_so_far": 5673590440427520.0,
      "budget_used_percent": 5.67359044042752
    },
    {
      "type": "training",
      "description": "Training step 239",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:54",
      "total_flops_so_far": 5697329312563200.0,
      "budget_used_percent": 5.6973293125632
    },
    {
      "type": "training",
      "description": "Training step 240",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:55",
      "total_flops_so_far": 5721068184698880.0,
      "budget_used_percent": 5.72106818469888
    },
    {
      "type": "training",
      "description": "Training step 241",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:57",
      "total_flops_so_far": 5744807056834560.0,
      "budget_used_percent": 5.74480705683456
    },
    {
      "type": "training",
      "description": "Training step 242",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:58",
      "total_flops_so_far": 5768545928970240.0,
      "budget_used_percent": 5.76854592897024
    },
    {
      "type": "training",
      "description": "Training step 243",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:36:59",
      "total_flops_so_far": 5792284801105920.0,
      "budget_used_percent": 5.79228480110592
    },
    {
      "type": "training",
      "description": "Training step 244",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:01",
      "total_flops_so_far": 5816023673241600.0,
      "budget_used_percent": 5.8160236732416
    },
    {
      "type": "training",
      "description": "Training step 245",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:02",
      "total_flops_so_far": 5839762545377280.0,
      "budget_used_percent": 5.83976254537728
    },
    {
      "type": "training",
      "description": "Training step 246",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:03",
      "total_flops_so_far": 5863501417512960.0,
      "budget_used_percent": 5.86350141751296
    },
    {
      "type": "training",
      "description": "Training step 247",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:04",
      "total_flops_so_far": 5887240289648640.0,
      "budget_used_percent": 5.88724028964864
    },
    {
      "type": "training",
      "description": "Training step 248",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:06",
      "total_flops_so_far": 5910979161784320.0,
      "budget_used_percent": 5.91097916178432
    },
    {
      "type": "training",
      "description": "Training step 249",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:07",
      "total_flops_so_far": 5934718033920000.0,
      "budget_used_percent": 5.93471803392
    },
    {
      "type": "training",
      "description": "Training step 250",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:09",
      "total_flops_so_far": 5958456906055680.0,
      "budget_used_percent": 5.95845690605568
    },
    {
      "type": "training",
      "description": "Training step 251",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:10",
      "total_flops_so_far": 5982195778191360.0,
      "budget_used_percent": 5.98219577819136
    },
    {
      "type": "training",
      "description": "Training step 252",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:11",
      "total_flops_so_far": 6005934650327040.0,
      "budget_used_percent": 6.00593465032704
    },
    {
      "type": "training",
      "description": "Training step 253",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:12",
      "total_flops_so_far": 6029673522462720.0,
      "budget_used_percent": 6.02967352246272
    },
    {
      "type": "training",
      "description": "Training step 254",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:14",
      "total_flops_so_far": 6053412394598400.0,
      "budget_used_percent": 6.0534123945984
    },
    {
      "type": "training",
      "description": "Training step 255",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:15",
      "total_flops_so_far": 6077151266734080.0,
      "budget_used_percent": 6.07715126673408
    },
    {
      "type": "training",
      "description": "Training step 256",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:16",
      "total_flops_so_far": 6100890138869760.0,
      "budget_used_percent": 6.10089013886976
    },
    {
      "type": "training",
      "description": "Training step 257",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:18",
      "total_flops_so_far": 6124629011005440.0,
      "budget_used_percent": 6.12462901100544
    },
    {
      "type": "training",
      "description": "Training step 258",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:19",
      "total_flops_so_far": 6148367883141120.0,
      "budget_used_percent": 6.14836788314112
    },
    {
      "type": "training",
      "description": "Training step 259",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:20",
      "total_flops_so_far": 6172106755276800.0,
      "budget_used_percent": 6.1721067552768005
    },
    {
      "type": "training",
      "description": "Training step 260",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:21",
      "total_flops_so_far": 6195845627412480.0,
      "budget_used_percent": 6.1958456274124805
    },
    {
      "type": "training",
      "description": "Training step 261",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:23",
      "total_flops_so_far": 6219584499548160.0,
      "budget_used_percent": 6.21958449954816
    },
    {
      "type": "training",
      "description": "Training step 262",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:24",
      "total_flops_so_far": 6243323371683840.0,
      "budget_used_percent": 6.243323371683839
    },
    {
      "type": "training",
      "description": "Training step 263",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:25",
      "total_flops_so_far": 6267062243819520.0,
      "budget_used_percent": 6.267062243819519
    },
    {
      "type": "training",
      "description": "Training step 264",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:26",
      "total_flops_so_far": 6290801115955200.0,
      "budget_used_percent": 6.290801115955199
    },
    {
      "type": "training",
      "description": "Training step 265",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:27",
      "total_flops_so_far": 6314539988090880.0,
      "budget_used_percent": 6.314539988090879
    },
    {
      "type": "training",
      "description": "Training step 266",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:28",
      "total_flops_so_far": 6338278860226560.0,
      "budget_used_percent": 6.338278860226559
    },
    {
      "type": "training",
      "description": "Training step 267",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:29",
      "total_flops_so_far": 6362017732362240.0,
      "budget_used_percent": 6.362017732362241
    },
    {
      "type": "training",
      "description": "Training step 268",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:31",
      "total_flops_so_far": 6385756604497920.0,
      "budget_used_percent": 6.385756604497921
    },
    {
      "type": "training",
      "description": "Training step 269",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:32",
      "total_flops_so_far": 6409495476633600.0,
      "budget_used_percent": 6.409495476633601
    },
    {
      "type": "training",
      "description": "Training step 270",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:33",
      "total_flops_so_far": 6433234348769280.0,
      "budget_used_percent": 6.433234348769281
    },
    {
      "type": "training",
      "description": "Training step 271",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:35",
      "total_flops_so_far": 6456973220904960.0,
      "budget_used_percent": 6.45697322090496
    },
    {
      "type": "training",
      "description": "Training step 272",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:36",
      "total_flops_so_far": 6480712093040640.0,
      "budget_used_percent": 6.48071209304064
    },
    {
      "type": "training",
      "description": "Training step 273",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:37",
      "total_flops_so_far": 6504450965176320.0,
      "budget_used_percent": 6.50445096517632
    },
    {
      "type": "training",
      "description": "Training step 274",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:38",
      "total_flops_so_far": 6528189837312000.0,
      "budget_used_percent": 6.5281898373119995
    },
    {
      "type": "training",
      "description": "Training step 275",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:40",
      "total_flops_so_far": 6551928709447680.0,
      "budget_used_percent": 6.5519287094476795
    },
    {
      "type": "training",
      "description": "Training step 276",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:41",
      "total_flops_so_far": 6575667581583360.0,
      "budget_used_percent": 6.575667581583359
    },
    {
      "type": "training",
      "description": "Training step 277",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:42",
      "total_flops_so_far": 6599406453719040.0,
      "budget_used_percent": 6.599406453719039
    },
    {
      "type": "training",
      "description": "Training step 278",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:44",
      "total_flops_so_far": 6623145325854720.0,
      "budget_used_percent": 6.623145325854719
    },
    {
      "type": "training",
      "description": "Training step 279",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:45",
      "total_flops_so_far": 6646884197990400.0,
      "budget_used_percent": 6.646884197990401
    },
    {
      "type": "training",
      "description": "Training step 280",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:46",
      "total_flops_so_far": 6670623070126080.0,
      "budget_used_percent": 6.67062307012608
    },
    {
      "type": "training",
      "description": "Training step 281",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:47",
      "total_flops_so_far": 6694361942261760.0,
      "budget_used_percent": 6.69436194226176
    },
    {
      "type": "training",
      "description": "Training step 282",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:49",
      "total_flops_so_far": 6718100814397440.0,
      "budget_used_percent": 6.71810081439744
    },
    {
      "type": "training",
      "description": "Training step 283",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:50",
      "total_flops_so_far": 6741839686533120.0,
      "budget_used_percent": 6.74183968653312
    },
    {
      "type": "training",
      "description": "Training step 284",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:51",
      "total_flops_so_far": 6765578558668800.0,
      "budget_used_percent": 6.7655785586688
    },
    {
      "type": "training",
      "description": "Training step 285",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:53",
      "total_flops_so_far": 6789317430804480.0,
      "budget_used_percent": 6.78931743080448
    },
    {
      "type": "training",
      "description": "Training step 286",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:54",
      "total_flops_so_far": 6813056302940160.0,
      "budget_used_percent": 6.81305630294016
    },
    {
      "type": "training",
      "description": "Training step 287",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:55",
      "total_flops_so_far": 6836795175075840.0,
      "budget_used_percent": 6.83679517507584
    },
    {
      "type": "training",
      "description": "Training step 288",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:56",
      "total_flops_so_far": 6860534047211520.0,
      "budget_used_percent": 6.8605340472115195
    },
    {
      "type": "training",
      "description": "Training step 289",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:58",
      "total_flops_so_far": 6884272919347200.0,
      "budget_used_percent": 6.884272919347199
    },
    {
      "type": "training",
      "description": "Training step 290",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:37:59",
      "total_flops_so_far": 6908011791482880.0,
      "budget_used_percent": 6.90801179148288
    },
    {
      "type": "training",
      "description": "Training step 291",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:00",
      "total_flops_so_far": 6931750663618560.0,
      "budget_used_percent": 6.93175066361856
    },
    {
      "type": "training",
      "description": "Training step 292",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:02",
      "total_flops_so_far": 6955489535754240.0,
      "budget_used_percent": 6.95548953575424
    },
    {
      "type": "training",
      "description": "Training step 293",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:03",
      "total_flops_so_far": 6979228407889920.0,
      "budget_used_percent": 6.97922840788992
    },
    {
      "type": "training",
      "description": "Training step 294",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:04",
      "total_flops_so_far": 7002967280025600.0,
      "budget_used_percent": 7.0029672800256
    },
    {
      "type": "training",
      "description": "Training step 295",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:05",
      "total_flops_so_far": 7026706152161280.0,
      "budget_used_percent": 7.02670615216128
    },
    {
      "type": "training",
      "description": "Training step 296",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:07",
      "total_flops_so_far": 7050445024296960.0,
      "budget_used_percent": 7.05044502429696
    },
    {
      "type": "training",
      "description": "Training step 297",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:08",
      "total_flops_so_far": 7074183896432640.0,
      "budget_used_percent": 7.07418389643264
    },
    {
      "type": "training",
      "description": "Training step 298",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:09",
      "total_flops_so_far": 7097922768568320.0,
      "budget_used_percent": 7.09792276856832
    },
    {
      "type": "training",
      "description": "Training step 299",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:11",
      "total_flops_so_far": 7121661640704000.0,
      "budget_used_percent": 7.121661640704
    },
    {
      "type": "training",
      "description": "Training step 300",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:12",
      "total_flops_so_far": 7145400512839680.0,
      "budget_used_percent": 7.14540051283968
    },
    {
      "type": "training",
      "description": "Training step 301",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:13",
      "total_flops_so_far": 7169139384975360.0,
      "budget_used_percent": 7.16913938497536
    },
    {
      "type": "training",
      "description": "Training step 302",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:15",
      "total_flops_so_far": 7192878257111040.0,
      "budget_used_percent": 7.19287825711104
    },
    {
      "type": "training",
      "description": "Training step 303",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:16",
      "total_flops_so_far": 7216617129246720.0,
      "budget_used_percent": 7.21661712924672
    },
    {
      "type": "training",
      "description": "Training step 304",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:17",
      "total_flops_so_far": 7240356001382400.0,
      "budget_used_percent": 7.2403560013824
    },
    {
      "type": "training",
      "description": "Training step 305",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:18",
      "total_flops_so_far": 7264094873518080.0,
      "budget_used_percent": 7.26409487351808
    },
    {
      "type": "training",
      "description": "Training step 306",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:20",
      "total_flops_so_far": 7287833745653760.0,
      "budget_used_percent": 7.28783374565376
    },
    {
      "type": "training",
      "description": "Training step 307",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:21",
      "total_flops_so_far": 7311572617789440.0,
      "budget_used_percent": 7.31157261778944
    },
    {
      "type": "training",
      "description": "Training step 308",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:22",
      "total_flops_so_far": 7335311489925120.0,
      "budget_used_percent": 7.33531148992512
    },
    {
      "type": "training",
      "description": "Training step 309",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:24",
      "total_flops_so_far": 7359050362060800.0,
      "budget_used_percent": 7.3590503620608
    },
    {
      "type": "training",
      "description": "Training step 310",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:25",
      "total_flops_so_far": 7382789234196480.0,
      "budget_used_percent": 7.38278923419648
    },
    {
      "type": "training",
      "description": "Training step 311",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:26",
      "total_flops_so_far": 7406528106332160.0,
      "budget_used_percent": 7.40652810633216
    },
    {
      "type": "training",
      "description": "Training step 312",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:27",
      "total_flops_so_far": 7430266978467840.0,
      "budget_used_percent": 7.43026697846784
    },
    {
      "type": "training",
      "description": "Training step 313",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:29",
      "total_flops_so_far": 7454005850603520.0,
      "budget_used_percent": 7.4540058506035205
    },
    {
      "type": "training",
      "description": "Training step 314",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:30",
      "total_flops_so_far": 7477744722739200.0,
      "budget_used_percent": 7.4777447227392
    },
    {
      "type": "training",
      "description": "Training step 315",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:31",
      "total_flops_so_far": 7501483594874880.0,
      "budget_used_percent": 7.50148359487488
    },
    {
      "type": "training",
      "description": "Training step 316",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:33",
      "total_flops_so_far": 7525222467010560.0,
      "budget_used_percent": 7.52522246701056
    },
    {
      "type": "training",
      "description": "Training step 317",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:34",
      "total_flops_so_far": 7548961339146240.0,
      "budget_used_percent": 7.54896133914624
    },
    {
      "type": "training",
      "description": "Training step 318",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:35",
      "total_flops_so_far": 7572700211281920.0,
      "budget_used_percent": 7.57270021128192
    },
    {
      "type": "training",
      "description": "Training step 319",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:36",
      "total_flops_so_far": 7596439083417600.0,
      "budget_used_percent": 7.5964390834176
    },
    {
      "type": "training",
      "description": "Training step 320",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:38",
      "total_flops_so_far": 7620177955553280.0,
      "budget_used_percent": 7.62017795555328
    },
    {
      "type": "training",
      "description": "Training step 321",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:39",
      "total_flops_so_far": 7643916827688960.0,
      "budget_used_percent": 7.64391682768896
    },
    {
      "type": "training",
      "description": "Training step 322",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:40",
      "total_flops_so_far": 7667655699824640.0,
      "budget_used_percent": 7.66765569982464
    },
    {
      "type": "training",
      "description": "Training step 323",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:42",
      "total_flops_so_far": 7691394571960320.0,
      "budget_used_percent": 7.69139457196032
    },
    {
      "type": "training",
      "description": "Training step 324",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:43",
      "total_flops_so_far": 7715133444096000.0,
      "budget_used_percent": 7.715133444096001
    },
    {
      "type": "training",
      "description": "Training step 325",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:44",
      "total_flops_so_far": 7738872316231680.0,
      "budget_used_percent": 7.738872316231681
    },
    {
      "type": "training",
      "description": "Training step 326",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:45",
      "total_flops_so_far": 7762611188367360.0,
      "budget_used_percent": 7.7626111883673605
    },
    {
      "type": "training",
      "description": "Training step 327",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:47",
      "total_flops_so_far": 7786350060503040.0,
      "budget_used_percent": 7.7863500605030405
    },
    {
      "type": "training",
      "description": "Training step 328",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:48",
      "total_flops_so_far": 7810088932638720.0,
      "budget_used_percent": 7.81008893263872
    },
    {
      "type": "training",
      "description": "Training step 329",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:49",
      "total_flops_so_far": 7833827804774400.0,
      "budget_used_percent": 7.8338278047744
    },
    {
      "type": "training",
      "description": "Training step 330",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:51",
      "total_flops_so_far": 7857566676910080.0,
      "budget_used_percent": 7.85756667691008
    },
    {
      "type": "training",
      "description": "Training step 331",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:52",
      "total_flops_so_far": 7881305549045760.0,
      "budget_used_percent": 7.88130554904576
    },
    {
      "type": "training",
      "description": "Training step 332",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:53",
      "total_flops_so_far": 7905044421181440.0,
      "budget_used_percent": 7.90504442118144
    },
    {
      "type": "training",
      "description": "Training step 333",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:54",
      "total_flops_so_far": 7928783293317120.0,
      "budget_used_percent": 7.928783293317119
    },
    {
      "type": "training",
      "description": "Training step 334",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:56",
      "total_flops_so_far": 7952522165452800.0,
      "budget_used_percent": 7.952522165452799
    },
    {
      "type": "training",
      "description": "Training step 335",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:57",
      "total_flops_so_far": 7976261037588480.0,
      "budget_used_percent": 7.976261037588479
    },
    {
      "type": "training",
      "description": "Training step 336",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:38:58",
      "total_flops_so_far": 7999999909724160.0,
      "budget_used_percent": 7.999999909724161
    },
    {
      "type": "training",
      "description": "Training step 337",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:00",
      "total_flops_so_far": 8023738781859840.0,
      "budget_used_percent": 8.02373878185984
    },
    {
      "type": "training",
      "description": "Training step 338",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:01",
      "total_flops_so_far": 8047477653995520.0,
      "budget_used_percent": 8.04747765399552
    },
    {
      "type": "training",
      "description": "Training step 339",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:02",
      "total_flops_so_far": 8071216526131200.0,
      "budget_used_percent": 8.0712165261312
    },
    {
      "type": "training",
      "description": "Training step 340",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:03",
      "total_flops_so_far": 8094955398266880.0,
      "budget_used_percent": 8.09495539826688
    },
    {
      "type": "training",
      "description": "Training step 341",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:05",
      "total_flops_so_far": 8118694270402560.0,
      "budget_used_percent": 8.11869427040256
    },
    {
      "type": "training",
      "description": "Training step 342",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:06",
      "total_flops_so_far": 8142433142538240.0,
      "budget_used_percent": 8.14243314253824
    },
    {
      "type": "training",
      "description": "Training step 343",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:07",
      "total_flops_so_far": 8166172014673920.0,
      "budget_used_percent": 8.16617201467392
    },
    {
      "type": "training",
      "description": "Training step 344",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:09",
      "total_flops_so_far": 8189910886809600.0,
      "budget_used_percent": 8.1899108868096
    },
    {
      "type": "training",
      "description": "Training step 345",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:10",
      "total_flops_so_far": 8213649758945280.0,
      "budget_used_percent": 8.21364975894528
    },
    {
      "type": "training",
      "description": "Training step 346",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:11",
      "total_flops_so_far": 8237388631080960.0,
      "budget_used_percent": 8.23738863108096
    },
    {
      "type": "training",
      "description": "Training step 347",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:12",
      "total_flops_so_far": 8261127503216640.0,
      "budget_used_percent": 8.26112750321664
    },
    {
      "type": "training",
      "description": "Training step 348",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:14",
      "total_flops_so_far": 8284866375352320.0,
      "budget_used_percent": 8.28486637535232
    },
    {
      "type": "training",
      "description": "Training step 349",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:15",
      "total_flops_so_far": 8308605247488000.0,
      "budget_used_percent": 8.308605247488
    },
    {
      "type": "training",
      "description": "Training step 350",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:16",
      "total_flops_so_far": 8332344119623680.0,
      "budget_used_percent": 8.33234411962368
    },
    {
      "type": "training",
      "description": "Training step 351",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:18",
      "total_flops_so_far": 8356082991759360.0,
      "budget_used_percent": 8.35608299175936
    },
    {
      "type": "training",
      "description": "Training step 352",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:19",
      "total_flops_so_far": 8379821863895040.0,
      "budget_used_percent": 8.37982186389504
    },
    {
      "type": "training",
      "description": "Training step 353",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:20",
      "total_flops_so_far": 8403560736030720.0,
      "budget_used_percent": 8.40356073603072
    },
    {
      "type": "training",
      "description": "Training step 354",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:21",
      "total_flops_so_far": 8427299608166400.0,
      "budget_used_percent": 8.4272996081664
    },
    {
      "type": "training",
      "description": "Training step 355",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:23",
      "total_flops_so_far": 8451038480302080.0,
      "budget_used_percent": 8.45103848030208
    },
    {
      "type": "training",
      "description": "Training step 356",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:24",
      "total_flops_so_far": 8474777352437760.0,
      "budget_used_percent": 8.47477735243776
    },
    {
      "type": "training",
      "description": "Training step 357",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:25",
      "total_flops_so_far": 8498516224573440.0,
      "budget_used_percent": 8.49851622457344
    },
    {
      "type": "training",
      "description": "Training step 358",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:27",
      "total_flops_so_far": 8522255096709120.0,
      "budget_used_percent": 8.52225509670912
    },
    {
      "type": "training",
      "description": "Training step 359",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:28",
      "total_flops_so_far": 8545993968844800.0,
      "budget_used_percent": 8.545993968844801
    },
    {
      "type": "training",
      "description": "Training step 360",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:29",
      "total_flops_so_far": 8569732840980480.0,
      "budget_used_percent": 8.56973284098048
    },
    {
      "type": "training",
      "description": "Training step 361",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:30",
      "total_flops_so_far": 8593471713116160.0,
      "budget_used_percent": 8.59347171311616
    },
    {
      "type": "training",
      "description": "Training step 362",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:32",
      "total_flops_so_far": 8617210585251840.0,
      "budget_used_percent": 8.61721058525184
    },
    {
      "type": "training",
      "description": "Training step 363",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:33",
      "total_flops_so_far": 8640949457387520.0,
      "budget_used_percent": 8.64094945738752
    },
    {
      "type": "training",
      "description": "Training step 364",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:34",
      "total_flops_so_far": 8664688329523200.0,
      "budget_used_percent": 8.6646883295232
    },
    {
      "type": "training",
      "description": "Training step 365",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:36",
      "total_flops_so_far": 8688427201658880.0,
      "budget_used_percent": 8.68842720165888
    },
    {
      "type": "training",
      "description": "Training step 366",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:37",
      "total_flops_so_far": 8712166073794560.0,
      "budget_used_percent": 8.71216607379456
    },
    {
      "type": "training",
      "description": "Training step 367",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:38",
      "total_flops_so_far": 8735904945930240.0,
      "budget_used_percent": 8.73590494593024
    },
    {
      "type": "training",
      "description": "Training step 368",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:39",
      "total_flops_so_far": 8759643818065920.0,
      "budget_used_percent": 8.75964381806592
    },
    {
      "type": "training",
      "description": "Training step 369",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:41",
      "total_flops_so_far": 8783382690201600.0,
      "budget_used_percent": 8.783382690201599
    },
    {
      "type": "training",
      "description": "Training step 370",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:42",
      "total_flops_so_far": 8807121562337280.0,
      "budget_used_percent": 8.807121562337281
    },
    {
      "type": "training",
      "description": "Training step 371",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:43",
      "total_flops_so_far": 8830860434472960.0,
      "budget_used_percent": 8.83086043447296
    },
    {
      "type": "training",
      "description": "Training step 372",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:45",
      "total_flops_so_far": 8854599306608640.0,
      "budget_used_percent": 8.854599306608641
    },
    {
      "type": "training",
      "description": "Training step 373",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:46",
      "total_flops_so_far": 8878338178744320.0,
      "budget_used_percent": 8.87833817874432
    },
    {
      "type": "training",
      "description": "Training step 374",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:47",
      "total_flops_so_far": 8902077050880000.0,
      "budget_used_percent": 8.902077050880001
    },
    {
      "type": "training",
      "description": "Training step 375",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:49",
      "total_flops_so_far": 8925815923015680.0,
      "budget_used_percent": 8.92581592301568
    },
    {
      "type": "training",
      "description": "Training step 376",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:50",
      "total_flops_so_far": 8949554795151360.0,
      "budget_used_percent": 8.94955479515136
    },
    {
      "type": "training",
      "description": "Training step 377",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:51",
      "total_flops_so_far": 8973293667287040.0,
      "budget_used_percent": 8.97329366728704
    },
    {
      "type": "training",
      "description": "Training step 378",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:52",
      "total_flops_so_far": 8997032539422720.0,
      "budget_used_percent": 8.997032539422719
    },
    {
      "type": "training",
      "description": "Training step 379",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:54",
      "total_flops_so_far": 9020771411558400.0,
      "budget_used_percent": 9.0207714115584
    },
    {
      "type": "training",
      "description": "Training step 380",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:55",
      "total_flops_so_far": 9044510283694080.0,
      "budget_used_percent": 9.044510283694079
    },
    {
      "type": "training",
      "description": "Training step 381",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:56",
      "total_flops_so_far": 9068249155829760.0,
      "budget_used_percent": 9.068249155829761
    },
    {
      "type": "training",
      "description": "Training step 382",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:58",
      "total_flops_so_far": 9091988027965440.0,
      "budget_used_percent": 9.09198802796544
    },
    {
      "type": "training",
      "description": "Training step 383",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:39:59",
      "total_flops_so_far": 9115726900101120.0,
      "budget_used_percent": 9.115726900101121
    },
    {
      "type": "training",
      "description": "Training step 384",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:00",
      "total_flops_so_far": 9139465772236800.0,
      "budget_used_percent": 9.1394657722368
    },
    {
      "type": "training",
      "description": "Training step 385",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:01",
      "total_flops_so_far": 9163204644372480.0,
      "budget_used_percent": 9.163204644372481
    },
    {
      "type": "training",
      "description": "Training step 386",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:03",
      "total_flops_so_far": 9186943516508160.0,
      "budget_used_percent": 9.18694351650816
    },
    {
      "type": "training",
      "description": "Training step 387",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:04",
      "total_flops_so_far": 9210682388643840.0,
      "budget_used_percent": 9.21068238864384
    },
    {
      "type": "training",
      "description": "Training step 388",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:06",
      "total_flops_so_far": 9234421260779520.0,
      "budget_used_percent": 9.23442126077952
    },
    {
      "type": "training",
      "description": "Training step 389",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:07",
      "total_flops_so_far": 9258160132915200.0,
      "budget_used_percent": 9.258160132915199
    },
    {
      "type": "training",
      "description": "Training step 390",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:09",
      "total_flops_so_far": 9281899005050880.0,
      "budget_used_percent": 9.28189900505088
    },
    {
      "type": "training",
      "description": "Training step 391",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:10",
      "total_flops_so_far": 9305637877186560.0,
      "budget_used_percent": 9.305637877186559
    },
    {
      "type": "training",
      "description": "Training step 392",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:11",
      "total_flops_so_far": 9329376749322240.0,
      "budget_used_percent": 9.32937674932224
    },
    {
      "type": "training",
      "description": "Training step 393",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:13",
      "total_flops_so_far": 9353115621457920.0,
      "budget_used_percent": 9.35311562145792
    },
    {
      "type": "training",
      "description": "Training step 394",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:14",
      "total_flops_so_far": 9376854493593600.0,
      "budget_used_percent": 9.376854493593601
    },
    {
      "type": "training",
      "description": "Training step 395",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:15",
      "total_flops_so_far": 9400593365729280.0,
      "budget_used_percent": 9.40059336572928
    },
    {
      "type": "training",
      "description": "Training step 396",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:16",
      "total_flops_so_far": 9424332237864960.0,
      "budget_used_percent": 9.42433223786496
    },
    {
      "type": "training",
      "description": "Training step 397",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:17",
      "total_flops_so_far": 9448071110000640.0,
      "budget_used_percent": 9.44807111000064
    },
    {
      "type": "training",
      "description": "Training step 398",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:18",
      "total_flops_so_far": 9471809982136320.0,
      "budget_used_percent": 9.47180998213632
    },
    {
      "type": "training",
      "description": "Training step 399",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:19",
      "total_flops_so_far": 9495548854272000.0,
      "budget_used_percent": 9.495548854272
    },
    {
      "type": "training",
      "description": "Training step 400",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:21",
      "total_flops_so_far": 9519287726407680.0,
      "budget_used_percent": 9.51928772640768
    },
    {
      "type": "training",
      "description": "Training step 401",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:22",
      "total_flops_so_far": 9543026598543360.0,
      "budget_used_percent": 9.54302659854336
    },
    {
      "type": "training",
      "description": "Training step 402",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:23",
      "total_flops_so_far": 9566765470679040.0,
      "budget_used_percent": 9.566765470679039
    },
    {
      "type": "training",
      "description": "Training step 403",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:25",
      "total_flops_so_far": 9590504342814720.0,
      "budget_used_percent": 9.59050434281472
    },
    {
      "type": "training",
      "description": "Training step 404",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:26",
      "total_flops_so_far": 9614243214950400.0,
      "budget_used_percent": 9.6142432149504
    },
    {
      "type": "training",
      "description": "Training step 405",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:27",
      "total_flops_so_far": 9637982087086080.0,
      "budget_used_percent": 9.63798208708608
    },
    {
      "type": "training",
      "description": "Training step 406",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:28",
      "total_flops_so_far": 9661720959221760.0,
      "budget_used_percent": 9.66172095922176
    },
    {
      "type": "training",
      "description": "Training step 407",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:30",
      "total_flops_so_far": 9685459831357440.0,
      "budget_used_percent": 9.68545983135744
    },
    {
      "type": "training",
      "description": "Training step 408",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:31",
      "total_flops_so_far": 9709198703493120.0,
      "budget_used_percent": 9.70919870349312
    },
    {
      "type": "training",
      "description": "Training step 409",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:32",
      "total_flops_so_far": 9732937575628800.0,
      "budget_used_percent": 9.7329375756288
    },
    {
      "type": "training",
      "description": "Training step 410",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:34",
      "total_flops_so_far": 9756676447764480.0,
      "budget_used_percent": 9.75667644776448
    },
    {
      "type": "training",
      "description": "Training step 411",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:35",
      "total_flops_so_far": 9780415319900160.0,
      "budget_used_percent": 9.78041531990016
    },
    {
      "type": "training",
      "description": "Training step 412",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:36",
      "total_flops_so_far": 9804154192035840.0,
      "budget_used_percent": 9.80415419203584
    },
    {
      "type": "training",
      "description": "Training step 413",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:37",
      "total_flops_so_far": 9827893064171520.0,
      "budget_used_percent": 9.82789306417152
    },
    {
      "type": "training",
      "description": "Training step 414",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:39",
      "total_flops_so_far": 9851631936307200.0,
      "budget_used_percent": 9.8516319363072
    },
    {
      "type": "training",
      "description": "Training step 415",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:40",
      "total_flops_so_far": 9875370808442880.0,
      "budget_used_percent": 9.87537080844288
    },
    {
      "type": "training",
      "description": "Training step 416",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:41",
      "total_flops_so_far": 9899109680578560.0,
      "budget_used_percent": 9.89910968057856
    },
    {
      "type": "training",
      "description": "Training step 417",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:43",
      "total_flops_so_far": 9922848552714240.0,
      "budget_used_percent": 9.92284855271424
    },
    {
      "type": "training",
      "description": "Training step 418",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:44",
      "total_flops_so_far": 9946587424849920.0,
      "budget_used_percent": 9.94658742484992
    },
    {
      "type": "training",
      "description": "Training step 419",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:45",
      "total_flops_so_far": 9970326296985600.0,
      "budget_used_percent": 9.9703262969856
    },
    {
      "type": "training",
      "description": "Training step 420",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:47",
      "total_flops_so_far": 9994065169121280.0,
      "budget_used_percent": 9.99406516912128
    },
    {
      "type": "training",
      "description": "Training step 421",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:48",
      "total_flops_so_far": 1.001780404125696e+16,
      "budget_used_percent": 10.01780404125696
    },
    {
      "type": "training",
      "description": "Training step 422",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:49",
      "total_flops_so_far": 1.004154291339264e+16,
      "budget_used_percent": 10.04154291339264
    },
    {
      "type": "training",
      "description": "Training step 423",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:51",
      "total_flops_so_far": 1.006528178552832e+16,
      "budget_used_percent": 10.06528178552832
    },
    {
      "type": "training",
      "description": "Training step 424",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:52",
      "total_flops_so_far": 1.0089020657664e+16,
      "budget_used_percent": 10.089020657664
    },
    {
      "type": "training",
      "description": "Training step 425",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:53",
      "total_flops_so_far": 1.011275952979968e+16,
      "budget_used_percent": 10.11275952979968
    },
    {
      "type": "training",
      "description": "Training step 426",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:55",
      "total_flops_so_far": 1.013649840193536e+16,
      "budget_used_percent": 10.13649840193536
    },
    {
      "type": "training",
      "description": "Training step 427",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:56",
      "total_flops_so_far": 1.016023727407104e+16,
      "budget_used_percent": 10.16023727407104
    },
    {
      "type": "training",
      "description": "Training step 428",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:57",
      "total_flops_so_far": 1.018397614620672e+16,
      "budget_used_percent": 10.18397614620672
    },
    {
      "type": "training",
      "description": "Training step 429",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:40:58",
      "total_flops_so_far": 1.02077150183424e+16,
      "budget_used_percent": 10.2077150183424
    },
    {
      "type": "training",
      "description": "Training step 430",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:00",
      "total_flops_so_far": 1.023145389047808e+16,
      "budget_used_percent": 10.23145389047808
    },
    {
      "type": "training",
      "description": "Training step 431",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:01",
      "total_flops_so_far": 1.025519276261376e+16,
      "budget_used_percent": 10.25519276261376
    },
    {
      "type": "training",
      "description": "Training step 432",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:02",
      "total_flops_so_far": 1.027893163474944e+16,
      "budget_used_percent": 10.27893163474944
    },
    {
      "type": "training",
      "description": "Training step 433",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:04",
      "total_flops_so_far": 1.030267050688512e+16,
      "budget_used_percent": 10.30267050688512
    },
    {
      "type": "training",
      "description": "Training step 434",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:05",
      "total_flops_so_far": 1.03264093790208e+16,
      "budget_used_percent": 10.3264093790208
    },
    {
      "type": "training",
      "description": "Training step 435",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:06",
      "total_flops_so_far": 1.035014825115648e+16,
      "budget_used_percent": 10.35014825115648
    },
    {
      "type": "training",
      "description": "Training step 436",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:08",
      "total_flops_so_far": 1.037388712329216e+16,
      "budget_used_percent": 10.37388712329216
    },
    {
      "type": "training",
      "description": "Training step 437",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:09",
      "total_flops_so_far": 1.039762599542784e+16,
      "budget_used_percent": 10.39762599542784
    },
    {
      "type": "training",
      "description": "Training step 438",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:10",
      "total_flops_so_far": 1.042136486756352e+16,
      "budget_used_percent": 10.42136486756352
    },
    {
      "type": "training",
      "description": "Training step 439",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:11",
      "total_flops_so_far": 1.04451037396992e+16,
      "budget_used_percent": 10.445103739699201
    },
    {
      "type": "training",
      "description": "Training step 440",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:13",
      "total_flops_so_far": 1.046884261183488e+16,
      "budget_used_percent": 10.46884261183488
    },
    {
      "type": "training",
      "description": "Training step 441",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:14",
      "total_flops_so_far": 1.049258148397056e+16,
      "budget_used_percent": 10.492581483970561
    },
    {
      "type": "training",
      "description": "Training step 442",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:15",
      "total_flops_so_far": 1.051632035610624e+16,
      "budget_used_percent": 10.51632035610624
    },
    {
      "type": "training",
      "description": "Training step 443",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:17",
      "total_flops_so_far": 1.054005922824192e+16,
      "budget_used_percent": 10.54005922824192
    },
    {
      "type": "training",
      "description": "Training step 444",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:18",
      "total_flops_so_far": 1.05637981003776e+16,
      "budget_used_percent": 10.5637981003776
    },
    {
      "type": "training",
      "description": "Training step 445",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:19",
      "total_flops_so_far": 1.058753697251328e+16,
      "budget_used_percent": 10.58753697251328
    },
    {
      "type": "training",
      "description": "Training step 446",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:20",
      "total_flops_so_far": 1.061127584464896e+16,
      "budget_used_percent": 10.61127584464896
    },
    {
      "type": "training",
      "description": "Training step 447",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:22",
      "total_flops_so_far": 1.063501471678464e+16,
      "budget_used_percent": 10.635014716784639
    },
    {
      "type": "training",
      "description": "Training step 448",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:23",
      "total_flops_so_far": 1.065875358892032e+16,
      "budget_used_percent": 10.65875358892032
    },
    {
      "type": "training",
      "description": "Training step 449",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:24",
      "total_flops_so_far": 1.0682492461056e+16,
      "budget_used_percent": 10.682492461055999
    },
    {
      "type": "training",
      "description": "Training step 450",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:26",
      "total_flops_so_far": 1.070623133319168e+16,
      "budget_used_percent": 10.706231333191681
    },
    {
      "type": "training",
      "description": "Training step 451",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:27",
      "total_flops_so_far": 1.072997020532736e+16,
      "budget_used_percent": 10.72997020532736
    },
    {
      "type": "training",
      "description": "Training step 452",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:28",
      "total_flops_so_far": 1.075370907746304e+16,
      "budget_used_percent": 10.753709077463041
    },
    {
      "type": "training",
      "description": "Training step 453",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:29",
      "total_flops_so_far": 1.077744794959872e+16,
      "budget_used_percent": 10.77744794959872
    },
    {
      "type": "training",
      "description": "Training step 454",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:32",
      "total_flops_so_far": 1.08011868217344e+16,
      "budget_used_percent": 10.801186821734401
    },
    {
      "type": "training",
      "description": "Training step 455",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:33",
      "total_flops_so_far": 1.082492569387008e+16,
      "budget_used_percent": 10.82492569387008
    },
    {
      "type": "training",
      "description": "Training step 456",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:34",
      "total_flops_so_far": 1.084866456600576e+16,
      "budget_used_percent": 10.848664566005759
    },
    {
      "type": "training",
      "description": "Training step 457",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:36",
      "total_flops_so_far": 1.087240343814144e+16,
      "budget_used_percent": 10.87240343814144
    },
    {
      "type": "training",
      "description": "Training step 458",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:37",
      "total_flops_so_far": 1.089614231027712e+16,
      "budget_used_percent": 10.896142310277119
    },
    {
      "type": "training",
      "description": "Training step 459",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:38",
      "total_flops_so_far": 1.09198811824128e+16,
      "budget_used_percent": 10.9198811824128
    },
    {
      "type": "training",
      "description": "Training step 460",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:39",
      "total_flops_so_far": 1.094362005454848e+16,
      "budget_used_percent": 10.943620054548479
    },
    {
      "type": "training",
      "description": "Training step 461",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:41",
      "total_flops_so_far": 1.096735892668416e+16,
      "budget_used_percent": 10.967358926684161
    },
    {
      "type": "training",
      "description": "Training step 462",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:42",
      "total_flops_so_far": 1.099109779881984e+16,
      "budget_used_percent": 10.99109779881984
    },
    {
      "type": "training",
      "description": "Training step 463",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:43",
      "total_flops_so_far": 1.101483667095552e+16,
      "budget_used_percent": 11.014836670955521
    },
    {
      "type": "training",
      "description": "Training step 464",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:45",
      "total_flops_so_far": 1.10385755430912e+16,
      "budget_used_percent": 11.0385755430912
    },
    {
      "type": "training",
      "description": "Training step 465",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:46",
      "total_flops_so_far": 1.106231441522688e+16,
      "budget_used_percent": 11.06231441522688
    },
    {
      "type": "training",
      "description": "Training step 466",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:47",
      "total_flops_so_far": 1.108605328736256e+16,
      "budget_used_percent": 11.08605328736256
    },
    {
      "type": "training",
      "description": "Training step 467",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:49",
      "total_flops_so_far": 1.110979215949824e+16,
      "budget_used_percent": 11.10979215949824
    },
    {
      "type": "training",
      "description": "Training step 468",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:50",
      "total_flops_so_far": 1.113353103163392e+16,
      "budget_used_percent": 11.13353103163392
    },
    {
      "type": "training",
      "description": "Training step 469",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:51",
      "total_flops_so_far": 1.11572699037696e+16,
      "budget_used_percent": 11.157269903769599
    },
    {
      "type": "training",
      "description": "Training step 470",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:52",
      "total_flops_so_far": 1.118100877590528e+16,
      "budget_used_percent": 11.18100877590528
    },
    {
      "type": "training",
      "description": "Training step 471",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:54",
      "total_flops_so_far": 1.120474764804096e+16,
      "budget_used_percent": 11.204747648040959
    },
    {
      "type": "training",
      "description": "Training step 472",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:55",
      "total_flops_so_far": 1.122848652017664e+16,
      "budget_used_percent": 11.228486520176642
    },
    {
      "type": "training",
      "description": "Training step 473",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:56",
      "total_flops_so_far": 1.125222539231232e+16,
      "budget_used_percent": 11.25222539231232
    },
    {
      "type": "training",
      "description": "Training step 474",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:58",
      "total_flops_so_far": 1.1275964264448e+16,
      "budget_used_percent": 11.275964264448
    },
    {
      "type": "training",
      "description": "Training step 475",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:41:59",
      "total_flops_so_far": 1.129970313658368e+16,
      "budget_used_percent": 11.29970313658368
    },
    {
      "type": "training",
      "description": "Training step 476",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:00",
      "total_flops_so_far": 1.132344200871936e+16,
      "budget_used_percent": 11.32344200871936
    },
    {
      "type": "training",
      "description": "Training step 477",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:01",
      "total_flops_so_far": 1.134718088085504e+16,
      "budget_used_percent": 11.34718088085504
    },
    {
      "type": "training",
      "description": "Training step 478",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:03",
      "total_flops_so_far": 1.137091975299072e+16,
      "budget_used_percent": 11.37091975299072
    },
    {
      "type": "training",
      "description": "Training step 479",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:04",
      "total_flops_so_far": 1.13946586251264e+16,
      "budget_used_percent": 11.3946586251264
    },
    {
      "type": "training",
      "description": "Training step 480",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:05",
      "total_flops_so_far": 1.141839749726208e+16,
      "budget_used_percent": 11.41839749726208
    },
    {
      "type": "training",
      "description": "Training step 481",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:07",
      "total_flops_so_far": 1.144213636939776e+16,
      "budget_used_percent": 11.44213636939776
    },
    {
      "type": "training",
      "description": "Training step 482",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:08",
      "total_flops_so_far": 1.146587524153344e+16,
      "budget_used_percent": 11.465875241533439
    },
    {
      "type": "training",
      "description": "Training step 483",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:09",
      "total_flops_so_far": 1.148961411366912e+16,
      "budget_used_percent": 11.48961411366912
    },
    {
      "type": "training",
      "description": "Training step 484",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:10",
      "total_flops_so_far": 1.15133529858048e+16,
      "budget_used_percent": 11.5133529858048
    },
    {
      "type": "training",
      "description": "Training step 485",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:12",
      "total_flops_so_far": 1.153709185794048e+16,
      "budget_used_percent": 11.53709185794048
    },
    {
      "type": "training",
      "description": "Training step 486",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:13",
      "total_flops_so_far": 1.156083073007616e+16,
      "budget_used_percent": 11.56083073007616
    },
    {
      "type": "training",
      "description": "Training step 487",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:14",
      "total_flops_so_far": 1.158456960221184e+16,
      "budget_used_percent": 11.58456960221184
    },
    {
      "type": "training",
      "description": "Training step 488",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:16",
      "total_flops_so_far": 1.160830847434752e+16,
      "budget_used_percent": 11.60830847434752
    },
    {
      "type": "training",
      "description": "Training step 489",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:17",
      "total_flops_so_far": 1.16320473464832e+16,
      "budget_used_percent": 11.6320473464832
    },
    {
      "type": "training",
      "description": "Training step 490",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:18",
      "total_flops_so_far": 1.165578621861888e+16,
      "budget_used_percent": 11.65578621861888
    },
    {
      "type": "training",
      "description": "Training step 491",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:20",
      "total_flops_so_far": 1.167952509075456e+16,
      "budget_used_percent": 11.67952509075456
    },
    {
      "type": "training",
      "description": "Training step 492",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:21",
      "total_flops_so_far": 1.170326396289024e+16,
      "budget_used_percent": 11.70326396289024
    },
    {
      "type": "training",
      "description": "Training step 493",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:22",
      "total_flops_so_far": 1.172700283502592e+16,
      "budget_used_percent": 11.72700283502592
    },
    {
      "type": "training",
      "description": "Training step 494",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:23",
      "total_flops_so_far": 1.17507417071616e+16,
      "budget_used_percent": 11.7507417071616
    },
    {
      "type": "training",
      "description": "Training step 495",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:25",
      "total_flops_so_far": 1.177448057929728e+16,
      "budget_used_percent": 11.77448057929728
    },
    {
      "type": "training",
      "description": "Training step 496",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:26",
      "total_flops_so_far": 1.179821945143296e+16,
      "budget_used_percent": 11.79821945143296
    },
    {
      "type": "training",
      "description": "Training step 497",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:27",
      "total_flops_so_far": 1.182195832356864e+16,
      "budget_used_percent": 11.82195832356864
    },
    {
      "type": "training",
      "description": "Training step 498",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:29",
      "total_flops_so_far": 1.184569719570432e+16,
      "budget_used_percent": 11.84569719570432
    },
    {
      "type": "training",
      "description": "Training step 499",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:30",
      "total_flops_so_far": 1.186943606784e+16,
      "budget_used_percent": 11.86943606784
    },
    {
      "type": "training",
      "description": "Training step 500",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:31",
      "total_flops_so_far": 1.189317493997568e+16,
      "budget_used_percent": 11.89317493997568
    },
    {
      "type": "training",
      "description": "Training step 501",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:32",
      "total_flops_so_far": 1.191691381211136e+16,
      "budget_used_percent": 11.91691381211136
    },
    {
      "type": "training",
      "description": "Training step 502",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:34",
      "total_flops_so_far": 1.194065268424704e+16,
      "budget_used_percent": 11.94065268424704
    },
    {
      "type": "training",
      "description": "Training step 503",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:35",
      "total_flops_so_far": 1.196439155638272e+16,
      "budget_used_percent": 11.96439155638272
    },
    {
      "type": "training",
      "description": "Training step 504",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:36",
      "total_flops_so_far": 1.19881304285184e+16,
      "budget_used_percent": 11.9881304285184
    },
    {
      "type": "training",
      "description": "Training step 505",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:38",
      "total_flops_so_far": 1.201186930065408e+16,
      "budget_used_percent": 12.01186930065408
    },
    {
      "type": "training",
      "description": "Training step 506",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:39",
      "total_flops_so_far": 1.203560817278976e+16,
      "budget_used_percent": 12.03560817278976
    },
    {
      "type": "training",
      "description": "Training step 507",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:40",
      "total_flops_so_far": 1.205934704492544e+16,
      "budget_used_percent": 12.05934704492544
    },
    {
      "type": "training",
      "description": "Training step 508",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:41",
      "total_flops_so_far": 1.208308591706112e+16,
      "budget_used_percent": 12.083085917061121
    },
    {
      "type": "training",
      "description": "Training step 509",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:43",
      "total_flops_so_far": 1.21068247891968e+16,
      "budget_used_percent": 12.1068247891968
    },
    {
      "type": "training",
      "description": "Training step 510",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:44",
      "total_flops_so_far": 1.213056366133248e+16,
      "budget_used_percent": 12.13056366133248
    },
    {
      "type": "training",
      "description": "Training step 511",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:45",
      "total_flops_so_far": 1.215430253346816e+16,
      "budget_used_percent": 12.15430253346816
    },
    {
      "type": "training",
      "description": "Training step 512",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:47",
      "total_flops_so_far": 1.217804140560384e+16,
      "budget_used_percent": 12.17804140560384
    },
    {
      "type": "training",
      "description": "Training step 513",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:48",
      "total_flops_so_far": 1.220178027773952e+16,
      "budget_used_percent": 12.20178027773952
    },
    {
      "type": "training",
      "description": "Training step 514",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:49",
      "total_flops_so_far": 1.22255191498752e+16,
      "budget_used_percent": 12.2255191498752
    },
    {
      "type": "training",
      "description": "Training step 515",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:51",
      "total_flops_so_far": 1.224925802201088e+16,
      "budget_used_percent": 12.24925802201088
    },
    {
      "type": "training",
      "description": "Training step 516",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:52",
      "total_flops_so_far": 1.227299689414656e+16,
      "budget_used_percent": 12.272996894146559
    },
    {
      "type": "training",
      "description": "Training step 517",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:53",
      "total_flops_so_far": 1.229673576628224e+16,
      "budget_used_percent": 12.29673576628224
    },
    {
      "type": "training",
      "description": "Training step 518",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:54",
      "total_flops_so_far": 1.232047463841792e+16,
      "budget_used_percent": 12.32047463841792
    },
    {
      "type": "training",
      "description": "Training step 519",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:56",
      "total_flops_so_far": 1.23442135105536e+16,
      "budget_used_percent": 12.344213510553601
    },
    {
      "type": "training",
      "description": "Training step 520",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:57",
      "total_flops_so_far": 1.236795238268928e+16,
      "budget_used_percent": 12.36795238268928
    },
    {
      "type": "training",
      "description": "Training step 521",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:42:58",
      "total_flops_so_far": 1.239169125482496e+16,
      "budget_used_percent": 12.391691254824961
    },
    {
      "type": "training",
      "description": "Training step 522",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:00",
      "total_flops_so_far": 1.241543012696064e+16,
      "budget_used_percent": 12.41543012696064
    },
    {
      "type": "training",
      "description": "Training step 523",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:01",
      "total_flops_so_far": 1.243916899909632e+16,
      "budget_used_percent": 12.43916899909632
    },
    {
      "type": "training",
      "description": "Training step 524",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:02",
      "total_flops_so_far": 1.2462907871232e+16,
      "budget_used_percent": 12.462907871232
    },
    {
      "type": "training",
      "description": "Training step 525",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:04",
      "total_flops_so_far": 1.248664674336768e+16,
      "budget_used_percent": 12.486646743367679
    },
    {
      "type": "training",
      "description": "Training step 526",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:05",
      "total_flops_so_far": 1.251038561550336e+16,
      "budget_used_percent": 12.510385615503361
    },
    {
      "type": "training",
      "description": "Training step 527",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:06",
      "total_flops_so_far": 1.253412448763904e+16,
      "budget_used_percent": 12.534124487639039
    },
    {
      "type": "training",
      "description": "Training step 528",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:06",
      "total_flops_so_far": 1.255786335977472e+16,
      "budget_used_percent": 12.557863359774721
    },
    {
      "type": "training",
      "description": "Training step 529",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:08",
      "total_flops_so_far": 1.25816022319104e+16,
      "budget_used_percent": 12.581602231910399
    },
    {
      "type": "training",
      "description": "Training step 530",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:09",
      "total_flops_so_far": 1.260534110404608e+16,
      "budget_used_percent": 12.605341104046081
    },
    {
      "type": "training",
      "description": "Training step 531",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:10",
      "total_flops_so_far": 1.262907997618176e+16,
      "budget_used_percent": 12.629079976181758
    },
    {
      "type": "training",
      "description": "Training step 532",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:12",
      "total_flops_so_far": 1.265281884831744e+16,
      "budget_used_percent": 12.652818848317441
    },
    {
      "type": "training",
      "description": "Training step 533",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:13",
      "total_flops_so_far": 1.267655772045312e+16,
      "budget_used_percent": 12.676557720453118
    },
    {
      "type": "training",
      "description": "Training step 534",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:14",
      "total_flops_so_far": 1.27002965925888e+16,
      "budget_used_percent": 12.7002965925888
    },
    {
      "type": "training",
      "description": "Training step 535",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:15",
      "total_flops_so_far": 1.272403546472448e+16,
      "budget_used_percent": 12.724035464724482
    },
    {
      "type": "training",
      "description": "Training step 536",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:17",
      "total_flops_so_far": 1.274777433686016e+16,
      "budget_used_percent": 12.747774336860159
    },
    {
      "type": "training",
      "description": "Training step 537",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:18",
      "total_flops_so_far": 1.277151320899584e+16,
      "budget_used_percent": 12.771513208995842
    },
    {
      "type": "training",
      "description": "Training step 538",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:19",
      "total_flops_so_far": 1.279525208113152e+16,
      "budget_used_percent": 12.795252081131519
    },
    {
      "type": "training",
      "description": "Training step 539",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:21",
      "total_flops_so_far": 1.28189909532672e+16,
      "budget_used_percent": 12.818990953267202
    },
    {
      "type": "training",
      "description": "Training step 540",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:22",
      "total_flops_so_far": 1.284272982540288e+16,
      "budget_used_percent": 12.842729825402879
    },
    {
      "type": "training",
      "description": "Training step 541",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:23",
      "total_flops_so_far": 1.286646869753856e+16,
      "budget_used_percent": 12.866468697538561
    },
    {
      "type": "training",
      "description": "Training step 542",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:25",
      "total_flops_so_far": 1.289020756967424e+16,
      "budget_used_percent": 12.890207569674239
    },
    {
      "type": "training",
      "description": "Training step 543",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:26",
      "total_flops_so_far": 1.291394644180992e+16,
      "budget_used_percent": 12.91394644180992
    },
    {
      "type": "training",
      "description": "Training step 544",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:27",
      "total_flops_so_far": 1.29376853139456e+16,
      "budget_used_percent": 12.937685313945599
    },
    {
      "type": "training",
      "description": "Training step 545",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:28",
      "total_flops_so_far": 1.296142418608128e+16,
      "budget_used_percent": 12.96142418608128
    },
    {
      "type": "training",
      "description": "Training step 546",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:30",
      "total_flops_so_far": 1.298516305821696e+16,
      "budget_used_percent": 12.985163058216962
    },
    {
      "type": "training",
      "description": "Training step 547",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:31",
      "total_flops_so_far": 1.300890193035264e+16,
      "budget_used_percent": 13.00890193035264
    },
    {
      "type": "training",
      "description": "Training step 548",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:32",
      "total_flops_so_far": 1.303264080248832e+16,
      "budget_used_percent": 13.032640802488322
    },
    {
      "type": "training",
      "description": "Training step 549",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:34",
      "total_flops_so_far": 1.3056379674624e+16,
      "budget_used_percent": 13.056379674623999
    },
    {
      "type": "training",
      "description": "Training step 550",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:35",
      "total_flops_so_far": 1.308011854675968e+16,
      "budget_used_percent": 13.080118546759682
    },
    {
      "type": "training",
      "description": "Training step 551",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:36",
      "total_flops_so_far": 1.310385741889536e+16,
      "budget_used_percent": 13.103857418895359
    },
    {
      "type": "training",
      "description": "Training step 552",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:37",
      "total_flops_so_far": 1.312759629103104e+16,
      "budget_used_percent": 13.12759629103104
    },
    {
      "type": "training",
      "description": "Training step 553",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:39",
      "total_flops_so_far": 1.315133516316672e+16,
      "budget_used_percent": 13.151335163166719
    },
    {
      "type": "training",
      "description": "Training step 554",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:40",
      "total_flops_so_far": 1.31750740353024e+16,
      "budget_used_percent": 13.1750740353024
    },
    {
      "type": "training",
      "description": "Training step 555",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:41",
      "total_flops_so_far": 1.319881290743808e+16,
      "budget_used_percent": 13.198812907438079
    },
    {
      "type": "training",
      "description": "Training step 556",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:43",
      "total_flops_so_far": 1.322255177957376e+16,
      "budget_used_percent": 13.22255177957376
    },
    {
      "type": "training",
      "description": "Training step 557",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:44",
      "total_flops_so_far": 1.324629065170944e+16,
      "budget_used_percent": 13.246290651709439
    },
    {
      "type": "training",
      "description": "Training step 558",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:45",
      "total_flops_so_far": 1.327002952384512e+16,
      "budget_used_percent": 13.27002952384512
    },
    {
      "type": "training",
      "description": "Training step 559",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:47",
      "total_flops_so_far": 1.32937683959808e+16,
      "budget_used_percent": 13.293768395980802
    },
    {
      "type": "training",
      "description": "Training step 560",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:48",
      "total_flops_so_far": 1.331750726811648e+16,
      "budget_used_percent": 13.31750726811648
    },
    {
      "type": "training",
      "description": "Training step 561",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:49",
      "total_flops_so_far": 1.334124614025216e+16,
      "budget_used_percent": 13.34124614025216
    },
    {
      "type": "training",
      "description": "Training step 562",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:50",
      "total_flops_so_far": 1.336498501238784e+16,
      "budget_used_percent": 13.364985012387839
    },
    {
      "type": "training",
      "description": "Training step 563",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:52",
      "total_flops_so_far": 1.338872388452352e+16,
      "budget_used_percent": 13.38872388452352
    },
    {
      "type": "training",
      "description": "Training step 564",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:53",
      "total_flops_so_far": 1.34124627566592e+16,
      "budget_used_percent": 13.412462756659199
    },
    {
      "type": "training",
      "description": "Training step 565",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:54",
      "total_flops_so_far": 1.343620162879488e+16,
      "budget_used_percent": 13.43620162879488
    },
    {
      "type": "training",
      "description": "Training step 566",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:56",
      "total_flops_so_far": 1.345994050093056e+16,
      "budget_used_percent": 13.459940500930559
    },
    {
      "type": "training",
      "description": "Training step 567",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:57",
      "total_flops_so_far": 1.348367937306624e+16,
      "budget_used_percent": 13.48367937306624
    },
    {
      "type": "training",
      "description": "Training step 568",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:58",
      "total_flops_so_far": 1.350741824520192e+16,
      "budget_used_percent": 13.507418245201919
    },
    {
      "type": "training",
      "description": "Training step 569",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:43:59",
      "total_flops_so_far": 1.35311571173376e+16,
      "budget_used_percent": 13.5311571173376
    },
    {
      "type": "training",
      "description": "Training step 570",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:01",
      "total_flops_so_far": 1.355489598947328e+16,
      "budget_used_percent": 13.55489598947328
    },
    {
      "type": "training",
      "description": "Training step 571",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:02",
      "total_flops_so_far": 1.357863486160896e+16,
      "budget_used_percent": 13.57863486160896
    },
    {
      "type": "training",
      "description": "Training step 572",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:03",
      "total_flops_so_far": 1.360237373374464e+16,
      "budget_used_percent": 13.60237373374464
    },
    {
      "type": "training",
      "description": "Training step 573",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:05",
      "total_flops_so_far": 1.362611260588032e+16,
      "budget_used_percent": 13.62611260588032
    },
    {
      "type": "training",
      "description": "Training step 574",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:06",
      "total_flops_so_far": 1.3649851478016e+16,
      "budget_used_percent": 13.649851478016
    },
    {
      "type": "training",
      "description": "Training step 575",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:07",
      "total_flops_so_far": 1.367359035015168e+16,
      "budget_used_percent": 13.67359035015168
    },
    {
      "type": "training",
      "description": "Training step 576",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:08",
      "total_flops_so_far": 1.369732922228736e+16,
      "budget_used_percent": 13.69732922228736
    },
    {
      "type": "training",
      "description": "Training step 577",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:10",
      "total_flops_so_far": 1.372106809442304e+16,
      "budget_used_percent": 13.721068094423039
    },
    {
      "type": "training",
      "description": "Training step 578",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:11",
      "total_flops_so_far": 1.374480696655872e+16,
      "budget_used_percent": 13.74480696655872
    },
    {
      "type": "training",
      "description": "Training step 579",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:12",
      "total_flops_so_far": 1.37685458386944e+16,
      "budget_used_percent": 13.768545838694399
    },
    {
      "type": "training",
      "description": "Training step 580",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:14",
      "total_flops_so_far": 1.379228471083008e+16,
      "budget_used_percent": 13.79228471083008
    },
    {
      "type": "training",
      "description": "Training step 581",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:15",
      "total_flops_so_far": 1.381602358296576e+16,
      "budget_used_percent": 13.81602358296576
    },
    {
      "type": "training",
      "description": "Training step 582",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:16",
      "total_flops_so_far": 1.383976245510144e+16,
      "budget_used_percent": 13.83976245510144
    },
    {
      "type": "training",
      "description": "Training step 583",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:18",
      "total_flops_so_far": 1.386350132723712e+16,
      "budget_used_percent": 13.86350132723712
    },
    {
      "type": "training",
      "description": "Training step 584",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:19",
      "total_flops_so_far": 1.38872401993728e+16,
      "budget_used_percent": 13.8872401993728
    },
    {
      "type": "training",
      "description": "Training step 585",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:20",
      "total_flops_so_far": 1.391097907150848e+16,
      "budget_used_percent": 13.91097907150848
    },
    {
      "type": "training",
      "description": "Training step 586",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:21",
      "total_flops_so_far": 1.393471794364416e+16,
      "budget_used_percent": 13.93471794364416
    },
    {
      "type": "training",
      "description": "Training step 587",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:23",
      "total_flops_so_far": 1.395845681577984e+16,
      "budget_used_percent": 13.95845681577984
    },
    {
      "type": "training",
      "description": "Training step 588",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:24",
      "total_flops_so_far": 1.398219568791552e+16,
      "budget_used_percent": 13.98219568791552
    },
    {
      "type": "training",
      "description": "Training step 589",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:25",
      "total_flops_so_far": 1.40059345600512e+16,
      "budget_used_percent": 14.0059345600512
    },
    {
      "type": "training",
      "description": "Training step 590",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:27",
      "total_flops_so_far": 1.402967343218688e+16,
      "budget_used_percent": 14.029673432186879
    },
    {
      "type": "training",
      "description": "Training step 591",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:28",
      "total_flops_so_far": 1.405341230432256e+16,
      "budget_used_percent": 14.05341230432256
    },
    {
      "type": "training",
      "description": "Training step 592",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:29",
      "total_flops_so_far": 1.407715117645824e+16,
      "budget_used_percent": 14.07715117645824
    },
    {
      "type": "training",
      "description": "Training step 593",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:30",
      "total_flops_so_far": 1.410089004859392e+16,
      "budget_used_percent": 14.10089004859392
    },
    {
      "type": "training",
      "description": "Training step 594",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:32",
      "total_flops_so_far": 1.41246289207296e+16,
      "budget_used_percent": 14.1246289207296
    },
    {
      "type": "training",
      "description": "Training step 595",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:33",
      "total_flops_so_far": 1.414836779286528e+16,
      "budget_used_percent": 14.14836779286528
    },
    {
      "type": "training",
      "description": "Training step 596",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:34",
      "total_flops_so_far": 1.417210666500096e+16,
      "budget_used_percent": 14.17210666500096
    },
    {
      "type": "training",
      "description": "Training step 597",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:36",
      "total_flops_so_far": 1.419584553713664e+16,
      "budget_used_percent": 14.19584553713664
    },
    {
      "type": "training",
      "description": "Training step 598",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:37",
      "total_flops_so_far": 1.421958440927232e+16,
      "budget_used_percent": 14.21958440927232
    },
    {
      "type": "training",
      "description": "Training step 599",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:38",
      "total_flops_so_far": 1.4243323281408e+16,
      "budget_used_percent": 14.243323281408
    },
    {
      "type": "training",
      "description": "Training step 600",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:40",
      "total_flops_so_far": 1.426706215354368e+16,
      "budget_used_percent": 14.26706215354368
    },
    {
      "type": "training",
      "description": "Training step 601",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:41",
      "total_flops_so_far": 1.429080102567936e+16,
      "budget_used_percent": 14.29080102567936
    },
    {
      "type": "training",
      "description": "Training step 602",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:42",
      "total_flops_so_far": 1.431453989781504e+16,
      "budget_used_percent": 14.31453989781504
    },
    {
      "type": "training",
      "description": "Training step 603",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:43",
      "total_flops_so_far": 1.433827876995072e+16,
      "budget_used_percent": 14.33827876995072
    },
    {
      "type": "training",
      "description": "Training step 604",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:45",
      "total_flops_so_far": 1.43620176420864e+16,
      "budget_used_percent": 14.3620176420864
    },
    {
      "type": "training",
      "description": "Training step 605",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:46",
      "total_flops_so_far": 1.438575651422208e+16,
      "budget_used_percent": 14.38575651422208
    },
    {
      "type": "training",
      "description": "Training step 606",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:47",
      "total_flops_so_far": 1.440949538635776e+16,
      "budget_used_percent": 14.40949538635776
    },
    {
      "type": "training",
      "description": "Training step 607",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:49",
      "total_flops_so_far": 1.443323425849344e+16,
      "budget_used_percent": 14.43323425849344
    },
    {
      "type": "training",
      "description": "Training step 608",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:50",
      "total_flops_so_far": 1.445697313062912e+16,
      "budget_used_percent": 14.45697313062912
    },
    {
      "type": "training",
      "description": "Training step 609",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:51",
      "total_flops_so_far": 1.44807120027648e+16,
      "budget_used_percent": 14.4807120027648
    },
    {
      "type": "training",
      "description": "Training step 610",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:52",
      "total_flops_so_far": 1.450445087490048e+16,
      "budget_used_percent": 14.50445087490048
    },
    {
      "type": "training",
      "description": "Training step 611",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:54",
      "total_flops_so_far": 1.452818974703616e+16,
      "budget_used_percent": 14.52818974703616
    },
    {
      "type": "training",
      "description": "Training step 612",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:55",
      "total_flops_so_far": 1.455192861917184e+16,
      "budget_used_percent": 14.55192861917184
    },
    {
      "type": "training",
      "description": "Training step 613",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:56",
      "total_flops_so_far": 1.457566749130752e+16,
      "budget_used_percent": 14.57566749130752
    },
    {
      "type": "training",
      "description": "Training step 614",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:58",
      "total_flops_so_far": 1.45994063634432e+16,
      "budget_used_percent": 14.5994063634432
    },
    {
      "type": "training",
      "description": "Training step 615",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:44:59",
      "total_flops_so_far": 1.462314523557888e+16,
      "budget_used_percent": 14.62314523557888
    },
    {
      "type": "training",
      "description": "Training step 616",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:00",
      "total_flops_so_far": 1.464688410771456e+16,
      "budget_used_percent": 14.64688410771456
    },
    {
      "type": "training",
      "description": "Training step 617",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:02",
      "total_flops_so_far": 1.467062297985024e+16,
      "budget_used_percent": 14.67062297985024
    },
    {
      "type": "training",
      "description": "Training step 618",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:03",
      "total_flops_so_far": 1.469436185198592e+16,
      "budget_used_percent": 14.69436185198592
    },
    {
      "type": "training",
      "description": "Training step 619",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:04",
      "total_flops_so_far": 1.47181007241216e+16,
      "budget_used_percent": 14.7181007241216
    },
    {
      "type": "training",
      "description": "Training step 620",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:05",
      "total_flops_so_far": 1.474183959625728e+16,
      "budget_used_percent": 14.74183959625728
    },
    {
      "type": "training",
      "description": "Training step 621",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:07",
      "total_flops_so_far": 1.476557846839296e+16,
      "budget_used_percent": 14.76557846839296
    },
    {
      "type": "training",
      "description": "Training step 622",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:08",
      "total_flops_so_far": 1.478931734052864e+16,
      "budget_used_percent": 14.78931734052864
    },
    {
      "type": "training",
      "description": "Training step 623",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:09",
      "total_flops_so_far": 1.481305621266432e+16,
      "budget_used_percent": 14.81305621266432
    },
    {
      "type": "training",
      "description": "Training step 624",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:11",
      "total_flops_so_far": 1.48367950848e+16,
      "budget_used_percent": 14.8367950848
    },
    {
      "type": "training",
      "description": "Training step 625",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:12",
      "total_flops_so_far": 1.486053395693568e+16,
      "budget_used_percent": 14.86053395693568
    },
    {
      "type": "training",
      "description": "Training step 626",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:13",
      "total_flops_so_far": 1.488427282907136e+16,
      "budget_used_percent": 14.88427282907136
    },
    {
      "type": "training",
      "description": "Training step 627",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:14",
      "total_flops_so_far": 1.490801170120704e+16,
      "budget_used_percent": 14.908011701207041
    },
    {
      "type": "training",
      "description": "Training step 628",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:16",
      "total_flops_so_far": 1.493175057334272e+16,
      "budget_used_percent": 14.93175057334272
    },
    {
      "type": "training",
      "description": "Training step 629",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:17",
      "total_flops_so_far": 1.49554894454784e+16,
      "budget_used_percent": 14.9554894454784
    },
    {
      "type": "training",
      "description": "Training step 630",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:18",
      "total_flops_so_far": 1.497922831761408e+16,
      "budget_used_percent": 14.97922831761408
    },
    {
      "type": "training",
      "description": "Training step 631",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:20",
      "total_flops_so_far": 1.500296718974976e+16,
      "budget_used_percent": 15.00296718974976
    },
    {
      "type": "training",
      "description": "Training step 632",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:21",
      "total_flops_so_far": 1.502670606188544e+16,
      "budget_used_percent": 15.02670606188544
    },
    {
      "type": "training",
      "description": "Training step 633",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:22",
      "total_flops_so_far": 1.505044493402112e+16,
      "budget_used_percent": 15.05044493402112
    },
    {
      "type": "training",
      "description": "Training step 634",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:24",
      "total_flops_so_far": 1.50741838061568e+16,
      "budget_used_percent": 15.0741838061568
    },
    {
      "type": "training",
      "description": "Training step 635",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:25",
      "total_flops_so_far": 1.509792267829248e+16,
      "budget_used_percent": 15.09792267829248
    },
    {
      "type": "training",
      "description": "Training step 636",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:26",
      "total_flops_so_far": 1.512166155042816e+16,
      "budget_used_percent": 15.12166155042816
    },
    {
      "type": "training",
      "description": "Training step 637",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:27",
      "total_flops_so_far": 1.514540042256384e+16,
      "budget_used_percent": 15.14540042256384
    },
    {
      "type": "training",
      "description": "Training step 638",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:29",
      "total_flops_so_far": 1.516913929469952e+16,
      "budget_used_percent": 15.169139294699521
    },
    {
      "type": "training",
      "description": "Training step 639",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:30",
      "total_flops_so_far": 1.51928781668352e+16,
      "budget_used_percent": 15.1928781668352
    },
    {
      "type": "training",
      "description": "Training step 640",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:31",
      "total_flops_so_far": 1.521661703897088e+16,
      "budget_used_percent": 15.216617038970881
    },
    {
      "type": "training",
      "description": "Training step 641",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:33",
      "total_flops_so_far": 1.524035591110656e+16,
      "budget_used_percent": 15.24035591110656
    },
    {
      "type": "training",
      "description": "Training step 642",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:34",
      "total_flops_so_far": 1.526409478324224e+16,
      "budget_used_percent": 15.264094783242241
    },
    {
      "type": "training",
      "description": "Training step 643",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:35",
      "total_flops_so_far": 1.528783365537792e+16,
      "budget_used_percent": 15.28783365537792
    },
    {
      "type": "training",
      "description": "Training step 644",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:37",
      "total_flops_so_far": 1.53115725275136e+16,
      "budget_used_percent": 15.3115725275136
    },
    {
      "type": "training",
      "description": "Training step 645",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:38",
      "total_flops_so_far": 1.533531139964928e+16,
      "budget_used_percent": 15.33531139964928
    },
    {
      "type": "training",
      "description": "Training step 646",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:39",
      "total_flops_so_far": 1.535905027178496e+16,
      "budget_used_percent": 15.35905027178496
    },
    {
      "type": "training",
      "description": "Training step 647",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:40",
      "total_flops_so_far": 1.538278914392064e+16,
      "budget_used_percent": 15.38278914392064
    },
    {
      "type": "training",
      "description": "Training step 648",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:42",
      "total_flops_so_far": 1.540652801605632e+16,
      "budget_used_percent": 15.40652801605632
    },
    {
      "type": "training",
      "description": "Training step 649",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:43",
      "total_flops_so_far": 1.5430266888192e+16,
      "budget_used_percent": 15.430266888192001
    },
    {
      "type": "training",
      "description": "Training step 650",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:44",
      "total_flops_so_far": 1.545400576032768e+16,
      "budget_used_percent": 15.45400576032768
    },
    {
      "type": "training",
      "description": "Training step 651",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:46",
      "total_flops_so_far": 1.547774463246336e+16,
      "budget_used_percent": 15.477744632463361
    },
    {
      "type": "training",
      "description": "Training step 652",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:47",
      "total_flops_so_far": 1.550148350459904e+16,
      "budget_used_percent": 15.50148350459904
    },
    {
      "type": "training",
      "description": "Training step 653",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:48",
      "total_flops_so_far": 1.552522237673472e+16,
      "budget_used_percent": 15.525222376734721
    },
    {
      "type": "training",
      "description": "Training step 654",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:49",
      "total_flops_so_far": 1.55489612488704e+16,
      "budget_used_percent": 15.5489612488704
    },
    {
      "type": "training",
      "description": "Training step 655",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:51",
      "total_flops_so_far": 1.557270012100608e+16,
      "budget_used_percent": 15.572700121006081
    },
    {
      "type": "training",
      "description": "Training step 656",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:52",
      "total_flops_so_far": 1.559643899314176e+16,
      "budget_used_percent": 15.59643899314176
    },
    {
      "type": "training",
      "description": "Training step 657",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:53",
      "total_flops_so_far": 1.562017786527744e+16,
      "budget_used_percent": 15.62017786527744
    },
    {
      "type": "training",
      "description": "Training step 658",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:55",
      "total_flops_so_far": 1.564391673741312e+16,
      "budget_used_percent": 15.643916737413118
    },
    {
      "type": "training",
      "description": "Training step 659",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:56",
      "total_flops_so_far": 1.56676556095488e+16,
      "budget_used_percent": 15.6676556095488
    },
    {
      "type": "training",
      "description": "Training step 660",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:56",
      "total_flops_so_far": 1.569139448168448e+16,
      "budget_used_percent": 15.691394481684481
    },
    {
      "type": "training",
      "description": "Training step 661",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:58",
      "total_flops_so_far": 1.571513335382016e+16,
      "budget_used_percent": 15.71513335382016
    },
    {
      "type": "training",
      "description": "Training step 662",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:45:59",
      "total_flops_so_far": 1.573887222595584e+16,
      "budget_used_percent": 15.738872225955841
    },
    {
      "type": "training",
      "description": "Training step 663",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:00",
      "total_flops_so_far": 1.576261109809152e+16,
      "budget_used_percent": 15.76261109809152
    },
    {
      "type": "training",
      "description": "Training step 664",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:01",
      "total_flops_so_far": 1.57863499702272e+16,
      "budget_used_percent": 15.786349970227201
    },
    {
      "type": "training",
      "description": "Training step 665",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:03",
      "total_flops_so_far": 1.581008884236288e+16,
      "budget_used_percent": 15.81008884236288
    },
    {
      "type": "training",
      "description": "Training step 666",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:04",
      "total_flops_so_far": 1.583382771449856e+16,
      "budget_used_percent": 15.833827714498561
    },
    {
      "type": "training",
      "description": "Training step 667",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:05",
      "total_flops_so_far": 1.585756658663424e+16,
      "budget_used_percent": 15.857566586634238
    },
    {
      "type": "training",
      "description": "Training step 668",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:07",
      "total_flops_so_far": 1.588130545876992e+16,
      "budget_used_percent": 15.881305458769921
    },
    {
      "type": "training",
      "description": "Training step 669",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:08",
      "total_flops_so_far": 1.59050443309056e+16,
      "budget_used_percent": 15.905044330905598
    },
    {
      "type": "training",
      "description": "Training step 670",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:09",
      "total_flops_so_far": 1.592878320304128e+16,
      "budget_used_percent": 15.92878320304128
    },
    {
      "type": "training",
      "description": "Training step 671",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:11",
      "total_flops_so_far": 1.595252207517696e+16,
      "budget_used_percent": 15.952522075176958
    },
    {
      "type": "training",
      "description": "Training step 672",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:12",
      "total_flops_so_far": 1.597626094731264e+16,
      "budget_used_percent": 15.97626094731264
    },
    {
      "type": "training",
      "description": "Training step 673",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:13",
      "total_flops_so_far": 1.599999981944832e+16,
      "budget_used_percent": 15.999999819448322
    },
    {
      "type": "training",
      "description": "Training step 674",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:14",
      "total_flops_so_far": 1.6023738691584e+16,
      "budget_used_percent": 16.023738691584
    },
    {
      "type": "training",
      "description": "Training step 675",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:16",
      "total_flops_so_far": 1.604747756371968e+16,
      "budget_used_percent": 16.04747756371968
    },
    {
      "type": "training",
      "description": "Training step 676",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:17",
      "total_flops_so_far": 1.607121643585536e+16,
      "budget_used_percent": 16.07121643585536
    },
    {
      "type": "training",
      "description": "Training step 677",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:18",
      "total_flops_so_far": 1.609495530799104e+16,
      "budget_used_percent": 16.09495530799104
    },
    {
      "type": "training",
      "description": "Training step 678",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:20",
      "total_flops_so_far": 1.611869418012672e+16,
      "budget_used_percent": 16.11869418012672
    },
    {
      "type": "training",
      "description": "Training step 679",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:21",
      "total_flops_so_far": 1.61424330522624e+16,
      "budget_used_percent": 16.1424330522624
    },
    {
      "type": "training",
      "description": "Training step 680",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:22",
      "total_flops_so_far": 1.616617192439808e+16,
      "budget_used_percent": 16.16617192439808
    },
    {
      "type": "training",
      "description": "Training step 681",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:23",
      "total_flops_so_far": 1.618991079653376e+16,
      "budget_used_percent": 16.18991079653376
    },
    {
      "type": "training",
      "description": "Training step 682",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:25",
      "total_flops_so_far": 1.621364966866944e+16,
      "budget_used_percent": 16.21364966866944
    },
    {
      "type": "training",
      "description": "Training step 683",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:26",
      "total_flops_so_far": 1.623738854080512e+16,
      "budget_used_percent": 16.23738854080512
    },
    {
      "type": "training",
      "description": "Training step 684",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:27",
      "total_flops_so_far": 1.62611274129408e+16,
      "budget_used_percent": 16.2611274129408
    },
    {
      "type": "training",
      "description": "Training step 685",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:29",
      "total_flops_so_far": 1.628486628507648e+16,
      "budget_used_percent": 16.28486628507648
    },
    {
      "type": "training",
      "description": "Training step 686",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:30",
      "total_flops_so_far": 1.630860515721216e+16,
      "budget_used_percent": 16.30860515721216
    },
    {
      "type": "training",
      "description": "Training step 687",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:31",
      "total_flops_so_far": 1.633234402934784e+16,
      "budget_used_percent": 16.33234402934784
    },
    {
      "type": "training",
      "description": "Training step 688",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:33",
      "total_flops_so_far": 1.635608290148352e+16,
      "budget_used_percent": 16.35608290148352
    },
    {
      "type": "training",
      "description": "Training step 689",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:34",
      "total_flops_so_far": 1.63798217736192e+16,
      "budget_used_percent": 16.3798217736192
    },
    {
      "type": "training",
      "description": "Training step 690",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:35",
      "total_flops_so_far": 1.640356064575488e+16,
      "budget_used_percent": 16.40356064575488
    },
    {
      "type": "training",
      "description": "Training step 691",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:36",
      "total_flops_so_far": 1.642729951789056e+16,
      "budget_used_percent": 16.42729951789056
    },
    {
      "type": "training",
      "description": "Training step 692",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:38",
      "total_flops_so_far": 1.645103839002624e+16,
      "budget_used_percent": 16.45103839002624
    },
    {
      "type": "training",
      "description": "Training step 693",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:39",
      "total_flops_so_far": 1.647477726216192e+16,
      "budget_used_percent": 16.47477726216192
    },
    {
      "type": "training",
      "description": "Training step 694",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:40",
      "total_flops_so_far": 1.64985161342976e+16,
      "budget_used_percent": 16.4985161342976
    },
    {
      "type": "training",
      "description": "Training step 695",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:42",
      "total_flops_so_far": 1.652225500643328e+16,
      "budget_used_percent": 16.52225500643328
    },
    {
      "type": "training",
      "description": "Training step 696",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:43",
      "total_flops_so_far": 1.654599387856896e+16,
      "budget_used_percent": 16.54599387856896
    },
    {
      "type": "training",
      "description": "Training step 697",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:44",
      "total_flops_so_far": 1.656973275070464e+16,
      "budget_used_percent": 16.56973275070464
    },
    {
      "type": "training",
      "description": "Training step 698",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:46",
      "total_flops_so_far": 1.659347162284032e+16,
      "budget_used_percent": 16.59347162284032
    },
    {
      "type": "training",
      "description": "Training step 699",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:47",
      "total_flops_so_far": 1.6617210494976e+16,
      "budget_used_percent": 16.617210494976
    },
    {
      "type": "training",
      "description": "Training step 700",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:48",
      "total_flops_so_far": 1.664094936711168e+16,
      "budget_used_percent": 16.64094936711168
    },
    {
      "type": "training",
      "description": "Training step 701",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:49",
      "total_flops_so_far": 1.666468823924736e+16,
      "budget_used_percent": 16.66468823924736
    },
    {
      "type": "training",
      "description": "Training step 702",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:51",
      "total_flops_so_far": 1.668842711138304e+16,
      "budget_used_percent": 16.68842711138304
    },
    {
      "type": "training",
      "description": "Training step 703",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:52",
      "total_flops_so_far": 1.671216598351872e+16,
      "budget_used_percent": 16.71216598351872
    },
    {
      "type": "training",
      "description": "Training step 704",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:53",
      "total_flops_so_far": 1.67359048556544e+16,
      "budget_used_percent": 16.7359048556544
    },
    {
      "type": "training",
      "description": "Training step 705",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:55",
      "total_flops_so_far": 1.675964372779008e+16,
      "budget_used_percent": 16.75964372779008
    },
    {
      "type": "training",
      "description": "Training step 706",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:56",
      "total_flops_so_far": 1.678338259992576e+16,
      "budget_used_percent": 16.783382599925762
    },
    {
      "type": "training",
      "description": "Training step 707",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:57",
      "total_flops_so_far": 1.680712147206144e+16,
      "budget_used_percent": 16.80712147206144
    },
    {
      "type": "training",
      "description": "Training step 708",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:46:59",
      "total_flops_so_far": 1.683086034419712e+16,
      "budget_used_percent": 16.83086034419712
    },
    {
      "type": "training",
      "description": "Training step 709",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:00",
      "total_flops_so_far": 1.68545992163328e+16,
      "budget_used_percent": 16.8545992163328
    },
    {
      "type": "training",
      "description": "Training step 710",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:01",
      "total_flops_so_far": 1.687833808846848e+16,
      "budget_used_percent": 16.87833808846848
    },
    {
      "type": "training",
      "description": "Training step 711",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:02",
      "total_flops_so_far": 1.690207696060416e+16,
      "budget_used_percent": 16.90207696060416
    },
    {
      "type": "training",
      "description": "Training step 712",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:04",
      "total_flops_so_far": 1.692581583273984e+16,
      "budget_used_percent": 16.92581583273984
    },
    {
      "type": "training",
      "description": "Training step 713",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:05",
      "total_flops_so_far": 1.694955470487552e+16,
      "budget_used_percent": 16.94955470487552
    },
    {
      "type": "training",
      "description": "Training step 714",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:06",
      "total_flops_so_far": 1.69732935770112e+16,
      "budget_used_percent": 16.9732935770112
    },
    {
      "type": "training",
      "description": "Training step 715",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:08",
      "total_flops_so_far": 1.699703244914688e+16,
      "budget_used_percent": 16.99703244914688
    },
    {
      "type": "training",
      "description": "Training step 716",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:09",
      "total_flops_so_far": 1.702077132128256e+16,
      "budget_used_percent": 17.02077132128256
    },
    {
      "type": "training",
      "description": "Training step 717",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:10",
      "total_flops_so_far": 1.704451019341824e+16,
      "budget_used_percent": 17.04451019341824
    },
    {
      "type": "training",
      "description": "Training step 718",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:11",
      "total_flops_so_far": 1.706824906555392e+16,
      "budget_used_percent": 17.06824906555392
    },
    {
      "type": "training",
      "description": "Training step 719",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:13",
      "total_flops_so_far": 1.70919879376896e+16,
      "budget_used_percent": 17.091987937689602
    },
    {
      "type": "training",
      "description": "Training step 720",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:14",
      "total_flops_so_far": 1.711572680982528e+16,
      "budget_used_percent": 17.11572680982528
    },
    {
      "type": "training",
      "description": "Training step 721",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:15",
      "total_flops_so_far": 1.713946568196096e+16,
      "budget_used_percent": 17.13946568196096
    },
    {
      "type": "training",
      "description": "Training step 722",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:17",
      "total_flops_so_far": 1.716320455409664e+16,
      "budget_used_percent": 17.16320455409664
    },
    {
      "type": "training",
      "description": "Training step 723",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:18",
      "total_flops_so_far": 1.718694342623232e+16,
      "budget_used_percent": 17.18694342623232
    },
    {
      "type": "training",
      "description": "Training step 724",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:19",
      "total_flops_so_far": 1.7210682298368e+16,
      "budget_used_percent": 17.210682298368
    },
    {
      "type": "training",
      "description": "Training step 725",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:21",
      "total_flops_so_far": 1.723442117050368e+16,
      "budget_used_percent": 17.23442117050368
    },
    {
      "type": "training",
      "description": "Training step 726",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:22",
      "total_flops_so_far": 1.725816004263936e+16,
      "budget_used_percent": 17.25816004263936
    },
    {
      "type": "training",
      "description": "Training step 727",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:23",
      "total_flops_so_far": 1.728189891477504e+16,
      "budget_used_percent": 17.28189891477504
    },
    {
      "type": "training",
      "description": "Training step 728",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:24",
      "total_flops_so_far": 1.730563778691072e+16,
      "budget_used_percent": 17.30563778691072
    },
    {
      "type": "training",
      "description": "Training step 729",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:26",
      "total_flops_so_far": 1.73293766590464e+16,
      "budget_used_percent": 17.3293766590464
    },
    {
      "type": "training",
      "description": "Training step 730",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:27",
      "total_flops_so_far": 1.735311553118208e+16,
      "budget_used_percent": 17.35311553118208
    },
    {
      "type": "training",
      "description": "Training step 731",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:28",
      "total_flops_so_far": 1.737685440331776e+16,
      "budget_used_percent": 17.37685440331776
    },
    {
      "type": "training",
      "description": "Training step 732",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:30",
      "total_flops_so_far": 1.740059327545344e+16,
      "budget_used_percent": 17.400593275453442
    },
    {
      "type": "training",
      "description": "Training step 733",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:31",
      "total_flops_so_far": 1.742433214758912e+16,
      "budget_used_percent": 17.42433214758912
    },
    {
      "type": "training",
      "description": "Training step 734",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:32",
      "total_flops_so_far": 1.74480710197248e+16,
      "budget_used_percent": 17.4480710197248
    },
    {
      "type": "training",
      "description": "Training step 735",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:34",
      "total_flops_so_far": 1.747180989186048e+16,
      "budget_used_percent": 17.47180989186048
    },
    {
      "type": "training",
      "description": "Training step 736",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:35",
      "total_flops_so_far": 1.749554876399616e+16,
      "budget_used_percent": 17.49554876399616
    },
    {
      "type": "training",
      "description": "Training step 737",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:36",
      "total_flops_so_far": 1.751928763613184e+16,
      "budget_used_percent": 17.51928763613184
    },
    {
      "type": "training",
      "description": "Training step 738",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:37",
      "total_flops_so_far": 1.754302650826752e+16,
      "budget_used_percent": 17.54302650826752
    },
    {
      "type": "training",
      "description": "Training step 739",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:39",
      "total_flops_so_far": 1.75667653804032e+16,
      "budget_used_percent": 17.566765380403197
    },
    {
      "type": "training",
      "description": "Training step 740",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:40",
      "total_flops_so_far": 1.759050425253888e+16,
      "budget_used_percent": 17.59050425253888
    },
    {
      "type": "training",
      "description": "Training step 741",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:41",
      "total_flops_so_far": 1.761424312467456e+16,
      "budget_used_percent": 17.614243124674562
    },
    {
      "type": "training",
      "description": "Training step 742",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:43",
      "total_flops_so_far": 1.763798199681024e+16,
      "budget_used_percent": 17.63798199681024
    },
    {
      "type": "training",
      "description": "Training step 743",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:44",
      "total_flops_so_far": 1.766172086894592e+16,
      "budget_used_percent": 17.66172086894592
    },
    {
      "type": "training",
      "description": "Training step 744",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:45",
      "total_flops_so_far": 1.76854597410816e+16,
      "budget_used_percent": 17.6854597410816
    },
    {
      "type": "training",
      "description": "Training step 745",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:46",
      "total_flops_so_far": 1.770919861321728e+16,
      "budget_used_percent": 17.709198613217282
    },
    {
      "type": "training",
      "description": "Training step 746",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:48",
      "total_flops_so_far": 1.773293748535296e+16,
      "budget_used_percent": 17.73293748535296
    },
    {
      "type": "training",
      "description": "Training step 747",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:49",
      "total_flops_so_far": 1.775667635748864e+16,
      "budget_used_percent": 17.75667635748864
    },
    {
      "type": "training",
      "description": "Training step 748",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:50",
      "total_flops_so_far": 1.778041522962432e+16,
      "budget_used_percent": 17.780415229624317
    },
    {
      "type": "training",
      "description": "Training step 749",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:52",
      "total_flops_so_far": 1.780415410176e+16,
      "budget_used_percent": 17.804154101760002
    },
    {
      "type": "training",
      "description": "Training step 750",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:53",
      "total_flops_so_far": 1.782789297389568e+16,
      "budget_used_percent": 17.82789297389568
    },
    {
      "type": "training",
      "description": "Training step 751",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:54",
      "total_flops_so_far": 1.785163184603136e+16,
      "budget_used_percent": 17.85163184603136
    },
    {
      "type": "training",
      "description": "Training step 752",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:56",
      "total_flops_so_far": 1.787537071816704e+16,
      "budget_used_percent": 17.87537071816704
    },
    {
      "type": "training",
      "description": "Training step 753",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:57",
      "total_flops_so_far": 1.789910959030272e+16,
      "budget_used_percent": 17.89910959030272
    },
    {
      "type": "training",
      "description": "Training step 754",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:58",
      "total_flops_so_far": 1.79228484624384e+16,
      "budget_used_percent": 17.922848462438402
    },
    {
      "type": "training",
      "description": "Training step 755",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:47:59",
      "total_flops_so_far": 1.794658733457408e+16,
      "budget_used_percent": 17.94658733457408
    },
    {
      "type": "training",
      "description": "Training step 756",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:01",
      "total_flops_so_far": 1.797032620670976e+16,
      "budget_used_percent": 17.97032620670976
    },
    {
      "type": "training",
      "description": "Training step 757",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:02",
      "total_flops_so_far": 1.799406507884544e+16,
      "budget_used_percent": 17.994065078845438
    },
    {
      "type": "training",
      "description": "Training step 758",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:03",
      "total_flops_so_far": 1.801780395098112e+16,
      "budget_used_percent": 18.017803950981122
    },
    {
      "type": "training",
      "description": "Training step 759",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:05",
      "total_flops_so_far": 1.80415428231168e+16,
      "budget_used_percent": 18.0415428231168
    },
    {
      "type": "training",
      "description": "Training step 760",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:06",
      "total_flops_so_far": 1.806528169525248e+16,
      "budget_used_percent": 18.06528169525248
    },
    {
      "type": "training",
      "description": "Training step 761",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:07",
      "total_flops_so_far": 1.808902056738816e+16,
      "budget_used_percent": 18.089020567388157
    },
    {
      "type": "training",
      "description": "Training step 762",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:09",
      "total_flops_so_far": 1.811275943952384e+16,
      "budget_used_percent": 18.112759439523842
    },
    {
      "type": "training",
      "description": "Training step 763",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:10",
      "total_flops_so_far": 1.813649831165952e+16,
      "budget_used_percent": 18.136498311659523
    },
    {
      "type": "training",
      "description": "Training step 764",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:11",
      "total_flops_so_far": 1.81602371837952e+16,
      "budget_used_percent": 18.1602371837952
    },
    {
      "type": "training",
      "description": "Training step 765",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:12",
      "total_flops_so_far": 1.818397605593088e+16,
      "budget_used_percent": 18.18397605593088
    },
    {
      "type": "training",
      "description": "Training step 766",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:14",
      "total_flops_so_far": 1.820771492806656e+16,
      "budget_used_percent": 18.207714928066558
    },
    {
      "type": "training",
      "description": "Training step 767",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:15",
      "total_flops_so_far": 1.823145380020224e+16,
      "budget_used_percent": 18.231453800202242
    },
    {
      "type": "training",
      "description": "Training step 768",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:16",
      "total_flops_so_far": 1.825519267233792e+16,
      "budget_used_percent": 18.25519267233792
    },
    {
      "type": "training",
      "description": "Training step 769",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:18",
      "total_flops_so_far": 1.82789315444736e+16,
      "budget_used_percent": 18.2789315444736
    },
    {
      "type": "training",
      "description": "Training step 770",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:19",
      "total_flops_so_far": 1.830267041660928e+16,
      "budget_used_percent": 18.302670416609278
    },
    {
      "type": "training",
      "description": "Training step 771",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:20",
      "total_flops_so_far": 1.832640928874496e+16,
      "budget_used_percent": 18.326409288744962
    },
    {
      "type": "training",
      "description": "Training step 772",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:22",
      "total_flops_so_far": 1.835014816088064e+16,
      "budget_used_percent": 18.35014816088064
    },
    {
      "type": "training",
      "description": "Training step 773",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:23",
      "total_flops_so_far": 1.837388703301632e+16,
      "budget_used_percent": 18.37388703301632
    },
    {
      "type": "training",
      "description": "Training step 774",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:24",
      "total_flops_so_far": 1.8397625905152e+16,
      "budget_used_percent": 18.397625905152
    },
    {
      "type": "training",
      "description": "Training step 775",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:26",
      "total_flops_so_far": 1.842136477728768e+16,
      "budget_used_percent": 18.42136477728768
    },
    {
      "type": "training",
      "description": "Training step 776",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:27",
      "total_flops_so_far": 1.844510364942336e+16,
      "budget_used_percent": 18.445103649423363
    },
    {
      "type": "training",
      "description": "Training step 777",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:28",
      "total_flops_so_far": 1.846884252155904e+16,
      "budget_used_percent": 18.46884252155904
    },
    {
      "type": "training",
      "description": "Training step 778",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:29",
      "total_flops_so_far": 1.849258139369472e+16,
      "budget_used_percent": 18.49258139369472
    },
    {
      "type": "training",
      "description": "Training step 779",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:31",
      "total_flops_so_far": 1.85163202658304e+16,
      "budget_used_percent": 18.516320265830398
    },
    {
      "type": "training",
      "description": "Training step 780",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:32",
      "total_flops_so_far": 1.854005913796608e+16,
      "budget_used_percent": 18.540059137966082
    },
    {
      "type": "training",
      "description": "Training step 781",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:33",
      "total_flops_so_far": 1.856379801010176e+16,
      "budget_used_percent": 18.56379801010176
    },
    {
      "type": "training",
      "description": "Training step 782",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:35",
      "total_flops_so_far": 1.858753688223744e+16,
      "budget_used_percent": 18.58753688223744
    },
    {
      "type": "training",
      "description": "Training step 783",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:36",
      "total_flops_so_far": 1.861127575437312e+16,
      "budget_used_percent": 18.611275754373118
    },
    {
      "type": "training",
      "description": "Training step 784",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:37",
      "total_flops_so_far": 1.86350146265088e+16,
      "budget_used_percent": 18.6350146265088
    },
    {
      "type": "training",
      "description": "Training step 785",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:39",
      "total_flops_so_far": 1.865875349864448e+16,
      "budget_used_percent": 18.65875349864448
    },
    {
      "type": "training",
      "description": "Training step 786",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:40",
      "total_flops_so_far": 1.868249237078016e+16,
      "budget_used_percent": 18.68249237078016
    },
    {
      "type": "training",
      "description": "Training step 787",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:41",
      "total_flops_so_far": 1.870623124291584e+16,
      "budget_used_percent": 18.70623124291584
    },
    {
      "type": "training",
      "description": "Training step 788",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:42",
      "total_flops_so_far": 1.872997011505152e+16,
      "budget_used_percent": 18.72997011505152
    },
    {
      "type": "training",
      "description": "Training step 789",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:44",
      "total_flops_so_far": 1.87537089871872e+16,
      "budget_used_percent": 18.753708987187203
    },
    {
      "type": "training",
      "description": "Training step 790",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:45",
      "total_flops_so_far": 1.877744785932288e+16,
      "budget_used_percent": 18.77744785932288
    },
    {
      "type": "training",
      "description": "Training step 791",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:46",
      "total_flops_so_far": 1.880118673145856e+16,
      "budget_used_percent": 18.80118673145856
    },
    {
      "type": "training",
      "description": "Training step 792",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:47",
      "total_flops_so_far": 1.882492560359424e+16,
      "budget_used_percent": 18.824925603594238
    },
    {
      "type": "training",
      "description": "Training step 793",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:48",
      "total_flops_so_far": 1.884866447572992e+16,
      "budget_used_percent": 18.84866447572992
    },
    {
      "type": "training",
      "description": "Training step 794",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:49",
      "total_flops_so_far": 1.88724033478656e+16,
      "budget_used_percent": 18.8724033478656
    },
    {
      "type": "training",
      "description": "Training step 795",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:51",
      "total_flops_so_far": 1.889614222000128e+16,
      "budget_used_percent": 18.89614222000128
    },
    {
      "type": "training",
      "description": "Training step 796",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:52",
      "total_flops_so_far": 1.891988109213696e+16,
      "budget_used_percent": 18.919881092136958
    },
    {
      "type": "training",
      "description": "Training step 797",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:53",
      "total_flops_so_far": 1.894361996427264e+16,
      "budget_used_percent": 18.94361996427264
    },
    {
      "type": "training",
      "description": "Training step 798",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:54",
      "total_flops_so_far": 1.896735883640832e+16,
      "budget_used_percent": 18.967358836408323
    },
    {
      "type": "training",
      "description": "Training step 799",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:56",
      "total_flops_so_far": 1.8991097708544e+16,
      "budget_used_percent": 18.991097708544
    },
    {
      "type": "training",
      "description": "Training step 800",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:57",
      "total_flops_so_far": 1.901483658067968e+16,
      "budget_used_percent": 19.01483658067968
    },
    {
      "type": "training",
      "description": "Training step 801",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:48:58",
      "total_flops_so_far": 1.903857545281536e+16,
      "budget_used_percent": 19.03857545281536
    },
    {
      "type": "training",
      "description": "Training step 802",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:00",
      "total_flops_so_far": 1.906231432495104e+16,
      "budget_used_percent": 19.06231432495104
    },
    {
      "type": "training",
      "description": "Training step 803",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:01",
      "total_flops_so_far": 1.908605319708672e+16,
      "budget_used_percent": 19.08605319708672
    },
    {
      "type": "training",
      "description": "Training step 804",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:02",
      "total_flops_so_far": 1.91097920692224e+16,
      "budget_used_percent": 19.1097920692224
    },
    {
      "type": "training",
      "description": "Training step 805",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:04",
      "total_flops_so_far": 1.913353094135808e+16,
      "budget_used_percent": 19.133530941358078
    },
    {
      "type": "training",
      "description": "Training step 806",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:05",
      "total_flops_so_far": 1.915726981349376e+16,
      "budget_used_percent": 19.15726981349376
    },
    {
      "type": "training",
      "description": "Training step 807",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:06",
      "total_flops_so_far": 1.918100868562944e+16,
      "budget_used_percent": 19.18100868562944
    },
    {
      "type": "training",
      "description": "Training step 808",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:07",
      "total_flops_so_far": 1.920474755776512e+16,
      "budget_used_percent": 19.20474755776512
    },
    {
      "type": "training",
      "description": "Training step 809",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:09",
      "total_flops_so_far": 1.92284864299008e+16,
      "budget_used_percent": 19.2284864299008
    },
    {
      "type": "training",
      "description": "Training step 810",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:10",
      "total_flops_so_far": 1.925222530203648e+16,
      "budget_used_percent": 19.25222530203648
    },
    {
      "type": "training",
      "description": "Training step 811",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:11",
      "total_flops_so_far": 1.927596417417216e+16,
      "budget_used_percent": 19.27596417417216
    },
    {
      "type": "training",
      "description": "Training step 812",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:13",
      "total_flops_so_far": 1.929970304630784e+16,
      "budget_used_percent": 19.29970304630784
    },
    {
      "type": "training",
      "description": "Training step 813",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:14",
      "total_flops_so_far": 1.932344191844352e+16,
      "budget_used_percent": 19.32344191844352
    },
    {
      "type": "training",
      "description": "Training step 814",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:15",
      "total_flops_so_far": 1.93471807905792e+16,
      "budget_used_percent": 19.3471807905792
    },
    {
      "type": "training",
      "description": "Training step 815",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:17",
      "total_flops_so_far": 1.937091966271488e+16,
      "budget_used_percent": 19.37091966271488
    },
    {
      "type": "training",
      "description": "Training step 816",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:18",
      "total_flops_so_far": 1.939465853485056e+16,
      "budget_used_percent": 19.39465853485056
    },
    {
      "type": "training",
      "description": "Training step 817",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:19",
      "total_flops_so_far": 1.941839740698624e+16,
      "budget_used_percent": 19.41839740698624
    },
    {
      "type": "training",
      "description": "Training step 818",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:20",
      "total_flops_so_far": 1.944213627912192e+16,
      "budget_used_percent": 19.442136279121918
    },
    {
      "type": "training",
      "description": "Training step 819",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:22",
      "total_flops_so_far": 1.94658751512576e+16,
      "budget_used_percent": 19.4658751512576
    },
    {
      "type": "training",
      "description": "Training step 820",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:23",
      "total_flops_so_far": 1.948961402339328e+16,
      "budget_used_percent": 19.48961402339328
    },
    {
      "type": "training",
      "description": "Training step 821",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:24",
      "total_flops_so_far": 1.951335289552896e+16,
      "budget_used_percent": 19.51335289552896
    },
    {
      "type": "training",
      "description": "Training step 822",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:26",
      "total_flops_so_far": 1.953709176766464e+16,
      "budget_used_percent": 19.53709176766464
    },
    {
      "type": "training",
      "description": "Training step 823",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:27",
      "total_flops_so_far": 1.956083063980032e+16,
      "budget_used_percent": 19.56083063980032
    },
    {
      "type": "training",
      "description": "Training step 824",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:28",
      "total_flops_so_far": 1.9584569511936e+16,
      "budget_used_percent": 19.584569511936
    },
    {
      "type": "training",
      "description": "Training step 825",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:30",
      "total_flops_so_far": 1.960830838407168e+16,
      "budget_used_percent": 19.60830838407168
    },
    {
      "type": "training",
      "description": "Training step 826",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:31",
      "total_flops_so_far": 1.963204725620736e+16,
      "budget_used_percent": 19.63204725620736
    },
    {
      "type": "training",
      "description": "Training step 827",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:32",
      "total_flops_so_far": 1.965578612834304e+16,
      "budget_used_percent": 19.65578612834304
    },
    {
      "type": "training",
      "description": "Training step 828",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:33",
      "total_flops_so_far": 1.967952500047872e+16,
      "budget_used_percent": 19.67952500047872
    },
    {
      "type": "training",
      "description": "Training step 829",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:35",
      "total_flops_so_far": 1.97032638726144e+16,
      "budget_used_percent": 19.7032638726144
    },
    {
      "type": "training",
      "description": "Training step 830",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:36",
      "total_flops_so_far": 1.972700274475008e+16,
      "budget_used_percent": 19.72700274475008
    },
    {
      "type": "training",
      "description": "Training step 831",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:37",
      "total_flops_so_far": 1.975074161688576e+16,
      "budget_used_percent": 19.75074161688576
    },
    {
      "type": "training",
      "description": "Training step 832",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:39",
      "total_flops_so_far": 1.977448048902144e+16,
      "budget_used_percent": 19.77448048902144
    },
    {
      "type": "training",
      "description": "Training step 833",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:40",
      "total_flops_so_far": 1.979821936115712e+16,
      "budget_used_percent": 19.79821936115712
    },
    {
      "type": "training",
      "description": "Training step 834",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:41",
      "total_flops_so_far": 1.98219582332928e+16,
      "budget_used_percent": 19.8219582332928
    },
    {
      "type": "training",
      "description": "Training step 835",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:43",
      "total_flops_so_far": 1.984569710542848e+16,
      "budget_used_percent": 19.84569710542848
    },
    {
      "type": "training",
      "description": "Training step 836",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:44",
      "total_flops_so_far": 1.986943597756416e+16,
      "budget_used_percent": 19.86943597756416
    },
    {
      "type": "training",
      "description": "Training step 837",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:45",
      "total_flops_so_far": 1.989317484969984e+16,
      "budget_used_percent": 19.89317484969984
    },
    {
      "type": "training",
      "description": "Training step 838",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:46",
      "total_flops_so_far": 1.991691372183552e+16,
      "budget_used_percent": 19.91691372183552
    },
    {
      "type": "training",
      "description": "Training step 839",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:48",
      "total_flops_so_far": 1.99406525939712e+16,
      "budget_used_percent": 19.9406525939712
    },
    {
      "type": "training",
      "description": "Training step 840",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:49",
      "total_flops_so_far": 1.996439146610688e+16,
      "budget_used_percent": 19.96439146610688
    },
    {
      "type": "training",
      "description": "Training step 841",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:50",
      "total_flops_so_far": 1.998813033824256e+16,
      "budget_used_percent": 19.98813033824256
    },
    {
      "type": "training",
      "description": "Training step 842",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:52",
      "total_flops_so_far": 2.001186921037824e+16,
      "budget_used_percent": 20.01186921037824
    },
    {
      "type": "training",
      "description": "Training step 843",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:53",
      "total_flops_so_far": 2.003560808251392e+16,
      "budget_used_percent": 20.03560808251392
    },
    {
      "type": "training",
      "description": "Training step 844",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:54",
      "total_flops_so_far": 2.00593469546496e+16,
      "budget_used_percent": 20.0593469546496
    },
    {
      "type": "training",
      "description": "Training step 845",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:56",
      "total_flops_so_far": 2.008308582678528e+16,
      "budget_used_percent": 20.08308582678528
    },
    {
      "type": "training",
      "description": "Training step 846",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:57",
      "total_flops_so_far": 2.010682469892096e+16,
      "budget_used_percent": 20.10682469892096
    },
    {
      "type": "training",
      "description": "Training step 847",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:58",
      "total_flops_so_far": 2.013056357105664e+16,
      "budget_used_percent": 20.13056357105664
    },
    {
      "type": "training",
      "description": "Training step 848",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:49:59",
      "total_flops_so_far": 2.015430244319232e+16,
      "budget_used_percent": 20.15430244319232
    },
    {
      "type": "training",
      "description": "Training step 849",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:01",
      "total_flops_so_far": 2.0178041315328e+16,
      "budget_used_percent": 20.178041315328
    },
    {
      "type": "training",
      "description": "Training step 850",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:02",
      "total_flops_so_far": 2.020178018746368e+16,
      "budget_used_percent": 20.20178018746368
    },
    {
      "type": "training",
      "description": "Training step 851",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:03",
      "total_flops_so_far": 2.022551905959936e+16,
      "budget_used_percent": 20.22551905959936
    },
    {
      "type": "training",
      "description": "Training step 852",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:05",
      "total_flops_so_far": 2.024925793173504e+16,
      "budget_used_percent": 20.24925793173504
    },
    {
      "type": "training",
      "description": "Training step 853",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:06",
      "total_flops_so_far": 2.027299680387072e+16,
      "budget_used_percent": 20.27299680387072
    },
    {
      "type": "training",
      "description": "Training step 854",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:07",
      "total_flops_so_far": 2.02967356760064e+16,
      "budget_used_percent": 20.2967356760064
    },
    {
      "type": "training",
      "description": "Training step 855",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:10",
      "total_flops_so_far": 2.032047454814208e+16,
      "budget_used_percent": 20.32047454814208
    },
    {
      "type": "training",
      "description": "Training step 856",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:11",
      "total_flops_so_far": 2.034421342027776e+16,
      "budget_used_percent": 20.34421342027776
    },
    {
      "type": "training",
      "description": "Training step 857",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:12",
      "total_flops_so_far": 2.036795229241344e+16,
      "budget_used_percent": 20.36795229241344
    },
    {
      "type": "training",
      "description": "Training step 858",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:13",
      "total_flops_so_far": 2.039169116454912e+16,
      "budget_used_percent": 20.39169116454912
    },
    {
      "type": "training",
      "description": "Training step 859",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:15",
      "total_flops_so_far": 2.04154300366848e+16,
      "budget_used_percent": 20.4154300366848
    },
    {
      "type": "training",
      "description": "Training step 860",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:16",
      "total_flops_so_far": 2.043916890882048e+16,
      "budget_used_percent": 20.43916890882048
    },
    {
      "type": "training",
      "description": "Training step 861",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:17",
      "total_flops_so_far": 2.046290778095616e+16,
      "budget_used_percent": 20.46290778095616
    },
    {
      "type": "training",
      "description": "Training step 862",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:19",
      "total_flops_so_far": 2.048664665309184e+16,
      "budget_used_percent": 20.48664665309184
    },
    {
      "type": "training",
      "description": "Training step 863",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:20",
      "total_flops_so_far": 2.051038552522752e+16,
      "budget_used_percent": 20.51038552522752
    },
    {
      "type": "training",
      "description": "Training step 864",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:21",
      "total_flops_so_far": 2.05341243973632e+16,
      "budget_used_percent": 20.5341243973632
    },
    {
      "type": "training",
      "description": "Training step 865",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:23",
      "total_flops_so_far": 2.055786326949888e+16,
      "budget_used_percent": 20.55786326949888
    },
    {
      "type": "training",
      "description": "Training step 866",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:24",
      "total_flops_so_far": 2.058160214163456e+16,
      "budget_used_percent": 20.581602141634562
    },
    {
      "type": "training",
      "description": "Training step 867",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:25",
      "total_flops_so_far": 2.060534101377024e+16,
      "budget_used_percent": 20.60534101377024
    },
    {
      "type": "training",
      "description": "Training step 868",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:26",
      "total_flops_so_far": 2.062907988590592e+16,
      "budget_used_percent": 20.62907988590592
    },
    {
      "type": "training",
      "description": "Training step 869",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:28",
      "total_flops_so_far": 2.06528187580416e+16,
      "budget_used_percent": 20.6528187580416
    },
    {
      "type": "training",
      "description": "Training step 870",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:29",
      "total_flops_so_far": 2.067655763017728e+16,
      "budget_used_percent": 20.67655763017728
    },
    {
      "type": "training",
      "description": "Training step 871",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:30",
      "total_flops_so_far": 2.070029650231296e+16,
      "budget_used_percent": 20.70029650231296
    },
    {
      "type": "training",
      "description": "Training step 872",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:32",
      "total_flops_so_far": 2.072403537444864e+16,
      "budget_used_percent": 20.72403537444864
    },
    {
      "type": "training",
      "description": "Training step 873",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:33",
      "total_flops_so_far": 2.074777424658432e+16,
      "budget_used_percent": 20.74777424658432
    },
    {
      "type": "training",
      "description": "Training step 874",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:34",
      "total_flops_so_far": 2.077151311872e+16,
      "budget_used_percent": 20.77151311872
    },
    {
      "type": "training",
      "description": "Training step 875",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:36",
      "total_flops_so_far": 2.079525199085568e+16,
      "budget_used_percent": 20.79525199085568
    },
    {
      "type": "training",
      "description": "Training step 876",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:37",
      "total_flops_so_far": 2.081899086299136e+16,
      "budget_used_percent": 20.81899086299136
    },
    {
      "type": "training",
      "description": "Training step 877",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:38",
      "total_flops_so_far": 2.084272973512704e+16,
      "budget_used_percent": 20.84272973512704
    },
    {
      "type": "training",
      "description": "Training step 878",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:39",
      "total_flops_so_far": 2.086646860726272e+16,
      "budget_used_percent": 20.86646860726272
    },
    {
      "type": "training",
      "description": "Training step 879",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:41",
      "total_flops_so_far": 2.08902074793984e+16,
      "budget_used_percent": 20.890207479398402
    },
    {
      "type": "training",
      "description": "Training step 880",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:42",
      "total_flops_so_far": 2.091394635153408e+16,
      "budget_used_percent": 20.91394635153408
    },
    {
      "type": "training",
      "description": "Training step 881",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:43",
      "total_flops_so_far": 2.093768522366976e+16,
      "budget_used_percent": 20.93768522366976
    },
    {
      "type": "training",
      "description": "Training step 882",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:45",
      "total_flops_so_far": 2.096142409580544e+16,
      "budget_used_percent": 20.96142409580544
    },
    {
      "type": "training",
      "description": "Training step 883",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:46",
      "total_flops_so_far": 2.098516296794112e+16,
      "budget_used_percent": 20.985162967941122
    },
    {
      "type": "training",
      "description": "Training step 884",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:47",
      "total_flops_so_far": 2.10089018400768e+16,
      "budget_used_percent": 21.0089018400768
    },
    {
      "type": "training",
      "description": "Training step 885",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:49",
      "total_flops_so_far": 2.103264071221248e+16,
      "budget_used_percent": 21.03264071221248
    },
    {
      "type": "training",
      "description": "Training step 886",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:50",
      "total_flops_so_far": 2.105637958434816e+16,
      "budget_used_percent": 21.056379584348157
    },
    {
      "type": "training",
      "description": "Training step 887",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:51",
      "total_flops_so_far": 2.108011845648384e+16,
      "budget_used_percent": 21.08011845648384
    },
    {
      "type": "training",
      "description": "Training step 888",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:52",
      "total_flops_so_far": 2.110385732861952e+16,
      "budget_used_percent": 21.103857328619522
    },
    {
      "type": "training",
      "description": "Training step 889",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:54",
      "total_flops_so_far": 2.11275962007552e+16,
      "budget_used_percent": 21.1275962007552
    },
    {
      "type": "training",
      "description": "Training step 890",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:55",
      "total_flops_so_far": 2.115133507289088e+16,
      "budget_used_percent": 21.15133507289088
    },
    {
      "type": "training",
      "description": "Training step 891",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:56",
      "total_flops_so_far": 2.117507394502656e+16,
      "budget_used_percent": 21.17507394502656
    },
    {
      "type": "training",
      "description": "Training step 892",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:58",
      "total_flops_so_far": 2.119881281716224e+16,
      "budget_used_percent": 21.198812817162242
    },
    {
      "type": "training",
      "description": "Training step 893",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:50:59",
      "total_flops_so_far": 2.122255168929792e+16,
      "budget_used_percent": 21.22255168929792
    },
    {
      "type": "training",
      "description": "Training step 894",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:00",
      "total_flops_so_far": 2.12462905614336e+16,
      "budget_used_percent": 21.2462905614336
    },
    {
      "type": "training",
      "description": "Training step 895",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:02",
      "total_flops_so_far": 2.127002943356928e+16,
      "budget_used_percent": 21.270029433569277
    },
    {
      "type": "training",
      "description": "Training step 896",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:03",
      "total_flops_so_far": 2.129376830570496e+16,
      "budget_used_percent": 21.293768305704962
    },
    {
      "type": "training",
      "description": "Training step 897",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:04",
      "total_flops_so_far": 2.131750717784064e+16,
      "budget_used_percent": 21.31750717784064
    },
    {
      "type": "training",
      "description": "Training step 898",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:05",
      "total_flops_so_far": 2.134124604997632e+16,
      "budget_used_percent": 21.34124604997632
    },
    {
      "type": "training",
      "description": "Training step 899",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:07",
      "total_flops_so_far": 2.1364984922112e+16,
      "budget_used_percent": 21.364984922111997
    },
    {
      "type": "training",
      "description": "Training step 900",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:08",
      "total_flops_so_far": 2.138872379424768e+16,
      "budget_used_percent": 21.38872379424768
    },
    {
      "type": "training",
      "description": "Training step 901",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:09",
      "total_flops_so_far": 2.141246266638336e+16,
      "budget_used_percent": 21.412462666383362
    },
    {
      "type": "training",
      "description": "Training step 902",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:11",
      "total_flops_so_far": 2.143620153851904e+16,
      "budget_used_percent": 21.43620153851904
    },
    {
      "type": "training",
      "description": "Training step 903",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:12",
      "total_flops_so_far": 2.145994041065472e+16,
      "budget_used_percent": 21.45994041065472
    },
    {
      "type": "training",
      "description": "Training step 904",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:13",
      "total_flops_so_far": 2.14836792827904e+16,
      "budget_used_percent": 21.483679282790398
    },
    {
      "type": "training",
      "description": "Training step 905",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:15",
      "total_flops_so_far": 2.150741815492608e+16,
      "budget_used_percent": 21.507418154926082
    },
    {
      "type": "training",
      "description": "Training step 906",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:16",
      "total_flops_so_far": 2.153115702706176e+16,
      "budget_used_percent": 21.53115702706176
    },
    {
      "type": "training",
      "description": "Training step 907",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:17",
      "total_flops_so_far": 2.155489589919744e+16,
      "budget_used_percent": 21.55489589919744
    },
    {
      "type": "training",
      "description": "Training step 908",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:18",
      "total_flops_so_far": 2.157863477133312e+16,
      "budget_used_percent": 21.578634771333117
    },
    {
      "type": "training",
      "description": "Training step 909",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:20",
      "total_flops_so_far": 2.16023736434688e+16,
      "budget_used_percent": 21.602373643468802
    },
    {
      "type": "training",
      "description": "Training step 910",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:21",
      "total_flops_so_far": 2.162611251560448e+16,
      "budget_used_percent": 21.62611251560448
    },
    {
      "type": "training",
      "description": "Training step 911",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:22",
      "total_flops_so_far": 2.164985138774016e+16,
      "budget_used_percent": 21.64985138774016
    },
    {
      "type": "training",
      "description": "Training step 912",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:24",
      "total_flops_so_far": 2.167359025987584e+16,
      "budget_used_percent": 21.67359025987584
    },
    {
      "type": "training",
      "description": "Training step 913",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:25",
      "total_flops_so_far": 2.169732913201152e+16,
      "budget_used_percent": 21.697329132011518
    },
    {
      "type": "training",
      "description": "Training step 914",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:26",
      "total_flops_so_far": 2.17210680041472e+16,
      "budget_used_percent": 21.721068004147202
    },
    {
      "type": "training",
      "description": "Training step 915",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:28",
      "total_flops_so_far": 2.174480687628288e+16,
      "budget_used_percent": 21.74480687628288
    },
    {
      "type": "training",
      "description": "Training step 916",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:29",
      "total_flops_so_far": 2.176854574841856e+16,
      "budget_used_percent": 21.76854574841856
    },
    {
      "type": "training",
      "description": "Training step 917",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:30",
      "total_flops_so_far": 2.179228462055424e+16,
      "budget_used_percent": 21.792284620554238
    },
    {
      "type": "training",
      "description": "Training step 918",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:31",
      "total_flops_so_far": 2.181602349268992e+16,
      "budget_used_percent": 21.816023492689922
    },
    {
      "type": "training",
      "description": "Training step 919",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:33",
      "total_flops_so_far": 2.18397623648256e+16,
      "budget_used_percent": 21.8397623648256
    },
    {
      "type": "training",
      "description": "Training step 920",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:34",
      "total_flops_so_far": 2.186350123696128e+16,
      "budget_used_percent": 21.86350123696128
    },
    {
      "type": "training",
      "description": "Training step 921",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:35",
      "total_flops_so_far": 2.188724010909696e+16,
      "budget_used_percent": 21.887240109096957
    },
    {
      "type": "training",
      "description": "Training step 922",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:37",
      "total_flops_so_far": 2.191097898123264e+16,
      "budget_used_percent": 21.91097898123264
    },
    {
      "type": "training",
      "description": "Training step 923",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:38",
      "total_flops_so_far": 2.193471785336832e+16,
      "budget_used_percent": 21.934717853368323
    },
    {
      "type": "training",
      "description": "Training step 924",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:38",
      "total_flops_so_far": 2.1958456725504e+16,
      "budget_used_percent": 21.958456725504
    },
    {
      "type": "training",
      "description": "Training step 925",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:40",
      "total_flops_so_far": 2.198219559763968e+16,
      "budget_used_percent": 21.98219559763968
    },
    {
      "type": "training",
      "description": "Training step 926",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:41",
      "total_flops_so_far": 2.200593446977536e+16,
      "budget_used_percent": 22.005934469775358
    },
    {
      "type": "training",
      "description": "Training step 927",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:42",
      "total_flops_so_far": 2.202967334191104e+16,
      "budget_used_percent": 22.029673341911042
    },
    {
      "type": "training",
      "description": "Training step 928",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:44",
      "total_flops_so_far": 2.205341221404672e+16,
      "budget_used_percent": 22.05341221404672
    },
    {
      "type": "training",
      "description": "Training step 929",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:45",
      "total_flops_so_far": 2.20771510861824e+16,
      "budget_used_percent": 22.0771510861824
    },
    {
      "type": "training",
      "description": "Training step 930",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:46",
      "total_flops_so_far": 2.210088995831808e+16,
      "budget_used_percent": 22.100889958318078
    },
    {
      "type": "training",
      "description": "Training step 931",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:47",
      "total_flops_so_far": 2.212462883045376e+16,
      "budget_used_percent": 22.12462883045376
    },
    {
      "type": "training",
      "description": "Training step 932",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:49",
      "total_flops_so_far": 2.214836770258944e+16,
      "budget_used_percent": 22.14836770258944
    },
    {
      "type": "training",
      "description": "Training step 933",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:50",
      "total_flops_so_far": 2.217210657472512e+16,
      "budget_used_percent": 22.17210657472512
    },
    {
      "type": "training",
      "description": "Training step 934",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:51",
      "total_flops_so_far": 2.21958454468608e+16,
      "budget_used_percent": 22.1958454468608
    },
    {
      "type": "training",
      "description": "Training step 935",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:53",
      "total_flops_so_far": 2.221958431899648e+16,
      "budget_used_percent": 22.21958431899648
    },
    {
      "type": "training",
      "description": "Training step 936",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:54",
      "total_flops_so_far": 2.224332319113216e+16,
      "budget_used_percent": 22.243323191132163
    },
    {
      "type": "training",
      "description": "Training step 937",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:55",
      "total_flops_so_far": 2.226706206326784e+16,
      "budget_used_percent": 22.26706206326784
    },
    {
      "type": "training",
      "description": "Training step 938",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:57",
      "total_flops_so_far": 2.229080093540352e+16,
      "budget_used_percent": 22.29080093540352
    },
    {
      "type": "training",
      "description": "Training step 939",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:58",
      "total_flops_so_far": 2.23145398075392e+16,
      "budget_used_percent": 22.314539807539198
    },
    {
      "type": "training",
      "description": "Training step 940",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:51:59",
      "total_flops_so_far": 2.233827867967488e+16,
      "budget_used_percent": 22.33827867967488
    },
    {
      "type": "training",
      "description": "Training step 941",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:01",
      "total_flops_so_far": 2.236201755181056e+16,
      "budget_used_percent": 22.36201755181056
    },
    {
      "type": "training",
      "description": "Training step 942",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:02",
      "total_flops_so_far": 2.238575642394624e+16,
      "budget_used_percent": 22.38575642394624
    },
    {
      "type": "training",
      "description": "Training step 943",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:03",
      "total_flops_so_far": 2.240949529608192e+16,
      "budget_used_percent": 22.409495296081918
    },
    {
      "type": "training",
      "description": "Training step 944",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:04",
      "total_flops_so_far": 2.24332341682176e+16,
      "budget_used_percent": 22.4332341682176
    },
    {
      "type": "training",
      "description": "Training step 945",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:06",
      "total_flops_so_far": 2.245697304035328e+16,
      "budget_used_percent": 22.456973040353283
    },
    {
      "type": "training",
      "description": "Training step 946",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:07",
      "total_flops_so_far": 2.248071191248896e+16,
      "budget_used_percent": 22.48071191248896
    },
    {
      "type": "training",
      "description": "Training step 947",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:08",
      "total_flops_so_far": 2.250445078462464e+16,
      "budget_used_percent": 22.50445078462464
    },
    {
      "type": "training",
      "description": "Training step 948",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:10",
      "total_flops_so_far": 2.252818965676032e+16,
      "budget_used_percent": 22.52818965676032
    },
    {
      "type": "training",
      "description": "Training step 949",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:11",
      "total_flops_so_far": 2.2551928528896e+16,
      "budget_used_percent": 22.551928528896
    },
    {
      "type": "training",
      "description": "Training step 950",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:12",
      "total_flops_so_far": 2.257566740103168e+16,
      "budget_used_percent": 22.57566740103168
    },
    {
      "type": "training",
      "description": "Training step 951",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:14",
      "total_flops_so_far": 2.259940627316736e+16,
      "budget_used_percent": 22.59940627316736
    },
    {
      "type": "training",
      "description": "Training step 952",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:15",
      "total_flops_so_far": 2.262314514530304e+16,
      "budget_used_percent": 22.623145145303038
    },
    {
      "type": "training",
      "description": "Training step 953",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:16",
      "total_flops_so_far": 2.264688401743872e+16,
      "budget_used_percent": 22.64688401743872
    },
    {
      "type": "training",
      "description": "Training step 954",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:18",
      "total_flops_so_far": 2.26706228895744e+16,
      "budget_used_percent": 22.6706228895744
    },
    {
      "type": "training",
      "description": "Training step 955",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:19",
      "total_flops_so_far": 2.269436176171008e+16,
      "budget_used_percent": 22.69436176171008
    },
    {
      "type": "training",
      "description": "Training step 956",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:20",
      "total_flops_so_far": 2.271810063384576e+16,
      "budget_used_percent": 22.718100633845758
    },
    {
      "type": "training",
      "description": "Training step 957",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:21",
      "total_flops_so_far": 2.274183950598144e+16,
      "budget_used_percent": 22.74183950598144
    },
    {
      "type": "training",
      "description": "Training step 958",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:23",
      "total_flops_so_far": 2.276557837811712e+16,
      "budget_used_percent": 22.76557837811712
    },
    {
      "type": "training",
      "description": "Training step 959",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:24",
      "total_flops_so_far": 2.27893172502528e+16,
      "budget_used_percent": 22.7893172502528
    },
    {
      "type": "training",
      "description": "Training step 960",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:25",
      "total_flops_so_far": 2.281305612238848e+16,
      "budget_used_percent": 22.81305612238848
    },
    {
      "type": "training",
      "description": "Training step 961",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:27",
      "total_flops_so_far": 2.283679499452416e+16,
      "budget_used_percent": 22.83679499452416
    },
    {
      "type": "training",
      "description": "Training step 962",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:28",
      "total_flops_so_far": 2.286053386665984e+16,
      "budget_used_percent": 22.86053386665984
    },
    {
      "type": "training",
      "description": "Training step 963",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:29",
      "total_flops_so_far": 2.288427273879552e+16,
      "budget_used_percent": 22.88427273879552
    },
    {
      "type": "training",
      "description": "Training step 964",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:31",
      "total_flops_so_far": 2.29080116109312e+16,
      "budget_used_percent": 22.9080116109312
    },
    {
      "type": "training",
      "description": "Training step 965",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:32",
      "total_flops_so_far": 2.293175048306688e+16,
      "budget_used_percent": 22.931750483066878
    },
    {
      "type": "training",
      "description": "Training step 966",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:33",
      "total_flops_so_far": 2.295548935520256e+16,
      "budget_used_percent": 22.95548935520256
    },
    {
      "type": "training",
      "description": "Training step 967",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:35",
      "total_flops_so_far": 2.297922822733824e+16,
      "budget_used_percent": 22.97922822733824
    },
    {
      "type": "training",
      "description": "Training step 968",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:36",
      "total_flops_so_far": 2.300296709947392e+16,
      "budget_used_percent": 23.00296709947392
    },
    {
      "type": "training",
      "description": "Training step 969",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:37",
      "total_flops_so_far": 2.30267059716096e+16,
      "budget_used_percent": 23.0267059716096
    },
    {
      "type": "training",
      "description": "Training step 970",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:38",
      "total_flops_so_far": 2.305044484374528e+16,
      "budget_used_percent": 23.05044484374528
    },
    {
      "type": "training",
      "description": "Training step 971",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:40",
      "total_flops_so_far": 2.307418371588096e+16,
      "budget_used_percent": 23.07418371588096
    },
    {
      "type": "training",
      "description": "Training step 972",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:41",
      "total_flops_so_far": 2.309792258801664e+16,
      "budget_used_percent": 23.09792258801664
    },
    {
      "type": "training",
      "description": "Training step 973",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:42",
      "total_flops_so_far": 2.312166146015232e+16,
      "budget_used_percent": 23.12166146015232
    },
    {
      "type": "training",
      "description": "Training step 974",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:44",
      "total_flops_so_far": 2.3145400332288e+16,
      "budget_used_percent": 23.145400332288
    },
    {
      "type": "training",
      "description": "Training step 975",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:45",
      "total_flops_so_far": 2.316913920442368e+16,
      "budget_used_percent": 23.16913920442368
    },
    {
      "type": "training",
      "description": "Training step 976",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:46",
      "total_flops_so_far": 2.319287807655936e+16,
      "budget_used_percent": 23.19287807655936
    },
    {
      "type": "training",
      "description": "Training step 977",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:48",
      "total_flops_so_far": 2.321661694869504e+16,
      "budget_used_percent": 23.21661694869504
    },
    {
      "type": "training",
      "description": "Training step 978",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:49",
      "total_flops_so_far": 2.324035582083072e+16,
      "budget_used_percent": 23.240355820830718
    },
    {
      "type": "training",
      "description": "Training step 979",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:50",
      "total_flops_so_far": 2.32640946929664e+16,
      "budget_used_percent": 23.2640946929664
    },
    {
      "type": "training",
      "description": "Training step 980",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:52",
      "total_flops_so_far": 2.328783356510208e+16,
      "budget_used_percent": 23.28783356510208
    },
    {
      "type": "training",
      "description": "Training step 981",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:53",
      "total_flops_so_far": 2.331157243723776e+16,
      "budget_used_percent": 23.31157243723776
    },
    {
      "type": "training",
      "description": "Training step 982",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:54",
      "total_flops_so_far": 2.333531130937344e+16,
      "budget_used_percent": 23.33531130937344
    },
    {
      "type": "training",
      "description": "Training step 983",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:55",
      "total_flops_so_far": 2.335905018150912e+16,
      "budget_used_percent": 23.35905018150912
    },
    {
      "type": "training",
      "description": "Training step 984",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:57",
      "total_flops_so_far": 2.33827890536448e+16,
      "budget_used_percent": 23.3827890536448
    },
    {
      "type": "training",
      "description": "Training step 985",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:58",
      "total_flops_so_far": 2.340652792578048e+16,
      "budget_used_percent": 23.40652792578048
    },
    {
      "type": "training",
      "description": "Training step 986",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:52:59",
      "total_flops_so_far": 2.343026679791616e+16,
      "budget_used_percent": 23.43026679791616
    },
    {
      "type": "training",
      "description": "Training step 987",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:53:01",
      "total_flops_so_far": 2.345400567005184e+16,
      "budget_used_percent": 23.45400567005184
    },
    {
      "type": "training",
      "description": "Training step 988",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:53:02",
      "total_flops_so_far": 2.347774454218752e+16,
      "budget_used_percent": 23.47774454218752
    },
    {
      "type": "training",
      "description": "Training step 989",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:53:03",
      "total_flops_so_far": 2.35014834143232e+16,
      "budget_used_percent": 23.5014834143232
    },
    {
      "type": "training",
      "description": "Training step 990",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:53:05",
      "total_flops_so_far": 2.352522228645888e+16,
      "budget_used_percent": 23.52522228645888
    },
    {
      "type": "training",
      "description": "Training step 991",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:53:06",
      "total_flops_so_far": 2.354896115859456e+16,
      "budget_used_percent": 23.54896115859456
    },
    {
      "type": "training",
      "description": "Training step 992",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:53:07",
      "total_flops_so_far": 2.357270003073024e+16,
      "budget_used_percent": 23.57270003073024
    },
    {
      "type": "training",
      "description": "Training step 993",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:53:08",
      "total_flops_so_far": 2.359643890286592e+16,
      "budget_used_percent": 23.59643890286592
    },
    {
      "type": "training",
      "description": "Training step 994",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:53:10",
      "total_flops_so_far": 2.36201777750016e+16,
      "budget_used_percent": 23.6201777750016
    },
    {
      "type": "training",
      "description": "Training step 995",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:53:11",
      "total_flops_so_far": 2.364391664713728e+16,
      "budget_used_percent": 23.64391664713728
    },
    {
      "type": "training",
      "description": "Training step 996",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:53:12",
      "total_flops_so_far": 2.366765551927296e+16,
      "budget_used_percent": 23.66765551927296
    },
    {
      "type": "training",
      "description": "Training step 997",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:53:14",
      "total_flops_so_far": 2.369139439140864e+16,
      "budget_used_percent": 23.69139439140864
    },
    {
      "type": "training",
      "description": "Training step 998",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:53:15",
      "total_flops_so_far": 2.371513326354432e+16,
      "budget_used_percent": 23.71513326354432
    },
    {
      "type": "training",
      "description": "Training step 999",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:53:16",
      "total_flops_so_far": 2.373887213568e+16,
      "budget_used_percent": 23.73887213568
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 709803614656.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:53:24",
      "total_flops_so_far": 2.3739581939294656e+16,
      "budget_used_percent": 23.739581939294656
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 713504058416.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:53:29",
      "total_flops_so_far": 2.374029544335307e+16,
      "budget_used_percent": 23.74029544335307
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711653476344.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:53:34",
      "total_flops_so_far": 2.3741007096829416e+16,
      "budget_used_percent": 23.741007096829417
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 709803614656.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:53:39",
      "total_flops_so_far": 2.374171690044407e+16,
      "budget_used_percent": 23.74171690044407
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712578677332.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:53:44",
      "total_flops_so_far": 2.3742429479121404e+16,
      "budget_used_percent": 23.742429479121405
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 5",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 709803614656.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:53:49",
      "total_flops_so_far": 2.374313928273606e+16,
      "budget_used_percent": 23.74313928273606
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 6",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711653476344.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:53:54",
      "total_flops_so_far": 2.3743850936212404e+16,
      "budget_used_percent": 23.743850936212404
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 7",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711653476344.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:53:59",
      "total_flops_so_far": 2.3744562589688748e+16,
      "budget_used_percent": 23.744562589688748
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 8",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711653476344.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:04",
      "total_flops_so_far": 2.3745274243165092e+16,
      "budget_used_percent": 23.745274243165092
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 9",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711653476344.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:09",
      "total_flops_so_far": 2.3745985896641436e+16,
      "budget_used_percent": 23.745985896641436
    },
    {
      "type": "training",
      "description": "Training step 1000",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:09",
      "total_flops_so_far": 2.3769724768777116e+16,
      "budget_used_percent": 23.769724768777117
    },
    {
      "type": "training",
      "description": "Training step 1001",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:10",
      "total_flops_so_far": 2.3793463640912796e+16,
      "budget_used_percent": 23.793463640912798
    },
    {
      "type": "training",
      "description": "Training step 1002",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:12",
      "total_flops_so_far": 2.3817202513048476e+16,
      "budget_used_percent": 23.81720251304848
    },
    {
      "type": "training",
      "description": "Training step 1003",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:13",
      "total_flops_so_far": 2.3840941385184156e+16,
      "budget_used_percent": 23.840941385184156
    },
    {
      "type": "training",
      "description": "Training step 1004",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:14",
      "total_flops_so_far": 2.3864680257319836e+16,
      "budget_used_percent": 23.864680257319836
    },
    {
      "type": "training",
      "description": "Training step 1005",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:16",
      "total_flops_so_far": 2.3888419129455516e+16,
      "budget_used_percent": 23.888419129455514
    },
    {
      "type": "training",
      "description": "Training step 1006",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:17",
      "total_flops_so_far": 2.3912158001591196e+16,
      "budget_used_percent": 23.912158001591198
    },
    {
      "type": "training",
      "description": "Training step 1007",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:18",
      "total_flops_so_far": 2.3935896873726876e+16,
      "budget_used_percent": 23.935896873726875
    },
    {
      "type": "training",
      "description": "Training step 1008",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:20",
      "total_flops_so_far": 2.3959635745862556e+16,
      "budget_used_percent": 23.959635745862556
    },
    {
      "type": "training",
      "description": "Training step 1009",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:21",
      "total_flops_so_far": 2.3983374617998236e+16,
      "budget_used_percent": 23.983374617998233
    },
    {
      "type": "training",
      "description": "Training step 1010",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:22",
      "total_flops_so_far": 2.4007113490133916e+16,
      "budget_used_percent": 24.007113490133918
    },
    {
      "type": "training",
      "description": "Training step 1011",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:23",
      "total_flops_so_far": 2.4030852362269596e+16,
      "budget_used_percent": 24.030852362269595
    },
    {
      "type": "training",
      "description": "Training step 1012",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:25",
      "total_flops_so_far": 2.4054591234405276e+16,
      "budget_used_percent": 24.054591234405276
    },
    {
      "type": "training",
      "description": "Training step 1013",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:26",
      "total_flops_so_far": 2.4078330106540956e+16,
      "budget_used_percent": 24.078330106540957
    },
    {
      "type": "training",
      "description": "Training step 1014",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:27",
      "total_flops_so_far": 2.4102068978676636e+16,
      "budget_used_percent": 24.102068978676634
    },
    {
      "type": "training",
      "description": "Training step 1015",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:29",
      "total_flops_so_far": 2.4125807850812316e+16,
      "budget_used_percent": 24.12580785081232
    },
    {
      "type": "training",
      "description": "Training step 1016",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:30",
      "total_flops_so_far": 2.4149546722947996e+16,
      "budget_used_percent": 24.149546722947996
    },
    {
      "type": "training",
      "description": "Training step 1017",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:31",
      "total_flops_so_far": 2.4173285595083676e+16,
      "budget_used_percent": 24.173285595083676
    },
    {
      "type": "training",
      "description": "Training step 1018",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:33",
      "total_flops_so_far": 2.4197024467219356e+16,
      "budget_used_percent": 24.197024467219354
    },
    {
      "type": "training",
      "description": "Training step 1019",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:34",
      "total_flops_so_far": 2.4220763339355036e+16,
      "budget_used_percent": 24.220763339355038
    },
    {
      "type": "training",
      "description": "Training step 1020",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:35",
      "total_flops_so_far": 2.4244502211490716e+16,
      "budget_used_percent": 24.244502211490715
    },
    {
      "type": "training",
      "description": "Training step 1021",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:36",
      "total_flops_so_far": 2.4268241083626396e+16,
      "budget_used_percent": 24.268241083626396
    },
    {
      "type": "training",
      "description": "Training step 1022",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:38",
      "total_flops_so_far": 2.4291979955762076e+16,
      "budget_used_percent": 24.291979955762073
    },
    {
      "type": "training",
      "description": "Training step 1023",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:39",
      "total_flops_so_far": 2.4315718827897756e+16,
      "budget_used_percent": 24.315718827897754
    },
    {
      "type": "training",
      "description": "Training step 1024",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:40",
      "total_flops_so_far": 2.4339457700033436e+16,
      "budget_used_percent": 24.33945770003344
    },
    {
      "type": "training",
      "description": "Training step 1025",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:42",
      "total_flops_so_far": 2.4363196572169116e+16,
      "budget_used_percent": 24.363196572169116
    },
    {
      "type": "training",
      "description": "Training step 1026",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:43",
      "total_flops_so_far": 2.4386935444304796e+16,
      "budget_used_percent": 24.386935444304797
    },
    {
      "type": "training",
      "description": "Training step 1027",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:44",
      "total_flops_so_far": 2.4410674316440476e+16,
      "budget_used_percent": 24.410674316440474
    },
    {
      "type": "training",
      "description": "Training step 1028",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:46",
      "total_flops_so_far": 2.4434413188576156e+16,
      "budget_used_percent": 24.43441318857616
    },
    {
      "type": "training",
      "description": "Training step 1029",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:47",
      "total_flops_so_far": 2.4458152060711836e+16,
      "budget_used_percent": 24.458152060711836
    },
    {
      "type": "training",
      "description": "Training step 1030",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:48",
      "total_flops_so_far": 2.4481890932847516e+16,
      "budget_used_percent": 24.481890932847516
    },
    {
      "type": "training",
      "description": "Training step 1031",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:50",
      "total_flops_so_far": 2.4505629804983196e+16,
      "budget_used_percent": 24.505629804983194
    },
    {
      "type": "training",
      "description": "Training step 1032",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:51",
      "total_flops_so_far": 2.4529368677118876e+16,
      "budget_used_percent": 24.529368677118875
    },
    {
      "type": "training",
      "description": "Training step 1033",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:52",
      "total_flops_so_far": 2.4553107549254556e+16,
      "budget_used_percent": 24.553107549254555
    },
    {
      "type": "training",
      "description": "Training step 1034",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:53",
      "total_flops_so_far": 2.4576846421390236e+16,
      "budget_used_percent": 24.576846421390236
    },
    {
      "type": "training",
      "description": "Training step 1035",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:55",
      "total_flops_so_far": 2.4600585293525916e+16,
      "budget_used_percent": 24.600585293525917
    },
    {
      "type": "training",
      "description": "Training step 1036",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:56",
      "total_flops_so_far": 2.4624324165661596e+16,
      "budget_used_percent": 24.624324165661594
    },
    {
      "type": "training",
      "description": "Training step 1037",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:57",
      "total_flops_so_far": 2.4648063037797276e+16,
      "budget_used_percent": 24.64806303779728
    },
    {
      "type": "training",
      "description": "Training step 1038",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:54:59",
      "total_flops_so_far": 2.4671801909932956e+16,
      "budget_used_percent": 24.671801909932956
    },
    {
      "type": "training",
      "description": "Training step 1039",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:00",
      "total_flops_so_far": 2.4695540782068636e+16,
      "budget_used_percent": 24.695540782068637
    },
    {
      "type": "training",
      "description": "Training step 1040",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:01",
      "total_flops_so_far": 2.4719279654204316e+16,
      "budget_used_percent": 24.719279654204314
    },
    {
      "type": "training",
      "description": "Training step 1041",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:03",
      "total_flops_so_far": 2.4743018526339996e+16,
      "budget_used_percent": 24.743018526339995
    },
    {
      "type": "training",
      "description": "Training step 1042",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:04",
      "total_flops_so_far": 2.4766757398475676e+16,
      "budget_used_percent": 24.766757398475676
    },
    {
      "type": "training",
      "description": "Training step 1043",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:05",
      "total_flops_so_far": 2.4790496270611356e+16,
      "budget_used_percent": 24.790496270611357
    },
    {
      "type": "training",
      "description": "Training step 1044",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:06",
      "total_flops_so_far": 2.4814235142747036e+16,
      "budget_used_percent": 24.814235142747034
    },
    {
      "type": "training",
      "description": "Training step 1045",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:08",
      "total_flops_so_far": 2.4837974014882716e+16,
      "budget_used_percent": 24.837974014882715
    },
    {
      "type": "training",
      "description": "Training step 1046",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:09",
      "total_flops_so_far": 2.4861712887018396e+16,
      "budget_used_percent": 24.8617128870184
    },
    {
      "type": "training",
      "description": "Training step 1047",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:10",
      "total_flops_so_far": 2.4885451759154076e+16,
      "budget_used_percent": 24.885451759154076
    },
    {
      "type": "training",
      "description": "Training step 1048",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:12",
      "total_flops_so_far": 2.4909190631289756e+16,
      "budget_used_percent": 24.909190631289757
    },
    {
      "type": "training",
      "description": "Training step 1049",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:13",
      "total_flops_so_far": 2.4932929503425436e+16,
      "budget_used_percent": 24.932929503425434
    },
    {
      "type": "training",
      "description": "Training step 1050",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:14",
      "total_flops_so_far": 2.4956668375561116e+16,
      "budget_used_percent": 24.956668375561115
    },
    {
      "type": "training",
      "description": "Training step 1051",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:16",
      "total_flops_so_far": 2.4980407247696796e+16,
      "budget_used_percent": 24.980407247696796
    },
    {
      "type": "training",
      "description": "Training step 1052",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:17",
      "total_flops_so_far": 2.5004146119832476e+16,
      "budget_used_percent": 25.004146119832477
    },
    {
      "type": "training",
      "description": "Training step 1053",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:18",
      "total_flops_so_far": 2.5027884991968156e+16,
      "budget_used_percent": 25.027884991968158
    },
    {
      "type": "training",
      "description": "Training step 1054",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:20",
      "total_flops_so_far": 2.5051623864103836e+16,
      "budget_used_percent": 25.051623864103835
    },
    {
      "type": "training",
      "description": "Training step 1055",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:21",
      "total_flops_so_far": 2.5075362736239516e+16,
      "budget_used_percent": 25.075362736239516
    },
    {
      "type": "training",
      "description": "Training step 1056",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:21",
      "total_flops_so_far": 2.5099101608375196e+16,
      "budget_used_percent": 25.099101608375197
    },
    {
      "type": "training",
      "description": "Training step 1057",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:23",
      "total_flops_so_far": 2.5122840480510876e+16,
      "budget_used_percent": 25.122840480510877
    },
    {
      "type": "training",
      "description": "Training step 1058",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:24",
      "total_flops_so_far": 2.5146579352646556e+16,
      "budget_used_percent": 25.146579352646558
    },
    {
      "type": "training",
      "description": "Training step 1059",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:25",
      "total_flops_so_far": 2.5170318224782236e+16,
      "budget_used_percent": 25.170318224782235
    },
    {
      "type": "training",
      "description": "Training step 1060",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:26",
      "total_flops_so_far": 2.5194057096917916e+16,
      "budget_used_percent": 25.194057096917916
    },
    {
      "type": "training",
      "description": "Training step 1061",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:28",
      "total_flops_so_far": 2.5217795969053596e+16,
      "budget_used_percent": 25.217795969053597
    },
    {
      "type": "training",
      "description": "Training step 1062",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:29",
      "total_flops_so_far": 2.5241534841189276e+16,
      "budget_used_percent": 25.241534841189278
    },
    {
      "type": "training",
      "description": "Training step 1063",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:30",
      "total_flops_so_far": 2.5265273713324956e+16,
      "budget_used_percent": 25.265273713324955
    },
    {
      "type": "training",
      "description": "Training step 1064",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:32",
      "total_flops_so_far": 2.5289012585460636e+16,
      "budget_used_percent": 25.289012585460636
    },
    {
      "type": "training",
      "description": "Training step 1065",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:33",
      "total_flops_so_far": 2.5312751457596316e+16,
      "budget_used_percent": 25.312751457596317
    },
    {
      "type": "training",
      "description": "Training step 1066",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:34",
      "total_flops_so_far": 2.5336490329731996e+16,
      "budget_used_percent": 25.336490329731998
    },
    {
      "type": "training",
      "description": "Training step 1067",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:36",
      "total_flops_so_far": 2.5360229201867676e+16,
      "budget_used_percent": 25.360229201867675
    },
    {
      "type": "training",
      "description": "Training step 1068",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:37",
      "total_flops_so_far": 2.5383968074003356e+16,
      "budget_used_percent": 25.383968074003356
    },
    {
      "type": "training",
      "description": "Training step 1069",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:38",
      "total_flops_so_far": 2.5407706946139036e+16,
      "budget_used_percent": 25.407706946139037
    },
    {
      "type": "training",
      "description": "Training step 1070",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:39",
      "total_flops_so_far": 2.5431445818274716e+16,
      "budget_used_percent": 25.431445818274717
    },
    {
      "type": "training",
      "description": "Training step 1071",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:41",
      "total_flops_so_far": 2.5455184690410396e+16,
      "budget_used_percent": 25.455184690410398
    },
    {
      "type": "training",
      "description": "Training step 1072",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:42",
      "total_flops_so_far": 2.5478923562546076e+16,
      "budget_used_percent": 25.478923562546075
    },
    {
      "type": "training",
      "description": "Training step 1073",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:43",
      "total_flops_so_far": 2.5502662434681756e+16,
      "budget_used_percent": 25.502662434681756
    },
    {
      "type": "training",
      "description": "Training step 1074",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:45",
      "total_flops_so_far": 2.5526401306817436e+16,
      "budget_used_percent": 25.526401306817437
    },
    {
      "type": "training",
      "description": "Training step 1075",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:46",
      "total_flops_so_far": 2.5550140178953116e+16,
      "budget_used_percent": 25.550140178953118
    },
    {
      "type": "training",
      "description": "Training step 1076",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:47",
      "total_flops_so_far": 2.5573879051088796e+16,
      "budget_used_percent": 25.573879051088795
    },
    {
      "type": "training",
      "description": "Training step 1077",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:49",
      "total_flops_so_far": 2.5597617923224476e+16,
      "budget_used_percent": 25.597617923224476
    },
    {
      "type": "training",
      "description": "Training step 1078",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:50",
      "total_flops_so_far": 2.5621356795360156e+16,
      "budget_used_percent": 25.621356795360157
    },
    {
      "type": "training",
      "description": "Training step 1079",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:51",
      "total_flops_so_far": 2.5645095667495836e+16,
      "budget_used_percent": 25.645095667495838
    },
    {
      "type": "training",
      "description": "Training step 1080",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:53",
      "total_flops_so_far": 2.5668834539631516e+16,
      "budget_used_percent": 25.66883453963152
    },
    {
      "type": "training",
      "description": "Training step 1081",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:54",
      "total_flops_so_far": 2.5692573411767196e+16,
      "budget_used_percent": 25.692573411767196
    },
    {
      "type": "training",
      "description": "Training step 1082",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:55",
      "total_flops_so_far": 2.5716312283902876e+16,
      "budget_used_percent": 25.716312283902877
    },
    {
      "type": "training",
      "description": "Training step 1083",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:56",
      "total_flops_so_far": 2.5740051156038556e+16,
      "budget_used_percent": 25.740051156038557
    },
    {
      "type": "training",
      "description": "Training step 1084",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:58",
      "total_flops_so_far": 2.5763790028174236e+16,
      "budget_used_percent": 25.76379002817424
    },
    {
      "type": "training",
      "description": "Training step 1085",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:55:59",
      "total_flops_so_far": 2.5787528900309916e+16,
      "budget_used_percent": 25.787528900309916
    },
    {
      "type": "training",
      "description": "Training step 1086",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:00",
      "total_flops_so_far": 2.5811267772445596e+16,
      "budget_used_percent": 25.811267772445596
    },
    {
      "type": "training",
      "description": "Training step 1087",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:02",
      "total_flops_so_far": 2.5835006644581276e+16,
      "budget_used_percent": 25.835006644581277
    },
    {
      "type": "training",
      "description": "Training step 1088",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:03",
      "total_flops_so_far": 2.5858745516716956e+16,
      "budget_used_percent": 25.858745516716958
    },
    {
      "type": "training",
      "description": "Training step 1089",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:04",
      "total_flops_so_far": 2.5882484388852636e+16,
      "budget_used_percent": 25.88248438885263
    },
    {
      "type": "training",
      "description": "Training step 1090",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:06",
      "total_flops_so_far": 2.5906223260988316e+16,
      "budget_used_percent": 25.906223260988316
    },
    {
      "type": "training",
      "description": "Training step 1091",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:07",
      "total_flops_so_far": 2.5929962133123996e+16,
      "budget_used_percent": 25.929962133123997
    },
    {
      "type": "training",
      "description": "Training step 1092",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:08",
      "total_flops_so_far": 2.5953701005259676e+16,
      "budget_used_percent": 25.953701005259678
    },
    {
      "type": "training",
      "description": "Training step 1093",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:10",
      "total_flops_so_far": 2.5977439877395356e+16,
      "budget_used_percent": 25.97743987739536
    },
    {
      "type": "training",
      "description": "Training step 1094",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:11",
      "total_flops_so_far": 2.6001178749531036e+16,
      "budget_used_percent": 26.001178749531036
    },
    {
      "type": "training",
      "description": "Training step 1095",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:12",
      "total_flops_so_far": 2.6024917621666716e+16,
      "budget_used_percent": 26.024917621666717
    },
    {
      "type": "training",
      "description": "Training step 1096",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:13",
      "total_flops_so_far": 2.6048656493802396e+16,
      "budget_used_percent": 26.048656493802397
    },
    {
      "type": "training",
      "description": "Training step 1097",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:15",
      "total_flops_so_far": 2.6072395365938076e+16,
      "budget_used_percent": 26.07239536593808
    },
    {
      "type": "training",
      "description": "Training step 1098",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:16",
      "total_flops_so_far": 2.6096134238073756e+16,
      "budget_used_percent": 26.096134238073752
    },
    {
      "type": "training",
      "description": "Training step 1099",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:17",
      "total_flops_so_far": 2.6119873110209436e+16,
      "budget_used_percent": 26.119873110209436
    },
    {
      "type": "training",
      "description": "Training step 1100",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:19",
      "total_flops_so_far": 2.6143611982345116e+16,
      "budget_used_percent": 26.143611982345117
    },
    {
      "type": "training",
      "description": "Training step 1101",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:20",
      "total_flops_so_far": 2.6167350854480796e+16,
      "budget_used_percent": 26.167350854480798
    },
    {
      "type": "training",
      "description": "Training step 1102",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:21",
      "total_flops_so_far": 2.6191089726616476e+16,
      "budget_used_percent": 26.19108972661647
    },
    {
      "type": "training",
      "description": "Training step 1103",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:23",
      "total_flops_so_far": 2.6214828598752156e+16,
      "budget_used_percent": 26.214828598752156
    },
    {
      "type": "training",
      "description": "Training step 1104",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:24",
      "total_flops_so_far": 2.6238567470887836e+16,
      "budget_used_percent": 26.238567470887837
    },
    {
      "type": "training",
      "description": "Training step 1105",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:25",
      "total_flops_so_far": 2.6262306343023516e+16,
      "budget_used_percent": 26.262306343023518
    },
    {
      "type": "training",
      "description": "Training step 1106",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:26",
      "total_flops_so_far": 2.6286045215159196e+16,
      "budget_used_percent": 26.2860452151592
    },
    {
      "type": "training",
      "description": "Training step 1107",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:28",
      "total_flops_so_far": 2.6309784087294876e+16,
      "budget_used_percent": 26.309784087294872
    },
    {
      "type": "training",
      "description": "Training step 1108",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:29",
      "total_flops_so_far": 2.6333522959430556e+16,
      "budget_used_percent": 26.333522959430557
    },
    {
      "type": "training",
      "description": "Training step 1109",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:30",
      "total_flops_so_far": 2.6357261831566236e+16,
      "budget_used_percent": 26.357261831566237
    },
    {
      "type": "training",
      "description": "Training step 1110",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:32",
      "total_flops_so_far": 2.6381000703701916e+16,
      "budget_used_percent": 26.38100070370192
    },
    {
      "type": "training",
      "description": "Training step 1111",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:33",
      "total_flops_so_far": 2.6404739575837596e+16,
      "budget_used_percent": 26.404739575837592
    },
    {
      "type": "training",
      "description": "Training step 1112",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:34",
      "total_flops_so_far": 2.6428478447973276e+16,
      "budget_used_percent": 26.428478447973276
    },
    {
      "type": "training",
      "description": "Training step 1113",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:36",
      "total_flops_so_far": 2.6452217320108956e+16,
      "budget_used_percent": 26.452217320108957
    },
    {
      "type": "training",
      "description": "Training step 1114",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:37",
      "total_flops_so_far": 2.6475956192244636e+16,
      "budget_used_percent": 26.475956192244638
    },
    {
      "type": "training",
      "description": "Training step 1115",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:38",
      "total_flops_so_far": 2.6499695064380316e+16,
      "budget_used_percent": 26.49969506438032
    },
    {
      "type": "training",
      "description": "Training step 1116",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:40",
      "total_flops_so_far": 2.6523433936515996e+16,
      "budget_used_percent": 26.523433936515993
    },
    {
      "type": "training",
      "description": "Training step 1117",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:41",
      "total_flops_so_far": 2.6547172808651676e+16,
      "budget_used_percent": 26.547172808651677
    },
    {
      "type": "training",
      "description": "Training step 1118",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:42",
      "total_flops_so_far": 2.6570911680787356e+16,
      "budget_used_percent": 26.570911680787358
    },
    {
      "type": "training",
      "description": "Training step 1119",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:43",
      "total_flops_so_far": 2.6594650552923036e+16,
      "budget_used_percent": 26.59465055292304
    },
    {
      "type": "training",
      "description": "Training step 1120",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:45",
      "total_flops_so_far": 2.6618389425058716e+16,
      "budget_used_percent": 26.618389425058712
    },
    {
      "type": "training",
      "description": "Training step 1121",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:46",
      "total_flops_so_far": 2.6642128297194396e+16,
      "budget_used_percent": 26.642128297194397
    },
    {
      "type": "training",
      "description": "Training step 1122",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:47",
      "total_flops_so_far": 2.6665867169330076e+16,
      "budget_used_percent": 26.665867169330078
    },
    {
      "type": "training",
      "description": "Training step 1123",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:49",
      "total_flops_so_far": 2.6689606041465756e+16,
      "budget_used_percent": 26.68960604146576
    },
    {
      "type": "training",
      "description": "Training step 1124",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:50",
      "total_flops_so_far": 2.6713344913601436e+16,
      "budget_used_percent": 26.713344913601432
    },
    {
      "type": "training",
      "description": "Training step 1125",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:51",
      "total_flops_so_far": 2.6737083785737116e+16,
      "budget_used_percent": 26.737083785737113
    },
    {
      "type": "training",
      "description": "Training step 1126",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:53",
      "total_flops_so_far": 2.6760822657872796e+16,
      "budget_used_percent": 26.760822657872797
    },
    {
      "type": "training",
      "description": "Training step 1127",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:54",
      "total_flops_so_far": 2.6784561530008476e+16,
      "budget_used_percent": 26.784561530008478
    },
    {
      "type": "training",
      "description": "Training step 1128",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:55",
      "total_flops_so_far": 2.6808300402144156e+16,
      "budget_used_percent": 26.80830040214416
    },
    {
      "type": "training",
      "description": "Training step 1129",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:57",
      "total_flops_so_far": 2.6832039274279836e+16,
      "budget_used_percent": 26.832039274279833
    },
    {
      "type": "training",
      "description": "Training step 1130",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:58",
      "total_flops_so_far": 2.6855778146415516e+16,
      "budget_used_percent": 26.855778146415517
    },
    {
      "type": "training",
      "description": "Training step 1131",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:56:59",
      "total_flops_so_far": 2.6879517018551196e+16,
      "budget_used_percent": 26.879517018551198
    },
    {
      "type": "training",
      "description": "Training step 1132",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:00",
      "total_flops_so_far": 2.6903255890686876e+16,
      "budget_used_percent": 26.90325589068688
    },
    {
      "type": "training",
      "description": "Training step 1133",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:02",
      "total_flops_so_far": 2.6926994762822556e+16,
      "budget_used_percent": 26.926994762822552
    },
    {
      "type": "training",
      "description": "Training step 1134",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:03",
      "total_flops_so_far": 2.6950733634958236e+16,
      "budget_used_percent": 26.950733634958233
    },
    {
      "type": "training",
      "description": "Training step 1135",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:04",
      "total_flops_so_far": 2.6974472507093916e+16,
      "budget_used_percent": 26.974472507093918
    },
    {
      "type": "training",
      "description": "Training step 1136",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:06",
      "total_flops_so_far": 2.6998211379229596e+16,
      "budget_used_percent": 26.9982113792296
    },
    {
      "type": "training",
      "description": "Training step 1137",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:07",
      "total_flops_so_far": 2.7021950251365276e+16,
      "budget_used_percent": 27.02195025136528
    },
    {
      "type": "training",
      "description": "Training step 1138",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:08",
      "total_flops_so_far": 2.7045689123500956e+16,
      "budget_used_percent": 27.045689123500953
    },
    {
      "type": "training",
      "description": "Training step 1139",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:10",
      "total_flops_so_far": 2.7069427995636636e+16,
      "budget_used_percent": 27.069427995636637
    },
    {
      "type": "training",
      "description": "Training step 1140",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:11",
      "total_flops_so_far": 2.7093166867772316e+16,
      "budget_used_percent": 27.093166867772318
    },
    {
      "type": "training",
      "description": "Training step 1141",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:12",
      "total_flops_so_far": 2.7116905739907996e+16,
      "budget_used_percent": 27.116905739908
    },
    {
      "type": "training",
      "description": "Training step 1142",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:13",
      "total_flops_so_far": 2.7140644612043676e+16,
      "budget_used_percent": 27.140644612043673
    },
    {
      "type": "training",
      "description": "Training step 1143",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:15",
      "total_flops_so_far": 2.7164383484179356e+16,
      "budget_used_percent": 27.164383484179353
    },
    {
      "type": "training",
      "description": "Training step 1144",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:16",
      "total_flops_so_far": 2.7188122356315036e+16,
      "budget_used_percent": 27.188122356315038
    },
    {
      "type": "training",
      "description": "Training step 1145",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:17",
      "total_flops_so_far": 2.7211861228450716e+16,
      "budget_used_percent": 27.21186122845072
    },
    {
      "type": "training",
      "description": "Training step 1146",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:19",
      "total_flops_so_far": 2.7235600100586396e+16,
      "budget_used_percent": 27.235600100586392
    },
    {
      "type": "training",
      "description": "Training step 1147",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:20",
      "total_flops_so_far": 2.7259338972722076e+16,
      "budget_used_percent": 27.259338972722073
    },
    {
      "type": "training",
      "description": "Training step 1148",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:21",
      "total_flops_so_far": 2.7283077844857756e+16,
      "budget_used_percent": 27.283077844857758
    },
    {
      "type": "training",
      "description": "Training step 1149",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:23",
      "total_flops_so_far": 2.7306816716993436e+16,
      "budget_used_percent": 27.30681671699344
    },
    {
      "type": "training",
      "description": "Training step 1150",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:24",
      "total_flops_so_far": 2.7330555589129116e+16,
      "budget_used_percent": 27.33055558912912
    },
    {
      "type": "training",
      "description": "Training step 1151",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:25",
      "total_flops_so_far": 2.7354294461264796e+16,
      "budget_used_percent": 27.354294461264793
    },
    {
      "type": "training",
      "description": "Training step 1152",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:27",
      "total_flops_so_far": 2.7378033333400476e+16,
      "budget_used_percent": 27.378033333400474
    },
    {
      "type": "training",
      "description": "Training step 1153",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:28",
      "total_flops_so_far": 2.7401772205536156e+16,
      "budget_used_percent": 27.401772205536158
    },
    {
      "type": "training",
      "description": "Training step 1154",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:29",
      "total_flops_so_far": 2.7425511077671836e+16,
      "budget_used_percent": 27.42551107767184
    },
    {
      "type": "training",
      "description": "Training step 1155",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:30",
      "total_flops_so_far": 2.7449249949807516e+16,
      "budget_used_percent": 27.449249949807513
    },
    {
      "type": "training",
      "description": "Training step 1156",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:32",
      "total_flops_so_far": 2.7472988821943196e+16,
      "budget_used_percent": 27.472988821943193
    },
    {
      "type": "training",
      "description": "Training step 1157",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:33",
      "total_flops_so_far": 2.7496727694078876e+16,
      "budget_used_percent": 27.496727694078878
    },
    {
      "type": "training",
      "description": "Training step 1158",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:34",
      "total_flops_so_far": 2.7520466566214556e+16,
      "budget_used_percent": 27.52046656621456
    },
    {
      "type": "training",
      "description": "Training step 1159",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:36",
      "total_flops_so_far": 2.7544205438350236e+16,
      "budget_used_percent": 27.544205438350232
    },
    {
      "type": "training",
      "description": "Training step 1160",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:37",
      "total_flops_so_far": 2.7567944310485916e+16,
      "budget_used_percent": 27.567944310485913
    },
    {
      "type": "training",
      "description": "Training step 1161",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:38",
      "total_flops_so_far": 2.7591683182621596e+16,
      "budget_used_percent": 27.591683182621594
    },
    {
      "type": "training",
      "description": "Training step 1162",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:40",
      "total_flops_so_far": 2.7615422054757276e+16,
      "budget_used_percent": 27.61542205475728
    },
    {
      "type": "training",
      "description": "Training step 1163",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:41",
      "total_flops_so_far": 2.7639160926892956e+16,
      "budget_used_percent": 27.63916092689296
    },
    {
      "type": "training",
      "description": "Training step 1164",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:42",
      "total_flops_so_far": 2.7662899799028636e+16,
      "budget_used_percent": 27.662899799028633
    },
    {
      "type": "training",
      "description": "Training step 1165",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:44",
      "total_flops_so_far": 2.7686638671164316e+16,
      "budget_used_percent": 27.686638671164314
    },
    {
      "type": "training",
      "description": "Training step 1166",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:45",
      "total_flops_so_far": 2.7710377543299996e+16,
      "budget_used_percent": 27.710377543299998
    },
    {
      "type": "training",
      "description": "Training step 1167",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:46",
      "total_flops_so_far": 2.7734116415435676e+16,
      "budget_used_percent": 27.73411641543568
    },
    {
      "type": "training",
      "description": "Training step 1168",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:47",
      "total_flops_so_far": 2.7757855287571356e+16,
      "budget_used_percent": 27.757855287571353
    },
    {
      "type": "training",
      "description": "Training step 1169",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:49",
      "total_flops_so_far": 2.7781594159707036e+16,
      "budget_used_percent": 27.781594159707033
    },
    {
      "type": "training",
      "description": "Training step 1170",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:50",
      "total_flops_so_far": 2.7805333031842716e+16,
      "budget_used_percent": 27.805333031842714
    },
    {
      "type": "training",
      "description": "Training step 1171",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:51",
      "total_flops_so_far": 2.7829071903978396e+16,
      "budget_used_percent": 27.8290719039784
    },
    {
      "type": "training",
      "description": "Training step 1172",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:53",
      "total_flops_so_far": 2.7852810776114076e+16,
      "budget_used_percent": 27.85281077611408
    },
    {
      "type": "training",
      "description": "Training step 1173",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:54",
      "total_flops_so_far": 2.7876549648249756e+16,
      "budget_used_percent": 27.876549648249753
    },
    {
      "type": "training",
      "description": "Training step 1174",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:55",
      "total_flops_so_far": 2.7900288520385436e+16,
      "budget_used_percent": 27.900288520385434
    },
    {
      "type": "training",
      "description": "Training step 1175",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:57",
      "total_flops_so_far": 2.7924027392521116e+16,
      "budget_used_percent": 27.92402739252112
    },
    {
      "type": "training",
      "description": "Training step 1176",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:58",
      "total_flops_so_far": 2.7947766264656796e+16,
      "budget_used_percent": 27.9477662646568
    },
    {
      "type": "training",
      "description": "Training step 1177",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:57:59",
      "total_flops_so_far": 2.7971505136792476e+16,
      "budget_used_percent": 27.971505136792473
    },
    {
      "type": "training",
      "description": "Training step 1178",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:00",
      "total_flops_so_far": 2.7995244008928156e+16,
      "budget_used_percent": 27.995244008928154
    },
    {
      "type": "training",
      "description": "Training step 1179",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:02",
      "total_flops_so_far": 2.8018982881063836e+16,
      "budget_used_percent": 28.018982881063835
    },
    {
      "type": "training",
      "description": "Training step 1180",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:03",
      "total_flops_so_far": 2.8042721753199516e+16,
      "budget_used_percent": 28.04272175319952
    },
    {
      "type": "training",
      "description": "Training step 1181",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:04",
      "total_flops_so_far": 2.8066460625335196e+16,
      "budget_used_percent": 28.066460625335193
    },
    {
      "type": "training",
      "description": "Training step 1182",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:06",
      "total_flops_so_far": 2.8090199497470876e+16,
      "budget_used_percent": 28.090199497470874
    },
    {
      "type": "training",
      "description": "Training step 1183",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:07",
      "total_flops_so_far": 2.8113938369606556e+16,
      "budget_used_percent": 28.113938369606554
    },
    {
      "type": "training",
      "description": "Training step 1184",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:08",
      "total_flops_so_far": 2.8137677241742236e+16,
      "budget_used_percent": 28.13767724174224
    },
    {
      "type": "training",
      "description": "Training step 1185",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:10",
      "total_flops_so_far": 2.8161416113877916e+16,
      "budget_used_percent": 28.16141611387792
    },
    {
      "type": "training",
      "description": "Training step 1186",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:11",
      "total_flops_so_far": 2.8185154986013596e+16,
      "budget_used_percent": 28.185154986013593
    },
    {
      "type": "training",
      "description": "Training step 1187",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:12",
      "total_flops_so_far": 2.8208893858149276e+16,
      "budget_used_percent": 28.208893858149274
    },
    {
      "type": "training",
      "description": "Training step 1188",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:13",
      "total_flops_so_far": 2.8232632730284956e+16,
      "budget_used_percent": 28.232632730284955
    },
    {
      "type": "training",
      "description": "Training step 1189",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:14",
      "total_flops_so_far": 2.8256371602420636e+16,
      "budget_used_percent": 28.25637160242064
    },
    {
      "type": "training",
      "description": "Training step 1190",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:15",
      "total_flops_so_far": 2.8280110474556316e+16,
      "budget_used_percent": 28.280110474556313
    },
    {
      "type": "training",
      "description": "Training step 1191",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:17",
      "total_flops_so_far": 2.8303849346691996e+16,
      "budget_used_percent": 28.303849346691994
    },
    {
      "type": "training",
      "description": "Training step 1192",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:18",
      "total_flops_so_far": 2.8327588218827676e+16,
      "budget_used_percent": 28.327588218827675
    },
    {
      "type": "training",
      "description": "Training step 1193",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:19",
      "total_flops_so_far": 2.8351327090963356e+16,
      "budget_used_percent": 28.35132709096336
    },
    {
      "type": "training",
      "description": "Training step 1194",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:20",
      "total_flops_so_far": 2.8375065963099036e+16,
      "budget_used_percent": 28.37506596309904
    },
    {
      "type": "training",
      "description": "Training step 1195",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:22",
      "total_flops_so_far": 2.8398804835234716e+16,
      "budget_used_percent": 28.398804835234714
    },
    {
      "type": "training",
      "description": "Training step 1196",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:23",
      "total_flops_so_far": 2.8422543707370396e+16,
      "budget_used_percent": 28.422543707370394
    },
    {
      "type": "training",
      "description": "Training step 1197",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:24",
      "total_flops_so_far": 2.8446282579506076e+16,
      "budget_used_percent": 28.446282579506075
    },
    {
      "type": "training",
      "description": "Training step 1198",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:26",
      "total_flops_so_far": 2.8470021451641756e+16,
      "budget_used_percent": 28.47002145164176
    },
    {
      "type": "training",
      "description": "Training step 1199",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:27",
      "total_flops_so_far": 2.8493760323777436e+16,
      "budget_used_percent": 28.493760323777433
    },
    {
      "type": "training",
      "description": "Training step 1200",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:28",
      "total_flops_so_far": 2.8517499195913116e+16,
      "budget_used_percent": 28.517499195913114
    },
    {
      "type": "training",
      "description": "Training step 1201",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:30",
      "total_flops_so_far": 2.8541238068048796e+16,
      "budget_used_percent": 28.541238068048795
    },
    {
      "type": "training",
      "description": "Training step 1202",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:31",
      "total_flops_so_far": 2.8564976940184476e+16,
      "budget_used_percent": 28.56497694018448
    },
    {
      "type": "training",
      "description": "Training step 1203",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:32",
      "total_flops_so_far": 2.8588715812320156e+16,
      "budget_used_percent": 28.588715812320153
    },
    {
      "type": "training",
      "description": "Training step 1204",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:34",
      "total_flops_so_far": 2.8612454684455836e+16,
      "budget_used_percent": 28.612454684455834
    },
    {
      "type": "training",
      "description": "Training step 1205",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:35",
      "total_flops_so_far": 2.8636193556591516e+16,
      "budget_used_percent": 28.636193556591515
    },
    {
      "type": "training",
      "description": "Training step 1206",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:36",
      "total_flops_so_far": 2.8659932428727196e+16,
      "budget_used_percent": 28.659932428727195
    },
    {
      "type": "training",
      "description": "Training step 1207",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:37",
      "total_flops_so_far": 2.8683671300862876e+16,
      "budget_used_percent": 28.68367130086288
    },
    {
      "type": "training",
      "description": "Training step 1208",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:39",
      "total_flops_so_far": 2.8707410172998556e+16,
      "budget_used_percent": 28.707410172998554
    },
    {
      "type": "training",
      "description": "Training step 1209",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:40",
      "total_flops_so_far": 2.8731149045134236e+16,
      "budget_used_percent": 28.731149045134234
    },
    {
      "type": "training",
      "description": "Training step 1210",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:41",
      "total_flops_so_far": 2.8754887917269916e+16,
      "budget_used_percent": 28.754887917269915
    },
    {
      "type": "training",
      "description": "Training step 1211",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:43",
      "total_flops_so_far": 2.8778626789405596e+16,
      "budget_used_percent": 28.7786267894056
    },
    {
      "type": "training",
      "description": "Training step 1212",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:44",
      "total_flops_so_far": 2.8802365661541276e+16,
      "budget_used_percent": 28.802365661541273
    },
    {
      "type": "training",
      "description": "Training step 1213",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:45",
      "total_flops_so_far": 2.8826104533676956e+16,
      "budget_used_percent": 28.826104533676954
    },
    {
      "type": "training",
      "description": "Training step 1214",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:47",
      "total_flops_so_far": 2.8849843405812636e+16,
      "budget_used_percent": 28.849843405812635
    },
    {
      "type": "training",
      "description": "Training step 1215",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:48",
      "total_flops_so_far": 2.8873582277948316e+16,
      "budget_used_percent": 28.873582277948316
    },
    {
      "type": "training",
      "description": "Training step 1216",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:49",
      "total_flops_so_far": 2.8897321150083996e+16,
      "budget_used_percent": 28.897321150083993
    },
    {
      "type": "training",
      "description": "Training step 1217",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:51",
      "total_flops_so_far": 2.8921060022219676e+16,
      "budget_used_percent": 28.921060022219674
    },
    {
      "type": "training",
      "description": "Training step 1218",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:52",
      "total_flops_so_far": 2.8944798894355356e+16,
      "budget_used_percent": 28.944798894355355
    },
    {
      "type": "training",
      "description": "Training step 1219",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:53",
      "total_flops_so_far": 2.8968537766491036e+16,
      "budget_used_percent": 28.968537766491036
    },
    {
      "type": "training",
      "description": "Training step 1220",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:54",
      "total_flops_so_far": 2.8992276638626716e+16,
      "budget_used_percent": 28.99227663862672
    },
    {
      "type": "training",
      "description": "Training step 1221",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:56",
      "total_flops_so_far": 2.9016015510762396e+16,
      "budget_used_percent": 29.016015510762394
    },
    {
      "type": "training",
      "description": "Training step 1222",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:57",
      "total_flops_so_far": 2.9039754382898076e+16,
      "budget_used_percent": 29.039754382898074
    },
    {
      "type": "training",
      "description": "Training step 1223",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:58:58",
      "total_flops_so_far": 2.9063493255033756e+16,
      "budget_used_percent": 29.063493255033755
    },
    {
      "type": "training",
      "description": "Training step 1224",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:00",
      "total_flops_so_far": 2.9087232127169436e+16,
      "budget_used_percent": 29.087232127169436
    },
    {
      "type": "training",
      "description": "Training step 1225",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:01",
      "total_flops_so_far": 2.9110970999305116e+16,
      "budget_used_percent": 29.110970999305113
    },
    {
      "type": "training",
      "description": "Training step 1226",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:02",
      "total_flops_so_far": 2.9134709871440796e+16,
      "budget_used_percent": 29.134709871440794
    },
    {
      "type": "training",
      "description": "Training step 1227",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:04",
      "total_flops_so_far": 2.9158448743576476e+16,
      "budget_used_percent": 29.158448743576475
    },
    {
      "type": "training",
      "description": "Training step 1228",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:05",
      "total_flops_so_far": 2.9182187615712156e+16,
      "budget_used_percent": 29.182187615712156
    },
    {
      "type": "training",
      "description": "Training step 1229",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:06",
      "total_flops_so_far": 2.9205926487847836e+16,
      "budget_used_percent": 29.20592648784784
    },
    {
      "type": "training",
      "description": "Training step 1230",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:08",
      "total_flops_so_far": 2.9229665359983516e+16,
      "budget_used_percent": 29.229665359983514
    },
    {
      "type": "training",
      "description": "Training step 1231",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:09",
      "total_flops_so_far": 2.9253404232119196e+16,
      "budget_used_percent": 29.253404232119195
    },
    {
      "type": "training",
      "description": "Training step 1232",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:10",
      "total_flops_so_far": 2.9277143104254876e+16,
      "budget_used_percent": 29.277143104254876
    },
    {
      "type": "training",
      "description": "Training step 1233",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:12",
      "total_flops_so_far": 2.9300881976390556e+16,
      "budget_used_percent": 29.300881976390556
    },
    {
      "type": "training",
      "description": "Training step 1234",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:13",
      "total_flops_so_far": 2.9324620848526236e+16,
      "budget_used_percent": 29.324620848526234
    },
    {
      "type": "training",
      "description": "Training step 1235",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:14",
      "total_flops_so_far": 2.9348359720661916e+16,
      "budget_used_percent": 29.348359720661914
    },
    {
      "type": "training",
      "description": "Training step 1236",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:15",
      "total_flops_so_far": 2.9372098592797596e+16,
      "budget_used_percent": 29.372098592797595
    },
    {
      "type": "training",
      "description": "Training step 1237",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:17",
      "total_flops_so_far": 2.9395837464933276e+16,
      "budget_used_percent": 29.395837464933276
    },
    {
      "type": "training",
      "description": "Training step 1238",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:18",
      "total_flops_so_far": 2.9419576337068956e+16,
      "budget_used_percent": 29.419576337068953
    },
    {
      "type": "training",
      "description": "Training step 1239",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:19",
      "total_flops_so_far": 2.9443315209204636e+16,
      "budget_used_percent": 29.443315209204634
    },
    {
      "type": "training",
      "description": "Training step 1240",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:21",
      "total_flops_so_far": 2.9467054081340316e+16,
      "budget_used_percent": 29.467054081340315
    },
    {
      "type": "training",
      "description": "Training step 1241",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:22",
      "total_flops_so_far": 2.9490792953475996e+16,
      "budget_used_percent": 29.490792953475996
    },
    {
      "type": "training",
      "description": "Training step 1242",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:23",
      "total_flops_so_far": 2.9514531825611676e+16,
      "budget_used_percent": 29.514531825611677
    },
    {
      "type": "training",
      "description": "Training step 1243",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:25",
      "total_flops_so_far": 2.9538270697747356e+16,
      "budget_used_percent": 29.538270697747354
    },
    {
      "type": "training",
      "description": "Training step 1244",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:26",
      "total_flops_so_far": 2.9562009569883036e+16,
      "budget_used_percent": 29.562009569883035
    },
    {
      "type": "training",
      "description": "Training step 1245",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:27",
      "total_flops_so_far": 2.9585748442018716e+16,
      "budget_used_percent": 29.585748442018716
    },
    {
      "type": "training",
      "description": "Training step 1246",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:29",
      "total_flops_so_far": 2.9609487314154396e+16,
      "budget_used_percent": 29.609487314154396
    },
    {
      "type": "training",
      "description": "Training step 1247",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:30",
      "total_flops_so_far": 2.9633226186290076e+16,
      "budget_used_percent": 29.633226186290074
    },
    {
      "type": "training",
      "description": "Training step 1248",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:31",
      "total_flops_so_far": 2.9656965058425756e+16,
      "budget_used_percent": 29.656965058425754
    },
    {
      "type": "training",
      "description": "Training step 1249",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:32",
      "total_flops_so_far": 2.9680703930561436e+16,
      "budget_used_percent": 29.680703930561435
    },
    {
      "type": "training",
      "description": "Training step 1250",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:34",
      "total_flops_so_far": 2.9704442802697116e+16,
      "budget_used_percent": 29.704442802697116
    },
    {
      "type": "training",
      "description": "Training step 1251",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:35",
      "total_flops_so_far": 2.9728181674832796e+16,
      "budget_used_percent": 29.728181674832797
    },
    {
      "type": "training",
      "description": "Training step 1252",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:36",
      "total_flops_so_far": 2.9751920546968476e+16,
      "budget_used_percent": 29.751920546968474
    },
    {
      "type": "training",
      "description": "Training step 1253",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:38",
      "total_flops_so_far": 2.9775659419104156e+16,
      "budget_used_percent": 29.775659419104155
    },
    {
      "type": "training",
      "description": "Training step 1254",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:39",
      "total_flops_so_far": 2.9799398291239836e+16,
      "budget_used_percent": 29.799398291239836
    },
    {
      "type": "training",
      "description": "Training step 1255",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:40",
      "total_flops_so_far": 2.9823137163375516e+16,
      "budget_used_percent": 29.823137163375517
    },
    {
      "type": "training",
      "description": "Training step 1256",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:42",
      "total_flops_so_far": 2.9846876035511196e+16,
      "budget_used_percent": 29.846876035511194
    },
    {
      "type": "training",
      "description": "Training step 1257",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:43",
      "total_flops_so_far": 2.9870614907646876e+16,
      "budget_used_percent": 29.870614907646875
    },
    {
      "type": "training",
      "description": "Training step 1258",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:44",
      "total_flops_so_far": 2.9894353779782556e+16,
      "budget_used_percent": 29.894353779782556
    },
    {
      "type": "training",
      "description": "Training step 1259",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:46",
      "total_flops_so_far": 2.9918092651918236e+16,
      "budget_used_percent": 29.918092651918236
    },
    {
      "type": "training",
      "description": "Training step 1260",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:47",
      "total_flops_so_far": 2.9941831524053916e+16,
      "budget_used_percent": 29.941831524053914
    },
    {
      "type": "training",
      "description": "Training step 1261",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:48",
      "total_flops_so_far": 2.9965570396189596e+16,
      "budget_used_percent": 29.965570396189595
    },
    {
      "type": "training",
      "description": "Training step 1262",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:49",
      "total_flops_so_far": 2.9989309268325276e+16,
      "budget_used_percent": 29.989309268325275
    },
    {
      "type": "training",
      "description": "Training step 1263",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:51",
      "total_flops_so_far": 3.0013048140460956e+16,
      "budget_used_percent": 30.013048140460956
    },
    {
      "type": "training",
      "description": "Training step 1264",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:52",
      "total_flops_so_far": 3.0036787012596636e+16,
      "budget_used_percent": 30.036787012596637
    },
    {
      "type": "training",
      "description": "Training step 1265",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:53",
      "total_flops_so_far": 3.0060525884732316e+16,
      "budget_used_percent": 30.060525884732314
    },
    {
      "type": "training",
      "description": "Training step 1266",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:55",
      "total_flops_so_far": 3.0084264756867996e+16,
      "budget_used_percent": 30.084264756867995
    },
    {
      "type": "training",
      "description": "Training step 1267",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:56",
      "total_flops_so_far": 3.0108003629003676e+16,
      "budget_used_percent": 30.108003629003676
    },
    {
      "type": "training",
      "description": "Training step 1268",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:57",
      "total_flops_so_far": 3.0131742501139356e+16,
      "budget_used_percent": 30.131742501139357
    },
    {
      "type": "training",
      "description": "Training step 1269",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 19:59:59",
      "total_flops_so_far": 3.0155481373275036e+16,
      "budget_used_percent": 30.155481373275034
    },
    {
      "type": "training",
      "description": "Training step 1270",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:00",
      "total_flops_so_far": 3.0179220245410716e+16,
      "budget_used_percent": 30.179220245410715
    },
    {
      "type": "training",
      "description": "Training step 1271",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:01",
      "total_flops_so_far": 3.0202959117546396e+16,
      "budget_used_percent": 30.202959117546396
    },
    {
      "type": "training",
      "description": "Training step 1272",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:03",
      "total_flops_so_far": 3.0226697989682076e+16,
      "budget_used_percent": 30.226697989682076
    },
    {
      "type": "training",
      "description": "Training step 1273",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:04",
      "total_flops_so_far": 3.0250436861817756e+16,
      "budget_used_percent": 30.250436861817754
    },
    {
      "type": "training",
      "description": "Training step 1274",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:05",
      "total_flops_so_far": 3.0274175733953436e+16,
      "budget_used_percent": 30.274175733953435
    },
    {
      "type": "training",
      "description": "Training step 1275",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:07",
      "total_flops_so_far": 3.0297914606089116e+16,
      "budget_used_percent": 30.297914606089115
    },
    {
      "type": "training",
      "description": "Training step 1276",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:08",
      "total_flops_so_far": 3.0321653478224796e+16,
      "budget_used_percent": 30.321653478224796
    },
    {
      "type": "training",
      "description": "Training step 1277",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:11",
      "total_flops_so_far": 3.0345392350360476e+16,
      "budget_used_percent": 30.345392350360477
    },
    {
      "type": "training",
      "description": "Training step 1278",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:12",
      "total_flops_so_far": 3.0369131222496156e+16,
      "budget_used_percent": 30.369131222496154
    },
    {
      "type": "training",
      "description": "Training step 1279",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:13",
      "total_flops_so_far": 3.0392870094631836e+16,
      "budget_used_percent": 30.392870094631835
    },
    {
      "type": "training",
      "description": "Training step 1280",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:15",
      "total_flops_so_far": 3.0416608966767516e+16,
      "budget_used_percent": 30.416608966767516
    },
    {
      "type": "training",
      "description": "Training step 1281",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:16",
      "total_flops_so_far": 3.0440347838903196e+16,
      "budget_used_percent": 30.440347838903197
    },
    {
      "type": "training",
      "description": "Training step 1282",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:17",
      "total_flops_so_far": 3.0464086711038876e+16,
      "budget_used_percent": 30.464086711038874
    },
    {
      "type": "training",
      "description": "Training step 1283",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:18",
      "total_flops_so_far": 3.0487825583174556e+16,
      "budget_used_percent": 30.487825583174555
    },
    {
      "type": "training",
      "description": "Training step 1284",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:20",
      "total_flops_so_far": 3.0511564455310236e+16,
      "budget_used_percent": 30.511564455310236
    },
    {
      "type": "training",
      "description": "Training step 1285",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:21",
      "total_flops_so_far": 3.0535303327445916e+16,
      "budget_used_percent": 30.535303327445916
    },
    {
      "type": "training",
      "description": "Training step 1286",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:22",
      "total_flops_so_far": 3.0559042199581596e+16,
      "budget_used_percent": 30.559042199581597
    },
    {
      "type": "training",
      "description": "Training step 1287",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:24",
      "total_flops_so_far": 3.0582781071717276e+16,
      "budget_used_percent": 30.582781071717275
    },
    {
      "type": "training",
      "description": "Training step 1288",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:25",
      "total_flops_so_far": 3.0606519943852956e+16,
      "budget_used_percent": 30.606519943852955
    },
    {
      "type": "training",
      "description": "Training step 1289",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:26",
      "total_flops_so_far": 3.0630258815988636e+16,
      "budget_used_percent": 30.630258815988636
    },
    {
      "type": "training",
      "description": "Training step 1290",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:28",
      "total_flops_so_far": 3.0653997688124316e+16,
      "budget_used_percent": 30.653997688124317
    },
    {
      "type": "training",
      "description": "Training step 1291",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:29",
      "total_flops_so_far": 3.0677736560259996e+16,
      "budget_used_percent": 30.677736560259994
    },
    {
      "type": "training",
      "description": "Training step 1292",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:30",
      "total_flops_so_far": 3.0701475432395676e+16,
      "budget_used_percent": 30.701475432395675
    },
    {
      "type": "training",
      "description": "Training step 1293",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:32",
      "total_flops_so_far": 3.0725214304531356e+16,
      "budget_used_percent": 30.725214304531356
    },
    {
      "type": "training",
      "description": "Training step 1294",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:33",
      "total_flops_so_far": 3.0748953176667036e+16,
      "budget_used_percent": 30.748953176667037
    },
    {
      "type": "training",
      "description": "Training step 1295",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:34",
      "total_flops_so_far": 3.0772692048802716e+16,
      "budget_used_percent": 30.772692048802714
    },
    {
      "type": "training",
      "description": "Training step 1296",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:35",
      "total_flops_so_far": 3.0796430920938396e+16,
      "budget_used_percent": 30.796430920938395
    },
    {
      "type": "training",
      "description": "Training step 1297",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:37",
      "total_flops_so_far": 3.0820169793074076e+16,
      "budget_used_percent": 30.820169793074076
    },
    {
      "type": "training",
      "description": "Training step 1298",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:38",
      "total_flops_so_far": 3.0843908665209756e+16,
      "budget_used_percent": 30.843908665209756
    },
    {
      "type": "training",
      "description": "Training step 1299",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:39",
      "total_flops_so_far": 3.0867647537345436e+16,
      "budget_used_percent": 30.867647537345437
    },
    {
      "type": "training",
      "description": "Training step 1300",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:41",
      "total_flops_so_far": 3.0891386409481116e+16,
      "budget_used_percent": 30.891386409481115
    },
    {
      "type": "training",
      "description": "Training step 1301",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:42",
      "total_flops_so_far": 3.0915125281616796e+16,
      "budget_used_percent": 30.915125281616795
    },
    {
      "type": "training",
      "description": "Training step 1302",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:43",
      "total_flops_so_far": 3.0938864153752476e+16,
      "budget_used_percent": 30.938864153752476
    },
    {
      "type": "training",
      "description": "Training step 1303",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:45",
      "total_flops_so_far": 3.0962603025888156e+16,
      "budget_used_percent": 30.962603025888157
    },
    {
      "type": "training",
      "description": "Training step 1304",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:46",
      "total_flops_so_far": 3.0986341898023836e+16,
      "budget_used_percent": 30.986341898023834
    },
    {
      "type": "training",
      "description": "Training step 1305",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:47",
      "total_flops_so_far": 3.1010080770159516e+16,
      "budget_used_percent": 31.010080770159515
    },
    {
      "type": "training",
      "description": "Training step 1306",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:49",
      "total_flops_so_far": 3.1033819642295196e+16,
      "budget_used_percent": 31.033819642295196
    },
    {
      "type": "training",
      "description": "Training step 1307",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:50",
      "total_flops_so_far": 3.1057558514430876e+16,
      "budget_used_percent": 31.057558514430877
    },
    {
      "type": "training",
      "description": "Training step 1308",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:51",
      "total_flops_so_far": 3.1081297386566556e+16,
      "budget_used_percent": 31.081297386566558
    },
    {
      "type": "training",
      "description": "Training step 1309",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:53",
      "total_flops_so_far": 3.1105036258702236e+16,
      "budget_used_percent": 31.105036258702235
    },
    {
      "type": "training",
      "description": "Training step 1310",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:54",
      "total_flops_so_far": 3.1128775130837916e+16,
      "budget_used_percent": 31.128775130837916
    },
    {
      "type": "training",
      "description": "Training step 1311",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:55",
      "total_flops_so_far": 3.1152514002973596e+16,
      "budget_used_percent": 31.152514002973597
    },
    {
      "type": "training",
      "description": "Training step 1312",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:56",
      "total_flops_so_far": 3.1176252875109276e+16,
      "budget_used_percent": 31.176252875109277
    },
    {
      "type": "training",
      "description": "Training step 1313",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:58",
      "total_flops_so_far": 3.1199991747244956e+16,
      "budget_used_percent": 31.199991747244955
    },
    {
      "type": "training",
      "description": "Training step 1314",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:00:59",
      "total_flops_so_far": 3.1223730619380636e+16,
      "budget_used_percent": 31.223730619380635
    },
    {
      "type": "training",
      "description": "Training step 1315",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:00",
      "total_flops_so_far": 3.1247469491516316e+16,
      "budget_used_percent": 31.247469491516316
    },
    {
      "type": "training",
      "description": "Training step 1316",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:02",
      "total_flops_so_far": 3.1271208363651996e+16,
      "budget_used_percent": 31.271208363651997
    },
    {
      "type": "training",
      "description": "Training step 1317",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:03",
      "total_flops_so_far": 3.1294947235787676e+16,
      "budget_used_percent": 31.294947235787674
    },
    {
      "type": "training",
      "description": "Training step 1318",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:04",
      "total_flops_so_far": 3.1318686107923356e+16,
      "budget_used_percent": 31.318686107923355
    },
    {
      "type": "training",
      "description": "Training step 1319",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:06",
      "total_flops_so_far": 3.1342424980059036e+16,
      "budget_used_percent": 31.342424980059036
    },
    {
      "type": "training",
      "description": "Training step 1320",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:06",
      "total_flops_so_far": 3.1366163852194716e+16,
      "budget_used_percent": 31.366163852194717
    },
    {
      "type": "training",
      "description": "Training step 1321",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:07",
      "total_flops_so_far": 3.1389902724330396e+16,
      "budget_used_percent": 31.389902724330398
    },
    {
      "type": "training",
      "description": "Training step 1322",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:09",
      "total_flops_so_far": 3.1413641596466076e+16,
      "budget_used_percent": 31.413641596466075
    },
    {
      "type": "training",
      "description": "Training step 1323",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:10",
      "total_flops_so_far": 3.1437380468601756e+16,
      "budget_used_percent": 31.437380468601756
    },
    {
      "type": "training",
      "description": "Training step 1324",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:11",
      "total_flops_so_far": 3.1461119340737436e+16,
      "budget_used_percent": 31.461119340737437
    },
    {
      "type": "training",
      "description": "Training step 1325",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:13",
      "total_flops_so_far": 3.1484858212873116e+16,
      "budget_used_percent": 31.484858212873117
    },
    {
      "type": "training",
      "description": "Training step 1326",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:14",
      "total_flops_so_far": 3.1508597085008796e+16,
      "budget_used_percent": 31.508597085008795
    },
    {
      "type": "training",
      "description": "Training step 1327",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:15",
      "total_flops_so_far": 3.1532335957144476e+16,
      "budget_used_percent": 31.532335957144475
    },
    {
      "type": "training",
      "description": "Training step 1328",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:16",
      "total_flops_so_far": 3.1556074829280156e+16,
      "budget_used_percent": 31.556074829280156
    },
    {
      "type": "training",
      "description": "Training step 1329",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:18",
      "total_flops_so_far": 3.1579813701415836e+16,
      "budget_used_percent": 31.579813701415837
    },
    {
      "type": "training",
      "description": "Training step 1330",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:19",
      "total_flops_so_far": 3.1603552573551516e+16,
      "budget_used_percent": 31.603552573551514
    },
    {
      "type": "training",
      "description": "Training step 1331",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:20",
      "total_flops_so_far": 3.1627291445687196e+16,
      "budget_used_percent": 31.627291445687195
    },
    {
      "type": "training",
      "description": "Training step 1332",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:22",
      "total_flops_so_far": 3.1651030317822876e+16,
      "budget_used_percent": 31.651030317822876
    },
    {
      "type": "training",
      "description": "Training step 1333",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:23",
      "total_flops_so_far": 3.1674769189958556e+16,
      "budget_used_percent": 31.674769189958557
    },
    {
      "type": "training",
      "description": "Training step 1334",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:24",
      "total_flops_so_far": 3.1698508062094236e+16,
      "budget_used_percent": 31.698508062094238
    },
    {
      "type": "training",
      "description": "Training step 1335",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:26",
      "total_flops_so_far": 3.1722246934229916e+16,
      "budget_used_percent": 31.722246934229915
    },
    {
      "type": "training",
      "description": "Training step 1336",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:27",
      "total_flops_so_far": 3.1745985806365596e+16,
      "budget_used_percent": 31.745985806365596
    },
    {
      "type": "training",
      "description": "Training step 1337",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:28",
      "total_flops_so_far": 3.1769724678501276e+16,
      "budget_used_percent": 31.769724678501277
    },
    {
      "type": "training",
      "description": "Training step 1338",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:30",
      "total_flops_so_far": 3.1793463550636956e+16,
      "budget_used_percent": 31.793463550636957
    },
    {
      "type": "training",
      "description": "Training step 1339",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:31",
      "total_flops_so_far": 3.1817202422772636e+16,
      "budget_used_percent": 31.817202422772635
    },
    {
      "type": "training",
      "description": "Training step 1340",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:32",
      "total_flops_so_far": 3.1840941294908316e+16,
      "budget_used_percent": 31.840941294908315
    },
    {
      "type": "training",
      "description": "Training step 1341",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:34",
      "total_flops_so_far": 3.1864680167043996e+16,
      "budget_used_percent": 31.864680167043996
    },
    {
      "type": "training",
      "description": "Training step 1342",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:35",
      "total_flops_so_far": 3.1888419039179676e+16,
      "budget_used_percent": 31.888419039179677
    },
    {
      "type": "training",
      "description": "Training step 1343",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:36",
      "total_flops_so_far": 3.1912157911315356e+16,
      "budget_used_percent": 31.912157911315358
    },
    {
      "type": "training",
      "description": "Training step 1344",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:37",
      "total_flops_so_far": 3.1935896783451036e+16,
      "budget_used_percent": 31.935896783451035
    },
    {
      "type": "training",
      "description": "Training step 1345",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:39",
      "total_flops_so_far": 3.1959635655586716e+16,
      "budget_used_percent": 31.959635655586716
    },
    {
      "type": "training",
      "description": "Training step 1346",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:40",
      "total_flops_so_far": 3.1983374527722396e+16,
      "budget_used_percent": 31.983374527722397
    },
    {
      "type": "training",
      "description": "Training step 1347",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:41",
      "total_flops_so_far": 3.2007113399858076e+16,
      "budget_used_percent": 32.00711339985808
    },
    {
      "type": "training",
      "description": "Training step 1348",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:43",
      "total_flops_so_far": 3.2030852271993756e+16,
      "budget_used_percent": 32.03085227199375
    },
    {
      "type": "training",
      "description": "Training step 1349",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:44",
      "total_flops_so_far": 3.2054591144129436e+16,
      "budget_used_percent": 32.05459114412943
    },
    {
      "type": "training",
      "description": "Training step 1350",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:45",
      "total_flops_so_far": 3.2078330016265116e+16,
      "budget_used_percent": 32.07833001626511
    },
    {
      "type": "training",
      "description": "Training step 1351",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:47",
      "total_flops_so_far": 3.2102068888400796e+16,
      "budget_used_percent": 32.1020688884008
    },
    {
      "type": "training",
      "description": "Training step 1352",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:48",
      "total_flops_so_far": 3.2125807760536476e+16,
      "budget_used_percent": 32.125807760536475
    },
    {
      "type": "training",
      "description": "Training step 1353",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:49",
      "total_flops_so_far": 3.2149546632672156e+16,
      "budget_used_percent": 32.149546632672156
    },
    {
      "type": "training",
      "description": "Training step 1354",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:51",
      "total_flops_so_far": 3.2173285504807836e+16,
      "budget_used_percent": 32.173285504807836
    },
    {
      "type": "training",
      "description": "Training step 1355",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:52",
      "total_flops_so_far": 3.2197024376943516e+16,
      "budget_used_percent": 32.19702437694352
    },
    {
      "type": "training",
      "description": "Training step 1356",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:53",
      "total_flops_so_far": 3.2220763249079196e+16,
      "budget_used_percent": 32.2207632490792
    },
    {
      "type": "training",
      "description": "Training step 1357",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:54",
      "total_flops_so_far": 3.2244502121214876e+16,
      "budget_used_percent": 32.24450212121487
    },
    {
      "type": "training",
      "description": "Training step 1358",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:56",
      "total_flops_so_far": 3.2268240993350556e+16,
      "budget_used_percent": 32.26824099335055
    },
    {
      "type": "training",
      "description": "Training step 1359",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:57",
      "total_flops_so_far": 3.2291979865486236e+16,
      "budget_used_percent": 32.29197986548623
    },
    {
      "type": "training",
      "description": "Training step 1360",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:01:58",
      "total_flops_so_far": 3.2315718737621916e+16,
      "budget_used_percent": 32.31571873762192
    },
    {
      "type": "training",
      "description": "Training step 1361",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:00",
      "total_flops_so_far": 3.2339457609757596e+16,
      "budget_used_percent": 32.339457609757595
    },
    {
      "type": "training",
      "description": "Training step 1362",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:01",
      "total_flops_so_far": 3.2363196481893276e+16,
      "budget_used_percent": 32.363196481893276
    },
    {
      "type": "training",
      "description": "Training step 1363",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:02",
      "total_flops_so_far": 3.2386935354028956e+16,
      "budget_used_percent": 32.38693535402896
    },
    {
      "type": "training",
      "description": "Training step 1364",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:04",
      "total_flops_so_far": 3.2410674226164636e+16,
      "budget_used_percent": 32.41067422616464
    },
    {
      "type": "training",
      "description": "Training step 1365",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:05",
      "total_flops_so_far": 3.2434413098300316e+16,
      "budget_used_percent": 32.43441309830032
    },
    {
      "type": "training",
      "description": "Training step 1366",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:06",
      "total_flops_so_far": 3.2458151970435996e+16,
      "budget_used_percent": 32.45815197043599
    },
    {
      "type": "training",
      "description": "Training step 1367",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:08",
      "total_flops_so_far": 3.2481890842571676e+16,
      "budget_used_percent": 32.48189084257167
    },
    {
      "type": "training",
      "description": "Training step 1368",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:09",
      "total_flops_so_far": 3.2505629714707356e+16,
      "budget_used_percent": 32.505629714707354
    },
    {
      "type": "training",
      "description": "Training step 1369",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:10",
      "total_flops_so_far": 3.2529368586843036e+16,
      "budget_used_percent": 32.52936858684304
    },
    {
      "type": "training",
      "description": "Training step 1370",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:11",
      "total_flops_so_far": 3.2553107458978716e+16,
      "budget_used_percent": 32.553107458978715
    },
    {
      "type": "training",
      "description": "Training step 1371",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:13",
      "total_flops_so_far": 3.2576846331114396e+16,
      "budget_used_percent": 32.576846331114396
    },
    {
      "type": "training",
      "description": "Training step 1372",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:14",
      "total_flops_so_far": 3.2600585203250076e+16,
      "budget_used_percent": 32.60058520325008
    },
    {
      "type": "training",
      "description": "Training step 1373",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:15",
      "total_flops_so_far": 3.2624324075385756e+16,
      "budget_used_percent": 32.62432407538576
    },
    {
      "type": "training",
      "description": "Training step 1374",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:17",
      "total_flops_so_far": 3.2648062947521436e+16,
      "budget_used_percent": 32.64806294752143
    },
    {
      "type": "training",
      "description": "Training step 1375",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:18",
      "total_flops_so_far": 3.2671801819657116e+16,
      "budget_used_percent": 32.67180181965711
    },
    {
      "type": "training",
      "description": "Training step 1376",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:19",
      "total_flops_so_far": 3.2695540691792796e+16,
      "budget_used_percent": 32.69554069179279
    },
    {
      "type": "training",
      "description": "Training step 1377",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:21",
      "total_flops_so_far": 3.2719279563928476e+16,
      "budget_used_percent": 32.719279563928474
    },
    {
      "type": "training",
      "description": "Training step 1378",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:22",
      "total_flops_so_far": 3.2743018436064156e+16,
      "budget_used_percent": 32.74301843606416
    },
    {
      "type": "training",
      "description": "Training step 1379",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:23",
      "total_flops_so_far": 3.2766757308199836e+16,
      "budget_used_percent": 32.766757308199836
    },
    {
      "type": "training",
      "description": "Training step 1380",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:25",
      "total_flops_so_far": 3.2790496180335516e+16,
      "budget_used_percent": 32.790496180335516
    },
    {
      "type": "training",
      "description": "Training step 1381",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:26",
      "total_flops_so_far": 3.2814235052471196e+16,
      "budget_used_percent": 32.8142350524712
    },
    {
      "type": "training",
      "description": "Training step 1382",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:27",
      "total_flops_so_far": 3.2837973924606876e+16,
      "budget_used_percent": 32.83797392460688
    },
    {
      "type": "training",
      "description": "Training step 1383",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:29",
      "total_flops_so_far": 3.2861712796742556e+16,
      "budget_used_percent": 32.86171279674255
    },
    {
      "type": "training",
      "description": "Training step 1384",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:30",
      "total_flops_so_far": 3.2885451668878236e+16,
      "budget_used_percent": 32.88545166887823
    },
    {
      "type": "training",
      "description": "Training step 1385",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:31",
      "total_flops_so_far": 3.2909190541013916e+16,
      "budget_used_percent": 32.90919054101391
    },
    {
      "type": "training",
      "description": "Training step 1386",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:32",
      "total_flops_so_far": 3.2932929413149596e+16,
      "budget_used_percent": 32.932929413149594
    },
    {
      "type": "training",
      "description": "Training step 1387",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:34",
      "total_flops_so_far": 3.2956668285285276e+16,
      "budget_used_percent": 32.956668285285275
    },
    {
      "type": "training",
      "description": "Training step 1388",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:35",
      "total_flops_so_far": 3.2980407157420956e+16,
      "budget_used_percent": 32.980407157420956
    },
    {
      "type": "training",
      "description": "Training step 1389",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:36",
      "total_flops_so_far": 3.3004146029556636e+16,
      "budget_used_percent": 33.00414602955664
    },
    {
      "type": "training",
      "description": "Training step 1390",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:38",
      "total_flops_so_far": 3.3027884901692316e+16,
      "budget_used_percent": 33.02788490169232
    },
    {
      "type": "training",
      "description": "Training step 1391",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:39",
      "total_flops_so_far": 3.3051623773827996e+16,
      "budget_used_percent": 33.051623773828
    },
    {
      "type": "training",
      "description": "Training step 1392",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:40",
      "total_flops_so_far": 3.3075362645963676e+16,
      "budget_used_percent": 33.07536264596367
    },
    {
      "type": "training",
      "description": "Training step 1393",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:42",
      "total_flops_so_far": 3.3099101518099356e+16,
      "budget_used_percent": 33.09910151809935
    },
    {
      "type": "training",
      "description": "Training step 1394",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:43",
      "total_flops_so_far": 3.3122840390235036e+16,
      "budget_used_percent": 33.122840390235034
    },
    {
      "type": "training",
      "description": "Training step 1395",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:44",
      "total_flops_so_far": 3.3146579262370716e+16,
      "budget_used_percent": 33.146579262370715
    },
    {
      "type": "training",
      "description": "Training step 1396",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:46",
      "total_flops_so_far": 3.3170318134506396e+16,
      "budget_used_percent": 33.170318134506395
    },
    {
      "type": "training",
      "description": "Training step 1397",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:47",
      "total_flops_so_far": 3.3194057006642076e+16,
      "budget_used_percent": 33.194057006642076
    },
    {
      "type": "training",
      "description": "Training step 1398",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:48",
      "total_flops_so_far": 3.3217795878777756e+16,
      "budget_used_percent": 33.21779587877776
    },
    {
      "type": "training",
      "description": "Training step 1399",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:49",
      "total_flops_so_far": 3.3241534750913436e+16,
      "budget_used_percent": 33.24153475091344
    },
    {
      "type": "training",
      "description": "Training step 1400",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:51",
      "total_flops_so_far": 3.3265273623049116e+16,
      "budget_used_percent": 33.26527362304912
    },
    {
      "type": "training",
      "description": "Training step 1401",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:52",
      "total_flops_so_far": 3.3289012495184796e+16,
      "budget_used_percent": 33.28901249518479
    },
    {
      "type": "training",
      "description": "Training step 1402",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:53",
      "total_flops_so_far": 3.3312751367320476e+16,
      "budget_used_percent": 33.31275136732047
    },
    {
      "type": "training",
      "description": "Training step 1403",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:55",
      "total_flops_so_far": 3.3336490239456156e+16,
      "budget_used_percent": 33.336490239456154
    },
    {
      "type": "training",
      "description": "Training step 1404",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:56",
      "total_flops_so_far": 3.3360229111591836e+16,
      "budget_used_percent": 33.360229111591835
    },
    {
      "type": "training",
      "description": "Training step 1405",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:57",
      "total_flops_so_far": 3.3383967983727516e+16,
      "budget_used_percent": 33.383967983727516
    },
    {
      "type": "training",
      "description": "Training step 1406",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:02:59",
      "total_flops_so_far": 3.3407706855863196e+16,
      "budget_used_percent": 33.407706855863196
    },
    {
      "type": "training",
      "description": "Training step 1407",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:00",
      "total_flops_so_far": 3.3431445727998876e+16,
      "budget_used_percent": 33.43144572799888
    },
    {
      "type": "training",
      "description": "Training step 1408",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:01",
      "total_flops_so_far": 3.3455184600134556e+16,
      "budget_used_percent": 33.45518460013456
    },
    {
      "type": "training",
      "description": "Training step 1409",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:03",
      "total_flops_so_far": 3.3478923472270236e+16,
      "budget_used_percent": 33.47892347227023
    },
    {
      "type": "training",
      "description": "Training step 1410",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:04",
      "total_flops_so_far": 3.3502662344405916e+16,
      "budget_used_percent": 33.50266234440591
    },
    {
      "type": "training",
      "description": "Training step 1411",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:05",
      "total_flops_so_far": 3.3526401216541596e+16,
      "budget_used_percent": 33.52640121654159
    },
    {
      "type": "training",
      "description": "Training step 1412",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:07",
      "total_flops_so_far": 3.3550140088677276e+16,
      "budget_used_percent": 33.550140088677274
    },
    {
      "type": "training",
      "description": "Training step 1413",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:08",
      "total_flops_so_far": 3.3573878960812956e+16,
      "budget_used_percent": 33.573878960812955
    },
    {
      "type": "training",
      "description": "Training step 1414",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:09",
      "total_flops_so_far": 3.3597617832948636e+16,
      "budget_used_percent": 33.597617832948636
    },
    {
      "type": "training",
      "description": "Training step 1415",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:10",
      "total_flops_so_far": 3.3621356705084316e+16,
      "budget_used_percent": 33.62135670508432
    },
    {
      "type": "training",
      "description": "Training step 1416",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:12",
      "total_flops_so_far": 3.3645095577219996e+16,
      "budget_used_percent": 33.64509557722
    },
    {
      "type": "training",
      "description": "Training step 1417",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:13",
      "total_flops_so_far": 3.3668834449355676e+16,
      "budget_used_percent": 33.66883444935568
    },
    {
      "type": "training",
      "description": "Training step 1418",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:14",
      "total_flops_so_far": 3.3692573321491356e+16,
      "budget_used_percent": 33.69257332149135
    },
    {
      "type": "training",
      "description": "Training step 1419",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:16",
      "total_flops_so_far": 3.3716312193627036e+16,
      "budget_used_percent": 33.71631219362703
    },
    {
      "type": "training",
      "description": "Training step 1420",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:17",
      "total_flops_so_far": 3.3740051065762716e+16,
      "budget_used_percent": 33.740051065762714
    },
    {
      "type": "training",
      "description": "Training step 1421",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:18",
      "total_flops_so_far": 3.3763789937898396e+16,
      "budget_used_percent": 33.763789937898395
    },
    {
      "type": "training",
      "description": "Training step 1422",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:20",
      "total_flops_so_far": 3.3787528810034076e+16,
      "budget_used_percent": 33.787528810034075
    },
    {
      "type": "training",
      "description": "Training step 1423",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:21",
      "total_flops_so_far": 3.3811267682169756e+16,
      "budget_used_percent": 33.811267682169756
    },
    {
      "type": "training",
      "description": "Training step 1424",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:22",
      "total_flops_so_far": 3.3835006554305436e+16,
      "budget_used_percent": 33.83500655430544
    },
    {
      "type": "training",
      "description": "Training step 1425",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:24",
      "total_flops_so_far": 3.3858745426441116e+16,
      "budget_used_percent": 33.85874542644112
    },
    {
      "type": "training",
      "description": "Training step 1426",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:25",
      "total_flops_so_far": 3.3882484298576796e+16,
      "budget_used_percent": 33.8824842985768
    },
    {
      "type": "training",
      "description": "Training step 1427",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:26",
      "total_flops_so_far": 3.3906223170712476e+16,
      "budget_used_percent": 33.90622317071247
    },
    {
      "type": "training",
      "description": "Training step 1428",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:27",
      "total_flops_so_far": 3.3929962042848156e+16,
      "budget_used_percent": 33.92996204284815
    },
    {
      "type": "training",
      "description": "Training step 1429",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:29",
      "total_flops_so_far": 3.3953700914983836e+16,
      "budget_used_percent": 33.953700914983834
    },
    {
      "type": "training",
      "description": "Training step 1430",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:30",
      "total_flops_so_far": 3.3977439787119516e+16,
      "budget_used_percent": 33.977439787119515
    },
    {
      "type": "training",
      "description": "Training step 1431",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:31",
      "total_flops_so_far": 3.4001178659255196e+16,
      "budget_used_percent": 34.001178659255196
    },
    {
      "type": "training",
      "description": "Training step 1432",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:33",
      "total_flops_so_far": 3.4024917531390876e+16,
      "budget_used_percent": 34.02491753139088
    },
    {
      "type": "training",
      "description": "Training step 1433",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:34",
      "total_flops_so_far": 3.4048656403526556e+16,
      "budget_used_percent": 34.04865640352656
    },
    {
      "type": "training",
      "description": "Training step 1434",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:35",
      "total_flops_so_far": 3.4072395275662236e+16,
      "budget_used_percent": 34.07239527566224
    },
    {
      "type": "training",
      "description": "Training step 1435",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:37",
      "total_flops_so_far": 3.4096134147797916e+16,
      "budget_used_percent": 34.09613414779792
    },
    {
      "type": "training",
      "description": "Training step 1436",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:38",
      "total_flops_so_far": 3.4119873019933596e+16,
      "budget_used_percent": 34.11987301993359
    },
    {
      "type": "training",
      "description": "Training step 1437",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:39",
      "total_flops_so_far": 3.4143611892069276e+16,
      "budget_used_percent": 34.14361189206927
    },
    {
      "type": "training",
      "description": "Training step 1438",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:41",
      "total_flops_so_far": 3.4167350764204956e+16,
      "budget_used_percent": 34.167350764204954
    },
    {
      "type": "training",
      "description": "Training step 1439",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:42",
      "total_flops_so_far": 3.4191089636340636e+16,
      "budget_used_percent": 34.191089636340635
    },
    {
      "type": "training",
      "description": "Training step 1440",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:43",
      "total_flops_so_far": 3.4214828508476316e+16,
      "budget_used_percent": 34.214828508476316
    },
    {
      "type": "training",
      "description": "Training step 1441",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:45",
      "total_flops_so_far": 3.4238567380611996e+16,
      "budget_used_percent": 34.238567380612
    },
    {
      "type": "training",
      "description": "Training step 1442",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:46",
      "total_flops_so_far": 3.4262306252747676e+16,
      "budget_used_percent": 34.26230625274768
    },
    {
      "type": "training",
      "description": "Training step 1443",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:47",
      "total_flops_so_far": 3.4286045124883356e+16,
      "budget_used_percent": 34.28604512488336
    },
    {
      "type": "training",
      "description": "Training step 1444",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:48",
      "total_flops_so_far": 3.4309783997019036e+16,
      "budget_used_percent": 34.30978399701903
    },
    {
      "type": "training",
      "description": "Training step 1445",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:50",
      "total_flops_so_far": 3.4333522869154716e+16,
      "budget_used_percent": 34.33352286915471
    },
    {
      "type": "training",
      "description": "Training step 1446",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:51",
      "total_flops_so_far": 3.4357261741290396e+16,
      "budget_used_percent": 34.357261741290394
    },
    {
      "type": "training",
      "description": "Training step 1447",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:52",
      "total_flops_so_far": 3.4381000613426076e+16,
      "budget_used_percent": 34.381000613426075
    },
    {
      "type": "training",
      "description": "Training step 1448",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:54",
      "total_flops_so_far": 3.4404739485561756e+16,
      "budget_used_percent": 34.404739485561755
    },
    {
      "type": "training",
      "description": "Training step 1449",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:55",
      "total_flops_so_far": 3.4428478357697436e+16,
      "budget_used_percent": 34.428478357697436
    },
    {
      "type": "training",
      "description": "Training step 1450",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:56",
      "total_flops_so_far": 3.4452217229833116e+16,
      "budget_used_percent": 34.45221722983312
    },
    {
      "type": "training",
      "description": "Training step 1451",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:58",
      "total_flops_so_far": 3.4475956101968796e+16,
      "budget_used_percent": 34.4759561019688
    },
    {
      "type": "training",
      "description": "Training step 1452",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:58",
      "total_flops_so_far": 3.4499694974104476e+16,
      "budget_used_percent": 34.49969497410448
    },
    {
      "type": "training",
      "description": "Training step 1453",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:03:59",
      "total_flops_so_far": 3.4523433846240156e+16,
      "budget_used_percent": 34.52343384624015
    },
    {
      "type": "training",
      "description": "Training step 1454",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:01",
      "total_flops_so_far": 3.4547172718375836e+16,
      "budget_used_percent": 34.54717271837583
    },
    {
      "type": "training",
      "description": "Training step 1455",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:02",
      "total_flops_so_far": 3.4570911590511516e+16,
      "budget_used_percent": 34.570911590511514
    },
    {
      "type": "training",
      "description": "Training step 1456",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:03",
      "total_flops_so_far": 3.4594650462647196e+16,
      "budget_used_percent": 34.594650462647195
    },
    {
      "type": "training",
      "description": "Training step 1457",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:05",
      "total_flops_so_far": 3.4618389334782876e+16,
      "budget_used_percent": 34.618389334782876
    },
    {
      "type": "training",
      "description": "Training step 1458",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:06",
      "total_flops_so_far": 3.4642128206918556e+16,
      "budget_used_percent": 34.64212820691856
    },
    {
      "type": "training",
      "description": "Training step 1459",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:07",
      "total_flops_so_far": 3.4665867079054236e+16,
      "budget_used_percent": 34.66586707905424
    },
    {
      "type": "training",
      "description": "Training step 1460",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:09",
      "total_flops_so_far": 3.4689605951189916e+16,
      "budget_used_percent": 34.68960595118992
    },
    {
      "type": "training",
      "description": "Training step 1461",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:10",
      "total_flops_so_far": 3.4713344823325596e+16,
      "budget_used_percent": 34.7133448233256
    },
    {
      "type": "training",
      "description": "Training step 1462",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:11",
      "total_flops_so_far": 3.4737083695461276e+16,
      "budget_used_percent": 34.73708369546127
    },
    {
      "type": "training",
      "description": "Training step 1463",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:12",
      "total_flops_so_far": 3.4760822567596956e+16,
      "budget_used_percent": 34.76082256759695
    },
    {
      "type": "training",
      "description": "Training step 1464",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:14",
      "total_flops_so_far": 3.4784561439732636e+16,
      "budget_used_percent": 34.784561439732634
    },
    {
      "type": "training",
      "description": "Training step 1465",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:15",
      "total_flops_so_far": 3.4808300311868316e+16,
      "budget_used_percent": 34.808300311868315
    },
    {
      "type": "training",
      "description": "Training step 1466",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:16",
      "total_flops_so_far": 3.4832039184003996e+16,
      "budget_used_percent": 34.832039184003996
    },
    {
      "type": "training",
      "description": "Training step 1467",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:18",
      "total_flops_so_far": 3.4855778056139676e+16,
      "budget_used_percent": 34.85577805613968
    },
    {
      "type": "training",
      "description": "Training step 1468",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:19",
      "total_flops_so_far": 3.4879516928275356e+16,
      "budget_used_percent": 34.87951692827536
    },
    {
      "type": "training",
      "description": "Training step 1469",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:20",
      "total_flops_so_far": 3.4903255800411036e+16,
      "budget_used_percent": 34.90325580041104
    },
    {
      "type": "training",
      "description": "Training step 1470",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:22",
      "total_flops_so_far": 3.4926994672546716e+16,
      "budget_used_percent": 34.92699467254672
    },
    {
      "type": "training",
      "description": "Training step 1471",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:23",
      "total_flops_so_far": 3.4950733544682396e+16,
      "budget_used_percent": 34.95073354468239
    },
    {
      "type": "training",
      "description": "Training step 1472",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:24",
      "total_flops_so_far": 3.4974472416818076e+16,
      "budget_used_percent": 34.974472416818074
    },
    {
      "type": "training",
      "description": "Training step 1473",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:26",
      "total_flops_so_far": 3.4998211288953756e+16,
      "budget_used_percent": 34.998211288953755
    },
    {
      "type": "training",
      "description": "Training step 1474",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:27",
      "total_flops_so_far": 3.5021950161089436e+16,
      "budget_used_percent": 35.021950161089435
    },
    {
      "type": "training",
      "description": "Training step 1475",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:28",
      "total_flops_so_far": 3.5045689033225116e+16,
      "budget_used_percent": 35.045689033225116
    },
    {
      "type": "training",
      "description": "Training step 1476",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:29",
      "total_flops_so_far": 3.5069427905360796e+16,
      "budget_used_percent": 35.0694279053608
    },
    {
      "type": "training",
      "description": "Training step 1477",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:31",
      "total_flops_so_far": 3.5093166777496476e+16,
      "budget_used_percent": 35.09316677749648
    },
    {
      "type": "training",
      "description": "Training step 1478",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:32",
      "total_flops_so_far": 3.5116905649632156e+16,
      "budget_used_percent": 35.11690564963216
    },
    {
      "type": "training",
      "description": "Training step 1479",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:33",
      "total_flops_so_far": 3.5140644521767836e+16,
      "budget_used_percent": 35.14064452176784
    },
    {
      "type": "training",
      "description": "Training step 1480",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:35",
      "total_flops_so_far": 3.5164383393903516e+16,
      "budget_used_percent": 35.16438339390351
    },
    {
      "type": "training",
      "description": "Training step 1481",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:36",
      "total_flops_so_far": 3.5188122266039196e+16,
      "budget_used_percent": 35.188122266039194
    },
    {
      "type": "training",
      "description": "Training step 1482",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:37",
      "total_flops_so_far": 3.5211861138174876e+16,
      "budget_used_percent": 35.211861138174875
    },
    {
      "type": "training",
      "description": "Training step 1483",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:39",
      "total_flops_so_far": 3.5235600010310556e+16,
      "budget_used_percent": 35.235600010310556
    },
    {
      "type": "training",
      "description": "Training step 1484",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:40",
      "total_flops_so_far": 3.5259338882446236e+16,
      "budget_used_percent": 35.25933888244624
    },
    {
      "type": "training",
      "description": "Training step 1485",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:41",
      "total_flops_so_far": 3.5283077754581916e+16,
      "budget_used_percent": 35.28307775458192
    },
    {
      "type": "training",
      "description": "Training step 1486",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:43",
      "total_flops_so_far": 3.5306816626717596e+16,
      "budget_used_percent": 35.3068166267176
    },
    {
      "type": "training",
      "description": "Training step 1487",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:44",
      "total_flops_so_far": 3.5330555498853276e+16,
      "budget_used_percent": 35.33055549885328
    },
    {
      "type": "training",
      "description": "Training step 1488",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:45",
      "total_flops_so_far": 3.5354294370988956e+16,
      "budget_used_percent": 35.35429437098895
    },
    {
      "type": "training",
      "description": "Training step 1489",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:47",
      "total_flops_so_far": 3.5378033243124636e+16,
      "budget_used_percent": 35.378033243124634
    },
    {
      "type": "training",
      "description": "Training step 1490",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:48",
      "total_flops_so_far": 3.5401772115260316e+16,
      "budget_used_percent": 35.401772115260314
    },
    {
      "type": "training",
      "description": "Training step 1491",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:49",
      "total_flops_so_far": 3.5425510987395996e+16,
      "budget_used_percent": 35.425510987395995
    },
    {
      "type": "training",
      "description": "Training step 1492",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:50",
      "total_flops_so_far": 3.5449249859531676e+16,
      "budget_used_percent": 35.449249859531676
    },
    {
      "type": "training",
      "description": "Training step 1493",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:52",
      "total_flops_so_far": 3.5472988731667356e+16,
      "budget_used_percent": 35.47298873166736
    },
    {
      "type": "training",
      "description": "Training step 1494",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:53",
      "total_flops_so_far": 3.5496727603803036e+16,
      "budget_used_percent": 35.49672760380304
    },
    {
      "type": "training",
      "description": "Training step 1495",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:54",
      "total_flops_so_far": 3.5520466475938716e+16,
      "budget_used_percent": 35.52046647593872
    },
    {
      "type": "training",
      "description": "Training step 1496",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:56",
      "total_flops_so_far": 3.5544205348074396e+16,
      "budget_used_percent": 35.5442053480744
    },
    {
      "type": "training",
      "description": "Training step 1497",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:57",
      "total_flops_so_far": 3.5567944220210076e+16,
      "budget_used_percent": 35.56794422021007
    },
    {
      "type": "training",
      "description": "Training step 1498",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:04:58",
      "total_flops_so_far": 3.5591683092345756e+16,
      "budget_used_percent": 35.591683092345754
    },
    {
      "type": "training",
      "description": "Training step 1499",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:00",
      "total_flops_so_far": 3.5615421964481436e+16,
      "budget_used_percent": 35.615421964481435
    },
    {
      "type": "training",
      "description": "Training step 1500",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:01",
      "total_flops_so_far": 3.5639160836617116e+16,
      "budget_used_percent": 35.639160836617116
    },
    {
      "type": "training",
      "description": "Training step 1501",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:02",
      "total_flops_so_far": 3.5662899708752796e+16,
      "budget_used_percent": 35.662899708752796
    },
    {
      "type": "training",
      "description": "Training step 1502",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:04",
      "total_flops_so_far": 3.5686638580888476e+16,
      "budget_used_percent": 35.68663858088848
    },
    {
      "type": "training",
      "description": "Training step 1503",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:05",
      "total_flops_so_far": 3.5710377453024156e+16,
      "budget_used_percent": 35.71037745302416
    },
    {
      "type": "training",
      "description": "Training step 1504",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:06",
      "total_flops_so_far": 3.5734116325159836e+16,
      "budget_used_percent": 35.73411632515984
    },
    {
      "type": "training",
      "description": "Training step 1505",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:08",
      "total_flops_so_far": 3.5757855197295516e+16,
      "budget_used_percent": 35.75785519729552
    },
    {
      "type": "training",
      "description": "Training step 1506",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:09",
      "total_flops_so_far": 3.5781594069431196e+16,
      "budget_used_percent": 35.78159406943119
    },
    {
      "type": "training",
      "description": "Training step 1507",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:10",
      "total_flops_so_far": 3.5805332941566876e+16,
      "budget_used_percent": 35.805332941566874
    },
    {
      "type": "training",
      "description": "Training step 1508",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:12",
      "total_flops_so_far": 3.5829071813702556e+16,
      "budget_used_percent": 35.829071813702555
    },
    {
      "type": "training",
      "description": "Training step 1509",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:13",
      "total_flops_so_far": 3.5852810685838236e+16,
      "budget_used_percent": 35.852810685838236
    },
    {
      "type": "training",
      "description": "Training step 1510",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:14",
      "total_flops_so_far": 3.5876549557973916e+16,
      "budget_used_percent": 35.87654955797392
    },
    {
      "type": "training",
      "description": "Training step 1511",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:15",
      "total_flops_so_far": 3.5900288430109596e+16,
      "budget_used_percent": 35.9002884301096
    },
    {
      "type": "training",
      "description": "Training step 1512",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:17",
      "total_flops_so_far": 3.5924027302245276e+16,
      "budget_used_percent": 35.92402730224528
    },
    {
      "type": "training",
      "description": "Training step 1513",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:18",
      "total_flops_so_far": 3.5947766174380956e+16,
      "budget_used_percent": 35.94776617438096
    },
    {
      "type": "training",
      "description": "Training step 1514",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:19",
      "total_flops_so_far": 3.5971505046516636e+16,
      "budget_used_percent": 35.97150504651664
    },
    {
      "type": "training",
      "description": "Training step 1515",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:21",
      "total_flops_so_far": 3.5995243918652316e+16,
      "budget_used_percent": 35.995243918652314
    },
    {
      "type": "training",
      "description": "Training step 1516",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:22",
      "total_flops_so_far": 3.6018982790787996e+16,
      "budget_used_percent": 36.018982790787994
    },
    {
      "type": "training",
      "description": "Training step 1517",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:23",
      "total_flops_so_far": 3.604272166292368e+16,
      "budget_used_percent": 36.04272166292368
    },
    {
      "type": "training",
      "description": "Training step 1518",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:25",
      "total_flops_so_far": 3.606646053505936e+16,
      "budget_used_percent": 36.066460535059356
    },
    {
      "type": "training",
      "description": "Training step 1519",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:26",
      "total_flops_so_far": 3.609019940719504e+16,
      "budget_used_percent": 36.09019940719504
    },
    {
      "type": "training",
      "description": "Training step 1520",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:27",
      "total_flops_so_far": 3.611393827933072e+16,
      "budget_used_percent": 36.11393827933072
    },
    {
      "type": "training",
      "description": "Training step 1521",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:29",
      "total_flops_so_far": 3.61376771514664e+16,
      "budget_used_percent": 36.1376771514664
    },
    {
      "type": "training",
      "description": "Training step 1522",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:30",
      "total_flops_so_far": 3.616141602360208e+16,
      "budget_used_percent": 36.16141602360208
    },
    {
      "type": "training",
      "description": "Training step 1523",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:31",
      "total_flops_so_far": 3.618515489573776e+16,
      "budget_used_percent": 36.18515489573776
    },
    {
      "type": "training",
      "description": "Training step 1524",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:33",
      "total_flops_so_far": 3.620889376787344e+16,
      "budget_used_percent": 36.20889376787344
    },
    {
      "type": "training",
      "description": "Training step 1525",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:34",
      "total_flops_so_far": 3.623263264000912e+16,
      "budget_used_percent": 36.23263264000912
    },
    {
      "type": "training",
      "description": "Training step 1526",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:35",
      "total_flops_so_far": 3.62563715121448e+16,
      "budget_used_percent": 36.2563715121448
    },
    {
      "type": "training",
      "description": "Training step 1527",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:36",
      "total_flops_so_far": 3.628011038428048e+16,
      "budget_used_percent": 36.280110384280476
    },
    {
      "type": "training",
      "description": "Training step 1528",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:38",
      "total_flops_so_far": 3.630384925641616e+16,
      "budget_used_percent": 36.30384925641616
    },
    {
      "type": "training",
      "description": "Training step 1529",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:39",
      "total_flops_so_far": 3.632758812855184e+16,
      "budget_used_percent": 36.32758812855184
    },
    {
      "type": "training",
      "description": "Training step 1530",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:40",
      "total_flops_so_far": 3.635132700068752e+16,
      "budget_used_percent": 36.35132700068752
    },
    {
      "type": "training",
      "description": "Training step 1531",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:42",
      "total_flops_so_far": 3.63750658728232e+16,
      "budget_used_percent": 36.3750658728232
    },
    {
      "type": "training",
      "description": "Training step 1532",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:43",
      "total_flops_so_far": 3.639880474495888e+16,
      "budget_used_percent": 36.39880474495888
    },
    {
      "type": "training",
      "description": "Training step 1533",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:44",
      "total_flops_so_far": 3.642254361709456e+16,
      "budget_used_percent": 36.42254361709456
    },
    {
      "type": "training",
      "description": "Training step 1534",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:46",
      "total_flops_so_far": 3.644628248923024e+16,
      "budget_used_percent": 36.44628248923024
    },
    {
      "type": "training",
      "description": "Training step 1535",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:47",
      "total_flops_so_far": 3.647002136136592e+16,
      "budget_used_percent": 36.470021361365916
    },
    {
      "type": "training",
      "description": "Training step 1536",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:48",
      "total_flops_so_far": 3.64937602335016e+16,
      "budget_used_percent": 36.4937602335016
    },
    {
      "type": "training",
      "description": "Training step 1537",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:50",
      "total_flops_so_far": 3.651749910563728e+16,
      "budget_used_percent": 36.51749910563728
    },
    {
      "type": "training",
      "description": "Training step 1538",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:51",
      "total_flops_so_far": 3.654123797777296e+16,
      "budget_used_percent": 36.54123797777296
    },
    {
      "type": "training",
      "description": "Training step 1539",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:52",
      "total_flops_so_far": 3.656497684990864e+16,
      "budget_used_percent": 36.56497684990864
    },
    {
      "type": "training",
      "description": "Training step 1540",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:54",
      "total_flops_so_far": 3.658871572204432e+16,
      "budget_used_percent": 36.58871572204432
    },
    {
      "type": "training",
      "description": "Training step 1541",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:55",
      "total_flops_so_far": 3.661245459418e+16,
      "budget_used_percent": 36.61245459418
    },
    {
      "type": "training",
      "description": "Training step 1542",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:56",
      "total_flops_so_far": 3.663619346631568e+16,
      "budget_used_percent": 36.63619346631568
    },
    {
      "type": "training",
      "description": "Training step 1543",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:57",
      "total_flops_so_far": 3.665993233845136e+16,
      "budget_used_percent": 36.65993233845136
    },
    {
      "type": "training",
      "description": "Training step 1544",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:05:59",
      "total_flops_so_far": 3.668367121058704e+16,
      "budget_used_percent": 36.683671210587036
    },
    {
      "type": "training",
      "description": "Training step 1545",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:00",
      "total_flops_so_far": 3.670741008272272e+16,
      "budget_used_percent": 36.70741008272272
    },
    {
      "type": "training",
      "description": "Training step 1546",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:01",
      "total_flops_so_far": 3.67311489548584e+16,
      "budget_used_percent": 36.7311489548584
    },
    {
      "type": "training",
      "description": "Training step 1547",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:03",
      "total_flops_so_far": 3.675488782699408e+16,
      "budget_used_percent": 36.75488782699408
    },
    {
      "type": "training",
      "description": "Training step 1548",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:04",
      "total_flops_so_far": 3.677862669912976e+16,
      "budget_used_percent": 36.77862669912976
    },
    {
      "type": "training",
      "description": "Training step 1549",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:05",
      "total_flops_so_far": 3.680236557126544e+16,
      "budget_used_percent": 36.80236557126544
    },
    {
      "type": "training",
      "description": "Training step 1550",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:07",
      "total_flops_so_far": 3.682610444340112e+16,
      "budget_used_percent": 36.82610444340112
    },
    {
      "type": "training",
      "description": "Training step 1551",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:08",
      "total_flops_so_far": 3.68498433155368e+16,
      "budget_used_percent": 36.8498433155368
    },
    {
      "type": "training",
      "description": "Training step 1552",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:09",
      "total_flops_so_far": 3.687358218767248e+16,
      "budget_used_percent": 36.87358218767248
    },
    {
      "type": "training",
      "description": "Training step 1553",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:11",
      "total_flops_so_far": 3.689732105980816e+16,
      "budget_used_percent": 36.89732105980816
    },
    {
      "type": "training",
      "description": "Training step 1554",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:12",
      "total_flops_so_far": 3.692105993194384e+16,
      "budget_used_percent": 36.92105993194384
    },
    {
      "type": "training",
      "description": "Training step 1555",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:13",
      "total_flops_so_far": 3.694479880407952e+16,
      "budget_used_percent": 36.94479880407952
    },
    {
      "type": "training",
      "description": "Training step 1556",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:15",
      "total_flops_so_far": 3.69685376762152e+16,
      "budget_used_percent": 36.9685376762152
    },
    {
      "type": "training",
      "description": "Training step 1557",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:16",
      "total_flops_so_far": 3.699227654835088e+16,
      "budget_used_percent": 36.99227654835088
    },
    {
      "type": "training",
      "description": "Training step 1558",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:17",
      "total_flops_so_far": 3.701601542048656e+16,
      "budget_used_percent": 37.01601542048656
    },
    {
      "type": "training",
      "description": "Training step 1559",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:19",
      "total_flops_so_far": 3.703975429262224e+16,
      "budget_used_percent": 37.03975429262224
    },
    {
      "type": "training",
      "description": "Training step 1560",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:20",
      "total_flops_so_far": 3.706349316475792e+16,
      "budget_used_percent": 37.06349316475792
    },
    {
      "type": "training",
      "description": "Training step 1561",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:21",
      "total_flops_so_far": 3.70872320368936e+16,
      "budget_used_percent": 37.0872320368936
    },
    {
      "type": "training",
      "description": "Training step 1562",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:22",
      "total_flops_so_far": 3.711097090902928e+16,
      "budget_used_percent": 37.11097090902928
    },
    {
      "type": "training",
      "description": "Training step 1563",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:24",
      "total_flops_so_far": 3.713470978116496e+16,
      "budget_used_percent": 37.13470978116496
    },
    {
      "type": "training",
      "description": "Training step 1564",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:25",
      "total_flops_so_far": 3.715844865330064e+16,
      "budget_used_percent": 37.15844865330064
    },
    {
      "type": "training",
      "description": "Training step 1565",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:26",
      "total_flops_so_far": 3.718218752543632e+16,
      "budget_used_percent": 37.18218752543632
    },
    {
      "type": "training",
      "description": "Training step 1566",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:28",
      "total_flops_so_far": 3.7205926397572e+16,
      "budget_used_percent": 37.205926397572
    },
    {
      "type": "training",
      "description": "Training step 1567",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:29",
      "total_flops_so_far": 3.722966526970768e+16,
      "budget_used_percent": 37.22966526970768
    },
    {
      "type": "training",
      "description": "Training step 1568",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:30",
      "total_flops_so_far": 3.725340414184336e+16,
      "budget_used_percent": 37.25340414184336
    },
    {
      "type": "training",
      "description": "Training step 1569",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:32",
      "total_flops_so_far": 3.727714301397904e+16,
      "budget_used_percent": 37.27714301397904
    },
    {
      "type": "training",
      "description": "Training step 1570",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:33",
      "total_flops_so_far": 3.730088188611472e+16,
      "budget_used_percent": 37.300881886114716
    },
    {
      "type": "training",
      "description": "Training step 1571",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:34",
      "total_flops_so_far": 3.73246207582504e+16,
      "budget_used_percent": 37.3246207582504
    },
    {
      "type": "training",
      "description": "Training step 1572",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:36",
      "total_flops_so_far": 3.734835963038608e+16,
      "budget_used_percent": 37.34835963038608
    },
    {
      "type": "training",
      "description": "Training step 1573",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:37",
      "total_flops_so_far": 3.737209850252176e+16,
      "budget_used_percent": 37.37209850252176
    },
    {
      "type": "training",
      "description": "Training step 1574",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:38",
      "total_flops_so_far": 3.739583737465744e+16,
      "budget_used_percent": 37.39583737465744
    },
    {
      "type": "training",
      "description": "Training step 1575",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:40",
      "total_flops_so_far": 3.741957624679312e+16,
      "budget_used_percent": 37.41957624679312
    },
    {
      "type": "training",
      "description": "Training step 1576",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:41",
      "total_flops_so_far": 3.74433151189288e+16,
      "budget_used_percent": 37.4433151189288
    },
    {
      "type": "training",
      "description": "Training step 1577",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:42",
      "total_flops_so_far": 3.746705399106448e+16,
      "budget_used_percent": 37.46705399106448
    },
    {
      "type": "training",
      "description": "Training step 1578",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:43",
      "total_flops_so_far": 3.749079286320016e+16,
      "budget_used_percent": 37.49079286320016
    },
    {
      "type": "training",
      "description": "Training step 1579",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:45",
      "total_flops_so_far": 3.751453173533584e+16,
      "budget_used_percent": 37.51453173533584
    },
    {
      "type": "training",
      "description": "Training step 1580",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:46",
      "total_flops_so_far": 3.753827060747152e+16,
      "budget_used_percent": 37.53827060747152
    },
    {
      "type": "training",
      "description": "Training step 1581",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:47",
      "total_flops_so_far": 3.75620094796072e+16,
      "budget_used_percent": 37.5620094796072
    },
    {
      "type": "training",
      "description": "Training step 1582",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:49",
      "total_flops_so_far": 3.758574835174288e+16,
      "budget_used_percent": 37.58574835174288
    },
    {
      "type": "training",
      "description": "Training step 1583",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:50",
      "total_flops_so_far": 3.760948722387856e+16,
      "budget_used_percent": 37.60948722387856
    },
    {
      "type": "training",
      "description": "Training step 1584",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:50",
      "total_flops_so_far": 3.763322609601424e+16,
      "budget_used_percent": 37.63322609601424
    },
    {
      "type": "training",
      "description": "Training step 1585",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:52",
      "total_flops_so_far": 3.765696496814992e+16,
      "budget_used_percent": 37.65696496814992
    },
    {
      "type": "training",
      "description": "Training step 1586",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:53",
      "total_flops_so_far": 3.76807038402856e+16,
      "budget_used_percent": 37.6807038402856
    },
    {
      "type": "training",
      "description": "Training step 1587",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:54",
      "total_flops_so_far": 3.770444271242128e+16,
      "budget_used_percent": 37.70444271242128
    },
    {
      "type": "training",
      "description": "Training step 1588",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:56",
      "total_flops_so_far": 3.772818158455696e+16,
      "budget_used_percent": 37.72818158455696
    },
    {
      "type": "training",
      "description": "Training step 1589",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:57",
      "total_flops_so_far": 3.775192045669264e+16,
      "budget_used_percent": 37.75192045669264
    },
    {
      "type": "training",
      "description": "Training step 1590",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:06:58",
      "total_flops_so_far": 3.777565932882832e+16,
      "budget_used_percent": 37.77565932882832
    },
    {
      "type": "training",
      "description": "Training step 1591",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:00",
      "total_flops_so_far": 3.7799398200964e+16,
      "budget_used_percent": 37.799398200964
    },
    {
      "type": "training",
      "description": "Training step 1592",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:01",
      "total_flops_so_far": 3.782313707309968e+16,
      "budget_used_percent": 37.82313707309968
    },
    {
      "type": "training",
      "description": "Training step 1593",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:02",
      "total_flops_so_far": 3.784687594523536e+16,
      "budget_used_percent": 37.84687594523536
    },
    {
      "type": "training",
      "description": "Training step 1594",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:04",
      "total_flops_so_far": 3.787061481737104e+16,
      "budget_used_percent": 37.87061481737104
    },
    {
      "type": "training",
      "description": "Training step 1595",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:05",
      "total_flops_so_far": 3.789435368950672e+16,
      "budget_used_percent": 37.89435368950672
    },
    {
      "type": "training",
      "description": "Training step 1596",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:06",
      "total_flops_so_far": 3.79180925616424e+16,
      "budget_used_percent": 37.9180925616424
    },
    {
      "type": "training",
      "description": "Training step 1597",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:09",
      "total_flops_so_far": 3.794183143377808e+16,
      "budget_used_percent": 37.94183143377808
    },
    {
      "type": "training",
      "description": "Training step 1598",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:10",
      "total_flops_so_far": 3.796557030591376e+16,
      "budget_used_percent": 37.96557030591376
    },
    {
      "type": "training",
      "description": "Training step 1599",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:11",
      "total_flops_so_far": 3.798930917804944e+16,
      "budget_used_percent": 37.98930917804944
    },
    {
      "type": "training",
      "description": "Training step 1600",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:13",
      "total_flops_so_far": 3.801304805018512e+16,
      "budget_used_percent": 38.01304805018512
    },
    {
      "type": "training",
      "description": "Training step 1601",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:14",
      "total_flops_so_far": 3.80367869223208e+16,
      "budget_used_percent": 38.0367869223208
    },
    {
      "type": "training",
      "description": "Training step 1602",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:15",
      "total_flops_so_far": 3.806052579445648e+16,
      "budget_used_percent": 38.06052579445648
    },
    {
      "type": "training",
      "description": "Training step 1603",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:17",
      "total_flops_so_far": 3.808426466659216e+16,
      "budget_used_percent": 38.08426466659216
    },
    {
      "type": "training",
      "description": "Training step 1604",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:18",
      "total_flops_so_far": 3.810800353872784e+16,
      "budget_used_percent": 38.10800353872784
    },
    {
      "type": "training",
      "description": "Training step 1605",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:19",
      "total_flops_so_far": 3.813174241086352e+16,
      "budget_used_percent": 38.13174241086352
    },
    {
      "type": "training",
      "description": "Training step 1606",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:21",
      "total_flops_so_far": 3.81554812829992e+16,
      "budget_used_percent": 38.1554812829992
    },
    {
      "type": "training",
      "description": "Training step 1607",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:22",
      "total_flops_so_far": 3.817922015513488e+16,
      "budget_used_percent": 38.17922015513488
    },
    {
      "type": "training",
      "description": "Training step 1608",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:23",
      "total_flops_so_far": 3.820295902727056e+16,
      "budget_used_percent": 38.20295902727056
    },
    {
      "type": "training",
      "description": "Training step 1609",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:24",
      "total_flops_so_far": 3.822669789940624e+16,
      "budget_used_percent": 38.22669789940624
    },
    {
      "type": "training",
      "description": "Training step 1610",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:26",
      "total_flops_so_far": 3.825043677154192e+16,
      "budget_used_percent": 38.25043677154192
    },
    {
      "type": "training",
      "description": "Training step 1611",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:27",
      "total_flops_so_far": 3.82741756436776e+16,
      "budget_used_percent": 38.2741756436776
    },
    {
      "type": "training",
      "description": "Training step 1612",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:28",
      "total_flops_so_far": 3.829791451581328e+16,
      "budget_used_percent": 38.29791451581328
    },
    {
      "type": "training",
      "description": "Training step 1613",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:30",
      "total_flops_so_far": 3.832165338794896e+16,
      "budget_used_percent": 38.32165338794896
    },
    {
      "type": "training",
      "description": "Training step 1614",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:31",
      "total_flops_so_far": 3.834539226008464e+16,
      "budget_used_percent": 38.34539226008464
    },
    {
      "type": "training",
      "description": "Training step 1615",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:32",
      "total_flops_so_far": 3.836913113222032e+16,
      "budget_used_percent": 38.36913113222032
    },
    {
      "type": "training",
      "description": "Training step 1616",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:34",
      "total_flops_so_far": 3.8392870004356e+16,
      "budget_used_percent": 38.392870004356
    },
    {
      "type": "training",
      "description": "Training step 1617",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:35",
      "total_flops_so_far": 3.841660887649168e+16,
      "budget_used_percent": 38.41660887649168
    },
    {
      "type": "training",
      "description": "Training step 1618",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:36",
      "total_flops_so_far": 3.844034774862736e+16,
      "budget_used_percent": 38.44034774862736
    },
    {
      "type": "training",
      "description": "Training step 1619",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:38",
      "total_flops_so_far": 3.846408662076304e+16,
      "budget_used_percent": 38.46408662076304
    },
    {
      "type": "training",
      "description": "Training step 1620",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:39",
      "total_flops_so_far": 3.848782549289872e+16,
      "budget_used_percent": 38.48782549289872
    },
    {
      "type": "training",
      "description": "Training step 1621",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:40",
      "total_flops_so_far": 3.85115643650344e+16,
      "budget_used_percent": 38.5115643650344
    },
    {
      "type": "training",
      "description": "Training step 1622",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:42",
      "total_flops_so_far": 3.853530323717008e+16,
      "budget_used_percent": 38.53530323717008
    },
    {
      "type": "training",
      "description": "Training step 1623",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:43",
      "total_flops_so_far": 3.855904210930576e+16,
      "budget_used_percent": 38.55904210930576
    },
    {
      "type": "training",
      "description": "Training step 1624",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:44",
      "total_flops_so_far": 3.858278098144144e+16,
      "budget_used_percent": 38.58278098144144
    },
    {
      "type": "training",
      "description": "Training step 1625",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:46",
      "total_flops_so_far": 3.860651985357712e+16,
      "budget_used_percent": 38.60651985357712
    },
    {
      "type": "training",
      "description": "Training step 1626",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:47",
      "total_flops_so_far": 3.86302587257128e+16,
      "budget_used_percent": 38.6302587257128
    },
    {
      "type": "training",
      "description": "Training step 1627",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:48",
      "total_flops_so_far": 3.865399759784848e+16,
      "budget_used_percent": 38.65399759784848
    },
    {
      "type": "training",
      "description": "Training step 1628",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:49",
      "total_flops_so_far": 3.867773646998416e+16,
      "budget_used_percent": 38.67773646998416
    },
    {
      "type": "training",
      "description": "Training step 1629",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:51",
      "total_flops_so_far": 3.870147534211984e+16,
      "budget_used_percent": 38.70147534211984
    },
    {
      "type": "training",
      "description": "Training step 1630",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:52",
      "total_flops_so_far": 3.872521421425552e+16,
      "budget_used_percent": 38.72521421425552
    },
    {
      "type": "training",
      "description": "Training step 1631",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:53",
      "total_flops_so_far": 3.87489530863912e+16,
      "budget_used_percent": 38.748953086391204
    },
    {
      "type": "training",
      "description": "Training step 1632",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:55",
      "total_flops_so_far": 3.877269195852688e+16,
      "budget_used_percent": 38.77269195852688
    },
    {
      "type": "training",
      "description": "Training step 1633",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:56",
      "total_flops_so_far": 3.879643083066256e+16,
      "budget_used_percent": 38.79643083066256
    },
    {
      "type": "training",
      "description": "Training step 1634",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:57",
      "total_flops_so_far": 3.882016970279824e+16,
      "budget_used_percent": 38.82016970279824
    },
    {
      "type": "training",
      "description": "Training step 1635",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:07:59",
      "total_flops_so_far": 3.884390857493392e+16,
      "budget_used_percent": 38.84390857493392
    },
    {
      "type": "training",
      "description": "Training step 1636",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:00",
      "total_flops_so_far": 3.88676474470696e+16,
      "budget_used_percent": 38.8676474470696
    },
    {
      "type": "training",
      "description": "Training step 1637",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:01",
      "total_flops_so_far": 3.889138631920528e+16,
      "budget_used_percent": 38.89138631920528
    },
    {
      "type": "training",
      "description": "Training step 1638",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:03",
      "total_flops_so_far": 3.891512519134096e+16,
      "budget_used_percent": 38.91512519134096
    },
    {
      "type": "training",
      "description": "Training step 1639",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:04",
      "total_flops_so_far": 3.893886406347664e+16,
      "budget_used_percent": 38.93886406347664
    },
    {
      "type": "training",
      "description": "Training step 1640",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:05",
      "total_flops_so_far": 3.896260293561232e+16,
      "budget_used_percent": 38.962602935612324
    },
    {
      "type": "training",
      "description": "Training step 1641",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:07",
      "total_flops_so_far": 3.8986341807748e+16,
      "budget_used_percent": 38.986341807748
    },
    {
      "type": "training",
      "description": "Training step 1642",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:08",
      "total_flops_so_far": 3.901008067988368e+16,
      "budget_used_percent": 39.01008067988368
    },
    {
      "type": "training",
      "description": "Training step 1643",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:09",
      "total_flops_so_far": 3.903381955201936e+16,
      "budget_used_percent": 39.03381955201936
    },
    {
      "type": "training",
      "description": "Training step 1644",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:11",
      "total_flops_so_far": 3.905755842415504e+16,
      "budget_used_percent": 39.05755842415504
    },
    {
      "type": "training",
      "description": "Training step 1645",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:12",
      "total_flops_so_far": 3.908129729629072e+16,
      "budget_used_percent": 39.08129729629072
    },
    {
      "type": "training",
      "description": "Training step 1646",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:13",
      "total_flops_so_far": 3.91050361684264e+16,
      "budget_used_percent": 39.1050361684264
    },
    {
      "type": "training",
      "description": "Training step 1647",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:14",
      "total_flops_so_far": 3.912877504056208e+16,
      "budget_used_percent": 39.12877504056208
    },
    {
      "type": "training",
      "description": "Training step 1648",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:16",
      "total_flops_so_far": 3.915251391269776e+16,
      "budget_used_percent": 39.15251391269776
    },
    {
      "type": "training",
      "description": "Training step 1649",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:17",
      "total_flops_so_far": 3.917625278483344e+16,
      "budget_used_percent": 39.17625278483344
    },
    {
      "type": "training",
      "description": "Training step 1650",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:18",
      "total_flops_so_far": 3.919999165696912e+16,
      "budget_used_percent": 39.19999165696912
    },
    {
      "type": "training",
      "description": "Training step 1651",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:20",
      "total_flops_so_far": 3.92237305291048e+16,
      "budget_used_percent": 39.2237305291048
    },
    {
      "type": "training",
      "description": "Training step 1652",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:21",
      "total_flops_so_far": 3.924746940124048e+16,
      "budget_used_percent": 39.24746940124048
    },
    {
      "type": "training",
      "description": "Training step 1653",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:22",
      "total_flops_so_far": 3.927120827337616e+16,
      "budget_used_percent": 39.27120827337616
    },
    {
      "type": "training",
      "description": "Training step 1654",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:24",
      "total_flops_so_far": 3.929494714551184e+16,
      "budget_used_percent": 39.29494714551184
    },
    {
      "type": "training",
      "description": "Training step 1655",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:25",
      "total_flops_so_far": 3.931868601764752e+16,
      "budget_used_percent": 39.31868601764752
    },
    {
      "type": "training",
      "description": "Training step 1656",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:26",
      "total_flops_so_far": 3.93424248897832e+16,
      "budget_used_percent": 39.3424248897832
    },
    {
      "type": "training",
      "description": "Training step 1657",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:28",
      "total_flops_so_far": 3.936616376191888e+16,
      "budget_used_percent": 39.366163761918884
    },
    {
      "type": "training",
      "description": "Training step 1658",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:29",
      "total_flops_so_far": 3.938990263405456e+16,
      "budget_used_percent": 39.38990263405456
    },
    {
      "type": "training",
      "description": "Training step 1659",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:30",
      "total_flops_so_far": 3.941364150619024e+16,
      "budget_used_percent": 39.41364150619024
    },
    {
      "type": "training",
      "description": "Training step 1660",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:32",
      "total_flops_so_far": 3.943738037832592e+16,
      "budget_used_percent": 39.43738037832592
    },
    {
      "type": "training",
      "description": "Training step 1661",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:33",
      "total_flops_so_far": 3.94611192504616e+16,
      "budget_used_percent": 39.4611192504616
    },
    {
      "type": "training",
      "description": "Training step 1662",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:34",
      "total_flops_so_far": 3.948485812259728e+16,
      "budget_used_percent": 39.484858122597274
    },
    {
      "type": "training",
      "description": "Training step 1663",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:36",
      "total_flops_so_far": 3.950859699473296e+16,
      "budget_used_percent": 39.50859699473296
    },
    {
      "type": "training",
      "description": "Training step 1664",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:37",
      "total_flops_so_far": 3.953233586686864e+16,
      "budget_used_percent": 39.53233586686864
    },
    {
      "type": "training",
      "description": "Training step 1665",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:38",
      "total_flops_so_far": 3.955607473900432e+16,
      "budget_used_percent": 39.55607473900432
    },
    {
      "type": "training",
      "description": "Training step 1666",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:39",
      "total_flops_so_far": 3.957981361114e+16,
      "budget_used_percent": 39.579813611140004
    },
    {
      "type": "training",
      "description": "Training step 1667",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:41",
      "total_flops_so_far": 3.960355248327568e+16,
      "budget_used_percent": 39.60355248327568
    },
    {
      "type": "training",
      "description": "Training step 1668",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:42",
      "total_flops_so_far": 3.962729135541136e+16,
      "budget_used_percent": 39.62729135541136
    },
    {
      "type": "training",
      "description": "Training step 1669",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:43",
      "total_flops_so_far": 3.965103022754704e+16,
      "budget_used_percent": 39.65103022754704
    },
    {
      "type": "training",
      "description": "Training step 1670",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:45",
      "total_flops_so_far": 3.967476909968272e+16,
      "budget_used_percent": 39.67476909968272
    },
    {
      "type": "training",
      "description": "Training step 1671",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:46",
      "total_flops_so_far": 3.96985079718184e+16,
      "budget_used_percent": 39.6985079718184
    },
    {
      "type": "training",
      "description": "Training step 1672",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:47",
      "total_flops_so_far": 3.972224684395408e+16,
      "budget_used_percent": 39.72224684395408
    },
    {
      "type": "training",
      "description": "Training step 1673",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:49",
      "total_flops_so_far": 3.974598571608976e+16,
      "budget_used_percent": 39.74598571608976
    },
    {
      "type": "training",
      "description": "Training step 1674",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:50",
      "total_flops_so_far": 3.976972458822544e+16,
      "budget_used_percent": 39.76972458822544
    },
    {
      "type": "training",
      "description": "Training step 1675",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:51",
      "total_flops_so_far": 3.979346346036112e+16,
      "budget_used_percent": 39.793463460361124
    },
    {
      "type": "training",
      "description": "Training step 1676",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:53",
      "total_flops_so_far": 3.98172023324968e+16,
      "budget_used_percent": 39.8172023324968
    },
    {
      "type": "training",
      "description": "Training step 1677",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:54",
      "total_flops_so_far": 3.984094120463248e+16,
      "budget_used_percent": 39.84094120463248
    },
    {
      "type": "training",
      "description": "Training step 1678",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:55",
      "total_flops_so_far": 3.986468007676816e+16,
      "budget_used_percent": 39.86468007676816
    },
    {
      "type": "training",
      "description": "Training step 1679",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:57",
      "total_flops_so_far": 3.988841894890384e+16,
      "budget_used_percent": 39.88841894890384
    },
    {
      "type": "training",
      "description": "Training step 1680",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:58",
      "total_flops_so_far": 3.991215782103952e+16,
      "budget_used_percent": 39.912157821039514
    },
    {
      "type": "training",
      "description": "Training step 1681",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:08:59",
      "total_flops_so_far": 3.99358966931752e+16,
      "budget_used_percent": 39.9358966931752
    },
    {
      "type": "training",
      "description": "Training step 1682",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:01",
      "total_flops_so_far": 3.995963556531088e+16,
      "budget_used_percent": 39.95963556531088
    },
    {
      "type": "training",
      "description": "Training step 1683",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:02",
      "total_flops_so_far": 3.998337443744656e+16,
      "budget_used_percent": 39.983374437446564
    },
    {
      "type": "training",
      "description": "Training step 1684",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:03",
      "total_flops_so_far": 4.000711330958224e+16,
      "budget_used_percent": 40.00711330958224
    },
    {
      "type": "training",
      "description": "Training step 1685",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:05",
      "total_flops_so_far": 4.003085218171792e+16,
      "budget_used_percent": 40.03085218171792
    },
    {
      "type": "training",
      "description": "Training step 1686",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:06",
      "total_flops_so_far": 4.00545910538536e+16,
      "budget_used_percent": 40.0545910538536
    },
    {
      "type": "training",
      "description": "Training step 1687",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:07",
      "total_flops_so_far": 4.007832992598928e+16,
      "budget_used_percent": 40.07832992598928
    },
    {
      "type": "training",
      "description": "Training step 1688",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:09",
      "total_flops_so_far": 4.010206879812496e+16,
      "budget_used_percent": 40.10206879812496
    },
    {
      "type": "training",
      "description": "Training step 1689",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:10",
      "total_flops_so_far": 4.012580767026064e+16,
      "budget_used_percent": 40.12580767026064
    },
    {
      "type": "training",
      "description": "Training step 1690",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:11",
      "total_flops_so_far": 4.014954654239632e+16,
      "budget_used_percent": 40.14954654239632
    },
    {
      "type": "training",
      "description": "Training step 1691",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:13",
      "total_flops_so_far": 4.0173285414532e+16,
      "budget_used_percent": 40.173285414532
    },
    {
      "type": "training",
      "description": "Training step 1692",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:14",
      "total_flops_so_far": 4.019702428666768e+16,
      "budget_used_percent": 40.197024286667684
    },
    {
      "type": "training",
      "description": "Training step 1693",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:15",
      "total_flops_so_far": 4.022076315880336e+16,
      "budget_used_percent": 40.22076315880336
    },
    {
      "type": "training",
      "description": "Training step 1694",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:16",
      "total_flops_so_far": 4.024450203093904e+16,
      "budget_used_percent": 40.24450203093904
    },
    {
      "type": "training",
      "description": "Training step 1695",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:18",
      "total_flops_so_far": 4.026824090307472e+16,
      "budget_used_percent": 40.26824090307472
    },
    {
      "type": "training",
      "description": "Training step 1696",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:19",
      "total_flops_so_far": 4.02919797752104e+16,
      "budget_used_percent": 40.2919797752104
    },
    {
      "type": "training",
      "description": "Training step 1697",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:20",
      "total_flops_so_far": 4.031571864734608e+16,
      "budget_used_percent": 40.31571864734608
    },
    {
      "type": "training",
      "description": "Training step 1698",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:22",
      "total_flops_so_far": 4.033945751948176e+16,
      "budget_used_percent": 40.339457519481755
    },
    {
      "type": "training",
      "description": "Training step 1699",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:23",
      "total_flops_so_far": 4.036319639161744e+16,
      "budget_used_percent": 40.36319639161744
    },
    {
      "type": "training",
      "description": "Training step 1700",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:24",
      "total_flops_so_far": 4.038693526375312e+16,
      "budget_used_percent": 40.386935263753124
    },
    {
      "type": "training",
      "description": "Training step 1701",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:26",
      "total_flops_so_far": 4.04106741358888e+16,
      "budget_used_percent": 40.410674135888804
    },
    {
      "type": "training",
      "description": "Training step 1702",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:27",
      "total_flops_so_far": 4.043441300802448e+16,
      "budget_used_percent": 40.43441300802448
    },
    {
      "type": "training",
      "description": "Training step 1703",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:28",
      "total_flops_so_far": 4.045815188016016e+16,
      "budget_used_percent": 40.45815188016016
    },
    {
      "type": "training",
      "description": "Training step 1704",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:30",
      "total_flops_so_far": 4.048189075229584e+16,
      "budget_used_percent": 40.48189075229584
    },
    {
      "type": "training",
      "description": "Training step 1705",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:31",
      "total_flops_so_far": 4.050562962443152e+16,
      "budget_used_percent": 40.50562962443152
    },
    {
      "type": "training",
      "description": "Training step 1706",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:32",
      "total_flops_so_far": 4.05293684965672e+16,
      "budget_used_percent": 40.529368496567194
    },
    {
      "type": "training",
      "description": "Training step 1707",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:34",
      "total_flops_so_far": 4.055310736870288e+16,
      "budget_used_percent": 40.55310736870288
    },
    {
      "type": "training",
      "description": "Training step 1708",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:35",
      "total_flops_so_far": 4.057684624083856e+16,
      "budget_used_percent": 40.57684624083856
    },
    {
      "type": "training",
      "description": "Training step 1709",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:36",
      "total_flops_so_far": 4.060058511297424e+16,
      "budget_used_percent": 40.600585112974244
    },
    {
      "type": "training",
      "description": "Training step 1710",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:38",
      "total_flops_so_far": 4.062432398510992e+16,
      "budget_used_percent": 40.624323985109925
    },
    {
      "type": "training",
      "description": "Training step 1711",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:39",
      "total_flops_so_far": 4.06480628572456e+16,
      "budget_used_percent": 40.6480628572456
    },
    {
      "type": "training",
      "description": "Training step 1712",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:40",
      "total_flops_so_far": 4.067180172938128e+16,
      "budget_used_percent": 40.67180172938128
    },
    {
      "type": "training",
      "description": "Training step 1713",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:42",
      "total_flops_so_far": 4.069554060151696e+16,
      "budget_used_percent": 40.69554060151696
    },
    {
      "type": "training",
      "description": "Training step 1714",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:43",
      "total_flops_so_far": 4.071927947365264e+16,
      "budget_used_percent": 40.71927947365264
    },
    {
      "type": "training",
      "description": "Training step 1715",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:44",
      "total_flops_so_far": 4.074301834578832e+16,
      "budget_used_percent": 40.743018345788315
    },
    {
      "type": "training",
      "description": "Training step 1716",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:45",
      "total_flops_so_far": 4.0766757217924e+16,
      "budget_used_percent": 40.766757217923995
    },
    {
      "type": "training",
      "description": "Training step 1717",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:46",
      "total_flops_so_far": 4.079049609005968e+16,
      "budget_used_percent": 40.79049609005968
    },
    {
      "type": "training",
      "description": "Training step 1718",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:47",
      "total_flops_so_far": 4.081423496219536e+16,
      "budget_used_percent": 40.814234962195364
    },
    {
      "type": "training",
      "description": "Training step 1719",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:49",
      "total_flops_so_far": 4.083797383433104e+16,
      "budget_used_percent": 40.83797383433104
    },
    {
      "type": "training",
      "description": "Training step 1720",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:50",
      "total_flops_so_far": 4.086171270646672e+16,
      "budget_used_percent": 40.86171270646672
    },
    {
      "type": "training",
      "description": "Training step 1721",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:51",
      "total_flops_so_far": 4.08854515786024e+16,
      "budget_used_percent": 40.8854515786024
    },
    {
      "type": "training",
      "description": "Training step 1722",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:53",
      "total_flops_so_far": 4.090919045073808e+16,
      "budget_used_percent": 40.90919045073808
    },
    {
      "type": "training",
      "description": "Training step 1723",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:54",
      "total_flops_so_far": 4.093292932287376e+16,
      "budget_used_percent": 40.93292932287376
    },
    {
      "type": "training",
      "description": "Training step 1724",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:55",
      "total_flops_so_far": 4.095666819500944e+16,
      "budget_used_percent": 40.956668195009435
    },
    {
      "type": "training",
      "description": "Training step 1725",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:56",
      "total_flops_so_far": 4.098040706714512e+16,
      "budget_used_percent": 40.98040706714512
    },
    {
      "type": "training",
      "description": "Training step 1726",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:58",
      "total_flops_so_far": 4.10041459392808e+16,
      "budget_used_percent": 41.004145939280804
    },
    {
      "type": "training",
      "description": "Training step 1727",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:09:59",
      "total_flops_so_far": 4.102788481141648e+16,
      "budget_used_percent": 41.027884811416484
    },
    {
      "type": "training",
      "description": "Training step 1728",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:00",
      "total_flops_so_far": 4.105162368355216e+16,
      "budget_used_percent": 41.05162368355216
    },
    {
      "type": "training",
      "description": "Training step 1729",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:02",
      "total_flops_so_far": 4.107536255568784e+16,
      "budget_used_percent": 41.07536255568784
    },
    {
      "type": "training",
      "description": "Training step 1730",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:03",
      "total_flops_so_far": 4.109910142782352e+16,
      "budget_used_percent": 41.09910142782352
    },
    {
      "type": "training",
      "description": "Training step 1731",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:04",
      "total_flops_so_far": 4.11228402999592e+16,
      "budget_used_percent": 41.1228402999592
    },
    {
      "type": "training",
      "description": "Training step 1732",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:06",
      "total_flops_so_far": 4.114657917209488e+16,
      "budget_used_percent": 41.14657917209488
    },
    {
      "type": "training",
      "description": "Training step 1733",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:08",
      "total_flops_so_far": 4.117031804423056e+16,
      "budget_used_percent": 41.170318044230555
    },
    {
      "type": "training",
      "description": "Training step 1734",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:10",
      "total_flops_so_far": 4.119405691636624e+16,
      "budget_used_percent": 41.194056916366236
    },
    {
      "type": "training",
      "description": "Training step 1735",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:12",
      "total_flops_so_far": 4.121779578850192e+16,
      "budget_used_percent": 41.217795788501924
    },
    {
      "type": "training",
      "description": "Training step 1736",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:14",
      "total_flops_so_far": 4.12415346606376e+16,
      "budget_used_percent": 41.241534660637605
    },
    {
      "type": "training",
      "description": "Training step 1737",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:15",
      "total_flops_so_far": 4.126527353277328e+16,
      "budget_used_percent": 41.26527353277328
    },
    {
      "type": "training",
      "description": "Training step 1738",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:16",
      "total_flops_so_far": 4.128901240490896e+16,
      "budget_used_percent": 41.28901240490896
    },
    {
      "type": "training",
      "description": "Training step 1739",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:17",
      "total_flops_so_far": 4.131275127704464e+16,
      "budget_used_percent": 41.31275127704464
    },
    {
      "type": "training",
      "description": "Training step 1740",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:19",
      "total_flops_so_far": 4.133649014918032e+16,
      "budget_used_percent": 41.33649014918032
    },
    {
      "type": "training",
      "description": "Training step 1741",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:20",
      "total_flops_so_far": 4.1360229021316e+16,
      "budget_used_percent": 41.360229021315995
    },
    {
      "type": "training",
      "description": "Training step 1742",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:21",
      "total_flops_so_far": 4.138396789345168e+16,
      "budget_used_percent": 41.383967893451675
    },
    {
      "type": "training",
      "description": "Training step 1743",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:23",
      "total_flops_so_far": 4.140770676558736e+16,
      "budget_used_percent": 41.40770676558736
    },
    {
      "type": "training",
      "description": "Training step 1744",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:24",
      "total_flops_so_far": 4.143144563772304e+16,
      "budget_used_percent": 41.431445637723044
    },
    {
      "type": "training",
      "description": "Training step 1745",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:25",
      "total_flops_so_far": 4.145518450985872e+16,
      "budget_used_percent": 41.455184509858725
    },
    {
      "type": "training",
      "description": "Training step 1746",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:27",
      "total_flops_so_far": 4.14789233819944e+16,
      "budget_used_percent": 41.4789233819944
    },
    {
      "type": "training",
      "description": "Training step 1747",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:28",
      "total_flops_so_far": 4.150266225413008e+16,
      "budget_used_percent": 41.50266225413008
    },
    {
      "type": "training",
      "description": "Training step 1748",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:29",
      "total_flops_so_far": 4.152640112626576e+16,
      "budget_used_percent": 41.52640112626576
    },
    {
      "type": "training",
      "description": "Training step 1749",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:31",
      "total_flops_so_far": 4.155013999840144e+16,
      "budget_used_percent": 41.55013999840144
    },
    {
      "type": "training",
      "description": "Training step 1750",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:32",
      "total_flops_so_far": 4.157387887053712e+16,
      "budget_used_percent": 41.573878870537115
    },
    {
      "type": "training",
      "description": "Training step 1751",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:33",
      "total_flops_so_far": 4.15976177426728e+16,
      "budget_used_percent": 41.597617742672796
    },
    {
      "type": "training",
      "description": "Training step 1752",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:35",
      "total_flops_so_far": 4.162135661480848e+16,
      "budget_used_percent": 41.62135661480848
    },
    {
      "type": "training",
      "description": "Training step 1753",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:36",
      "total_flops_so_far": 4.164509548694416e+16,
      "budget_used_percent": 41.645095486944165
    },
    {
      "type": "training",
      "description": "Training step 1754",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:37",
      "total_flops_so_far": 4.166883435907984e+16,
      "budget_used_percent": 41.668834359079845
    },
    {
      "type": "training",
      "description": "Training step 1755",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:39",
      "total_flops_so_far": 4.169257323121552e+16,
      "budget_used_percent": 41.69257323121552
    },
    {
      "type": "training",
      "description": "Training step 1756",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:40",
      "total_flops_so_far": 4.17163121033512e+16,
      "budget_used_percent": 41.7163121033512
    },
    {
      "type": "training",
      "description": "Training step 1757",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:41",
      "total_flops_so_far": 4.174005097548688e+16,
      "budget_used_percent": 41.74005097548688
    },
    {
      "type": "training",
      "description": "Training step 1758",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:43",
      "total_flops_so_far": 4.176378984762256e+16,
      "budget_used_percent": 41.76378984762256
    },
    {
      "type": "training",
      "description": "Training step 1759",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:44",
      "total_flops_so_far": 4.178752871975824e+16,
      "budget_used_percent": 41.787528719758235
    },
    {
      "type": "training",
      "description": "Training step 1760",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:45",
      "total_flops_so_far": 4.181126759189392e+16,
      "budget_used_percent": 41.811267591893916
    },
    {
      "type": "training",
      "description": "Training step 1761",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:46",
      "total_flops_so_far": 4.18350064640296e+16,
      "budget_used_percent": 41.835006464029604
    },
    {
      "type": "training",
      "description": "Training step 1762",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:48",
      "total_flops_so_far": 4.185874533616528e+16,
      "budget_used_percent": 41.858745336165285
    },
    {
      "type": "training",
      "description": "Training step 1763",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:49",
      "total_flops_so_far": 4.188248420830096e+16,
      "budget_used_percent": 41.88248420830096
    },
    {
      "type": "training",
      "description": "Training step 1764",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:50",
      "total_flops_so_far": 4.190622308043664e+16,
      "budget_used_percent": 41.90622308043664
    },
    {
      "type": "training",
      "description": "Training step 1765",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:52",
      "total_flops_so_far": 4.192996195257232e+16,
      "budget_used_percent": 41.92996195257232
    },
    {
      "type": "training",
      "description": "Training step 1766",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:53",
      "total_flops_so_far": 4.1953700824708e+16,
      "budget_used_percent": 41.953700824708
    },
    {
      "type": "training",
      "description": "Training step 1767",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:54",
      "total_flops_so_far": 4.197743969684368e+16,
      "budget_used_percent": 41.97743969684368
    },
    {
      "type": "training",
      "description": "Training step 1768",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:56",
      "total_flops_so_far": 4.200117856897936e+16,
      "budget_used_percent": 42.001178568979356
    },
    {
      "type": "training",
      "description": "Training step 1769",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:57",
      "total_flops_so_far": 4.202491744111504e+16,
      "budget_used_percent": 42.024917441115036
    },
    {
      "type": "training",
      "description": "Training step 1770",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:10:58",
      "total_flops_so_far": 4.204865631325072e+16,
      "budget_used_percent": 42.04865631325072
    },
    {
      "type": "training",
      "description": "Training step 1771",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:00",
      "total_flops_so_far": 4.20723951853864e+16,
      "budget_used_percent": 42.072395185386405
    },
    {
      "type": "training",
      "description": "Training step 1772",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:01",
      "total_flops_so_far": 4.209613405752208e+16,
      "budget_used_percent": 42.09613405752208
    },
    {
      "type": "training",
      "description": "Training step 1773",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:02",
      "total_flops_so_far": 4.211987292965776e+16,
      "budget_used_percent": 42.11987292965776
    },
    {
      "type": "training",
      "description": "Training step 1774",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:04",
      "total_flops_so_far": 4.214361180179344e+16,
      "budget_used_percent": 42.14361180179344
    },
    {
      "type": "training",
      "description": "Training step 1775",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:05",
      "total_flops_so_far": 4.216735067392912e+16,
      "budget_used_percent": 42.16735067392912
    },
    {
      "type": "training",
      "description": "Training step 1776",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:06",
      "total_flops_so_far": 4.21910895460648e+16,
      "budget_used_percent": 42.1910895460648
    },
    {
      "type": "training",
      "description": "Training step 1777",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:08",
      "total_flops_so_far": 4.221482841820048e+16,
      "budget_used_percent": 42.214828418200476
    },
    {
      "type": "training",
      "description": "Training step 1778",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:09",
      "total_flops_so_far": 4.223856729033616e+16,
      "budget_used_percent": 42.23856729033616
    },
    {
      "type": "training",
      "description": "Training step 1779",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:10",
      "total_flops_so_far": 4.226230616247184e+16,
      "budget_used_percent": 42.262306162471845
    },
    {
      "type": "training",
      "description": "Training step 1780",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:12",
      "total_flops_so_far": 4.228604503460752e+16,
      "budget_used_percent": 42.286045034607525
    },
    {
      "type": "training",
      "description": "Training step 1781",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:13",
      "total_flops_so_far": 4.23097839067432e+16,
      "budget_used_percent": 42.3097839067432
    },
    {
      "type": "training",
      "description": "Training step 1782",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:14",
      "total_flops_so_far": 4.233352277887888e+16,
      "budget_used_percent": 42.33352277887888
    },
    {
      "type": "training",
      "description": "Training step 1783",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:16",
      "total_flops_so_far": 4.235726165101456e+16,
      "budget_used_percent": 42.35726165101456
    },
    {
      "type": "training",
      "description": "Training step 1784",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:17",
      "total_flops_so_far": 4.238100052315024e+16,
      "budget_used_percent": 42.38100052315024
    },
    {
      "type": "training",
      "description": "Training step 1785",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:18",
      "total_flops_so_far": 4.240473939528592e+16,
      "budget_used_percent": 42.404739395285915
    },
    {
      "type": "training",
      "description": "Training step 1786",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:19",
      "total_flops_so_far": 4.24284782674216e+16,
      "budget_used_percent": 42.428478267421596
    },
    {
      "type": "training",
      "description": "Training step 1787",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:21",
      "total_flops_so_far": 4.245221713955728e+16,
      "budget_used_percent": 42.45221713955728
    },
    {
      "type": "training",
      "description": "Training step 1788",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:22",
      "total_flops_so_far": 4.247595601169296e+16,
      "budget_used_percent": 42.47595601169296
    },
    {
      "type": "training",
      "description": "Training step 1789",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:23",
      "total_flops_so_far": 4.249969488382864e+16,
      "budget_used_percent": 42.499694883828646
    },
    {
      "type": "training",
      "description": "Training step 1790",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:25",
      "total_flops_so_far": 4.252343375596432e+16,
      "budget_used_percent": 42.52343375596432
    },
    {
      "type": "training",
      "description": "Training step 1791",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:26",
      "total_flops_so_far": 4.25471726281e+16,
      "budget_used_percent": 42.5471726281
    },
    {
      "type": "training",
      "description": "Training step 1792",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:27",
      "total_flops_so_far": 4.257091150023568e+16,
      "budget_used_percent": 42.57091150023568
    },
    {
      "type": "training",
      "description": "Training step 1793",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:29",
      "total_flops_so_far": 4.259465037237136e+16,
      "budget_used_percent": 42.59465037237136
    },
    {
      "type": "training",
      "description": "Training step 1794",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:30",
      "total_flops_so_far": 4.261838924450704e+16,
      "budget_used_percent": 42.618389244507036
    },
    {
      "type": "training",
      "description": "Training step 1795",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:31",
      "total_flops_so_far": 4.264212811664272e+16,
      "budget_used_percent": 42.642128116642716
    },
    {
      "type": "training",
      "description": "Training step 1796",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:33",
      "total_flops_so_far": 4.26658669887784e+16,
      "budget_used_percent": 42.6658669887784
    },
    {
      "type": "training",
      "description": "Training step 1797",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:34",
      "total_flops_so_far": 4.268960586091408e+16,
      "budget_used_percent": 42.689605860914085
    },
    {
      "type": "training",
      "description": "Training step 1798",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:35",
      "total_flops_so_far": 4.271334473304976e+16,
      "budget_used_percent": 42.71334473304976
    },
    {
      "type": "training",
      "description": "Training step 1799",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:37",
      "total_flops_so_far": 4.273708360518544e+16,
      "budget_used_percent": 42.73708360518544
    },
    {
      "type": "training",
      "description": "Training step 1800",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:38",
      "total_flops_so_far": 4.276082247732112e+16,
      "budget_used_percent": 42.76082247732112
    },
    {
      "type": "training",
      "description": "Training step 1801",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:39",
      "total_flops_so_far": 4.27845613494568e+16,
      "budget_used_percent": 42.7845613494568
    },
    {
      "type": "training",
      "description": "Training step 1802",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:41",
      "total_flops_so_far": 4.280830022159248e+16,
      "budget_used_percent": 42.80830022159248
    },
    {
      "type": "training",
      "description": "Training step 1803",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:42",
      "total_flops_so_far": 4.283203909372816e+16,
      "budget_used_percent": 42.832039093728156
    },
    {
      "type": "training",
      "description": "Training step 1804",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:43",
      "total_flops_so_far": 4.285577796586384e+16,
      "budget_used_percent": 42.85577796586384
    },
    {
      "type": "training",
      "description": "Training step 1805",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:45",
      "total_flops_so_far": 4.287951683799952e+16,
      "budget_used_percent": 42.87951683799952
    },
    {
      "type": "training",
      "description": "Training step 1806",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:46",
      "total_flops_so_far": 4.29032557101352e+16,
      "budget_used_percent": 42.9032557101352
    },
    {
      "type": "training",
      "description": "Training step 1807",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:47",
      "total_flops_so_far": 4.292699458227088e+16,
      "budget_used_percent": 42.92699458227088
    },
    {
      "type": "training",
      "description": "Training step 1808",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:49",
      "total_flops_so_far": 4.295073345440656e+16,
      "budget_used_percent": 42.95073345440656
    },
    {
      "type": "training",
      "description": "Training step 1809",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:50",
      "total_flops_so_far": 4.297447232654224e+16,
      "budget_used_percent": 42.97447232654224
    },
    {
      "type": "training",
      "description": "Training step 1810",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:51",
      "total_flops_so_far": 4.299821119867792e+16,
      "budget_used_percent": 42.99821119867792
    },
    {
      "type": "training",
      "description": "Training step 1811",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:52",
      "total_flops_so_far": 4.30219500708136e+16,
      "budget_used_percent": 43.0219500708136
    },
    {
      "type": "training",
      "description": "Training step 1812",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:54",
      "total_flops_so_far": 4.304568894294928e+16,
      "budget_used_percent": 43.045688942949276
    },
    {
      "type": "training",
      "description": "Training step 1813",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:55",
      "total_flops_so_far": 4.306942781508496e+16,
      "budget_used_percent": 43.06942781508496
    },
    {
      "type": "training",
      "description": "Training step 1814",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:56",
      "total_flops_so_far": 4.309316668722064e+16,
      "budget_used_percent": 43.09316668722064
    },
    {
      "type": "training",
      "description": "Training step 1815",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:58",
      "total_flops_so_far": 4.311690555935632e+16,
      "budget_used_percent": 43.116905559356326
    },
    {
      "type": "training",
      "description": "Training step 1816",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:11:59",
      "total_flops_so_far": 4.3140644431492e+16,
      "budget_used_percent": 43.140644431492
    },
    {
      "type": "training",
      "description": "Training step 1817",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:00",
      "total_flops_so_far": 4.316438330362768e+16,
      "budget_used_percent": 43.16438330362768
    },
    {
      "type": "training",
      "description": "Training step 1818",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:02",
      "total_flops_so_far": 4.318812217576336e+16,
      "budget_used_percent": 43.18812217576336
    },
    {
      "type": "training",
      "description": "Training step 1819",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:03",
      "total_flops_so_far": 4.321186104789904e+16,
      "budget_used_percent": 43.21186104789904
    },
    {
      "type": "training",
      "description": "Training step 1820",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:04",
      "total_flops_so_far": 4.323559992003472e+16,
      "budget_used_percent": 43.235599920034716
    },
    {
      "type": "training",
      "description": "Training step 1821",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:06",
      "total_flops_so_far": 4.32593387921704e+16,
      "budget_used_percent": 43.2593387921704
    },
    {
      "type": "training",
      "description": "Training step 1822",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:07",
      "total_flops_so_far": 4.328307766430608e+16,
      "budget_used_percent": 43.28307766430608
    },
    {
      "type": "training",
      "description": "Training step 1823",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:08",
      "total_flops_so_far": 4.330681653644176e+16,
      "budget_used_percent": 43.30681653644176
    },
    {
      "type": "training",
      "description": "Training step 1824",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:10",
      "total_flops_so_far": 4.333055540857744e+16,
      "budget_used_percent": 43.33055540857744
    },
    {
      "type": "training",
      "description": "Training step 1825",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:11",
      "total_flops_so_far": 4.335429428071312e+16,
      "budget_used_percent": 43.35429428071312
    },
    {
      "type": "training",
      "description": "Training step 1826",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:12",
      "total_flops_so_far": 4.33780331528488e+16,
      "budget_used_percent": 43.3780331528488
    },
    {
      "type": "training",
      "description": "Training step 1827",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:14",
      "total_flops_so_far": 4.340177202498448e+16,
      "budget_used_percent": 43.40177202498448
    },
    {
      "type": "training",
      "description": "Training step 1828",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:15",
      "total_flops_so_far": 4.342551089712016e+16,
      "budget_used_percent": 43.42551089712016
    },
    {
      "type": "training",
      "description": "Training step 1829",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:16",
      "total_flops_so_far": 4.344924976925584e+16,
      "budget_used_percent": 43.449249769255836
    },
    {
      "type": "training",
      "description": "Training step 1830",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:18",
      "total_flops_so_far": 4.347298864139152e+16,
      "budget_used_percent": 43.47298864139152
    },
    {
      "type": "training",
      "description": "Training step 1831",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:19",
      "total_flops_so_far": 4.34967275135272e+16,
      "budget_used_percent": 43.4967275135272
    },
    {
      "type": "training",
      "description": "Training step 1832",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:20",
      "total_flops_so_far": 4.352046638566288e+16,
      "budget_used_percent": 43.52046638566288
    },
    {
      "type": "training",
      "description": "Training step 1833",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:22",
      "total_flops_so_far": 4.354420525779856e+16,
      "budget_used_percent": 43.544205257798566
    },
    {
      "type": "training",
      "description": "Training step 1834",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:23",
      "total_flops_so_far": 4.356794412993424e+16,
      "budget_used_percent": 43.56794412993424
    },
    {
      "type": "training",
      "description": "Training step 1835",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:24",
      "total_flops_so_far": 4.359168300206992e+16,
      "budget_used_percent": 43.59168300206992
    },
    {
      "type": "training",
      "description": "Training step 1836",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:26",
      "total_flops_so_far": 4.36154218742056e+16,
      "budget_used_percent": 43.6154218742056
    },
    {
      "type": "training",
      "description": "Training step 1837",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:27",
      "total_flops_so_far": 4.363916074634128e+16,
      "budget_used_percent": 43.63916074634128
    },
    {
      "type": "training",
      "description": "Training step 1838",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:28",
      "total_flops_so_far": 4.366289961847696e+16,
      "budget_used_percent": 43.662899618476956
    },
    {
      "type": "training",
      "description": "Training step 1839",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:30",
      "total_flops_so_far": 4.368663849061264e+16,
      "budget_used_percent": 43.68663849061264
    },
    {
      "type": "training",
      "description": "Training step 1840",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:31",
      "total_flops_so_far": 4.371037736274832e+16,
      "budget_used_percent": 43.71037736274832
    },
    {
      "type": "training",
      "description": "Training step 1841",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:32",
      "total_flops_so_far": 4.3734116234884e+16,
      "budget_used_percent": 43.734116234884
    },
    {
      "type": "training",
      "description": "Training step 1842",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:34",
      "total_flops_so_far": 4.375785510701968e+16,
      "budget_used_percent": 43.75785510701968
    },
    {
      "type": "training",
      "description": "Training step 1843",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:35",
      "total_flops_so_far": 4.378159397915536e+16,
      "budget_used_percent": 43.78159397915536
    },
    {
      "type": "training",
      "description": "Training step 1844",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:36",
      "total_flops_so_far": 4.380533285129104e+16,
      "budget_used_percent": 43.80533285129104
    },
    {
      "type": "training",
      "description": "Training step 1845",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:38",
      "total_flops_so_far": 4.382907172342672e+16,
      "budget_used_percent": 43.82907172342672
    },
    {
      "type": "training",
      "description": "Training step 1846",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:39",
      "total_flops_so_far": 4.38528105955624e+16,
      "budget_used_percent": 43.8528105955624
    },
    {
      "type": "training",
      "description": "Training step 1847",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:40",
      "total_flops_so_far": 4.387654946769808e+16,
      "budget_used_percent": 43.87654946769808
    },
    {
      "type": "training",
      "description": "Training step 1848",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:41",
      "total_flops_so_far": 4.390028833983376e+16,
      "budget_used_percent": 43.90028833983376
    },
    {
      "type": "training",
      "description": "Training step 1849",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:42",
      "total_flops_so_far": 4.392402721196944e+16,
      "budget_used_percent": 43.92402721196944
    },
    {
      "type": "training",
      "description": "Training step 1850",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:43",
      "total_flops_so_far": 4.394776608410512e+16,
      "budget_used_percent": 43.94776608410512
    },
    {
      "type": "training",
      "description": "Training step 1851",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:45",
      "total_flops_so_far": 4.39715049562408e+16,
      "budget_used_percent": 43.9715049562408
    },
    {
      "type": "training",
      "description": "Training step 1852",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:46",
      "total_flops_so_far": 4.399524382837648e+16,
      "budget_used_percent": 43.99524382837648
    },
    {
      "type": "training",
      "description": "Training step 1853",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:47",
      "total_flops_so_far": 4.401898270051216e+16,
      "budget_used_percent": 44.01898270051216
    },
    {
      "type": "training",
      "description": "Training step 1854",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:49",
      "total_flops_so_far": 4.404272157264784e+16,
      "budget_used_percent": 44.04272157264784
    },
    {
      "type": "training",
      "description": "Training step 1855",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:50",
      "total_flops_so_far": 4.406646044478352e+16,
      "budget_used_percent": 44.066460444783516
    },
    {
      "type": "training",
      "description": "Training step 1856",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:51",
      "total_flops_so_far": 4.40901993169192e+16,
      "budget_used_percent": 44.0901993169192
    },
    {
      "type": "training",
      "description": "Training step 1857",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:52",
      "total_flops_so_far": 4.411393818905488e+16,
      "budget_used_percent": 44.11393818905488
    },
    {
      "type": "training",
      "description": "Training step 1858",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:54",
      "total_flops_so_far": 4.413767706119056e+16,
      "budget_used_percent": 44.13767706119056
    },
    {
      "type": "training",
      "description": "Training step 1859",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:55",
      "total_flops_so_far": 4.416141593332624e+16,
      "budget_used_percent": 44.16141593332624
    },
    {
      "type": "training",
      "description": "Training step 1860",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:56",
      "total_flops_so_far": 4.418515480546192e+16,
      "budget_used_percent": 44.18515480546192
    },
    {
      "type": "training",
      "description": "Training step 1861",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:58",
      "total_flops_so_far": 4.42088936775976e+16,
      "budget_used_percent": 44.2088936775976
    },
    {
      "type": "training",
      "description": "Training step 1862",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:12:59",
      "total_flops_so_far": 4.423263254973328e+16,
      "budget_used_percent": 44.23263254973328
    },
    {
      "type": "training",
      "description": "Training step 1863",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:00",
      "total_flops_so_far": 4.425637142186896e+16,
      "budget_used_percent": 44.25637142186896
    },
    {
      "type": "training",
      "description": "Training step 1864",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:02",
      "total_flops_so_far": 4.428011029400464e+16,
      "budget_used_percent": 44.280110294004636
    },
    {
      "type": "training",
      "description": "Training step 1865",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:03",
      "total_flops_so_far": 4.430384916614032e+16,
      "budget_used_percent": 44.30384916614032
    },
    {
      "type": "training",
      "description": "Training step 1866",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:04",
      "total_flops_so_far": 4.4327588038276e+16,
      "budget_used_percent": 44.327588038276
    },
    {
      "type": "training",
      "description": "Training step 1867",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:06",
      "total_flops_so_far": 4.435132691041168e+16,
      "budget_used_percent": 44.35132691041168
    },
    {
      "type": "training",
      "description": "Training step 1868",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:07",
      "total_flops_so_far": 4.437506578254736e+16,
      "budget_used_percent": 44.37506578254736
    },
    {
      "type": "training",
      "description": "Training step 1869",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:08",
      "total_flops_so_far": 4.439880465468304e+16,
      "budget_used_percent": 44.39880465468304
    },
    {
      "type": "training",
      "description": "Training step 1870",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:10",
      "total_flops_so_far": 4.442254352681872e+16,
      "budget_used_percent": 44.42254352681872
    },
    {
      "type": "training",
      "description": "Training step 1871",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:11",
      "total_flops_so_far": 4.44462823989544e+16,
      "budget_used_percent": 44.4462823989544
    },
    {
      "type": "training",
      "description": "Training step 1872",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:12",
      "total_flops_so_far": 4.447002127109008e+16,
      "budget_used_percent": 44.47002127109008
    },
    {
      "type": "training",
      "description": "Training step 1873",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:14",
      "total_flops_so_far": 4.449376014322576e+16,
      "budget_used_percent": 44.49376014322576
    },
    {
      "type": "training",
      "description": "Training step 1874",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:15",
      "total_flops_so_far": 4.451749901536144e+16,
      "budget_used_percent": 44.51749901536144
    },
    {
      "type": "training",
      "description": "Training step 1875",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:16",
      "total_flops_so_far": 4.454123788749712e+16,
      "budget_used_percent": 44.54123788749712
    },
    {
      "type": "training",
      "description": "Training step 1876",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:18",
      "total_flops_so_far": 4.45649767596328e+16,
      "budget_used_percent": 44.5649767596328
    },
    {
      "type": "training",
      "description": "Training step 1877",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:19",
      "total_flops_so_far": 4.458871563176848e+16,
      "budget_used_percent": 44.58871563176848
    },
    {
      "type": "training",
      "description": "Training step 1878",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:20",
      "total_flops_so_far": 4.461245450390416e+16,
      "budget_used_percent": 44.61245450390416
    },
    {
      "type": "training",
      "description": "Training step 1879",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:22",
      "total_flops_so_far": 4.463619337603984e+16,
      "budget_used_percent": 44.63619337603984
    },
    {
      "type": "training",
      "description": "Training step 1880",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:23",
      "total_flops_so_far": 4.465993224817552e+16,
      "budget_used_percent": 44.65993224817552
    },
    {
      "type": "training",
      "description": "Training step 1881",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:24",
      "total_flops_so_far": 4.46836711203112e+16,
      "budget_used_percent": 44.6836711203112
    },
    {
      "type": "training",
      "description": "Training step 1882",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:26",
      "total_flops_so_far": 4.470740999244688e+16,
      "budget_used_percent": 44.70740999244688
    },
    {
      "type": "training",
      "description": "Training step 1883",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:27",
      "total_flops_so_far": 4.473114886458256e+16,
      "budget_used_percent": 44.73114886458256
    },
    {
      "type": "training",
      "description": "Training step 1884",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:28",
      "total_flops_so_far": 4.475488773671824e+16,
      "budget_used_percent": 44.75488773671824
    },
    {
      "type": "training",
      "description": "Training step 1885",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:30",
      "total_flops_so_far": 4.477862660885392e+16,
      "budget_used_percent": 44.77862660885392
    },
    {
      "type": "training",
      "description": "Training step 1886",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:31",
      "total_flops_so_far": 4.48023654809896e+16,
      "budget_used_percent": 44.8023654809896
    },
    {
      "type": "training",
      "description": "Training step 1887",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:32",
      "total_flops_so_far": 4.482610435312528e+16,
      "budget_used_percent": 44.82610435312528
    },
    {
      "type": "training",
      "description": "Training step 1888",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:33",
      "total_flops_so_far": 4.484984322526096e+16,
      "budget_used_percent": 44.84984322526096
    },
    {
      "type": "training",
      "description": "Training step 1889",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:35",
      "total_flops_so_far": 4.487358209739664e+16,
      "budget_used_percent": 44.87358209739664
    },
    {
      "type": "training",
      "description": "Training step 1890",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:36",
      "total_flops_so_far": 4.489732096953232e+16,
      "budget_used_percent": 44.89732096953232
    },
    {
      "type": "training",
      "description": "Training step 1891",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:37",
      "total_flops_so_far": 4.4921059841668e+16,
      "budget_used_percent": 44.921059841668
    },
    {
      "type": "training",
      "description": "Training step 1892",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:39",
      "total_flops_so_far": 4.494479871380368e+16,
      "budget_used_percent": 44.94479871380368
    },
    {
      "type": "training",
      "description": "Training step 1893",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:40",
      "total_flops_so_far": 4.496853758593936e+16,
      "budget_used_percent": 44.96853758593936
    },
    {
      "type": "training",
      "description": "Training step 1894",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:41",
      "total_flops_so_far": 4.499227645807504e+16,
      "budget_used_percent": 44.99227645807504
    },
    {
      "type": "training",
      "description": "Training step 1895",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:43",
      "total_flops_so_far": 4.501601533021072e+16,
      "budget_used_percent": 45.01601533021072
    },
    {
      "type": "training",
      "description": "Training step 1896",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:44",
      "total_flops_so_far": 4.50397542023464e+16,
      "budget_used_percent": 45.0397542023464
    },
    {
      "type": "training",
      "description": "Training step 1897",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:45",
      "total_flops_so_far": 4.506349307448208e+16,
      "budget_used_percent": 45.06349307448208
    },
    {
      "type": "training",
      "description": "Training step 1898",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:47",
      "total_flops_so_far": 4.508723194661776e+16,
      "budget_used_percent": 45.08723194661776
    },
    {
      "type": "training",
      "description": "Training step 1899",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:48",
      "total_flops_so_far": 4.511097081875344e+16,
      "budget_used_percent": 45.11097081875344
    },
    {
      "type": "training",
      "description": "Training step 1900",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:49",
      "total_flops_so_far": 4.513470969088912e+16,
      "budget_used_percent": 45.13470969088912
    },
    {
      "type": "training",
      "description": "Training step 1901",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:51",
      "total_flops_so_far": 4.51584485630248e+16,
      "budget_used_percent": 45.1584485630248
    },
    {
      "type": "training",
      "description": "Training step 1902",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:52",
      "total_flops_so_far": 4.518218743516048e+16,
      "budget_used_percent": 45.18218743516048
    },
    {
      "type": "training",
      "description": "Training step 1903",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:53",
      "total_flops_so_far": 4.520592630729616e+16,
      "budget_used_percent": 45.20592630729616
    },
    {
      "type": "training",
      "description": "Training step 1904",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:55",
      "total_flops_so_far": 4.522966517943184e+16,
      "budget_used_percent": 45.22966517943184
    },
    {
      "type": "training",
      "description": "Training step 1905",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:56",
      "total_flops_so_far": 4.525340405156752e+16,
      "budget_used_percent": 45.25340405156752
    },
    {
      "type": "training",
      "description": "Training step 1906",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:57",
      "total_flops_so_far": 4.52771429237032e+16,
      "budget_used_percent": 45.2771429237032
    },
    {
      "type": "training",
      "description": "Training step 1907",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:13:59",
      "total_flops_so_far": 4.530088179583888e+16,
      "budget_used_percent": 45.30088179583888
    },
    {
      "type": "training",
      "description": "Training step 1908",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:00",
      "total_flops_so_far": 4.532462066797456e+16,
      "budget_used_percent": 45.32462066797456
    },
    {
      "type": "training",
      "description": "Training step 1909",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:01",
      "total_flops_so_far": 4.534835954011024e+16,
      "budget_used_percent": 45.34835954011024
    },
    {
      "type": "training",
      "description": "Training step 1910",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:03",
      "total_flops_so_far": 4.537209841224592e+16,
      "budget_used_percent": 45.37209841224592
    },
    {
      "type": "training",
      "description": "Training step 1911",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:04",
      "total_flops_so_far": 4.53958372843816e+16,
      "budget_used_percent": 45.3958372843816
    },
    {
      "type": "training",
      "description": "Training step 1912",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:05",
      "total_flops_so_far": 4.541957615651728e+16,
      "budget_used_percent": 45.41957615651728
    },
    {
      "type": "training",
      "description": "Training step 1913",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:07",
      "total_flops_so_far": 4.544331502865296e+16,
      "budget_used_percent": 45.44331502865296
    },
    {
      "type": "training",
      "description": "Training step 1914",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:08",
      "total_flops_so_far": 4.546705390078864e+16,
      "budget_used_percent": 45.46705390078864
    },
    {
      "type": "training",
      "description": "Training step 1915",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:09",
      "total_flops_so_far": 4.549079277292432e+16,
      "budget_used_percent": 45.49079277292432
    },
    {
      "type": "training",
      "description": "Training step 1916",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:11",
      "total_flops_so_far": 4.551453164506e+16,
      "budget_used_percent": 45.51453164506
    },
    {
      "type": "training",
      "description": "Training step 1917",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:12",
      "total_flops_so_far": 4.553827051719568e+16,
      "budget_used_percent": 45.53827051719568
    },
    {
      "type": "training",
      "description": "Training step 1918",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:13",
      "total_flops_so_far": 4.556200938933136e+16,
      "budget_used_percent": 45.56200938933136
    },
    {
      "type": "training",
      "description": "Training step 1919",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:15",
      "total_flops_so_far": 4.558574826146704e+16,
      "budget_used_percent": 45.58574826146704
    },
    {
      "type": "training",
      "description": "Training step 1920",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:16",
      "total_flops_so_far": 4.560948713360272e+16,
      "budget_used_percent": 45.60948713360272
    },
    {
      "type": "training",
      "description": "Training step 1921",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:17",
      "total_flops_so_far": 4.56332260057384e+16,
      "budget_used_percent": 45.6332260057384
    },
    {
      "type": "training",
      "description": "Training step 1922",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:18",
      "total_flops_so_far": 4.565696487787408e+16,
      "budget_used_percent": 45.65696487787408
    },
    {
      "type": "training",
      "description": "Training step 1923",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:20",
      "total_flops_so_far": 4.568070375000976e+16,
      "budget_used_percent": 45.68070375000976
    },
    {
      "type": "training",
      "description": "Training step 1924",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:21",
      "total_flops_so_far": 4.570444262214544e+16,
      "budget_used_percent": 45.70444262214544
    },
    {
      "type": "training",
      "description": "Training step 1925",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:22",
      "total_flops_so_far": 4.572818149428112e+16,
      "budget_used_percent": 45.728181494281124
    },
    {
      "type": "training",
      "description": "Training step 1926",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:24",
      "total_flops_so_far": 4.57519203664168e+16,
      "budget_used_percent": 45.7519203664168
    },
    {
      "type": "training",
      "description": "Training step 1927",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:25",
      "total_flops_so_far": 4.577565923855248e+16,
      "budget_used_percent": 45.77565923855248
    },
    {
      "type": "training",
      "description": "Training step 1928",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:26",
      "total_flops_so_far": 4.579939811068816e+16,
      "budget_used_percent": 45.79939811068816
    },
    {
      "type": "training",
      "description": "Training step 1929",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:28",
      "total_flops_so_far": 4.582313698282384e+16,
      "budget_used_percent": 45.82313698282384
    },
    {
      "type": "training",
      "description": "Training step 1930",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:29",
      "total_flops_so_far": 4.584687585495952e+16,
      "budget_used_percent": 45.84687585495952
    },
    {
      "type": "training",
      "description": "Training step 1931",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:30",
      "total_flops_so_far": 4.58706147270952e+16,
      "budget_used_percent": 45.8706147270952
    },
    {
      "type": "training",
      "description": "Training step 1932",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:32",
      "total_flops_so_far": 4.589435359923088e+16,
      "budget_used_percent": 45.89435359923088
    },
    {
      "type": "training",
      "description": "Training step 1933",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:33",
      "total_flops_so_far": 4.591809247136656e+16,
      "budget_used_percent": 45.91809247136656
    },
    {
      "type": "training",
      "description": "Training step 1934",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:34",
      "total_flops_so_far": 4.594183134350224e+16,
      "budget_used_percent": 45.94183134350224
    },
    {
      "type": "training",
      "description": "Training step 1935",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:36",
      "total_flops_so_far": 4.596557021563792e+16,
      "budget_used_percent": 45.96557021563792
    },
    {
      "type": "training",
      "description": "Training step 1936",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:37",
      "total_flops_so_far": 4.59893090877736e+16,
      "budget_used_percent": 45.9893090877736
    },
    {
      "type": "training",
      "description": "Training step 1937",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:38",
      "total_flops_so_far": 4.601304795990928e+16,
      "budget_used_percent": 46.01304795990928
    },
    {
      "type": "training",
      "description": "Training step 1938",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:40",
      "total_flops_so_far": 4.603678683204496e+16,
      "budget_used_percent": 46.03678683204496
    },
    {
      "type": "training",
      "description": "Training step 1939",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:41",
      "total_flops_so_far": 4.606052570418064e+16,
      "budget_used_percent": 46.06052570418064
    },
    {
      "type": "training",
      "description": "Training step 1940",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:42",
      "total_flops_so_far": 4.608426457631632e+16,
      "budget_used_percent": 46.08426457631632
    },
    {
      "type": "training",
      "description": "Training step 1941",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:44",
      "total_flops_so_far": 4.6108003448452e+16,
      "budget_used_percent": 46.108003448452
    },
    {
      "type": "training",
      "description": "Training step 1942",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:45",
      "total_flops_so_far": 4.613174232058768e+16,
      "budget_used_percent": 46.13174232058768
    },
    {
      "type": "training",
      "description": "Training step 1943",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:46",
      "total_flops_so_far": 4.615548119272336e+16,
      "budget_used_percent": 46.15548119272336
    },
    {
      "type": "training",
      "description": "Training step 1944",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:48",
      "total_flops_so_far": 4.617922006485904e+16,
      "budget_used_percent": 46.17922006485904
    },
    {
      "type": "training",
      "description": "Training step 1945",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:49",
      "total_flops_so_far": 4.620295893699472e+16,
      "budget_used_percent": 46.20295893699472
    },
    {
      "type": "training",
      "description": "Training step 1946",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:50",
      "total_flops_so_far": 4.62266978091304e+16,
      "budget_used_percent": 46.2266978091304
    },
    {
      "type": "training",
      "description": "Training step 1947",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:52",
      "total_flops_so_far": 4.625043668126608e+16,
      "budget_used_percent": 46.25043668126608
    },
    {
      "type": "training",
      "description": "Training step 1948",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:53",
      "total_flops_so_far": 4.627417555340176e+16,
      "budget_used_percent": 46.27417555340176
    },
    {
      "type": "training",
      "description": "Training step 1949",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:54",
      "total_flops_so_far": 4.629791442553744e+16,
      "budget_used_percent": 46.29791442553744
    },
    {
      "type": "training",
      "description": "Training step 1950",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:55",
      "total_flops_so_far": 4.632165329767312e+16,
      "budget_used_percent": 46.32165329767312
    },
    {
      "type": "training",
      "description": "Training step 1951",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:57",
      "total_flops_so_far": 4.63453921698088e+16,
      "budget_used_percent": 46.345392169808804
    },
    {
      "type": "training",
      "description": "Training step 1952",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:58",
      "total_flops_so_far": 4.636913104194448e+16,
      "budget_used_percent": 46.36913104194448
    },
    {
      "type": "training",
      "description": "Training step 1953",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:14:59",
      "total_flops_so_far": 4.639286991408016e+16,
      "budget_used_percent": 46.39286991408016
    },
    {
      "type": "training",
      "description": "Training step 1954",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:01",
      "total_flops_so_far": 4.641660878621584e+16,
      "budget_used_percent": 46.41660878621584
    },
    {
      "type": "training",
      "description": "Training step 1955",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:02",
      "total_flops_so_far": 4.644034765835152e+16,
      "budget_used_percent": 46.44034765835152
    },
    {
      "type": "training",
      "description": "Training step 1956",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:03",
      "total_flops_so_far": 4.64640865304872e+16,
      "budget_used_percent": 46.4640865304872
    },
    {
      "type": "training",
      "description": "Training step 1957",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:05",
      "total_flops_so_far": 4.648782540262288e+16,
      "budget_used_percent": 46.48782540262288
    },
    {
      "type": "training",
      "description": "Training step 1958",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:06",
      "total_flops_so_far": 4.651156427475856e+16,
      "budget_used_percent": 46.51156427475856
    },
    {
      "type": "training",
      "description": "Training step 1959",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:07",
      "total_flops_so_far": 4.653530314689424e+16,
      "budget_used_percent": 46.53530314689424
    },
    {
      "type": "training",
      "description": "Training step 1960",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:09",
      "total_flops_so_far": 4.655904201902992e+16,
      "budget_used_percent": 46.559042019029924
    },
    {
      "type": "training",
      "description": "Training step 1961",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:10",
      "total_flops_so_far": 4.65827808911656e+16,
      "budget_used_percent": 46.5827808911656
    },
    {
      "type": "training",
      "description": "Training step 1962",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:11",
      "total_flops_so_far": 4.660651976330128e+16,
      "budget_used_percent": 46.60651976330128
    },
    {
      "type": "training",
      "description": "Training step 1963",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:13",
      "total_flops_so_far": 4.663025863543696e+16,
      "budget_used_percent": 46.63025863543696
    },
    {
      "type": "training",
      "description": "Training step 1964",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:14",
      "total_flops_so_far": 4.665399750757264e+16,
      "budget_used_percent": 46.65399750757264
    },
    {
      "type": "training",
      "description": "Training step 1965",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:15",
      "total_flops_so_far": 4.667773637970832e+16,
      "budget_used_percent": 46.67773637970832
    },
    {
      "type": "training",
      "description": "Training step 1966",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:17",
      "total_flops_so_far": 4.6701475251844e+16,
      "budget_used_percent": 46.701475251844
    },
    {
      "type": "training",
      "description": "Training step 1967",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:18",
      "total_flops_so_far": 4.672521412397968e+16,
      "budget_used_percent": 46.72521412397968
    },
    {
      "type": "training",
      "description": "Training step 1968",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:19",
      "total_flops_so_far": 4.674895299611536e+16,
      "budget_used_percent": 46.748952996115364
    },
    {
      "type": "training",
      "description": "Training step 1969",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:21",
      "total_flops_so_far": 4.677269186825104e+16,
      "budget_used_percent": 46.77269186825104
    },
    {
      "type": "training",
      "description": "Training step 1970",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:22",
      "total_flops_so_far": 4.679643074038672e+16,
      "budget_used_percent": 46.79643074038672
    },
    {
      "type": "training",
      "description": "Training step 1971",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:23",
      "total_flops_so_far": 4.68201696125224e+16,
      "budget_used_percent": 46.8201696125224
    },
    {
      "type": "training",
      "description": "Training step 1972",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:25",
      "total_flops_so_far": 4.684390848465808e+16,
      "budget_used_percent": 46.84390848465808
    },
    {
      "type": "training",
      "description": "Training step 1973",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:26",
      "total_flops_so_far": 4.686764735679376e+16,
      "budget_used_percent": 46.86764735679376
    },
    {
      "type": "training",
      "description": "Training step 1974",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:27",
      "total_flops_so_far": 4.689138622892944e+16,
      "budget_used_percent": 46.89138622892944
    },
    {
      "type": "training",
      "description": "Training step 1975",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:29",
      "total_flops_so_far": 4.691512510106512e+16,
      "budget_used_percent": 46.91512510106512
    },
    {
      "type": "training",
      "description": "Training step 1976",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:30",
      "total_flops_so_far": 4.69388639732008e+16,
      "budget_used_percent": 46.9388639732008
    },
    {
      "type": "training",
      "description": "Training step 1977",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:31",
      "total_flops_so_far": 4.696260284533648e+16,
      "budget_used_percent": 46.962602845336484
    },
    {
      "type": "training",
      "description": "Training step 1978",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:33",
      "total_flops_so_far": 4.698634171747216e+16,
      "budget_used_percent": 46.98634171747216
    },
    {
      "type": "training",
      "description": "Training step 1979",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:34",
      "total_flops_so_far": 4.701008058960784e+16,
      "budget_used_percent": 47.01008058960784
    },
    {
      "type": "training",
      "description": "Training step 1980",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:34",
      "total_flops_so_far": 4.703381946174352e+16,
      "budget_used_percent": 47.03381946174352
    },
    {
      "type": "training",
      "description": "Training step 1981",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:36",
      "total_flops_so_far": 4.70575583338792e+16,
      "budget_used_percent": 47.0575583338792
    },
    {
      "type": "training",
      "description": "Training step 1982",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:37",
      "total_flops_so_far": 4.708129720601488e+16,
      "budget_used_percent": 47.08129720601488
    },
    {
      "type": "training",
      "description": "Training step 1983",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:38",
      "total_flops_so_far": 4.710503607815056e+16,
      "budget_used_percent": 47.10503607815056
    },
    {
      "type": "training",
      "description": "Training step 1984",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:40",
      "total_flops_so_far": 4.712877495028624e+16,
      "budget_used_percent": 47.12877495028624
    },
    {
      "type": "training",
      "description": "Training step 1985",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:41",
      "total_flops_so_far": 4.715251382242192e+16,
      "budget_used_percent": 47.15251382242192
    },
    {
      "type": "training",
      "description": "Training step 1986",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:42",
      "total_flops_so_far": 4.71762526945576e+16,
      "budget_used_percent": 47.176252694557604
    },
    {
      "type": "training",
      "description": "Training step 1987",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:44",
      "total_flops_so_far": 4.719999156669328e+16,
      "budget_used_percent": 47.19999156669328
    },
    {
      "type": "training",
      "description": "Training step 1988",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:45",
      "total_flops_so_far": 4.722373043882896e+16,
      "budget_used_percent": 47.22373043882896
    },
    {
      "type": "training",
      "description": "Training step 1989",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:46",
      "total_flops_so_far": 4.724746931096464e+16,
      "budget_used_percent": 47.24746931096464
    },
    {
      "type": "training",
      "description": "Training step 1990",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:48",
      "total_flops_so_far": 4.727120818310032e+16,
      "budget_used_percent": 47.27120818310032
    },
    {
      "type": "training",
      "description": "Training step 1991",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:49",
      "total_flops_so_far": 4.7294947055236e+16,
      "budget_used_percent": 47.294947055235994
    },
    {
      "type": "training",
      "description": "Training step 1992",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:50",
      "total_flops_so_far": 4.731868592737168e+16,
      "budget_used_percent": 47.31868592737168
    },
    {
      "type": "training",
      "description": "Training step 1993",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:52",
      "total_flops_so_far": 4.734242479950736e+16,
      "budget_used_percent": 47.34242479950736
    },
    {
      "type": "training",
      "description": "Training step 1994",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:53",
      "total_flops_so_far": 4.736616367164304e+16,
      "budget_used_percent": 47.366163671643044
    },
    {
      "type": "training",
      "description": "Training step 1995",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:54",
      "total_flops_so_far": 4.738990254377872e+16,
      "budget_used_percent": 47.389902543778724
    },
    {
      "type": "training",
      "description": "Training step 1996",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:56",
      "total_flops_so_far": 4.74136414159144e+16,
      "budget_used_percent": 47.4136414159144
    },
    {
      "type": "training",
      "description": "Training step 1997",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:57",
      "total_flops_so_far": 4.743738028805008e+16,
      "budget_used_percent": 47.43738028805008
    },
    {
      "type": "training",
      "description": "Training step 1998",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:15:58",
      "total_flops_so_far": 4.746111916018576e+16,
      "budget_used_percent": 47.46111916018576
    },
    {
      "type": "training",
      "description": "Training step 1999",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:16:00",
      "total_flops_so_far": 4.748485803232144e+16,
      "budget_used_percent": 47.48485803232144
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 709803614656.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:16:06",
      "total_flops_so_far": 4.74855678359361e+16,
      "budget_used_percent": 47.485567835936095
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 713504058416.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:16:11",
      "total_flops_so_far": 4.748628133999451e+16,
      "budget_used_percent": 47.48628133999451
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711653476344.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:16:16",
      "total_flops_so_far": 4.7486992993470856e+16,
      "budget_used_percent": 47.48699299347086
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 709803614656.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:16:21",
      "total_flops_so_far": 4.748770279708551e+16,
      "budget_used_percent": 47.48770279708551
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712578677332.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:16:26",
      "total_flops_so_far": 4.748841537576285e+16,
      "budget_used_percent": 47.48841537576285
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 5",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 709803614656.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:16:31",
      "total_flops_so_far": 4.74891251793775e+16,
      "budget_used_percent": 47.4891251793775
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 6",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711653476344.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:16:36",
      "total_flops_so_far": 4.748983683285385e+16,
      "budget_used_percent": 47.48983683285385
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 7",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711653476344.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:16:41",
      "total_flops_so_far": 4.749054848633019e+16,
      "budget_used_percent": 47.49054848633019
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 8",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711653476344.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:16:46",
      "total_flops_so_far": 4.7491260139806536e+16,
      "budget_used_percent": 47.491260139806535
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 9",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711653476344.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:16:51",
      "total_flops_so_far": 4.749197179328288e+16,
      "budget_used_percent": 47.49197179328288
    },
    {
      "type": "training",
      "description": "Training step 2000",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:16:51",
      "total_flops_so_far": 4.751571066541856e+16,
      "budget_used_percent": 47.51571066541856
    },
    {
      "type": "training",
      "description": "Training step 2001",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:16:52",
      "total_flops_so_far": 4.753944953755424e+16,
      "budget_used_percent": 47.53944953755424
    },
    {
      "type": "training",
      "description": "Training step 2002",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:16:54",
      "total_flops_so_far": 4.756318840968992e+16,
      "budget_used_percent": 47.56318840968992
    },
    {
      "type": "training",
      "description": "Training step 2003",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:16:55",
      "total_flops_so_far": 4.75869272818256e+16,
      "budget_used_percent": 47.586927281825595
    },
    {
      "type": "training",
      "description": "Training step 2004",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:16:56",
      "total_flops_so_far": 4.761066615396128e+16,
      "budget_used_percent": 47.610666153961276
    },
    {
      "type": "training",
      "description": "Training step 2005",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:16:58",
      "total_flops_so_far": 4.763440502609696e+16,
      "budget_used_percent": 47.63440502609696
    },
    {
      "type": "training",
      "description": "Training step 2006",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:16:59",
      "total_flops_so_far": 4.765814389823264e+16,
      "budget_used_percent": 47.65814389823264
    },
    {
      "type": "training",
      "description": "Training step 2007",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:00",
      "total_flops_so_far": 4.768188277036832e+16,
      "budget_used_percent": 47.681882770368325
    },
    {
      "type": "training",
      "description": "Training step 2008",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:02",
      "total_flops_so_far": 4.7705621642504e+16,
      "budget_used_percent": 47.705621642504
    },
    {
      "type": "training",
      "description": "Training step 2009",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:03",
      "total_flops_so_far": 4.772936051463968e+16,
      "budget_used_percent": 47.72936051463968
    },
    {
      "type": "training",
      "description": "Training step 2010",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:04",
      "total_flops_so_far": 4.775309938677536e+16,
      "budget_used_percent": 47.75309938677536
    },
    {
      "type": "training",
      "description": "Training step 2011",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:06",
      "total_flops_so_far": 4.777683825891104e+16,
      "budget_used_percent": 47.77683825891104
    },
    {
      "type": "training",
      "description": "Training step 2012",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:07",
      "total_flops_so_far": 4.780057713104672e+16,
      "budget_used_percent": 47.800577131046715
    },
    {
      "type": "training",
      "description": "Training step 2013",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:08",
      "total_flops_so_far": 4.78243160031824e+16,
      "budget_used_percent": 47.824316003182396
    },
    {
      "type": "training",
      "description": "Training step 2014",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:10",
      "total_flops_so_far": 4.784805487531808e+16,
      "budget_used_percent": 47.84805487531808
    },
    {
      "type": "training",
      "description": "Training step 2015",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:11",
      "total_flops_so_far": 4.787179374745376e+16,
      "budget_used_percent": 47.871793747453765
    },
    {
      "type": "training",
      "description": "Training step 2016",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:12",
      "total_flops_so_far": 4.789553261958944e+16,
      "budget_used_percent": 47.895532619589446
    },
    {
      "type": "training",
      "description": "Training step 2017",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:14",
      "total_flops_so_far": 4.791927149172512e+16,
      "budget_used_percent": 47.91927149172512
    },
    {
      "type": "training",
      "description": "Training step 2018",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:15",
      "total_flops_so_far": 4.79430103638608e+16,
      "budget_used_percent": 47.9430103638608
    },
    {
      "type": "training",
      "description": "Training step 2019",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:16",
      "total_flops_so_far": 4.796674923599648e+16,
      "budget_used_percent": 47.96674923599648
    },
    {
      "type": "training",
      "description": "Training step 2020",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:18",
      "total_flops_so_far": 4.799048810813216e+16,
      "budget_used_percent": 47.99048810813216
    },
    {
      "type": "training",
      "description": "Training step 2021",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:19",
      "total_flops_so_far": 4.801422698026784e+16,
      "budget_used_percent": 48.014226980267836
    },
    {
      "type": "training",
      "description": "Training step 2022",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:20",
      "total_flops_so_far": 4.803796585240352e+16,
      "budget_used_percent": 48.03796585240352
    },
    {
      "type": "training",
      "description": "Training step 2023",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:22",
      "total_flops_so_far": 4.80617047245392e+16,
      "budget_used_percent": 48.0617047245392
    },
    {
      "type": "training",
      "description": "Training step 2024",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:23",
      "total_flops_so_far": 4.808544359667488e+16,
      "budget_used_percent": 48.08544359667488
    },
    {
      "type": "training",
      "description": "Training step 2025",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:24",
      "total_flops_so_far": 4.810918246881056e+16,
      "budget_used_percent": 48.10918246881056
    },
    {
      "type": "training",
      "description": "Training step 2026",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:26",
      "total_flops_so_far": 4.813292134094624e+16,
      "budget_used_percent": 48.13292134094624
    },
    {
      "type": "training",
      "description": "Training step 2027",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:27",
      "total_flops_so_far": 4.815666021308192e+16,
      "budget_used_percent": 48.15666021308192
    },
    {
      "type": "training",
      "description": "Training step 2028",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:28",
      "total_flops_so_far": 4.81803990852176e+16,
      "budget_used_percent": 48.1803990852176
    },
    {
      "type": "training",
      "description": "Training step 2029",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:30",
      "total_flops_so_far": 4.820413795735328e+16,
      "budget_used_percent": 48.20413795735328
    },
    {
      "type": "training",
      "description": "Training step 2030",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:31",
      "total_flops_so_far": 4.822787682948896e+16,
      "budget_used_percent": 48.227876829488956
    },
    {
      "type": "training",
      "description": "Training step 2031",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:32",
      "total_flops_so_far": 4.825161570162464e+16,
      "budget_used_percent": 48.25161570162464
    },
    {
      "type": "training",
      "description": "Training step 2032",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:34",
      "total_flops_so_far": 4.827535457376032e+16,
      "budget_used_percent": 48.27535457376032
    },
    {
      "type": "training",
      "description": "Training step 2033",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:35",
      "total_flops_so_far": 4.8299093445896e+16,
      "budget_used_percent": 48.299093445896006
    },
    {
      "type": "training",
      "description": "Training step 2034",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:36",
      "total_flops_so_far": 4.832283231803168e+16,
      "budget_used_percent": 48.32283231803168
    },
    {
      "type": "training",
      "description": "Training step 2035",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:37",
      "total_flops_so_far": 4.834657119016736e+16,
      "budget_used_percent": 48.34657119016736
    },
    {
      "type": "training",
      "description": "Training step 2036",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:39",
      "total_flops_so_far": 4.837031006230304e+16,
      "budget_used_percent": 48.37031006230304
    },
    {
      "type": "training",
      "description": "Training step 2037",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:40",
      "total_flops_so_far": 4.839404893443872e+16,
      "budget_used_percent": 48.39404893443872
    },
    {
      "type": "training",
      "description": "Training step 2038",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:41",
      "total_flops_so_far": 4.84177878065744e+16,
      "budget_used_percent": 48.4177878065744
    },
    {
      "type": "training",
      "description": "Training step 2039",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:43",
      "total_flops_so_far": 4.844152667871008e+16,
      "budget_used_percent": 48.441526678710076
    },
    {
      "type": "training",
      "description": "Training step 2040",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:44",
      "total_flops_so_far": 4.846526555084576e+16,
      "budget_used_percent": 48.46526555084576
    },
    {
      "type": "training",
      "description": "Training step 2041",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:45",
      "total_flops_so_far": 4.848900442298144e+16,
      "budget_used_percent": 48.48900442298144
    },
    {
      "type": "training",
      "description": "Training step 2042",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:47",
      "total_flops_so_far": 4.851274329511712e+16,
      "budget_used_percent": 48.51274329511712
    },
    {
      "type": "training",
      "description": "Training step 2043",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:48",
      "total_flops_so_far": 4.85364821672528e+16,
      "budget_used_percent": 48.5364821672528
    },
    {
      "type": "training",
      "description": "Training step 2044",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:49",
      "total_flops_so_far": 4.856022103938848e+16,
      "budget_used_percent": 48.56022103938848
    },
    {
      "type": "training",
      "description": "Training step 2045",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:51",
      "total_flops_so_far": 4.858395991152416e+16,
      "budget_used_percent": 48.58395991152416
    },
    {
      "type": "training",
      "description": "Training step 2046",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:52",
      "total_flops_so_far": 4.860769878365984e+16,
      "budget_used_percent": 48.60769878365984
    },
    {
      "type": "training",
      "description": "Training step 2047",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:53",
      "total_flops_so_far": 4.863143765579552e+16,
      "budget_used_percent": 48.631437655795516
    },
    {
      "type": "training",
      "description": "Training step 2048",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:55",
      "total_flops_so_far": 4.86551765279312e+16,
      "budget_used_percent": 48.6551765279312
    },
    {
      "type": "training",
      "description": "Training step 2049",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:56",
      "total_flops_so_far": 4.867891540006688e+16,
      "budget_used_percent": 48.67891540006688
    },
    {
      "type": "training",
      "description": "Training step 2050",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:57",
      "total_flops_so_far": 4.870265427220256e+16,
      "budget_used_percent": 48.70265427220256
    },
    {
      "type": "training",
      "description": "Training step 2051",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:17:59",
      "total_flops_so_far": 4.872639314433824e+16,
      "budget_used_percent": 48.726393144338246
    },
    {
      "type": "training",
      "description": "Training step 2052",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:00",
      "total_flops_so_far": 4.875013201647392e+16,
      "budget_used_percent": 48.75013201647392
    },
    {
      "type": "training",
      "description": "Training step 2053",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:01",
      "total_flops_so_far": 4.87738708886096e+16,
      "budget_used_percent": 48.7738708886096
    },
    {
      "type": "training",
      "description": "Training step 2054",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:03",
      "total_flops_so_far": 4.879760976074528e+16,
      "budget_used_percent": 48.79760976074528
    },
    {
      "type": "training",
      "description": "Training step 2055",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:04",
      "total_flops_so_far": 4.882134863288096e+16,
      "budget_used_percent": 48.82134863288096
    },
    {
      "type": "training",
      "description": "Training step 2056",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:05",
      "total_flops_so_far": 4.884508750501664e+16,
      "budget_used_percent": 48.845087505016636
    },
    {
      "type": "training",
      "description": "Training step 2057",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:07",
      "total_flops_so_far": 4.886882637715232e+16,
      "budget_used_percent": 48.86882637715232
    },
    {
      "type": "training",
      "description": "Training step 2058",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:08",
      "total_flops_so_far": 4.8892565249288e+16,
      "budget_used_percent": 48.892565249288
    },
    {
      "type": "training",
      "description": "Training step 2059",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:09",
      "total_flops_so_far": 4.891630412142368e+16,
      "budget_used_percent": 48.91630412142368
    },
    {
      "type": "training",
      "description": "Training step 2060",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:11",
      "total_flops_so_far": 4.894004299355936e+16,
      "budget_used_percent": 48.94004299355936
    },
    {
      "type": "training",
      "description": "Training step 2061",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:12",
      "total_flops_so_far": 4.896378186569504e+16,
      "budget_used_percent": 48.96378186569504
    },
    {
      "type": "training",
      "description": "Training step 2062",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:13",
      "total_flops_so_far": 4.898752073783072e+16,
      "budget_used_percent": 48.98752073783072
    },
    {
      "type": "training",
      "description": "Training step 2063",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:15",
      "total_flops_so_far": 4.90112596099664e+16,
      "budget_used_percent": 49.0112596099664
    },
    {
      "type": "training",
      "description": "Training step 2064",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:16",
      "total_flops_so_far": 4.903499848210208e+16,
      "budget_used_percent": 49.03499848210208
    },
    {
      "type": "training",
      "description": "Training step 2065",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:17",
      "total_flops_so_far": 4.905873735423776e+16,
      "budget_used_percent": 49.058737354237756
    },
    {
      "type": "training",
      "description": "Training step 2066",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:19",
      "total_flops_so_far": 4.908247622637344e+16,
      "budget_used_percent": 49.08247622637344
    },
    {
      "type": "training",
      "description": "Training step 2067",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:20",
      "total_flops_so_far": 4.910621509850912e+16,
      "budget_used_percent": 49.10621509850912
    },
    {
      "type": "training",
      "description": "Training step 2068",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:21",
      "total_flops_so_far": 4.91299539706448e+16,
      "budget_used_percent": 49.1299539706448
    },
    {
      "type": "training",
      "description": "Training step 2069",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:23",
      "total_flops_so_far": 4.915369284278048e+16,
      "budget_used_percent": 49.15369284278048
    },
    {
      "type": "training",
      "description": "Training step 2070",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:24",
      "total_flops_so_far": 4.917743171491616e+16,
      "budget_used_percent": 49.17743171491616
    },
    {
      "type": "training",
      "description": "Training step 2071",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:25",
      "total_flops_so_far": 4.920117058705184e+16,
      "budget_used_percent": 49.20117058705184
    },
    {
      "type": "training",
      "description": "Training step 2072",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:27",
      "total_flops_so_far": 4.922490945918752e+16,
      "budget_used_percent": 49.22490945918752
    },
    {
      "type": "training",
      "description": "Training step 2073",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:28",
      "total_flops_so_far": 4.92486483313232e+16,
      "budget_used_percent": 49.2486483313232
    },
    {
      "type": "training",
      "description": "Training step 2074",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:29",
      "total_flops_so_far": 4.927238720345888e+16,
      "budget_used_percent": 49.27238720345888
    },
    {
      "type": "training",
      "description": "Training step 2075",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:31",
      "total_flops_so_far": 4.929612607559456e+16,
      "budget_used_percent": 49.29612607559456
    },
    {
      "type": "training",
      "description": "Training step 2076",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:32",
      "total_flops_so_far": 4.931986494773024e+16,
      "budget_used_percent": 49.31986494773024
    },
    {
      "type": "training",
      "description": "Training step 2077",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:33",
      "total_flops_so_far": 4.934360381986592e+16,
      "budget_used_percent": 49.34360381986592
    },
    {
      "type": "training",
      "description": "Training step 2078",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:35",
      "total_flops_so_far": 4.93673426920016e+16,
      "budget_used_percent": 49.3673426920016
    },
    {
      "type": "training",
      "description": "Training step 2079",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:36",
      "total_flops_so_far": 4.939108156413728e+16,
      "budget_used_percent": 49.39108156413728
    },
    {
      "type": "training",
      "description": "Training step 2080",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:37",
      "total_flops_so_far": 4.941482043627296e+16,
      "budget_used_percent": 49.41482043627296
    },
    {
      "type": "training",
      "description": "Training step 2081",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:39",
      "total_flops_so_far": 4.943855930840864e+16,
      "budget_used_percent": 49.43855930840864
    },
    {
      "type": "training",
      "description": "Training step 2082",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:40",
      "total_flops_so_far": 4.946229818054432e+16,
      "budget_used_percent": 49.462298180544316
    },
    {
      "type": "training",
      "description": "Training step 2083",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:41",
      "total_flops_so_far": 4.948603705268e+16,
      "budget_used_percent": 49.48603705268
    },
    {
      "type": "training",
      "description": "Training step 2084",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:43",
      "total_flops_so_far": 4.950977592481568e+16,
      "budget_used_percent": 49.50977592481568
    },
    {
      "type": "training",
      "description": "Training step 2085",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:44",
      "total_flops_so_far": 4.953351479695136e+16,
      "budget_used_percent": 49.53351479695136
    },
    {
      "type": "training",
      "description": "Training step 2086",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:45",
      "total_flops_so_far": 4.955725366908704e+16,
      "budget_used_percent": 49.55725366908704
    },
    {
      "type": "training",
      "description": "Training step 2087",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:47",
      "total_flops_so_far": 4.958099254122272e+16,
      "budget_used_percent": 49.58099254122272
    },
    {
      "type": "training",
      "description": "Training step 2088",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:48",
      "total_flops_so_far": 4.96047314133584e+16,
      "budget_used_percent": 49.6047314133584
    },
    {
      "type": "training",
      "description": "Training step 2089",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:49",
      "total_flops_so_far": 4.962847028549408e+16,
      "budget_used_percent": 49.62847028549408
    },
    {
      "type": "training",
      "description": "Training step 2090",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:51",
      "total_flops_so_far": 4.965220915762976e+16,
      "budget_used_percent": 49.65220915762976
    },
    {
      "type": "training",
      "description": "Training step 2091",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:52",
      "total_flops_so_far": 4.967594802976544e+16,
      "budget_used_percent": 49.675948029765436
    },
    {
      "type": "training",
      "description": "Training step 2092",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:53",
      "total_flops_so_far": 4.969968690190112e+16,
      "budget_used_percent": 49.69968690190112
    },
    {
      "type": "training",
      "description": "Training step 2093",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:55",
      "total_flops_so_far": 4.97234257740368e+16,
      "budget_used_percent": 49.7234257740368
    },
    {
      "type": "training",
      "description": "Training step 2094",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:56",
      "total_flops_so_far": 4.974716464617248e+16,
      "budget_used_percent": 49.74716464617248
    },
    {
      "type": "training",
      "description": "Training step 2095",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:57",
      "total_flops_so_far": 4.977090351830816e+16,
      "budget_used_percent": 49.77090351830816
    },
    {
      "type": "training",
      "description": "Training step 2096",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:18:59",
      "total_flops_so_far": 4.979464239044384e+16,
      "budget_used_percent": 49.79464239044384
    },
    {
      "type": "training",
      "description": "Training step 2097",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:00",
      "total_flops_so_far": 4.981838126257952e+16,
      "budget_used_percent": 49.81838126257952
    },
    {
      "type": "training",
      "description": "Training step 2098",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:01",
      "total_flops_so_far": 4.98421201347152e+16,
      "budget_used_percent": 49.8421201347152
    },
    {
      "type": "training",
      "description": "Training step 2099",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:03",
      "total_flops_so_far": 4.986585900685088e+16,
      "budget_used_percent": 49.86585900685088
    },
    {
      "type": "training",
      "description": "Training step 2100",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:04",
      "total_flops_so_far": 4.988959787898656e+16,
      "budget_used_percent": 49.88959787898656
    },
    {
      "type": "training",
      "description": "Training step 2101",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:05",
      "total_flops_so_far": 4.991333675112224e+16,
      "budget_used_percent": 49.91333675112224
    },
    {
      "type": "training",
      "description": "Training step 2102",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:07",
      "total_flops_so_far": 4.993707562325792e+16,
      "budget_used_percent": 49.93707562325792
    },
    {
      "type": "training",
      "description": "Training step 2103",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:08",
      "total_flops_so_far": 4.99608144953936e+16,
      "budget_used_percent": 49.9608144953936
    },
    {
      "type": "training",
      "description": "Training step 2104",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:09",
      "total_flops_so_far": 4.998455336752928e+16,
      "budget_used_percent": 49.98455336752928
    },
    {
      "type": "training",
      "description": "Training step 2105",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:10",
      "total_flops_so_far": 5.000829223966496e+16,
      "budget_used_percent": 50.00829223966497
    },
    {
      "type": "training",
      "description": "Training step 2106",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:12",
      "total_flops_so_far": 5.003203111180064e+16,
      "budget_used_percent": 50.03203111180065
    },
    {
      "type": "training",
      "description": "Training step 2107",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:13",
      "total_flops_so_far": 5.005576998393632e+16,
      "budget_used_percent": 50.055769983936315
    },
    {
      "type": "training",
      "description": "Training step 2108",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:15",
      "total_flops_so_far": 5.0079508856072e+16,
      "budget_used_percent": 50.079508856071996
    },
    {
      "type": "training",
      "description": "Training step 2109",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:16",
      "total_flops_so_far": 5.010324772820768e+16,
      "budget_used_percent": 50.10324772820768
    },
    {
      "type": "training",
      "description": "Training step 2110",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:17",
      "total_flops_so_far": 5.012698660034336e+16,
      "budget_used_percent": 50.12698660034336
    },
    {
      "type": "training",
      "description": "Training step 2111",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:18",
      "total_flops_so_far": 5.015072547247904e+16,
      "budget_used_percent": 50.15072547247904
    },
    {
      "type": "training",
      "description": "Training step 2112",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:19",
      "total_flops_so_far": 5.017446434461472e+16,
      "budget_used_percent": 50.17446434461472
    },
    {
      "type": "training",
      "description": "Training step 2113",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:20",
      "total_flops_so_far": 5.01982032167504e+16,
      "budget_used_percent": 50.1982032167504
    },
    {
      "type": "training",
      "description": "Training step 2114",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:22",
      "total_flops_so_far": 5.022194208888608e+16,
      "budget_used_percent": 50.22194208888608
    },
    {
      "type": "training",
      "description": "Training step 2115",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:23",
      "total_flops_so_far": 5.024568096102176e+16,
      "budget_used_percent": 50.245680961021755
    },
    {
      "type": "training",
      "description": "Training step 2116",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:24",
      "total_flops_so_far": 5.026941983315744e+16,
      "budget_used_percent": 50.269419833157436
    },
    {
      "type": "training",
      "description": "Training step 2117",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:26",
      "total_flops_so_far": 5.029315870529312e+16,
      "budget_used_percent": 50.293158705293116
    },
    {
      "type": "training",
      "description": "Training step 2118",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:27",
      "total_flops_so_far": 5.03168975774288e+16,
      "budget_used_percent": 50.3168975774288
    },
    {
      "type": "training",
      "description": "Training step 2119",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:28",
      "total_flops_so_far": 5.034063644956448e+16,
      "budget_used_percent": 50.34063644956448
    },
    {
      "type": "training",
      "description": "Training step 2120",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:30",
      "total_flops_so_far": 5.036437532170016e+16,
      "budget_used_percent": 50.36437532170016
    },
    {
      "type": "training",
      "description": "Training step 2121",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:31",
      "total_flops_so_far": 5.038811419383584e+16,
      "budget_used_percent": 50.38811419383584
    },
    {
      "type": "training",
      "description": "Training step 2122",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:32",
      "total_flops_so_far": 5.041185306597152e+16,
      "budget_used_percent": 50.41185306597152
    },
    {
      "type": "training",
      "description": "Training step 2123",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:34",
      "total_flops_so_far": 5.04355919381072e+16,
      "budget_used_percent": 50.43559193810721
    },
    {
      "type": "training",
      "description": "Training step 2124",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:35",
      "total_flops_so_far": 5.045933081024288e+16,
      "budget_used_percent": 50.459330810242875
    },
    {
      "type": "training",
      "description": "Training step 2125",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:36",
      "total_flops_so_far": 5.048306968237856e+16,
      "budget_used_percent": 50.483069682378556
    },
    {
      "type": "training",
      "description": "Training step 2126",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:38",
      "total_flops_so_far": 5.050680855451424e+16,
      "budget_used_percent": 50.50680855451424
    },
    {
      "type": "training",
      "description": "Training step 2127",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:39",
      "total_flops_so_far": 5.053054742664992e+16,
      "budget_used_percent": 50.53054742664992
    },
    {
      "type": "training",
      "description": "Training step 2128",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:40",
      "total_flops_so_far": 5.05542862987856e+16,
      "budget_used_percent": 50.5542862987856
    },
    {
      "type": "training",
      "description": "Training step 2129",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:41",
      "total_flops_so_far": 5.057802517092128e+16,
      "budget_used_percent": 50.57802517092128
    },
    {
      "type": "training",
      "description": "Training step 2130",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:43",
      "total_flops_so_far": 5.060176404305696e+16,
      "budget_used_percent": 50.60176404305696
    },
    {
      "type": "training",
      "description": "Training step 2131",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:44",
      "total_flops_so_far": 5.062550291519264e+16,
      "budget_used_percent": 50.62550291519264
    },
    {
      "type": "training",
      "description": "Training step 2132",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:45",
      "total_flops_so_far": 5.064924178732832e+16,
      "budget_used_percent": 50.64924178732832
    },
    {
      "type": "training",
      "description": "Training step 2133",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:47",
      "total_flops_so_far": 5.0672980659464e+16,
      "budget_used_percent": 50.672980659463995
    },
    {
      "type": "training",
      "description": "Training step 2134",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:48",
      "total_flops_so_far": 5.069671953159968e+16,
      "budget_used_percent": 50.696719531599676
    },
    {
      "type": "training",
      "description": "Training step 2135",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:49",
      "total_flops_so_far": 5.072045840373536e+16,
      "budget_used_percent": 50.72045840373536
    },
    {
      "type": "training",
      "description": "Training step 2136",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:51",
      "total_flops_so_far": 5.074419727587104e+16,
      "budget_used_percent": 50.74419727587104
    },
    {
      "type": "training",
      "description": "Training step 2137",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:52",
      "total_flops_so_far": 5.076793614800672e+16,
      "budget_used_percent": 50.76793614800672
    },
    {
      "type": "training",
      "description": "Training step 2138",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:53",
      "total_flops_so_far": 5.07916750201424e+16,
      "budget_used_percent": 50.7916750201424
    },
    {
      "type": "training",
      "description": "Training step 2139",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:55",
      "total_flops_so_far": 5.081541389227808e+16,
      "budget_used_percent": 50.81541389227808
    },
    {
      "type": "training",
      "description": "Training step 2140",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:56",
      "total_flops_so_far": 5.083915276441376e+16,
      "budget_used_percent": 50.83915276441376
    },
    {
      "type": "training",
      "description": "Training step 2141",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:57",
      "total_flops_so_far": 5.086289163654944e+16,
      "budget_used_percent": 50.86289163654945
    },
    {
      "type": "training",
      "description": "Training step 2142",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:19:59",
      "total_flops_so_far": 5.088663050868512e+16,
      "budget_used_percent": 50.886630508685116
    },
    {
      "type": "training",
      "description": "Training step 2143",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:00",
      "total_flops_so_far": 5.09103693808208e+16,
      "budget_used_percent": 50.910369380820796
    },
    {
      "type": "training",
      "description": "Training step 2144",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:01",
      "total_flops_so_far": 5.093410825295648e+16,
      "budget_used_percent": 50.93410825295648
    },
    {
      "type": "training",
      "description": "Training step 2145",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:03",
      "total_flops_so_far": 5.095784712509216e+16,
      "budget_used_percent": 50.95784712509216
    },
    {
      "type": "training",
      "description": "Training step 2146",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:04",
      "total_flops_so_far": 5.098158599722784e+16,
      "budget_used_percent": 50.98158599722784
    },
    {
      "type": "training",
      "description": "Training step 2147",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:05",
      "total_flops_so_far": 5.100532486936352e+16,
      "budget_used_percent": 51.00532486936352
    },
    {
      "type": "training",
      "description": "Training step 2148",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:07",
      "total_flops_so_far": 5.10290637414992e+16,
      "budget_used_percent": 51.0290637414992
    },
    {
      "type": "training",
      "description": "Training step 2149",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:08",
      "total_flops_so_far": 5.105280261363488e+16,
      "budget_used_percent": 51.05280261363488
    },
    {
      "type": "training",
      "description": "Training step 2150",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:10",
      "total_flops_so_far": 5.107654148577056e+16,
      "budget_used_percent": 51.076541485770555
    },
    {
      "type": "training",
      "description": "Training step 2151",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:11",
      "total_flops_so_far": 5.110028035790624e+16,
      "budget_used_percent": 51.100280357906236
    },
    {
      "type": "training",
      "description": "Training step 2152",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:12",
      "total_flops_so_far": 5.112401923004192e+16,
      "budget_used_percent": 51.12401923004192
    },
    {
      "type": "training",
      "description": "Training step 2153",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:14",
      "total_flops_so_far": 5.11477581021776e+16,
      "budget_used_percent": 51.1477581021776
    },
    {
      "type": "training",
      "description": "Training step 2154",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:15",
      "total_flops_so_far": 5.117149697431328e+16,
      "budget_used_percent": 51.17149697431328
    },
    {
      "type": "training",
      "description": "Training step 2155",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:16",
      "total_flops_so_far": 5.119523584644896e+16,
      "budget_used_percent": 51.19523584644896
    },
    {
      "type": "training",
      "description": "Training step 2156",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:18",
      "total_flops_so_far": 5.121897471858464e+16,
      "budget_used_percent": 51.21897471858464
    },
    {
      "type": "training",
      "description": "Training step 2157",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:19",
      "total_flops_so_far": 5.124271359072032e+16,
      "budget_used_percent": 51.24271359072032
    },
    {
      "type": "training",
      "description": "Training step 2158",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:20",
      "total_flops_so_far": 5.1266452462856e+16,
      "budget_used_percent": 51.266452462856
    },
    {
      "type": "training",
      "description": "Training step 2159",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:22",
      "total_flops_so_far": 5.129019133499168e+16,
      "budget_used_percent": 51.290191334991675
    },
    {
      "type": "training",
      "description": "Training step 2160",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:23",
      "total_flops_so_far": 5.131393020712736e+16,
      "budget_used_percent": 51.313930207127356
    },
    {
      "type": "training",
      "description": "Training step 2161",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:24",
      "total_flops_so_far": 5.133766907926304e+16,
      "budget_used_percent": 51.33766907926304
    },
    {
      "type": "training",
      "description": "Training step 2162",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:26",
      "total_flops_so_far": 5.136140795139872e+16,
      "budget_used_percent": 51.36140795139872
    },
    {
      "type": "training",
      "description": "Training step 2163",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:27",
      "total_flops_so_far": 5.13851468235344e+16,
      "budget_used_percent": 51.3851468235344
    },
    {
      "type": "training",
      "description": "Training step 2164",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:28",
      "total_flops_so_far": 5.140888569567008e+16,
      "budget_used_percent": 51.40888569567008
    },
    {
      "type": "training",
      "description": "Training step 2165",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:30",
      "total_flops_so_far": 5.143262456780576e+16,
      "budget_used_percent": 51.43262456780576
    },
    {
      "type": "training",
      "description": "Training step 2166",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:31",
      "total_flops_so_far": 5.145636343994144e+16,
      "budget_used_percent": 51.45636343994144
    },
    {
      "type": "training",
      "description": "Training step 2167",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:32",
      "total_flops_so_far": 5.148010231207712e+16,
      "budget_used_percent": 51.48010231207712
    },
    {
      "type": "training",
      "description": "Training step 2168",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:34",
      "total_flops_so_far": 5.15038411842128e+16,
      "budget_used_percent": 51.503841184212796
    },
    {
      "type": "training",
      "description": "Training step 2169",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:35",
      "total_flops_so_far": 5.152758005634848e+16,
      "budget_used_percent": 51.52758005634848
    },
    {
      "type": "training",
      "description": "Training step 2170",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:36",
      "total_flops_so_far": 5.155131892848416e+16,
      "budget_used_percent": 51.55131892848416
    },
    {
      "type": "training",
      "description": "Training step 2171",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:38",
      "total_flops_so_far": 5.157505780061984e+16,
      "budget_used_percent": 51.57505780061984
    },
    {
      "type": "training",
      "description": "Training step 2172",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:39",
      "total_flops_so_far": 5.159879667275552e+16,
      "budget_used_percent": 51.59879667275552
    },
    {
      "type": "training",
      "description": "Training step 2173",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:40",
      "total_flops_so_far": 5.16225355448912e+16,
      "budget_used_percent": 51.6225355448912
    },
    {
      "type": "training",
      "description": "Training step 2174",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:42",
      "total_flops_so_far": 5.164627441702688e+16,
      "budget_used_percent": 51.64627441702688
    },
    {
      "type": "training",
      "description": "Training step 2175",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:43",
      "total_flops_so_far": 5.167001328916256e+16,
      "budget_used_percent": 51.67001328916256
    },
    {
      "type": "training",
      "description": "Training step 2176",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:44",
      "total_flops_so_far": 5.169375216129824e+16,
      "budget_used_percent": 51.69375216129824
    },
    {
      "type": "training",
      "description": "Training step 2177",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:46",
      "total_flops_so_far": 5.171749103343392e+16,
      "budget_used_percent": 51.717491033433916
    },
    {
      "type": "training",
      "description": "Training step 2178",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:47",
      "total_flops_so_far": 5.17412299055696e+16,
      "budget_used_percent": 51.7412299055696
    },
    {
      "type": "training",
      "description": "Training step 2179",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:48",
      "total_flops_so_far": 5.176496877770528e+16,
      "budget_used_percent": 51.76496877770528
    },
    {
      "type": "training",
      "description": "Training step 2180",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:49",
      "total_flops_so_far": 5.178870764984096e+16,
      "budget_used_percent": 51.78870764984096
    },
    {
      "type": "training",
      "description": "Training step 2181",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:51",
      "total_flops_so_far": 5.181244652197664e+16,
      "budget_used_percent": 51.81244652197664
    },
    {
      "type": "training",
      "description": "Training step 2182",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:52",
      "total_flops_so_far": 5.183618539411232e+16,
      "budget_used_percent": 51.83618539411232
    },
    {
      "type": "training",
      "description": "Training step 2183",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:53",
      "total_flops_so_far": 5.1859924266248e+16,
      "budget_used_percent": 51.859924266248
    },
    {
      "type": "training",
      "description": "Training step 2184",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:55",
      "total_flops_so_far": 5.188366313838368e+16,
      "budget_used_percent": 51.88366313838368
    },
    {
      "type": "training",
      "description": "Training step 2185",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:56",
      "total_flops_so_far": 5.190740201051936e+16,
      "budget_used_percent": 51.907402010519355
    },
    {
      "type": "training",
      "description": "Training step 2186",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:57",
      "total_flops_so_far": 5.193114088265504e+16,
      "budget_used_percent": 51.931140882655036
    },
    {
      "type": "training",
      "description": "Training step 2187",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:20:59",
      "total_flops_so_far": 5.195487975479072e+16,
      "budget_used_percent": 51.95487975479072
    },
    {
      "type": "training",
      "description": "Training step 2188",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:00",
      "total_flops_so_far": 5.19786186269264e+16,
      "budget_used_percent": 51.9786186269264
    },
    {
      "type": "training",
      "description": "Training step 2189",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:01",
      "total_flops_so_far": 5.200235749906208e+16,
      "budget_used_percent": 52.00235749906208
    },
    {
      "type": "training",
      "description": "Training step 2190",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:03",
      "total_flops_so_far": 5.202609637119776e+16,
      "budget_used_percent": 52.02609637119776
    },
    {
      "type": "training",
      "description": "Training step 2191",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:04",
      "total_flops_so_far": 5.204983524333344e+16,
      "budget_used_percent": 52.04983524333344
    },
    {
      "type": "training",
      "description": "Training step 2192",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:05",
      "total_flops_so_far": 5.207357411546912e+16,
      "budget_used_percent": 52.07357411546912
    },
    {
      "type": "training",
      "description": "Training step 2193",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:07",
      "total_flops_so_far": 5.20973129876048e+16,
      "budget_used_percent": 52.0973129876048
    },
    {
      "type": "training",
      "description": "Training step 2194",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:08",
      "total_flops_so_far": 5.212105185974048e+16,
      "budget_used_percent": 52.121051859740476
    },
    {
      "type": "training",
      "description": "Training step 2195",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:09",
      "total_flops_so_far": 5.214479073187616e+16,
      "budget_used_percent": 52.14479073187616
    },
    {
      "type": "training",
      "description": "Training step 2196",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:11",
      "total_flops_so_far": 5.216852960401184e+16,
      "budget_used_percent": 52.16852960401184
    },
    {
      "type": "training",
      "description": "Training step 2197",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:12",
      "total_flops_so_far": 5.219226847614752e+16,
      "budget_used_percent": 52.19226847614752
    },
    {
      "type": "training",
      "description": "Training step 2198",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:13",
      "total_flops_so_far": 5.22160073482832e+16,
      "budget_used_percent": 52.2160073482832
    },
    {
      "type": "training",
      "description": "Training step 2199",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:15",
      "total_flops_so_far": 5.223974622041888e+16,
      "budget_used_percent": 52.23974622041888
    },
    {
      "type": "training",
      "description": "Training step 2200",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:16",
      "total_flops_so_far": 5.226348509255456e+16,
      "budget_used_percent": 52.26348509255456
    },
    {
      "type": "training",
      "description": "Training step 2201",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:17",
      "total_flops_so_far": 5.228722396469024e+16,
      "budget_used_percent": 52.28722396469024
    },
    {
      "type": "training",
      "description": "Training step 2202",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:19",
      "total_flops_so_far": 5.231096283682592e+16,
      "budget_used_percent": 52.31096283682592
    },
    {
      "type": "training",
      "description": "Training step 2203",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:20",
      "total_flops_so_far": 5.23347017089616e+16,
      "budget_used_percent": 52.334701708961596
    },
    {
      "type": "training",
      "description": "Training step 2204",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:21",
      "total_flops_so_far": 5.235844058109728e+16,
      "budget_used_percent": 52.35844058109728
    },
    {
      "type": "training",
      "description": "Training step 2205",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:23",
      "total_flops_so_far": 5.238217945323296e+16,
      "budget_used_percent": 52.38217945323296
    },
    {
      "type": "training",
      "description": "Training step 2206",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:24",
      "total_flops_so_far": 5.240591832536864e+16,
      "budget_used_percent": 52.40591832536864
    },
    {
      "type": "training",
      "description": "Training step 2207",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:25",
      "total_flops_so_far": 5.242965719750432e+16,
      "budget_used_percent": 52.42965719750432
    },
    {
      "type": "training",
      "description": "Training step 2208",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:27",
      "total_flops_so_far": 5.245339606964e+16,
      "budget_used_percent": 52.45339606964
    },
    {
      "type": "training",
      "description": "Training step 2209",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:28",
      "total_flops_so_far": 5.247713494177568e+16,
      "budget_used_percent": 52.47713494177568
    },
    {
      "type": "training",
      "description": "Training step 2210",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:29",
      "total_flops_so_far": 5.250087381391136e+16,
      "budget_used_percent": 52.50087381391136
    },
    {
      "type": "training",
      "description": "Training step 2211",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:31",
      "total_flops_so_far": 5.252461268604704e+16,
      "budget_used_percent": 52.52461268604704
    },
    {
      "type": "training",
      "description": "Training step 2212",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:32",
      "total_flops_so_far": 5.254835155818272e+16,
      "budget_used_percent": 52.548351558182716
    },
    {
      "type": "training",
      "description": "Training step 2213",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:33",
      "total_flops_so_far": 5.25720904303184e+16,
      "budget_used_percent": 52.5720904303184
    },
    {
      "type": "training",
      "description": "Training step 2214",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:35",
      "total_flops_so_far": 5.259582930245408e+16,
      "budget_used_percent": 52.59582930245408
    },
    {
      "type": "training",
      "description": "Training step 2215",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:36",
      "total_flops_so_far": 5.261956817458976e+16,
      "budget_used_percent": 52.61956817458976
    },
    {
      "type": "training",
      "description": "Training step 2216",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:37",
      "total_flops_so_far": 5.264330704672544e+16,
      "budget_used_percent": 52.64330704672544
    },
    {
      "type": "training",
      "description": "Training step 2217",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:39",
      "total_flops_so_far": 5.266704591886112e+16,
      "budget_used_percent": 52.66704591886112
    },
    {
      "type": "training",
      "description": "Training step 2218",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:40",
      "total_flops_so_far": 5.26907847909968e+16,
      "budget_used_percent": 52.6907847909968
    },
    {
      "type": "training",
      "description": "Training step 2219",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:41",
      "total_flops_so_far": 5.271452366313248e+16,
      "budget_used_percent": 52.71452366313248
    },
    {
      "type": "training",
      "description": "Training step 2220",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:43",
      "total_flops_so_far": 5.273826253526816e+16,
      "budget_used_percent": 52.73826253526816
    },
    {
      "type": "training",
      "description": "Training step 2221",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:44",
      "total_flops_so_far": 5.276200140740384e+16,
      "budget_used_percent": 52.76200140740384
    },
    {
      "type": "training",
      "description": "Training step 2222",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:45",
      "total_flops_so_far": 5.278574027953952e+16,
      "budget_used_percent": 52.78574027953952
    },
    {
      "type": "training",
      "description": "Training step 2223",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:47",
      "total_flops_so_far": 5.28094791516752e+16,
      "budget_used_percent": 52.8094791516752
    },
    {
      "type": "training",
      "description": "Training step 2224",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:48",
      "total_flops_so_far": 5.283321802381088e+16,
      "budget_used_percent": 52.83321802381088
    },
    {
      "type": "training",
      "description": "Training step 2225",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:49",
      "total_flops_so_far": 5.285695689594656e+16,
      "budget_used_percent": 52.85695689594656
    },
    {
      "type": "training",
      "description": "Training step 2226",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:51",
      "total_flops_so_far": 5.288069576808224e+16,
      "budget_used_percent": 52.88069576808224
    },
    {
      "type": "training",
      "description": "Training step 2227",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:52",
      "total_flops_so_far": 5.290443464021792e+16,
      "budget_used_percent": 52.90443464021792
    },
    {
      "type": "training",
      "description": "Training step 2228",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:53",
      "total_flops_so_far": 5.29281735123536e+16,
      "budget_used_percent": 52.9281735123536
    },
    {
      "type": "training",
      "description": "Training step 2229",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:55",
      "total_flops_so_far": 5.295191238448928e+16,
      "budget_used_percent": 52.951912384489276
    },
    {
      "type": "training",
      "description": "Training step 2230",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:56",
      "total_flops_so_far": 5.297565125662496e+16,
      "budget_used_percent": 52.97565125662496
    },
    {
      "type": "training",
      "description": "Training step 2231",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:57",
      "total_flops_so_far": 5.299939012876064e+16,
      "budget_used_percent": 52.99939012876064
    },
    {
      "type": "training",
      "description": "Training step 2232",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:21:59",
      "total_flops_so_far": 5.302312900089632e+16,
      "budget_used_percent": 53.02312900089632
    },
    {
      "type": "training",
      "description": "Training step 2233",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:00",
      "total_flops_so_far": 5.3046867873032e+16,
      "budget_used_percent": 53.046867873032
    },
    {
      "type": "training",
      "description": "Training step 2234",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:01",
      "total_flops_so_far": 5.307060674516768e+16,
      "budget_used_percent": 53.07060674516768
    },
    {
      "type": "training",
      "description": "Training step 2235",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:03",
      "total_flops_so_far": 5.309434561730336e+16,
      "budget_used_percent": 53.09434561730336
    },
    {
      "type": "training",
      "description": "Training step 2236",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:04",
      "total_flops_so_far": 5.311808448943904e+16,
      "budget_used_percent": 53.11808448943904
    },
    {
      "type": "training",
      "description": "Training step 2237",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:05",
      "total_flops_so_far": 5.314182336157472e+16,
      "budget_used_percent": 53.14182336157472
    },
    {
      "type": "training",
      "description": "Training step 2238",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:07",
      "total_flops_so_far": 5.31655622337104e+16,
      "budget_used_percent": 53.165562233710396
    },
    {
      "type": "training",
      "description": "Training step 2239",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:08",
      "total_flops_so_far": 5.318930110584608e+16,
      "budget_used_percent": 53.18930110584608
    },
    {
      "type": "training",
      "description": "Training step 2240",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:09",
      "total_flops_so_far": 5.321303997798176e+16,
      "budget_used_percent": 53.21303997798176
    },
    {
      "type": "training",
      "description": "Training step 2241",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:11",
      "total_flops_so_far": 5.323677885011744e+16,
      "budget_used_percent": 53.23677885011744
    },
    {
      "type": "training",
      "description": "Training step 2242",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:12",
      "total_flops_so_far": 5.326051772225312e+16,
      "budget_used_percent": 53.26051772225312
    },
    {
      "type": "training",
      "description": "Training step 2243",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:13",
      "total_flops_so_far": 5.32842565943888e+16,
      "budget_used_percent": 53.2842565943888
    },
    {
      "type": "training",
      "description": "Training step 2244",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:14",
      "total_flops_so_far": 5.330799546652448e+16,
      "budget_used_percent": 53.30799546652448
    },
    {
      "type": "training",
      "description": "Training step 2245",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:15",
      "total_flops_so_far": 5.333173433866016e+16,
      "budget_used_percent": 53.33173433866016
    },
    {
      "type": "training",
      "description": "Training step 2246",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:17",
      "total_flops_so_far": 5.335547321079584e+16,
      "budget_used_percent": 53.35547321079584
    },
    {
      "type": "training",
      "description": "Training step 2247",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:18",
      "total_flops_so_far": 5.337921208293152e+16,
      "budget_used_percent": 53.37921208293152
    },
    {
      "type": "training",
      "description": "Training step 2248",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:19",
      "total_flops_so_far": 5.34029509550672e+16,
      "budget_used_percent": 53.4029509550672
    },
    {
      "type": "training",
      "description": "Training step 2249",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:20",
      "total_flops_so_far": 5.342668982720288e+16,
      "budget_used_percent": 53.42668982720288
    },
    {
      "type": "training",
      "description": "Training step 2250",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:22",
      "total_flops_so_far": 5.345042869933856e+16,
      "budget_used_percent": 53.45042869933856
    },
    {
      "type": "training",
      "description": "Training step 2251",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:23",
      "total_flops_so_far": 5.347416757147424e+16,
      "budget_used_percent": 53.47416757147424
    },
    {
      "type": "training",
      "description": "Training step 2252",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:24",
      "total_flops_so_far": 5.349790644360992e+16,
      "budget_used_percent": 53.49790644360992
    },
    {
      "type": "training",
      "description": "Training step 2253",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:26",
      "total_flops_so_far": 5.35216453157456e+16,
      "budget_used_percent": 53.5216453157456
    },
    {
      "type": "training",
      "description": "Training step 2254",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:27",
      "total_flops_so_far": 5.354538418788128e+16,
      "budget_used_percent": 53.54538418788128
    },
    {
      "type": "training",
      "description": "Training step 2255",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:28",
      "total_flops_so_far": 5.356912306001696e+16,
      "budget_used_percent": 53.56912306001696
    },
    {
      "type": "training",
      "description": "Training step 2256",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:30",
      "total_flops_so_far": 5.359286193215264e+16,
      "budget_used_percent": 53.59286193215264
    },
    {
      "type": "training",
      "description": "Training step 2257",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:31",
      "total_flops_so_far": 5.361660080428832e+16,
      "budget_used_percent": 53.61660080428832
    },
    {
      "type": "training",
      "description": "Training step 2258",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:32",
      "total_flops_so_far": 5.3640339676424e+16,
      "budget_used_percent": 53.640339676424
    },
    {
      "type": "training",
      "description": "Training step 2259",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:34",
      "total_flops_so_far": 5.366407854855968e+16,
      "budget_used_percent": 53.66407854855968
    },
    {
      "type": "training",
      "description": "Training step 2260",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:35",
      "total_flops_so_far": 5.368781742069536e+16,
      "budget_used_percent": 53.68781742069536
    },
    {
      "type": "training",
      "description": "Training step 2261",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:36",
      "total_flops_so_far": 5.371155629283104e+16,
      "budget_used_percent": 53.71155629283104
    },
    {
      "type": "training",
      "description": "Training step 2262",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:38",
      "total_flops_so_far": 5.373529516496672e+16,
      "budget_used_percent": 53.73529516496672
    },
    {
      "type": "training",
      "description": "Training step 2263",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:39",
      "total_flops_so_far": 5.37590340371024e+16,
      "budget_used_percent": 53.7590340371024
    },
    {
      "type": "training",
      "description": "Training step 2264",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:40",
      "total_flops_so_far": 5.378277290923808e+16,
      "budget_used_percent": 53.782772909238076
    },
    {
      "type": "training",
      "description": "Training step 2265",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:42",
      "total_flops_so_far": 5.380651178137376e+16,
      "budget_used_percent": 53.80651178137376
    },
    {
      "type": "training",
      "description": "Training step 2266",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:43",
      "total_flops_so_far": 5.383025065350944e+16,
      "budget_used_percent": 53.83025065350944
    },
    {
      "type": "training",
      "description": "Training step 2267",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:44",
      "total_flops_so_far": 5.385398952564512e+16,
      "budget_used_percent": 53.85398952564512
    },
    {
      "type": "training",
      "description": "Training step 2268",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:46",
      "total_flops_so_far": 5.38777283977808e+16,
      "budget_used_percent": 53.8777283977808
    },
    {
      "type": "training",
      "description": "Training step 2269",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:47",
      "total_flops_so_far": 5.390146726991648e+16,
      "budget_used_percent": 53.90146726991648
    },
    {
      "type": "training",
      "description": "Training step 2270",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:48",
      "total_flops_so_far": 5.392520614205216e+16,
      "budget_used_percent": 53.92520614205216
    },
    {
      "type": "training",
      "description": "Training step 2271",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:50",
      "total_flops_so_far": 5.394894501418784e+16,
      "budget_used_percent": 53.94894501418784
    },
    {
      "type": "training",
      "description": "Training step 2272",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:51",
      "total_flops_so_far": 5.397268388632352e+16,
      "budget_used_percent": 53.97268388632352
    },
    {
      "type": "training",
      "description": "Training step 2273",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:52",
      "total_flops_so_far": 5.39964227584592e+16,
      "budget_used_percent": 53.9964227584592
    },
    {
      "type": "training",
      "description": "Training step 2274",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:54",
      "total_flops_so_far": 5.402016163059488e+16,
      "budget_used_percent": 54.02016163059488
    },
    {
      "type": "training",
      "description": "Training step 2275",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:55",
      "total_flops_so_far": 5.404390050273056e+16,
      "budget_used_percent": 54.04390050273056
    },
    {
      "type": "training",
      "description": "Training step 2276",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:56",
      "total_flops_so_far": 5.406763937486624e+16,
      "budget_used_percent": 54.06763937486624
    },
    {
      "type": "training",
      "description": "Training step 2277",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:58",
      "total_flops_so_far": 5.409137824700192e+16,
      "budget_used_percent": 54.09137824700192
    },
    {
      "type": "training",
      "description": "Training step 2278",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:22:59",
      "total_flops_so_far": 5.41151171191376e+16,
      "budget_used_percent": 54.1151171191376
    },
    {
      "type": "training",
      "description": "Training step 2279",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:00",
      "total_flops_so_far": 5.413885599127328e+16,
      "budget_used_percent": 54.13885599127328
    },
    {
      "type": "training",
      "description": "Training step 2280",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:02",
      "total_flops_so_far": 5.416259486340896e+16,
      "budget_used_percent": 54.16259486340896
    },
    {
      "type": "training",
      "description": "Training step 2281",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:03",
      "total_flops_so_far": 5.418633373554464e+16,
      "budget_used_percent": 54.18633373554464
    },
    {
      "type": "training",
      "description": "Training step 2282",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:04",
      "total_flops_so_far": 5.421007260768032e+16,
      "budget_used_percent": 54.21007260768032
    },
    {
      "type": "training",
      "description": "Training step 2283",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:06",
      "total_flops_so_far": 5.4233811479816e+16,
      "budget_used_percent": 54.233811479816
    },
    {
      "type": "training",
      "description": "Training step 2284",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:07",
      "total_flops_so_far": 5.425755035195168e+16,
      "budget_used_percent": 54.25755035195168
    },
    {
      "type": "training",
      "description": "Training step 2285",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:08",
      "total_flops_so_far": 5.428128922408736e+16,
      "budget_used_percent": 54.28128922408736
    },
    {
      "type": "training",
      "description": "Training step 2286",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:10",
      "total_flops_so_far": 5.430502809622304e+16,
      "budget_used_percent": 54.30502809622304
    },
    {
      "type": "training",
      "description": "Training step 2287",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:11",
      "total_flops_so_far": 5.432876696835872e+16,
      "budget_used_percent": 54.32876696835872
    },
    {
      "type": "training",
      "description": "Training step 2288",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:12",
      "total_flops_so_far": 5.43525058404944e+16,
      "budget_used_percent": 54.3525058404944
    },
    {
      "type": "training",
      "description": "Training step 2289",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:14",
      "total_flops_so_far": 5.437624471263008e+16,
      "budget_used_percent": 54.37624471263008
    },
    {
      "type": "training",
      "description": "Training step 2290",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:15",
      "total_flops_so_far": 5.439998358476576e+16,
      "budget_used_percent": 54.399983584765764
    },
    {
      "type": "training",
      "description": "Training step 2291",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:16",
      "total_flops_so_far": 5.442372245690144e+16,
      "budget_used_percent": 54.42372245690144
    },
    {
      "type": "training",
      "description": "Training step 2292",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:18",
      "total_flops_so_far": 5.444746132903712e+16,
      "budget_used_percent": 54.44746132903712
    },
    {
      "type": "training",
      "description": "Training step 2293",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:19",
      "total_flops_so_far": 5.44712002011728e+16,
      "budget_used_percent": 54.4712002011728
    },
    {
      "type": "training",
      "description": "Training step 2294",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:20",
      "total_flops_so_far": 5.449493907330848e+16,
      "budget_used_percent": 54.49493907330848
    },
    {
      "type": "training",
      "description": "Training step 2295",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:22",
      "total_flops_so_far": 5.451867794544416e+16,
      "budget_used_percent": 54.51867794544416
    },
    {
      "type": "training",
      "description": "Training step 2296",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:23",
      "total_flops_so_far": 5.454241681757984e+16,
      "budget_used_percent": 54.54241681757984
    },
    {
      "type": "training",
      "description": "Training step 2297",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:24",
      "total_flops_so_far": 5.456615568971552e+16,
      "budget_used_percent": 54.56615568971552
    },
    {
      "type": "training",
      "description": "Training step 2298",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:26",
      "total_flops_so_far": 5.45898945618512e+16,
      "budget_used_percent": 54.5898945618512
    },
    {
      "type": "training",
      "description": "Training step 2299",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:27",
      "total_flops_so_far": 5.461363343398688e+16,
      "budget_used_percent": 54.61363343398688
    },
    {
      "type": "training",
      "description": "Training step 2300",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:28",
      "total_flops_so_far": 5.463737230612256e+16,
      "budget_used_percent": 54.63737230612256
    },
    {
      "type": "training",
      "description": "Training step 2301",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:30",
      "total_flops_so_far": 5.466111117825824e+16,
      "budget_used_percent": 54.66111117825824
    },
    {
      "type": "training",
      "description": "Training step 2302",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:31",
      "total_flops_so_far": 5.468485005039392e+16,
      "budget_used_percent": 54.68485005039392
    },
    {
      "type": "training",
      "description": "Training step 2303",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:32",
      "total_flops_so_far": 5.47085889225296e+16,
      "budget_used_percent": 54.7085889225296
    },
    {
      "type": "training",
      "description": "Training step 2304",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:34",
      "total_flops_so_far": 5.473232779466528e+16,
      "budget_used_percent": 54.73232779466528
    },
    {
      "type": "training",
      "description": "Training step 2305",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:35",
      "total_flops_so_far": 5.475606666680096e+16,
      "budget_used_percent": 54.75606666680096
    },
    {
      "type": "training",
      "description": "Training step 2306",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:36",
      "total_flops_so_far": 5.477980553893664e+16,
      "budget_used_percent": 54.77980553893664
    },
    {
      "type": "training",
      "description": "Training step 2307",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:38",
      "total_flops_so_far": 5.480354441107232e+16,
      "budget_used_percent": 54.80354441107232
    },
    {
      "type": "training",
      "description": "Training step 2308",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:39",
      "total_flops_so_far": 5.4827283283208e+16,
      "budget_used_percent": 54.827283283208
    },
    {
      "type": "training",
      "description": "Training step 2309",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:40",
      "total_flops_so_far": 5.485102215534368e+16,
      "budget_used_percent": 54.85102215534368
    },
    {
      "type": "training",
      "description": "Training step 2310",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:42",
      "total_flops_so_far": 5.487476102747936e+16,
      "budget_used_percent": 54.87476102747936
    },
    {
      "type": "training",
      "description": "Training step 2311",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:43",
      "total_flops_so_far": 5.489849989961504e+16,
      "budget_used_percent": 54.89849989961504
    },
    {
      "type": "training",
      "description": "Training step 2312",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:44",
      "total_flops_so_far": 5.492223877175072e+16,
      "budget_used_percent": 54.92223877175072
    },
    {
      "type": "training",
      "description": "Training step 2313",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:46",
      "total_flops_so_far": 5.49459776438864e+16,
      "budget_used_percent": 54.9459776438864
    },
    {
      "type": "training",
      "description": "Training step 2314",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:47",
      "total_flops_so_far": 5.496971651602208e+16,
      "budget_used_percent": 54.96971651602208
    },
    {
      "type": "training",
      "description": "Training step 2315",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:48",
      "total_flops_so_far": 5.499345538815776e+16,
      "budget_used_percent": 54.99345538815776
    },
    {
      "type": "training",
      "description": "Training step 2316",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:50",
      "total_flops_so_far": 5.501719426029344e+16,
      "budget_used_percent": 55.017194260293444
    },
    {
      "type": "training",
      "description": "Training step 2317",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:51",
      "total_flops_so_far": 5.504093313242912e+16,
      "budget_used_percent": 55.04093313242912
    },
    {
      "type": "training",
      "description": "Training step 2318",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:52",
      "total_flops_so_far": 5.50646720045648e+16,
      "budget_used_percent": 55.0646720045648
    },
    {
      "type": "training",
      "description": "Training step 2319",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:54",
      "total_flops_so_far": 5.508841087670048e+16,
      "budget_used_percent": 55.08841087670048
    },
    {
      "type": "training",
      "description": "Training step 2320",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:55",
      "total_flops_so_far": 5.511214974883616e+16,
      "budget_used_percent": 55.11214974883616
    },
    {
      "type": "training",
      "description": "Training step 2321",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:56",
      "total_flops_so_far": 5.513588862097184e+16,
      "budget_used_percent": 55.13588862097184
    },
    {
      "type": "training",
      "description": "Training step 2322",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:58",
      "total_flops_so_far": 5.515962749310752e+16,
      "budget_used_percent": 55.15962749310752
    },
    {
      "type": "training",
      "description": "Training step 2323",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:23:59",
      "total_flops_so_far": 5.51833663652432e+16,
      "budget_used_percent": 55.1833663652432
    },
    {
      "type": "training",
      "description": "Training step 2324",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:00",
      "total_flops_so_far": 5.520710523737888e+16,
      "budget_used_percent": 55.20710523737888
    },
    {
      "type": "training",
      "description": "Training step 2325",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:02",
      "total_flops_so_far": 5.523084410951456e+16,
      "budget_used_percent": 55.230844109514564
    },
    {
      "type": "training",
      "description": "Training step 2326",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:03",
      "total_flops_so_far": 5.525458298165024e+16,
      "budget_used_percent": 55.25458298165024
    },
    {
      "type": "training",
      "description": "Training step 2327",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:04",
      "total_flops_so_far": 5.527832185378592e+16,
      "budget_used_percent": 55.27832185378592
    },
    {
      "type": "training",
      "description": "Training step 2328",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:06",
      "total_flops_so_far": 5.53020607259216e+16,
      "budget_used_percent": 55.3020607259216
    },
    {
      "type": "training",
      "description": "Training step 2329",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:07",
      "total_flops_so_far": 5.532579959805728e+16,
      "budget_used_percent": 55.32579959805728
    },
    {
      "type": "training",
      "description": "Training step 2330",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:08",
      "total_flops_so_far": 5.534953847019296e+16,
      "budget_used_percent": 55.34953847019296
    },
    {
      "type": "training",
      "description": "Training step 2331",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:10",
      "total_flops_so_far": 5.537327734232864e+16,
      "budget_used_percent": 55.37327734232864
    },
    {
      "type": "training",
      "description": "Training step 2332",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:11",
      "total_flops_so_far": 5.539701621446432e+16,
      "budget_used_percent": 55.39701621446432
    },
    {
      "type": "training",
      "description": "Training step 2333",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:12",
      "total_flops_so_far": 5.54207550866e+16,
      "budget_used_percent": 55.4207550866
    },
    {
      "type": "training",
      "description": "Training step 2334",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:14",
      "total_flops_so_far": 5.544449395873568e+16,
      "budget_used_percent": 55.444493958735684
    },
    {
      "type": "training",
      "description": "Training step 2335",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:15",
      "total_flops_so_far": 5.546823283087136e+16,
      "budget_used_percent": 55.46823283087136
    },
    {
      "type": "training",
      "description": "Training step 2336",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:16",
      "total_flops_so_far": 5.549197170300704e+16,
      "budget_used_percent": 55.49197170300704
    },
    {
      "type": "training",
      "description": "Training step 2337",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:18",
      "total_flops_so_far": 5.551571057514272e+16,
      "budget_used_percent": 55.51571057514272
    },
    {
      "type": "training",
      "description": "Training step 2338",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:19",
      "total_flops_so_far": 5.55394494472784e+16,
      "budget_used_percent": 55.5394494472784
    },
    {
      "type": "training",
      "description": "Training step 2339",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:20",
      "total_flops_so_far": 5.556318831941408e+16,
      "budget_used_percent": 55.56318831941408
    },
    {
      "type": "training",
      "description": "Training step 2340",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:22",
      "total_flops_so_far": 5.558692719154976e+16,
      "budget_used_percent": 55.58692719154976
    },
    {
      "type": "training",
      "description": "Training step 2341",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:23",
      "total_flops_so_far": 5.561066606368544e+16,
      "budget_used_percent": 55.61066606368544
    },
    {
      "type": "training",
      "description": "Training step 2342",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:24",
      "total_flops_so_far": 5.563440493582112e+16,
      "budget_used_percent": 55.634404935821124
    },
    {
      "type": "training",
      "description": "Training step 2343",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:26",
      "total_flops_so_far": 5.56581438079568e+16,
      "budget_used_percent": 55.6581438079568
    },
    {
      "type": "training",
      "description": "Training step 2344",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:27",
      "total_flops_so_far": 5.568188268009248e+16,
      "budget_used_percent": 55.68188268009248
    },
    {
      "type": "training",
      "description": "Training step 2345",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:28",
      "total_flops_so_far": 5.570562155222816e+16,
      "budget_used_percent": 55.70562155222816
    },
    {
      "type": "training",
      "description": "Training step 2346",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:30",
      "total_flops_so_far": 5.572936042436384e+16,
      "budget_used_percent": 55.72936042436384
    },
    {
      "type": "training",
      "description": "Training step 2347",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:31",
      "total_flops_so_far": 5.575309929649952e+16,
      "budget_used_percent": 55.75309929649952
    },
    {
      "type": "training",
      "description": "Training step 2348",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:33",
      "total_flops_so_far": 5.57768381686352e+16,
      "budget_used_percent": 55.7768381686352
    },
    {
      "type": "training",
      "description": "Training step 2349",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:34",
      "total_flops_so_far": 5.580057704077088e+16,
      "budget_used_percent": 55.80057704077088
    },
    {
      "type": "training",
      "description": "Training step 2350",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:35",
      "total_flops_so_far": 5.582431591290656e+16,
      "budget_used_percent": 55.82431591290656
    },
    {
      "type": "training",
      "description": "Training step 2351",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:37",
      "total_flops_so_far": 5.584805478504224e+16,
      "budget_used_percent": 55.848054785042244
    },
    {
      "type": "training",
      "description": "Training step 2352",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:38",
      "total_flops_so_far": 5.587179365717792e+16,
      "budget_used_percent": 55.87179365717792
    },
    {
      "type": "training",
      "description": "Training step 2353",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:39",
      "total_flops_so_far": 5.58955325293136e+16,
      "budget_used_percent": 55.8955325293136
    },
    {
      "type": "training",
      "description": "Training step 2354",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:41",
      "total_flops_so_far": 5.591927140144928e+16,
      "budget_used_percent": 55.91927140144928
    },
    {
      "type": "training",
      "description": "Training step 2355",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:42",
      "total_flops_so_far": 5.594301027358496e+16,
      "budget_used_percent": 55.94301027358496
    },
    {
      "type": "training",
      "description": "Training step 2356",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:44",
      "total_flops_so_far": 5.596674914572064e+16,
      "budget_used_percent": 55.96674914572064
    },
    {
      "type": "training",
      "description": "Training step 2357",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:45",
      "total_flops_so_far": 5.599048801785632e+16,
      "budget_used_percent": 55.99048801785632
    },
    {
      "type": "training",
      "description": "Training step 2358",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:46",
      "total_flops_so_far": 5.6014226889992e+16,
      "budget_used_percent": 56.014226889992
    },
    {
      "type": "training",
      "description": "Training step 2359",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:48",
      "total_flops_so_far": 5.603796576212768e+16,
      "budget_used_percent": 56.03796576212768
    },
    {
      "type": "training",
      "description": "Training step 2360",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:49",
      "total_flops_so_far": 5.606170463426336e+16,
      "budget_used_percent": 56.061704634263364
    },
    {
      "type": "training",
      "description": "Training step 2361",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:50",
      "total_flops_so_far": 5.608544350639904e+16,
      "budget_used_percent": 56.08544350639904
    },
    {
      "type": "training",
      "description": "Training step 2362",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:52",
      "total_flops_so_far": 5.610918237853472e+16,
      "budget_used_percent": 56.10918237853472
    },
    {
      "type": "training",
      "description": "Training step 2363",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:53",
      "total_flops_so_far": 5.61329212506704e+16,
      "budget_used_percent": 56.1329212506704
    },
    {
      "type": "training",
      "description": "Training step 2364",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:54",
      "total_flops_so_far": 5.615666012280608e+16,
      "budget_used_percent": 56.15666012280608
    },
    {
      "type": "training",
      "description": "Training step 2365",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:56",
      "total_flops_so_far": 5.618039899494176e+16,
      "budget_used_percent": 56.18039899494176
    },
    {
      "type": "training",
      "description": "Training step 2366",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:57",
      "total_flops_so_far": 5.620413786707744e+16,
      "budget_used_percent": 56.20413786707744
    },
    {
      "type": "training",
      "description": "Training step 2367",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:24:58",
      "total_flops_so_far": 5.622787673921312e+16,
      "budget_used_percent": 56.22787673921312
    },
    {
      "type": "training",
      "description": "Training step 2368",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:00",
      "total_flops_so_far": 5.62516156113488e+16,
      "budget_used_percent": 56.251615611348804
    },
    {
      "type": "training",
      "description": "Training step 2369",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:01",
      "total_flops_so_far": 5.627535448348448e+16,
      "budget_used_percent": 56.275354483484485
    },
    {
      "type": "training",
      "description": "Training step 2370",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:02",
      "total_flops_so_far": 5.629909335562016e+16,
      "budget_used_percent": 56.29909335562016
    },
    {
      "type": "training",
      "description": "Training step 2371",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:04",
      "total_flops_so_far": 5.632283222775584e+16,
      "budget_used_percent": 56.32283222775584
    },
    {
      "type": "training",
      "description": "Training step 2372",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:05",
      "total_flops_so_far": 5.634657109989152e+16,
      "budget_used_percent": 56.34657109989152
    },
    {
      "type": "training",
      "description": "Training step 2373",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:06",
      "total_flops_so_far": 5.63703099720272e+16,
      "budget_used_percent": 56.3703099720272
    },
    {
      "type": "training",
      "description": "Training step 2374",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:08",
      "total_flops_so_far": 5.639404884416288e+16,
      "budget_used_percent": 56.39404884416288
    },
    {
      "type": "training",
      "description": "Training step 2375",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:09",
      "total_flops_so_far": 5.641778771629856e+16,
      "budget_used_percent": 56.41778771629856
    },
    {
      "type": "training",
      "description": "Training step 2376",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:10",
      "total_flops_so_far": 5.644152658843424e+16,
      "budget_used_percent": 56.44152658843424
    },
    {
      "type": "training",
      "description": "Training step 2377",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:11",
      "total_flops_so_far": 5.646526546056992e+16,
      "budget_used_percent": 56.465265460569924
    },
    {
      "type": "training",
      "description": "Training step 2378",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:12",
      "total_flops_so_far": 5.64890043327056e+16,
      "budget_used_percent": 56.4890043327056
    },
    {
      "type": "training",
      "description": "Training step 2379",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:14",
      "total_flops_so_far": 5.651274320484128e+16,
      "budget_used_percent": 56.51274320484128
    },
    {
      "type": "training",
      "description": "Training step 2380",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:15",
      "total_flops_so_far": 5.653648207697696e+16,
      "budget_used_percent": 56.53648207697696
    },
    {
      "type": "training",
      "description": "Training step 2381",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:16",
      "total_flops_so_far": 5.656022094911264e+16,
      "budget_used_percent": 56.56022094911264
    },
    {
      "type": "training",
      "description": "Training step 2382",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:18",
      "total_flops_so_far": 5.658395982124832e+16,
      "budget_used_percent": 56.58395982124832
    },
    {
      "type": "training",
      "description": "Training step 2383",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:19",
      "total_flops_so_far": 5.6607698693384e+16,
      "budget_used_percent": 56.607698693384
    },
    {
      "type": "training",
      "description": "Training step 2384",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:20",
      "total_flops_so_far": 5.663143756551968e+16,
      "budget_used_percent": 56.63143756551968
    },
    {
      "type": "training",
      "description": "Training step 2385",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:22",
      "total_flops_so_far": 5.665517643765536e+16,
      "budget_used_percent": 56.65517643765536
    },
    {
      "type": "training",
      "description": "Training step 2386",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:23",
      "total_flops_so_far": 5.667891530979104e+16,
      "budget_used_percent": 56.678915309791044
    },
    {
      "type": "training",
      "description": "Training step 2387",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:24",
      "total_flops_so_far": 5.670265418192672e+16,
      "budget_used_percent": 56.70265418192672
    },
    {
      "type": "training",
      "description": "Training step 2388",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:26",
      "total_flops_so_far": 5.67263930540624e+16,
      "budget_used_percent": 56.7263930540624
    },
    {
      "type": "training",
      "description": "Training step 2389",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:27",
      "total_flops_so_far": 5.675013192619808e+16,
      "budget_used_percent": 56.75013192619808
    },
    {
      "type": "training",
      "description": "Training step 2390",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:28",
      "total_flops_so_far": 5.677387079833376e+16,
      "budget_used_percent": 56.77387079833376
    },
    {
      "type": "training",
      "description": "Training step 2391",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:30",
      "total_flops_so_far": 5.679760967046944e+16,
      "budget_used_percent": 56.79760967046944
    },
    {
      "type": "training",
      "description": "Training step 2392",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:31",
      "total_flops_so_far": 5.682134854260512e+16,
      "budget_used_percent": 56.82134854260512
    },
    {
      "type": "training",
      "description": "Training step 2393",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:32",
      "total_flops_so_far": 5.68450874147408e+16,
      "budget_used_percent": 56.8450874147408
    },
    {
      "type": "training",
      "description": "Training step 2394",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:34",
      "total_flops_so_far": 5.686882628687648e+16,
      "budget_used_percent": 56.868826286876484
    },
    {
      "type": "training",
      "description": "Training step 2395",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:35",
      "total_flops_so_far": 5.689256515901216e+16,
      "budget_used_percent": 56.892565159012165
    },
    {
      "type": "training",
      "description": "Training step 2396",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:36",
      "total_flops_so_far": 5.691630403114784e+16,
      "budget_used_percent": 56.91630403114784
    },
    {
      "type": "training",
      "description": "Training step 2397",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:38",
      "total_flops_so_far": 5.694004290328352e+16,
      "budget_used_percent": 56.94004290328352
    },
    {
      "type": "training",
      "description": "Training step 2398",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:39",
      "total_flops_so_far": 5.69637817754192e+16,
      "budget_used_percent": 56.9637817754192
    },
    {
      "type": "training",
      "description": "Training step 2399",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:40",
      "total_flops_so_far": 5.698752064755488e+16,
      "budget_used_percent": 56.98752064755488
    },
    {
      "type": "training",
      "description": "Training step 2400",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:42",
      "total_flops_so_far": 5.701125951969056e+16,
      "budget_used_percent": 57.01125951969056
    },
    {
      "type": "training",
      "description": "Training step 2401",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:43",
      "total_flops_so_far": 5.703499839182624e+16,
      "budget_used_percent": 57.03499839182624
    },
    {
      "type": "training",
      "description": "Training step 2402",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:44",
      "total_flops_so_far": 5.705873726396192e+16,
      "budget_used_percent": 57.05873726396192
    },
    {
      "type": "training",
      "description": "Training step 2403",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:46",
      "total_flops_so_far": 5.70824761360976e+16,
      "budget_used_percent": 57.082476136097604
    },
    {
      "type": "training",
      "description": "Training step 2404",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:47",
      "total_flops_so_far": 5.710621500823328e+16,
      "budget_used_percent": 57.106215008233285
    },
    {
      "type": "training",
      "description": "Training step 2405",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:48",
      "total_flops_so_far": 5.712995388036896e+16,
      "budget_used_percent": 57.12995388036896
    },
    {
      "type": "training",
      "description": "Training step 2406",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:50",
      "total_flops_so_far": 5.715369275250464e+16,
      "budget_used_percent": 57.15369275250464
    },
    {
      "type": "training",
      "description": "Training step 2407",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:51",
      "total_flops_so_far": 5.717743162464032e+16,
      "budget_used_percent": 57.17743162464032
    },
    {
      "type": "training",
      "description": "Training step 2408",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:52",
      "total_flops_so_far": 5.7201170496776e+16,
      "budget_used_percent": 57.201170496776
    },
    {
      "type": "training",
      "description": "Training step 2409",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:54",
      "total_flops_so_far": 5.722490936891168e+16,
      "budget_used_percent": 57.22490936891168
    },
    {
      "type": "training",
      "description": "Training step 2410",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:55",
      "total_flops_so_far": 5.724864824104736e+16,
      "budget_used_percent": 57.24864824104736
    },
    {
      "type": "training",
      "description": "Training step 2411",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:56",
      "total_flops_so_far": 5.727238711318304e+16,
      "budget_used_percent": 57.27238711318304
    },
    {
      "type": "training",
      "description": "Training step 2412",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:58",
      "total_flops_so_far": 5.729612598531872e+16,
      "budget_used_percent": 57.296125985318724
    },
    {
      "type": "training",
      "description": "Training step 2413",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:25:59",
      "total_flops_so_far": 5.73198648574544e+16,
      "budget_used_percent": 57.31986485745439
    },
    {
      "type": "training",
      "description": "Training step 2414",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:00",
      "total_flops_so_far": 5.734360372959008e+16,
      "budget_used_percent": 57.34360372959008
    },
    {
      "type": "training",
      "description": "Training step 2415",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:02",
      "total_flops_so_far": 5.736734260172576e+16,
      "budget_used_percent": 57.36734260172576
    },
    {
      "type": "training",
      "description": "Training step 2416",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:03",
      "total_flops_so_far": 5.739108147386144e+16,
      "budget_used_percent": 57.39108147386144
    },
    {
      "type": "training",
      "description": "Training step 2417",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:04",
      "total_flops_so_far": 5.741482034599712e+16,
      "budget_used_percent": 57.41482034599712
    },
    {
      "type": "training",
      "description": "Training step 2418",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:06",
      "total_flops_so_far": 5.74385592181328e+16,
      "budget_used_percent": 57.4385592181328
    },
    {
      "type": "training",
      "description": "Training step 2419",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:07",
      "total_flops_so_far": 5.746229809026848e+16,
      "budget_used_percent": 57.46229809026848
    },
    {
      "type": "training",
      "description": "Training step 2420",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:08",
      "total_flops_so_far": 5.748603696240416e+16,
      "budget_used_percent": 57.486036962404164
    },
    {
      "type": "training",
      "description": "Training step 2421",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:10",
      "total_flops_so_far": 5.750977583453984e+16,
      "budget_used_percent": 57.509775834539845
    },
    {
      "type": "training",
      "description": "Training step 2422",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:11",
      "total_flops_so_far": 5.753351470667552e+16,
      "budget_used_percent": 57.53351470667552
    },
    {
      "type": "training",
      "description": "Training step 2423",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:12",
      "total_flops_so_far": 5.75572535788112e+16,
      "budget_used_percent": 57.5572535788112
    },
    {
      "type": "training",
      "description": "Training step 2424",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:14",
      "total_flops_so_far": 5.758099245094688e+16,
      "budget_used_percent": 57.58099245094688
    },
    {
      "type": "training",
      "description": "Training step 2425",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:15",
      "total_flops_so_far": 5.760473132308256e+16,
      "budget_used_percent": 57.60473132308256
    },
    {
      "type": "training",
      "description": "Training step 2426",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:16",
      "total_flops_so_far": 5.762847019521824e+16,
      "budget_used_percent": 57.62847019521824
    },
    {
      "type": "training",
      "description": "Training step 2427",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:18",
      "total_flops_so_far": 5.765220906735392e+16,
      "budget_used_percent": 57.65220906735392
    },
    {
      "type": "training",
      "description": "Training step 2428",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:19",
      "total_flops_so_far": 5.76759479394896e+16,
      "budget_used_percent": 57.6759479394896
    },
    {
      "type": "training",
      "description": "Training step 2429",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:20",
      "total_flops_so_far": 5.769968681162528e+16,
      "budget_used_percent": 57.699686811625284
    },
    {
      "type": "training",
      "description": "Training step 2430",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:22",
      "total_flops_so_far": 5.772342568376096e+16,
      "budget_used_percent": 57.723425683760965
    },
    {
      "type": "training",
      "description": "Training step 2431",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:23",
      "total_flops_so_far": 5.774716455589664e+16,
      "budget_used_percent": 57.74716455589663
    },
    {
      "type": "training",
      "description": "Training step 2432",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:24",
      "total_flops_so_far": 5.777090342803232e+16,
      "budget_used_percent": 57.77090342803232
    },
    {
      "type": "training",
      "description": "Training step 2433",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:26",
      "total_flops_so_far": 5.7794642300168e+16,
      "budget_used_percent": 57.794642300168
    },
    {
      "type": "training",
      "description": "Training step 2434",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:27",
      "total_flops_so_far": 5.781838117230368e+16,
      "budget_used_percent": 57.81838117230368
    },
    {
      "type": "training",
      "description": "Training step 2435",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:28",
      "total_flops_so_far": 5.784212004443936e+16,
      "budget_used_percent": 57.84212004443936
    },
    {
      "type": "training",
      "description": "Training step 2436",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:30",
      "total_flops_so_far": 5.786585891657504e+16,
      "budget_used_percent": 57.86585891657504
    },
    {
      "type": "training",
      "description": "Training step 2437",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:31",
      "total_flops_so_far": 5.788959778871072e+16,
      "budget_used_percent": 57.889597788710724
    },
    {
      "type": "training",
      "description": "Training step 2438",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:33",
      "total_flops_so_far": 5.79133366608464e+16,
      "budget_used_percent": 57.913336660846404
    },
    {
      "type": "training",
      "description": "Training step 2439",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:34",
      "total_flops_so_far": 5.793707553298208e+16,
      "budget_used_percent": 57.937075532982085
    },
    {
      "type": "training",
      "description": "Training step 2440",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:35",
      "total_flops_so_far": 5.796081440511776e+16,
      "budget_used_percent": 57.96081440511776
    },
    {
      "type": "training",
      "description": "Training step 2441",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:37",
      "total_flops_so_far": 5.798455327725344e+16,
      "budget_used_percent": 57.98455327725344
    },
    {
      "type": "training",
      "description": "Training step 2442",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:38",
      "total_flops_so_far": 5.800829214938912e+16,
      "budget_used_percent": 58.00829214938912
    },
    {
      "type": "training",
      "description": "Training step 2443",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:39",
      "total_flops_so_far": 5.80320310215248e+16,
      "budget_used_percent": 58.0320310215248
    },
    {
      "type": "training",
      "description": "Training step 2444",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:41",
      "total_flops_so_far": 5.805576989366048e+16,
      "budget_used_percent": 58.05576989366048
    },
    {
      "type": "training",
      "description": "Training step 2445",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:42",
      "total_flops_so_far": 5.807950876579616e+16,
      "budget_used_percent": 58.07950876579616
    },
    {
      "type": "training",
      "description": "Training step 2446",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:43",
      "total_flops_so_far": 5.810324763793184e+16,
      "budget_used_percent": 58.103247637931844
    },
    {
      "type": "training",
      "description": "Training step 2447",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:45",
      "total_flops_so_far": 5.812698651006752e+16,
      "budget_used_percent": 58.126986510067525
    },
    {
      "type": "training",
      "description": "Training step 2448",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:46",
      "total_flops_so_far": 5.81507253822032e+16,
      "budget_used_percent": 58.150725382203206
    },
    {
      "type": "training",
      "description": "Training step 2449",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:47",
      "total_flops_so_far": 5.817446425433888e+16,
      "budget_used_percent": 58.17446425433887
    },
    {
      "type": "training",
      "description": "Training step 2450",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:49",
      "total_flops_so_far": 5.819820312647456e+16,
      "budget_used_percent": 58.19820312647456
    },
    {
      "type": "training",
      "description": "Training step 2451",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:50",
      "total_flops_so_far": 5.822194199861024e+16,
      "budget_used_percent": 58.22194199861024
    },
    {
      "type": "training",
      "description": "Training step 2452",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:51",
      "total_flops_so_far": 5.824568087074592e+16,
      "budget_used_percent": 58.24568087074592
    },
    {
      "type": "training",
      "description": "Training step 2453",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:53",
      "total_flops_so_far": 5.82694197428816e+16,
      "budget_used_percent": 58.2694197428816
    },
    {
      "type": "training",
      "description": "Training step 2454",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:54",
      "total_flops_so_far": 5.829315861501728e+16,
      "budget_used_percent": 58.29315861501728
    },
    {
      "type": "training",
      "description": "Training step 2455",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:55",
      "total_flops_so_far": 5.831689748715296e+16,
      "budget_used_percent": 58.316897487152964
    },
    {
      "type": "training",
      "description": "Training step 2456",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:57",
      "total_flops_so_far": 5.834063635928864e+16,
      "budget_used_percent": 58.340636359288645
    },
    {
      "type": "training",
      "description": "Training step 2457",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:58",
      "total_flops_so_far": 5.836437523142432e+16,
      "budget_used_percent": 58.36437523142431
    },
    {
      "type": "training",
      "description": "Training step 2458",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:26:59",
      "total_flops_so_far": 5.838811410356e+16,
      "budget_used_percent": 58.38811410356
    },
    {
      "type": "training",
      "description": "Training step 2459",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:01",
      "total_flops_so_far": 5.841185297569568e+16,
      "budget_used_percent": 58.41185297569568
    },
    {
      "type": "training",
      "description": "Training step 2460",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:02",
      "total_flops_so_far": 5.843559184783136e+16,
      "budget_used_percent": 58.43559184783136
    },
    {
      "type": "training",
      "description": "Training step 2461",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:03",
      "total_flops_so_far": 5.845933071996704e+16,
      "budget_used_percent": 58.45933071996704
    },
    {
      "type": "training",
      "description": "Training step 2462",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:05",
      "total_flops_so_far": 5.848306959210272e+16,
      "budget_used_percent": 58.48306959210272
    },
    {
      "type": "training",
      "description": "Training step 2463",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:06",
      "total_flops_so_far": 5.85068084642384e+16,
      "budget_used_percent": 58.506808464238404
    },
    {
      "type": "training",
      "description": "Training step 2464",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:07",
      "total_flops_so_far": 5.853054733637408e+16,
      "budget_used_percent": 58.530547336374084
    },
    {
      "type": "training",
      "description": "Training step 2465",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:09",
      "total_flops_so_far": 5.855428620850976e+16,
      "budget_used_percent": 58.554286208509765
    },
    {
      "type": "training",
      "description": "Training step 2466",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:10",
      "total_flops_so_far": 5.857802508064544e+16,
      "budget_used_percent": 58.57802508064543
    },
    {
      "type": "training",
      "description": "Training step 2467",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:11",
      "total_flops_so_far": 5.860176395278112e+16,
      "budget_used_percent": 58.60176395278111
    },
    {
      "type": "training",
      "description": "Training step 2468",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:13",
      "total_flops_so_far": 5.86255028249168e+16,
      "budget_used_percent": 58.6255028249168
    },
    {
      "type": "training",
      "description": "Training step 2469",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:14",
      "total_flops_so_far": 5.864924169705248e+16,
      "budget_used_percent": 58.64924169705248
    },
    {
      "type": "training",
      "description": "Training step 2470",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:15",
      "total_flops_so_far": 5.867298056918816e+16,
      "budget_used_percent": 58.67298056918816
    },
    {
      "type": "training",
      "description": "Training step 2471",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:17",
      "total_flops_so_far": 5.869671944132384e+16,
      "budget_used_percent": 58.69671944132384
    },
    {
      "type": "training",
      "description": "Training step 2472",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:18",
      "total_flops_so_far": 5.872045831345952e+16,
      "budget_used_percent": 58.720458313459524
    },
    {
      "type": "training",
      "description": "Training step 2473",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:19",
      "total_flops_so_far": 5.87441971855952e+16,
      "budget_used_percent": 58.744197185595205
    },
    {
      "type": "training",
      "description": "Training step 2474",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:21",
      "total_flops_so_far": 5.876793605773088e+16,
      "budget_used_percent": 58.767936057730886
    },
    {
      "type": "training",
      "description": "Training step 2475",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:22",
      "total_flops_so_far": 5.879167492986656e+16,
      "budget_used_percent": 58.79167492986655
    },
    {
      "type": "training",
      "description": "Training step 2476",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:23",
      "total_flops_so_far": 5.881541380200224e+16,
      "budget_used_percent": 58.81541380200224
    },
    {
      "type": "training",
      "description": "Training step 2477",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:25",
      "total_flops_so_far": 5.883915267413792e+16,
      "budget_used_percent": 58.83915267413792
    },
    {
      "type": "training",
      "description": "Training step 2478",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:26",
      "total_flops_so_far": 5.88628915462736e+16,
      "budget_used_percent": 58.8628915462736
    },
    {
      "type": "training",
      "description": "Training step 2479",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:27",
      "total_flops_so_far": 5.888663041840928e+16,
      "budget_used_percent": 58.88663041840928
    },
    {
      "type": "training",
      "description": "Training step 2480",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:29",
      "total_flops_so_far": 5.891036929054496e+16,
      "budget_used_percent": 58.91036929054496
    },
    {
      "type": "training",
      "description": "Training step 2481",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:30",
      "total_flops_so_far": 5.893410816268064e+16,
      "budget_used_percent": 58.934108162680644
    },
    {
      "type": "training",
      "description": "Training step 2482",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:31",
      "total_flops_so_far": 5.895784703481632e+16,
      "budget_used_percent": 58.957847034816325
    },
    {
      "type": "training",
      "description": "Training step 2483",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:33",
      "total_flops_so_far": 5.8981585906952e+16,
      "budget_used_percent": 58.981585906952006
    },
    {
      "type": "training",
      "description": "Training step 2484",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:34",
      "total_flops_so_far": 5.900532477908768e+16,
      "budget_used_percent": 59.00532477908767
    },
    {
      "type": "training",
      "description": "Training step 2485",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:35",
      "total_flops_so_far": 5.902906365122336e+16,
      "budget_used_percent": 59.02906365122335
    },
    {
      "type": "training",
      "description": "Training step 2486",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:37",
      "total_flops_so_far": 5.905280252335904e+16,
      "budget_used_percent": 59.05280252335904
    },
    {
      "type": "training",
      "description": "Training step 2487",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:38",
      "total_flops_so_far": 5.907654139549472e+16,
      "budget_used_percent": 59.07654139549472
    },
    {
      "type": "training",
      "description": "Training step 2488",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:39",
      "total_flops_so_far": 5.91002802676304e+16,
      "budget_used_percent": 59.1002802676304
    },
    {
      "type": "training",
      "description": "Training step 2489",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:41",
      "total_flops_so_far": 5.912401913976608e+16,
      "budget_used_percent": 59.124019139766084
    },
    {
      "type": "training",
      "description": "Training step 2490",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:42",
      "total_flops_so_far": 5.914775801190176e+16,
      "budget_used_percent": 59.147758011901765
    },
    {
      "type": "training",
      "description": "Training step 2491",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:43",
      "total_flops_so_far": 5.917149688403744e+16,
      "budget_used_percent": 59.171496884037445
    },
    {
      "type": "training",
      "description": "Training step 2492",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:45",
      "total_flops_so_far": 5.919523575617312e+16,
      "budget_used_percent": 59.19523575617311
    },
    {
      "type": "training",
      "description": "Training step 2493",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:46",
      "total_flops_so_far": 5.92189746283088e+16,
      "budget_used_percent": 59.21897462830879
    },
    {
      "type": "training",
      "description": "Training step 2494",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:47",
      "total_flops_so_far": 5.924271350044448e+16,
      "budget_used_percent": 59.24271350044448
    },
    {
      "type": "training",
      "description": "Training step 2495",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:49",
      "total_flops_so_far": 5.926645237258016e+16,
      "budget_used_percent": 59.26645237258016
    },
    {
      "type": "training",
      "description": "Training step 2496",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:50",
      "total_flops_so_far": 5.929019124471584e+16,
      "budget_used_percent": 59.29019124471584
    },
    {
      "type": "training",
      "description": "Training step 2497",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:51",
      "total_flops_so_far": 5.931393011685152e+16,
      "budget_used_percent": 59.31393011685152
    },
    {
      "type": "training",
      "description": "Training step 2498",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:53",
      "total_flops_so_far": 5.93376689889872e+16,
      "budget_used_percent": 59.337668988987204
    },
    {
      "type": "training",
      "description": "Training step 2499",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:54",
      "total_flops_so_far": 5.936140786112288e+16,
      "budget_used_percent": 59.361407861122885
    },
    {
      "type": "training",
      "description": "Training step 2500",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:55",
      "total_flops_so_far": 5.938514673325856e+16,
      "budget_used_percent": 59.385146733258566
    },
    {
      "type": "training",
      "description": "Training step 2501",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:57",
      "total_flops_so_far": 5.940888560539424e+16,
      "budget_used_percent": 59.40888560539423
    },
    {
      "type": "training",
      "description": "Training step 2502",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:58",
      "total_flops_so_far": 5.943262447752992e+16,
      "budget_used_percent": 59.43262447752991
    },
    {
      "type": "training",
      "description": "Training step 2503",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:27:59",
      "total_flops_so_far": 5.94563633496656e+16,
      "budget_used_percent": 59.456363349665594
    },
    {
      "type": "training",
      "description": "Training step 2504",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:01",
      "total_flops_so_far": 5.948010222180128e+16,
      "budget_used_percent": 59.48010222180128
    },
    {
      "type": "training",
      "description": "Training step 2505",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:02",
      "total_flops_so_far": 5.950384109393696e+16,
      "budget_used_percent": 59.50384109393696
    },
    {
      "type": "training",
      "description": "Training step 2506",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:03",
      "total_flops_so_far": 5.952757996607264e+16,
      "budget_used_percent": 59.52757996607264
    },
    {
      "type": "training",
      "description": "Training step 2507",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:05",
      "total_flops_so_far": 5.955131883820832e+16,
      "budget_used_percent": 59.551318838208324
    },
    {
      "type": "training",
      "description": "Training step 2508",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:05",
      "total_flops_so_far": 5.9575057710344e+16,
      "budget_used_percent": 59.575057710344005
    },
    {
      "type": "training",
      "description": "Training step 2509",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:07",
      "total_flops_so_far": 5.959879658247968e+16,
      "budget_used_percent": 59.598796582479686
    },
    {
      "type": "training",
      "description": "Training step 2510",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:08",
      "total_flops_so_far": 5.962253545461536e+16,
      "budget_used_percent": 59.62253545461535
    },
    {
      "type": "training",
      "description": "Training step 2511",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:09",
      "total_flops_so_far": 5.964627432675104e+16,
      "budget_used_percent": 59.64627432675103
    },
    {
      "type": "training",
      "description": "Training step 2512",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:11",
      "total_flops_so_far": 5.967001319888672e+16,
      "budget_used_percent": 59.67001319888672
    },
    {
      "type": "training",
      "description": "Training step 2513",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:12",
      "total_flops_so_far": 5.96937520710224e+16,
      "budget_used_percent": 59.6937520710224
    },
    {
      "type": "training",
      "description": "Training step 2514",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:13",
      "total_flops_so_far": 5.971749094315808e+16,
      "budget_used_percent": 59.71749094315808
    },
    {
      "type": "training",
      "description": "Training step 2515",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:15",
      "total_flops_so_far": 5.974122981529376e+16,
      "budget_used_percent": 59.741229815293764
    },
    {
      "type": "training",
      "description": "Training step 2516",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:16",
      "total_flops_so_far": 5.976496868742944e+16,
      "budget_used_percent": 59.764968687429445
    },
    {
      "type": "training",
      "description": "Training step 2517",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:17",
      "total_flops_so_far": 5.978870755956512e+16,
      "budget_used_percent": 59.788707559565125
    },
    {
      "type": "training",
      "description": "Training step 2518",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:19",
      "total_flops_so_far": 5.98124464317008e+16,
      "budget_used_percent": 59.812446431700806
    },
    {
      "type": "training",
      "description": "Training step 2519",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:20",
      "total_flops_so_far": 5.983618530383648e+16,
      "budget_used_percent": 59.83618530383647
    },
    {
      "type": "training",
      "description": "Training step 2520",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:21",
      "total_flops_so_far": 5.985992417597216e+16,
      "budget_used_percent": 59.859924175972154
    },
    {
      "type": "training",
      "description": "Training step 2521",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:23",
      "total_flops_so_far": 5.988366304810784e+16,
      "budget_used_percent": 59.883663048107834
    },
    {
      "type": "training",
      "description": "Training step 2522",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:24",
      "total_flops_so_far": 5.990740192024352e+16,
      "budget_used_percent": 59.90740192024352
    },
    {
      "type": "training",
      "description": "Training step 2523",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:25",
      "total_flops_so_far": 5.99311407923792e+16,
      "budget_used_percent": 59.9311407923792
    },
    {
      "type": "training",
      "description": "Training step 2524",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:27",
      "total_flops_so_far": 5.995487966451488e+16,
      "budget_used_percent": 59.954879664514884
    },
    {
      "type": "training",
      "description": "Training step 2525",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:28",
      "total_flops_so_far": 5.997861853665056e+16,
      "budget_used_percent": 59.978618536650565
    },
    {
      "type": "training",
      "description": "Training step 2526",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:29",
      "total_flops_so_far": 6.000235740878624e+16,
      "budget_used_percent": 60.002357408786246
    },
    {
      "type": "training",
      "description": "Training step 2527",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:31",
      "total_flops_so_far": 6.002609628092192e+16,
      "budget_used_percent": 60.02609628092191
    },
    {
      "type": "training",
      "description": "Training step 2528",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:32",
      "total_flops_so_far": 6.00498351530576e+16,
      "budget_used_percent": 60.04983515305759
    },
    {
      "type": "training",
      "description": "Training step 2529",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:33",
      "total_flops_so_far": 6.007357402519328e+16,
      "budget_used_percent": 60.073574025193274
    },
    {
      "type": "training",
      "description": "Training step 2530",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:35",
      "total_flops_so_far": 6.009731289732896e+16,
      "budget_used_percent": 60.09731289732896
    },
    {
      "type": "training",
      "description": "Training step 2531",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:36",
      "total_flops_so_far": 6.012105176946464e+16,
      "budget_used_percent": 60.12105176946464
    },
    {
      "type": "training",
      "description": "Training step 2532",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:37",
      "total_flops_so_far": 6.014479064160032e+16,
      "budget_used_percent": 60.14479064160032
    },
    {
      "type": "training",
      "description": "Training step 2533",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:39",
      "total_flops_so_far": 6.0168529513736e+16,
      "budget_used_percent": 60.168529513736004
    },
    {
      "type": "training",
      "description": "Training step 2534",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:40",
      "total_flops_so_far": 6.019226838587168e+16,
      "budget_used_percent": 60.192268385871685
    },
    {
      "type": "training",
      "description": "Training step 2535",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:41",
      "total_flops_so_far": 6.021600725800736e+16,
      "budget_used_percent": 60.216007258007366
    },
    {
      "type": "training",
      "description": "Training step 2536",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:43",
      "total_flops_so_far": 6.023974613014304e+16,
      "budget_used_percent": 60.23974613014303
    },
    {
      "type": "training",
      "description": "Training step 2537",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:44",
      "total_flops_so_far": 6.026348500227872e+16,
      "budget_used_percent": 60.26348500227871
    },
    {
      "type": "training",
      "description": "Training step 2538",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:45",
      "total_flops_so_far": 6.02872238744144e+16,
      "budget_used_percent": 60.287223874414394
    },
    {
      "type": "training",
      "description": "Training step 2539",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:47",
      "total_flops_so_far": 6.031096274655008e+16,
      "budget_used_percent": 60.310962746550075
    },
    {
      "type": "training",
      "description": "Training step 2540",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:48",
      "total_flops_so_far": 6.033470161868576e+16,
      "budget_used_percent": 60.33470161868576
    },
    {
      "type": "training",
      "description": "Training step 2541",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:49",
      "total_flops_so_far": 6.035844049082144e+16,
      "budget_used_percent": 60.358440490821444
    },
    {
      "type": "training",
      "description": "Training step 2542",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:51",
      "total_flops_so_far": 6.038217936295712e+16,
      "budget_used_percent": 60.382179362957125
    },
    {
      "type": "training",
      "description": "Training step 2543",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:52",
      "total_flops_so_far": 6.04059182350928e+16,
      "budget_used_percent": 60.405918235092805
    },
    {
      "type": "training",
      "description": "Training step 2544",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:53",
      "total_flops_so_far": 6.042965710722848e+16,
      "budget_used_percent": 60.429657107228486
    },
    {
      "type": "training",
      "description": "Training step 2545",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:55",
      "total_flops_so_far": 6.045339597936416e+16,
      "budget_used_percent": 60.45339597936415
    },
    {
      "type": "training",
      "description": "Training step 2546",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:56",
      "total_flops_so_far": 6.047713485149984e+16,
      "budget_used_percent": 60.477134851499834
    },
    {
      "type": "training",
      "description": "Training step 2547",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:57",
      "total_flops_so_far": 6.050087372363552e+16,
      "budget_used_percent": 60.500873723635515
    },
    {
      "type": "training",
      "description": "Training step 2548",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:28:59",
      "total_flops_so_far": 6.05246125957712e+16,
      "budget_used_percent": 60.5246125957712
    },
    {
      "type": "training",
      "description": "Training step 2549",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:00",
      "total_flops_so_far": 6.054835146790688e+16,
      "budget_used_percent": 60.54835146790688
    },
    {
      "type": "training",
      "description": "Training step 2550",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:01",
      "total_flops_so_far": 6.057209034004256e+16,
      "budget_used_percent": 60.572090340042564
    },
    {
      "type": "training",
      "description": "Training step 2551",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:03",
      "total_flops_so_far": 6.059582921217824e+16,
      "budget_used_percent": 60.595829212178245
    },
    {
      "type": "training",
      "description": "Training step 2552",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:04",
      "total_flops_so_far": 6.061956808431392e+16,
      "budget_used_percent": 60.619568084313926
    },
    {
      "type": "training",
      "description": "Training step 2553",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:05",
      "total_flops_so_far": 6.06433069564496e+16,
      "budget_used_percent": 60.64330695644961
    },
    {
      "type": "training",
      "description": "Training step 2554",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:07",
      "total_flops_so_far": 6.066704582858528e+16,
      "budget_used_percent": 60.66704582858527
    },
    {
      "type": "training",
      "description": "Training step 2555",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:08",
      "total_flops_so_far": 6.069078470072096e+16,
      "budget_used_percent": 60.690784700720954
    },
    {
      "type": "training",
      "description": "Training step 2556",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:09",
      "total_flops_so_far": 6.071452357285664e+16,
      "budget_used_percent": 60.714523572856635
    },
    {
      "type": "training",
      "description": "Training step 2557",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:11",
      "total_flops_so_far": 6.073826244499232e+16,
      "budget_used_percent": 60.738262444992316
    },
    {
      "type": "training",
      "description": "Training step 2558",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:12",
      "total_flops_so_far": 6.0762001317128e+16,
      "budget_used_percent": 60.762001317128004
    },
    {
      "type": "training",
      "description": "Training step 2559",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:14",
      "total_flops_so_far": 6.078574018926368e+16,
      "budget_used_percent": 60.785740189263684
    },
    {
      "type": "training",
      "description": "Training step 2560",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:15",
      "total_flops_so_far": 6.080947906139936e+16,
      "budget_used_percent": 60.809479061399365
    },
    {
      "type": "training",
      "description": "Training step 2561",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:16",
      "total_flops_so_far": 6.083321793353504e+16,
      "budget_used_percent": 60.833217933535046
    },
    {
      "type": "training",
      "description": "Training step 2562",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:18",
      "total_flops_so_far": 6.085695680567072e+16,
      "budget_used_percent": 60.85695680567073
    },
    {
      "type": "training",
      "description": "Training step 2563",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:19",
      "total_flops_so_far": 6.08806956778064e+16,
      "budget_used_percent": 60.88069567780639
    },
    {
      "type": "training",
      "description": "Training step 2564",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:20",
      "total_flops_so_far": 6.090443454994208e+16,
      "budget_used_percent": 60.904434549942074
    },
    {
      "type": "training",
      "description": "Training step 2565",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:22",
      "total_flops_so_far": 6.092817342207776e+16,
      "budget_used_percent": 60.928173422077755
    },
    {
      "type": "training",
      "description": "Training step 2566",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:23",
      "total_flops_so_far": 6.095191229421344e+16,
      "budget_used_percent": 60.95191229421344
    },
    {
      "type": "training",
      "description": "Training step 2567",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:24",
      "total_flops_so_far": 6.097565116634912e+16,
      "budget_used_percent": 60.975651166349124
    },
    {
      "type": "training",
      "description": "Training step 2568",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:26",
      "total_flops_so_far": 6.09993900384848e+16,
      "budget_used_percent": 60.999390038484805
    },
    {
      "type": "training",
      "description": "Training step 2569",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:27",
      "total_flops_so_far": 6.102312891062048e+16,
      "budget_used_percent": 61.023128910620485
    },
    {
      "type": "training",
      "description": "Training step 2570",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:28",
      "total_flops_so_far": 6.104686778275616e+16,
      "budget_used_percent": 61.046867782756166
    },
    {
      "type": "training",
      "description": "Training step 2571",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:30",
      "total_flops_so_far": 6.107060665489184e+16,
      "budget_used_percent": 61.07060665489183
    },
    {
      "type": "training",
      "description": "Training step 2572",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:31",
      "total_flops_so_far": 6.109434552702752e+16,
      "budget_used_percent": 61.094345527027514
    },
    {
      "type": "training",
      "description": "Training step 2573",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:32",
      "total_flops_so_far": 6.11180843991632e+16,
      "budget_used_percent": 61.118084399163195
    },
    {
      "type": "training",
      "description": "Training step 2574",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:34",
      "total_flops_so_far": 6.114182327129888e+16,
      "budget_used_percent": 61.141823271298875
    },
    {
      "type": "training",
      "description": "Training step 2575",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:35",
      "total_flops_so_far": 6.116556214343456e+16,
      "budget_used_percent": 61.165562143434556
    },
    {
      "type": "training",
      "description": "Training step 2576",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:36",
      "total_flops_so_far": 6.118930101557024e+16,
      "budget_used_percent": 61.189301015570244
    },
    {
      "type": "training",
      "description": "Training step 2577",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:38",
      "total_flops_so_far": 6.121303988770592e+16,
      "budget_used_percent": 61.213039887705925
    },
    {
      "type": "training",
      "description": "Training step 2578",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:39",
      "total_flops_so_far": 6.12367787598416e+16,
      "budget_used_percent": 61.236778759841606
    },
    {
      "type": "training",
      "description": "Training step 2579",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:40",
      "total_flops_so_far": 6.126051763197728e+16,
      "budget_used_percent": 61.26051763197729
    },
    {
      "type": "training",
      "description": "Training step 2580",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:42",
      "total_flops_so_far": 6.128425650411296e+16,
      "budget_used_percent": 61.28425650411295
    },
    {
      "type": "training",
      "description": "Training step 2581",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:43",
      "total_flops_so_far": 6.130799537624864e+16,
      "budget_used_percent": 61.307995376248634
    },
    {
      "type": "training",
      "description": "Training step 2582",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:44",
      "total_flops_so_far": 6.133173424838432e+16,
      "budget_used_percent": 61.331734248384315
    },
    {
      "type": "training",
      "description": "Training step 2583",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:46",
      "total_flops_so_far": 6.135547312052e+16,
      "budget_used_percent": 61.355473120519996
    },
    {
      "type": "training",
      "description": "Training step 2584",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:47",
      "total_flops_so_far": 6.137921199265568e+16,
      "budget_used_percent": 61.379211992655684
    },
    {
      "type": "training",
      "description": "Training step 2585",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:49",
      "total_flops_so_far": 6.140295086479136e+16,
      "budget_used_percent": 61.402950864791364
    },
    {
      "type": "training",
      "description": "Training step 2586",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:50",
      "total_flops_so_far": 6.142668973692704e+16,
      "budget_used_percent": 61.426689736927045
    },
    {
      "type": "training",
      "description": "Training step 2587",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:51",
      "total_flops_so_far": 6.145042860906272e+16,
      "budget_used_percent": 61.450428609062726
    },
    {
      "type": "training",
      "description": "Training step 2588",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:53",
      "total_flops_so_far": 6.14741674811984e+16,
      "budget_used_percent": 61.47416748119841
    },
    {
      "type": "training",
      "description": "Training step 2589",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:54",
      "total_flops_so_far": 6.149790635333408e+16,
      "budget_used_percent": 61.49790635333407
    },
    {
      "type": "training",
      "description": "Training step 2590",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:55",
      "total_flops_so_far": 6.152164522546976e+16,
      "budget_used_percent": 61.521645225469754
    },
    {
      "type": "training",
      "description": "Training step 2591",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:57",
      "total_flops_so_far": 6.154538409760544e+16,
      "budget_used_percent": 61.545384097605435
    },
    {
      "type": "training",
      "description": "Training step 2592",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:58",
      "total_flops_so_far": 6.156912296974112e+16,
      "budget_used_percent": 61.569122969741116
    },
    {
      "type": "training",
      "description": "Training step 2593",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:29:59",
      "total_flops_so_far": 6.15928618418768e+16,
      "budget_used_percent": 61.5928618418768
    },
    {
      "type": "training",
      "description": "Training step 2594",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:01",
      "total_flops_so_far": 6.161660071401248e+16,
      "budget_used_percent": 61.616600714012485
    },
    {
      "type": "training",
      "description": "Training step 2595",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:02",
      "total_flops_so_far": 6.164033958614816e+16,
      "budget_used_percent": 61.640339586148166
    },
    {
      "type": "training",
      "description": "Training step 2596",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:03",
      "total_flops_so_far": 6.166407845828384e+16,
      "budget_used_percent": 61.664078458283846
    },
    {
      "type": "training",
      "description": "Training step 2597",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:06",
      "total_flops_so_far": 6.168781733041952e+16,
      "budget_used_percent": 61.68781733041953
    },
    {
      "type": "training",
      "description": "Training step 2598",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:08",
      "total_flops_so_far": 6.17115562025552e+16,
      "budget_used_percent": 61.711556202555194
    },
    {
      "type": "training",
      "description": "Training step 2599",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:10",
      "total_flops_so_far": 6.173529507469088e+16,
      "budget_used_percent": 61.735295074690875
    },
    {
      "type": "training",
      "description": "Training step 2600",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:11",
      "total_flops_so_far": 6.175903394682656e+16,
      "budget_used_percent": 61.759033946826555
    },
    {
      "type": "training",
      "description": "Training step 2601",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:12",
      "total_flops_so_far": 6.178277281896224e+16,
      "budget_used_percent": 61.782772818962236
    },
    {
      "type": "training",
      "description": "Training step 2602",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:14",
      "total_flops_so_far": 6.180651169109792e+16,
      "budget_used_percent": 61.806511691097924
    },
    {
      "type": "training",
      "description": "Training step 2603",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:15",
      "total_flops_so_far": 6.18302505632336e+16,
      "budget_used_percent": 61.830250563233605
    },
    {
      "type": "training",
      "description": "Training step 2604",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:16",
      "total_flops_so_far": 6.185398943536928e+16,
      "budget_used_percent": 61.853989435369286
    },
    {
      "type": "training",
      "description": "Training step 2605",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:18",
      "total_flops_so_far": 6.187772830750496e+16,
      "budget_used_percent": 61.87772830750497
    },
    {
      "type": "training",
      "description": "Training step 2606",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:19",
      "total_flops_so_far": 6.190146717964064e+16,
      "budget_used_percent": 61.90146717964063
    },
    {
      "type": "training",
      "description": "Training step 2607",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:20",
      "total_flops_so_far": 6.192520605177632e+16,
      "budget_used_percent": 61.925206051776314
    },
    {
      "type": "training",
      "description": "Training step 2608",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:22",
      "total_flops_so_far": 6.1948944923912e+16,
      "budget_used_percent": 61.948944923911995
    },
    {
      "type": "training",
      "description": "Training step 2609",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:23",
      "total_flops_so_far": 6.197268379604768e+16,
      "budget_used_percent": 61.972683796047676
    },
    {
      "type": "training",
      "description": "Training step 2610",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:24",
      "total_flops_so_far": 6.199642266818336e+16,
      "budget_used_percent": 61.99642266818336
    },
    {
      "type": "training",
      "description": "Training step 2611",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:26",
      "total_flops_so_far": 6.202016154031904e+16,
      "budget_used_percent": 62.02016154031904
    },
    {
      "type": "training",
      "description": "Training step 2612",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:27",
      "total_flops_so_far": 6.204390041245472e+16,
      "budget_used_percent": 62.043900412454725
    },
    {
      "type": "training",
      "description": "Training step 2613",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:28",
      "total_flops_so_far": 6.20676392845904e+16,
      "budget_used_percent": 62.067639284590406
    },
    {
      "type": "training",
      "description": "Training step 2614",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:30",
      "total_flops_so_far": 6.209137815672608e+16,
      "budget_used_percent": 62.09137815672609
    },
    {
      "type": "training",
      "description": "Training step 2615",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:31",
      "total_flops_so_far": 6.211511702886176e+16,
      "budget_used_percent": 62.115117028861754
    },
    {
      "type": "training",
      "description": "Training step 2616",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:32",
      "total_flops_so_far": 6.213885590099744e+16,
      "budget_used_percent": 62.138855900997434
    },
    {
      "type": "training",
      "description": "Training step 2617",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:34",
      "total_flops_so_far": 6.216259477313312e+16,
      "budget_used_percent": 62.162594773133115
    },
    {
      "type": "training",
      "description": "Training step 2618",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:35",
      "total_flops_so_far": 6.21863336452688e+16,
      "budget_used_percent": 62.186333645268796
    },
    {
      "type": "training",
      "description": "Training step 2619",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:36",
      "total_flops_so_far": 6.221007251740448e+16,
      "budget_used_percent": 62.21007251740448
    },
    {
      "type": "training",
      "description": "Training step 2620",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:38",
      "total_flops_so_far": 6.223381138954016e+16,
      "budget_used_percent": 62.233811389540165
    },
    {
      "type": "training",
      "description": "Training step 2621",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:39",
      "total_flops_so_far": 6.225755026167584e+16,
      "budget_used_percent": 62.257550261675846
    },
    {
      "type": "training",
      "description": "Training step 2622",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:40",
      "total_flops_so_far": 6.228128913381152e+16,
      "budget_used_percent": 62.281289133811526
    },
    {
      "type": "training",
      "description": "Training step 2623",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:42",
      "total_flops_so_far": 6.23050280059472e+16,
      "budget_used_percent": 62.30502800594721
    },
    {
      "type": "training",
      "description": "Training step 2624",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:43",
      "total_flops_so_far": 6.232876687808288e+16,
      "budget_used_percent": 62.328766878082874
    },
    {
      "type": "training",
      "description": "Training step 2625",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:44",
      "total_flops_so_far": 6.235250575021856e+16,
      "budget_used_percent": 62.352505750218555
    },
    {
      "type": "training",
      "description": "Training step 2626",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:46",
      "total_flops_so_far": 6.237624462235424e+16,
      "budget_used_percent": 62.376244622354236
    },
    {
      "type": "training",
      "description": "Training step 2627",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:47",
      "total_flops_so_far": 6.239998349448992e+16,
      "budget_used_percent": 62.399983494489916
    },
    {
      "type": "training",
      "description": "Training step 2628",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:48",
      "total_flops_so_far": 6.24237223666256e+16,
      "budget_used_percent": 62.4237223666256
    },
    {
      "type": "training",
      "description": "Training step 2629",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:50",
      "total_flops_so_far": 6.244746123876128e+16,
      "budget_used_percent": 62.44746123876128
    },
    {
      "type": "training",
      "description": "Training step 2630",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:51",
      "total_flops_so_far": 6.247120011089696e+16,
      "budget_used_percent": 62.471200110896966
    },
    {
      "type": "training",
      "description": "Training step 2631",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:52",
      "total_flops_so_far": 6.249493898303264e+16,
      "budget_used_percent": 62.49493898303265
    },
    {
      "type": "training",
      "description": "Training step 2632",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:54",
      "total_flops_so_far": 6.251867785516832e+16,
      "budget_used_percent": 62.51867785516833
    },
    {
      "type": "training",
      "description": "Training step 2633",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:55",
      "total_flops_so_far": 6.2542416727304e+16,
      "budget_used_percent": 62.542416727303994
    },
    {
      "type": "training",
      "description": "Training step 2634",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:57",
      "total_flops_so_far": 6.256615559943968e+16,
      "budget_used_percent": 62.566155599439675
    },
    {
      "type": "training",
      "description": "Training step 2635",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:58",
      "total_flops_so_far": 6.258989447157536e+16,
      "budget_used_percent": 62.589894471575356
    },
    {
      "type": "training",
      "description": "Training step 2636",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:30:59",
      "total_flops_so_far": 6.261363334371104e+16,
      "budget_used_percent": 62.61363334371104
    },
    {
      "type": "training",
      "description": "Training step 2637",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:01",
      "total_flops_so_far": 6.263737221584672e+16,
      "budget_used_percent": 62.63737221584672
    },
    {
      "type": "training",
      "description": "Training step 2638",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:02",
      "total_flops_so_far": 6.26611110879824e+16,
      "budget_used_percent": 62.661111087982405
    },
    {
      "type": "training",
      "description": "Training step 2639",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:03",
      "total_flops_so_far": 6.268484996011808e+16,
      "budget_used_percent": 62.684849960118086
    },
    {
      "type": "training",
      "description": "Training step 2640",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:04",
      "total_flops_so_far": 6.270858883225376e+16,
      "budget_used_percent": 62.70858883225377
    },
    {
      "type": "training",
      "description": "Training step 2641",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:05",
      "total_flops_so_far": 6.273232770438944e+16,
      "budget_used_percent": 62.732327704389434
    },
    {
      "type": "training",
      "description": "Training step 2642",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:06",
      "total_flops_so_far": 6.275606657652512e+16,
      "budget_used_percent": 62.756066576525114
    },
    {
      "type": "training",
      "description": "Training step 2643",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:08",
      "total_flops_so_far": 6.27798054486608e+16,
      "budget_used_percent": 62.779805448660795
    },
    {
      "type": "training",
      "description": "Training step 2644",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:09",
      "total_flops_so_far": 6.280354432079648e+16,
      "budget_used_percent": 62.803544320796476
    },
    {
      "type": "training",
      "description": "Training step 2645",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:10",
      "total_flops_so_far": 6.282728319293216e+16,
      "budget_used_percent": 62.82728319293216
    },
    {
      "type": "training",
      "description": "Training step 2646",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:12",
      "total_flops_so_far": 6.285102206506784e+16,
      "budget_used_percent": 62.85102206506784
    },
    {
      "type": "training",
      "description": "Training step 2647",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:13",
      "total_flops_so_far": 6.287476093720352e+16,
      "budget_used_percent": 62.87476093720352
    },
    {
      "type": "training",
      "description": "Training step 2648",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:14",
      "total_flops_so_far": 6.28984998093392e+16,
      "budget_used_percent": 62.89849980933921
    },
    {
      "type": "training",
      "description": "Training step 2649",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:16",
      "total_flops_so_far": 6.292223868147488e+16,
      "budget_used_percent": 62.92223868147489
    },
    {
      "type": "training",
      "description": "Training step 2650",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:17",
      "total_flops_so_far": 6.294597755361056e+16,
      "budget_used_percent": 62.945977553610554
    },
    {
      "type": "training",
      "description": "Training step 2651",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:18",
      "total_flops_so_far": 6.296971642574624e+16,
      "budget_used_percent": 62.969716425746235
    },
    {
      "type": "training",
      "description": "Training step 2652",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:20",
      "total_flops_so_far": 6.299345529788192e+16,
      "budget_used_percent": 62.993455297881916
    },
    {
      "type": "training",
      "description": "Training step 2653",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:21",
      "total_flops_so_far": 6.30171941700176e+16,
      "budget_used_percent": 63.017194170017596
    },
    {
      "type": "training",
      "description": "Training step 2654",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:22",
      "total_flops_so_far": 6.304093304215328e+16,
      "budget_used_percent": 63.04093304215328
    },
    {
      "type": "training",
      "description": "Training step 2655",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:24",
      "total_flops_so_far": 6.306467191428896e+16,
      "budget_used_percent": 63.06467191428896
    },
    {
      "type": "training",
      "description": "Training step 2656",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:25",
      "total_flops_so_far": 6.308841078642464e+16,
      "budget_used_percent": 63.088410786424646
    },
    {
      "type": "training",
      "description": "Training step 2657",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:27",
      "total_flops_so_far": 6.311214965856032e+16,
      "budget_used_percent": 63.11214965856033
    },
    {
      "type": "training",
      "description": "Training step 2658",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:28",
      "total_flops_so_far": 6.3135888530696e+16,
      "budget_used_percent": 63.13588853069601
    },
    {
      "type": "training",
      "description": "Training step 2659",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:29",
      "total_flops_so_far": 6.315962740283168e+16,
      "budget_used_percent": 63.159627402831674
    },
    {
      "type": "training",
      "description": "Training step 2660",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:31",
      "total_flops_so_far": 6.318336627496736e+16,
      "budget_used_percent": 63.183366274967355
    },
    {
      "type": "training",
      "description": "Training step 2661",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:32",
      "total_flops_so_far": 6.320710514710304e+16,
      "budget_used_percent": 63.207105147103036
    },
    {
      "type": "training",
      "description": "Training step 2662",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:33",
      "total_flops_so_far": 6.323084401923872e+16,
      "budget_used_percent": 63.23084401923872
    },
    {
      "type": "training",
      "description": "Training step 2663",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:35",
      "total_flops_so_far": 6.32545828913744e+16,
      "budget_used_percent": 63.2545828913744
    },
    {
      "type": "training",
      "description": "Training step 2664",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:36",
      "total_flops_so_far": 6.327832176351008e+16,
      "budget_used_percent": 63.27832176351008
    },
    {
      "type": "training",
      "description": "Training step 2665",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:37",
      "total_flops_so_far": 6.330206063564576e+16,
      "budget_used_percent": 63.30206063564576
    },
    {
      "type": "training",
      "description": "Training step 2666",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:39",
      "total_flops_so_far": 6.332579950778144e+16,
      "budget_used_percent": 63.32579950778145
    },
    {
      "type": "training",
      "description": "Training step 2667",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:40",
      "total_flops_so_far": 6.334953837991712e+16,
      "budget_used_percent": 63.34953837991713
    },
    {
      "type": "training",
      "description": "Training step 2668",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:41",
      "total_flops_so_far": 6.33732772520528e+16,
      "budget_used_percent": 63.373277252052794
    },
    {
      "type": "training",
      "description": "Training step 2669",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:43",
      "total_flops_so_far": 6.339701612418848e+16,
      "budget_used_percent": 63.397016124188475
    },
    {
      "type": "training",
      "description": "Training step 2670",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:44",
      "total_flops_so_far": 6.342075499632416e+16,
      "budget_used_percent": 63.420754996324156
    },
    {
      "type": "training",
      "description": "Training step 2671",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:45",
      "total_flops_so_far": 6.344449386845984e+16,
      "budget_used_percent": 63.44449386845984
    },
    {
      "type": "training",
      "description": "Training step 2672",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:47",
      "total_flops_so_far": 6.346823274059552e+16,
      "budget_used_percent": 63.46823274059552
    },
    {
      "type": "training",
      "description": "Training step 2673",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:48",
      "total_flops_so_far": 6.34919716127312e+16,
      "budget_used_percent": 63.4919716127312
    },
    {
      "type": "training",
      "description": "Training step 2674",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:49",
      "total_flops_so_far": 6.351571048486688e+16,
      "budget_used_percent": 63.51571048486689
    },
    {
      "type": "training",
      "description": "Training step 2675",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:51",
      "total_flops_so_far": 6.353944935700256e+16,
      "budget_used_percent": 63.53944935700257
    },
    {
      "type": "training",
      "description": "Training step 2676",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:52",
      "total_flops_so_far": 6.356318822913824e+16,
      "budget_used_percent": 63.56318822913825
    },
    {
      "type": "training",
      "description": "Training step 2677",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:53",
      "total_flops_so_far": 6.358692710127392e+16,
      "budget_used_percent": 63.586927101273915
    },
    {
      "type": "training",
      "description": "Training step 2678",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:55",
      "total_flops_so_far": 6.36106659734096e+16,
      "budget_used_percent": 63.610665973409596
    },
    {
      "type": "training",
      "description": "Training step 2679",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:56",
      "total_flops_so_far": 6.363440484554528e+16,
      "budget_used_percent": 63.634404845545276
    },
    {
      "type": "training",
      "description": "Training step 2680",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:57",
      "total_flops_so_far": 6.365814371768096e+16,
      "budget_used_percent": 63.65814371768096
    },
    {
      "type": "training",
      "description": "Training step 2681",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:31:59",
      "total_flops_so_far": 6.368188258981664e+16,
      "budget_used_percent": 63.68188258981664
    },
    {
      "type": "training",
      "description": "Training step 2682",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:00",
      "total_flops_so_far": 6.370562146195232e+16,
      "budget_used_percent": 63.70562146195232
    },
    {
      "type": "training",
      "description": "Training step 2683",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:01",
      "total_flops_so_far": 6.3729360334088e+16,
      "budget_used_percent": 63.729360334088
    },
    {
      "type": "training",
      "description": "Training step 2684",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:03",
      "total_flops_so_far": 6.375309920622368e+16,
      "budget_used_percent": 63.75309920622369
    },
    {
      "type": "training",
      "description": "Training step 2685",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:04",
      "total_flops_so_far": 6.377683807835936e+16,
      "budget_used_percent": 63.776838078359354
    },
    {
      "type": "training",
      "description": "Training step 2686",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:05",
      "total_flops_so_far": 6.380057695049504e+16,
      "budget_used_percent": 63.800576950495035
    },
    {
      "type": "training",
      "description": "Training step 2687",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:07",
      "total_flops_so_far": 6.382431582263072e+16,
      "budget_used_percent": 63.824315822630716
    },
    {
      "type": "training",
      "description": "Training step 2688",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:08",
      "total_flops_so_far": 6.38480546947664e+16,
      "budget_used_percent": 63.8480546947664
    },
    {
      "type": "training",
      "description": "Training step 2689",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:10",
      "total_flops_so_far": 6.387179356690208e+16,
      "budget_used_percent": 63.87179356690208
    },
    {
      "type": "training",
      "description": "Training step 2690",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:11",
      "total_flops_so_far": 6.389553243903776e+16,
      "budget_used_percent": 63.89553243903776
    },
    {
      "type": "training",
      "description": "Training step 2691",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:12",
      "total_flops_so_far": 6.391927131117344e+16,
      "budget_used_percent": 63.91927131117344
    },
    {
      "type": "training",
      "description": "Training step 2692",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:14",
      "total_flops_so_far": 6.394301018330912e+16,
      "budget_used_percent": 63.94301018330913
    },
    {
      "type": "training",
      "description": "Training step 2693",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:15",
      "total_flops_so_far": 6.39667490554448e+16,
      "budget_used_percent": 63.96674905544481
    },
    {
      "type": "training",
      "description": "Training step 2694",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:16",
      "total_flops_so_far": 6.399048792758048e+16,
      "budget_used_percent": 63.990487927580475
    },
    {
      "type": "training",
      "description": "Training step 2695",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:18",
      "total_flops_so_far": 6.401422679971616e+16,
      "budget_used_percent": 64.01422679971616
    },
    {
      "type": "training",
      "description": "Training step 2696",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:19",
      "total_flops_so_far": 6.403796567185184e+16,
      "budget_used_percent": 64.03796567185184
    },
    {
      "type": "training",
      "description": "Training step 2697",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:20",
      "total_flops_so_far": 6.406170454398752e+16,
      "budget_used_percent": 64.06170454398752
    },
    {
      "type": "training",
      "description": "Training step 2698",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:22",
      "total_flops_so_far": 6.40854434161232e+16,
      "budget_used_percent": 64.0854434161232
    },
    {
      "type": "training",
      "description": "Training step 2699",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:23",
      "total_flops_so_far": 6.410918228825888e+16,
      "budget_used_percent": 64.10918228825888
    },
    {
      "type": "training",
      "description": "Training step 2700",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:24",
      "total_flops_so_far": 6.413292116039456e+16,
      "budget_used_percent": 64.13292116039456
    },
    {
      "type": "training",
      "description": "Training step 2701",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:26",
      "total_flops_so_far": 6.415666003253024e+16,
      "budget_used_percent": 64.15666003253024
    },
    {
      "type": "training",
      "description": "Training step 2702",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:27",
      "total_flops_so_far": 6.418039890466592e+16,
      "budget_used_percent": 64.18039890466592
    },
    {
      "type": "training",
      "description": "Training step 2703",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:28",
      "total_flops_so_far": 6.42041377768016e+16,
      "budget_used_percent": 64.2041377768016
    },
    {
      "type": "training",
      "description": "Training step 2704",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:30",
      "total_flops_so_far": 6.422787664893728e+16,
      "budget_used_percent": 64.22787664893728
    },
    {
      "type": "training",
      "description": "Training step 2705",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:31",
      "total_flops_so_far": 6.425161552107296e+16,
      "budget_used_percent": 64.25161552107296
    },
    {
      "type": "training",
      "description": "Training step 2706",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:32",
      "total_flops_so_far": 6.427535439320864e+16,
      "budget_used_percent": 64.27535439320864
    },
    {
      "type": "training",
      "description": "Training step 2707",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:34",
      "total_flops_so_far": 6.429909326534432e+16,
      "budget_used_percent": 64.29909326534433
    },
    {
      "type": "training",
      "description": "Training step 2708",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:35",
      "total_flops_so_far": 6.432283213748e+16,
      "budget_used_percent": 64.32283213748
    },
    {
      "type": "training",
      "description": "Training step 2709",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:36",
      "total_flops_so_far": 6.434657100961568e+16,
      "budget_used_percent": 64.34657100961569
    },
    {
      "type": "training",
      "description": "Training step 2710",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:38",
      "total_flops_so_far": 6.437030988175136e+16,
      "budget_used_percent": 64.37030988175137
    },
    {
      "type": "training",
      "description": "Training step 2711",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:39",
      "total_flops_so_far": 6.439404875388704e+16,
      "budget_used_percent": 64.39404875388705
    },
    {
      "type": "training",
      "description": "Training step 2712",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:40",
      "total_flops_so_far": 6.441778762602272e+16,
      "budget_used_percent": 64.41778762602272
    },
    {
      "type": "training",
      "description": "Training step 2713",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:42",
      "total_flops_so_far": 6.44415264981584e+16,
      "budget_used_percent": 64.4415264981584
    },
    {
      "type": "training",
      "description": "Training step 2714",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:43",
      "total_flops_so_far": 6.446526537029408e+16,
      "budget_used_percent": 64.46526537029408
    },
    {
      "type": "training",
      "description": "Training step 2715",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:44",
      "total_flops_so_far": 6.448900424242976e+16,
      "budget_used_percent": 64.48900424242976
    },
    {
      "type": "training",
      "description": "Training step 2716",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:46",
      "total_flops_so_far": 6.451274311456544e+16,
      "budget_used_percent": 64.51274311456544
    },
    {
      "type": "training",
      "description": "Training step 2717",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:47",
      "total_flops_so_far": 6.453648198670112e+16,
      "budget_used_percent": 64.53648198670112
    },
    {
      "type": "training",
      "description": "Training step 2718",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:48",
      "total_flops_so_far": 6.45602208588368e+16,
      "budget_used_percent": 64.5602208588368
    },
    {
      "type": "training",
      "description": "Training step 2719",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:50",
      "total_flops_so_far": 6.458395973097248e+16,
      "budget_used_percent": 64.58395973097248
    },
    {
      "type": "training",
      "description": "Training step 2720",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:51",
      "total_flops_so_far": 6.460769860310816e+16,
      "budget_used_percent": 64.60769860310816
    },
    {
      "type": "training",
      "description": "Training step 2721",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:53",
      "total_flops_so_far": 6.463143747524384e+16,
      "budget_used_percent": 64.63143747524384
    },
    {
      "type": "training",
      "description": "Training step 2722",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:54",
      "total_flops_so_far": 6.465517634737952e+16,
      "budget_used_percent": 64.65517634737952
    },
    {
      "type": "training",
      "description": "Training step 2723",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:55",
      "total_flops_so_far": 6.46789152195152e+16,
      "budget_used_percent": 64.6789152195152
    },
    {
      "type": "training",
      "description": "Training step 2724",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:57",
      "total_flops_so_far": 6.470265409165088e+16,
      "budget_used_percent": 64.70265409165088
    },
    {
      "type": "training",
      "description": "Training step 2725",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:58",
      "total_flops_so_far": 6.472639296378656e+16,
      "budget_used_percent": 64.72639296378657
    },
    {
      "type": "training",
      "description": "Training step 2726",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:32:59",
      "total_flops_so_far": 6.475013183592224e+16,
      "budget_used_percent": 64.75013183592225
    },
    {
      "type": "training",
      "description": "Training step 2727",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:01",
      "total_flops_so_far": 6.477387070805792e+16,
      "budget_used_percent": 64.77387070805793
    },
    {
      "type": "training",
      "description": "Training step 2728",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:02",
      "total_flops_so_far": 6.47976095801936e+16,
      "budget_used_percent": 64.79760958019361
    },
    {
      "type": "training",
      "description": "Training step 2729",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:03",
      "total_flops_so_far": 6.482134845232928e+16,
      "budget_used_percent": 64.82134845232927
    },
    {
      "type": "training",
      "description": "Training step 2730",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:05",
      "total_flops_so_far": 6.484508732446496e+16,
      "budget_used_percent": 64.84508732446496
    },
    {
      "type": "training",
      "description": "Training step 2731",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:06",
      "total_flops_so_far": 6.486882619660064e+16,
      "budget_used_percent": 64.86882619660064
    },
    {
      "type": "training",
      "description": "Training step 2732",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:07",
      "total_flops_so_far": 6.489256506873632e+16,
      "budget_used_percent": 64.89256506873632
    },
    {
      "type": "training",
      "description": "Training step 2733",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:09",
      "total_flops_so_far": 6.4916303940872e+16,
      "budget_used_percent": 64.916303940872
    },
    {
      "type": "training",
      "description": "Training step 2734",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:10",
      "total_flops_so_far": 6.494004281300768e+16,
      "budget_used_percent": 64.94004281300768
    },
    {
      "type": "training",
      "description": "Training step 2735",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:12",
      "total_flops_so_far": 6.496378168514336e+16,
      "budget_used_percent": 64.96378168514336
    },
    {
      "type": "training",
      "description": "Training step 2736",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:13",
      "total_flops_so_far": 6.498752055727904e+16,
      "budget_used_percent": 64.98752055727904
    },
    {
      "type": "training",
      "description": "Training step 2737",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:14",
      "total_flops_so_far": 6.501125942941472e+16,
      "budget_used_percent": 65.01125942941472
    },
    {
      "type": "training",
      "description": "Training step 2738",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:16",
      "total_flops_so_far": 6.50349983015504e+16,
      "budget_used_percent": 65.03499830155039
    },
    {
      "type": "training",
      "description": "Training step 2739",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:17",
      "total_flops_so_far": 6.505873717368608e+16,
      "budget_used_percent": 65.05873717368608
    },
    {
      "type": "training",
      "description": "Training step 2740",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:18",
      "total_flops_so_far": 6.508247604582176e+16,
      "budget_used_percent": 65.08247604582176
    },
    {
      "type": "training",
      "description": "Training step 2741",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:20",
      "total_flops_so_far": 6.510621491795744e+16,
      "budget_used_percent": 65.10621491795744
    },
    {
      "type": "training",
      "description": "Training step 2742",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:21",
      "total_flops_so_far": 6.512995379009312e+16,
      "budget_used_percent": 65.12995379009313
    },
    {
      "type": "training",
      "description": "Training step 2743",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:22",
      "total_flops_so_far": 6.51536926622288e+16,
      "budget_used_percent": 65.1536926622288
    },
    {
      "type": "training",
      "description": "Training step 2744",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:24",
      "total_flops_so_far": 6.517743153436448e+16,
      "budget_used_percent": 65.17743153436449
    },
    {
      "type": "training",
      "description": "Training step 2745",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:25",
      "total_flops_so_far": 6.520117040650016e+16,
      "budget_used_percent": 65.20117040650017
    },
    {
      "type": "training",
      "description": "Training step 2746",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:26",
      "total_flops_so_far": 6.522490927863584e+16,
      "budget_used_percent": 65.22490927863585
    },
    {
      "type": "training",
      "description": "Training step 2747",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:28",
      "total_flops_so_far": 6.524864815077152e+16,
      "budget_used_percent": 65.24864815077152
    },
    {
      "type": "training",
      "description": "Training step 2748",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:29",
      "total_flops_so_far": 6.52723870229072e+16,
      "budget_used_percent": 65.2723870229072
    },
    {
      "type": "training",
      "description": "Training step 2749",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:30",
      "total_flops_so_far": 6.529612589504288e+16,
      "budget_used_percent": 65.29612589504288
    },
    {
      "type": "training",
      "description": "Training step 2750",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:32",
      "total_flops_so_far": 6.531986476717856e+16,
      "budget_used_percent": 65.31986476717856
    },
    {
      "type": "training",
      "description": "Training step 2751",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:33",
      "total_flops_so_far": 6.534360363931424e+16,
      "budget_used_percent": 65.34360363931424
    },
    {
      "type": "training",
      "description": "Training step 2752",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:34",
      "total_flops_so_far": 6.536734251144992e+16,
      "budget_used_percent": 65.36734251144992
    },
    {
      "type": "training",
      "description": "Training step 2753",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:36",
      "total_flops_so_far": 6.53910813835856e+16,
      "budget_used_percent": 65.3910813835856
    },
    {
      "type": "training",
      "description": "Training step 2754",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:37",
      "total_flops_so_far": 6.541482025572128e+16,
      "budget_used_percent": 65.41482025572128
    },
    {
      "type": "training",
      "description": "Training step 2755",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:38",
      "total_flops_so_far": 6.543855912785696e+16,
      "budget_used_percent": 65.43855912785695
    },
    {
      "type": "training",
      "description": "Training step 2756",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:40",
      "total_flops_so_far": 6.546229799999264e+16,
      "budget_used_percent": 65.46229799999264
    },
    {
      "type": "training",
      "description": "Training step 2757",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:41",
      "total_flops_so_far": 6.548603687212832e+16,
      "budget_used_percent": 65.48603687212832
    },
    {
      "type": "training",
      "description": "Training step 2758",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:42",
      "total_flops_so_far": 6.5509775744264e+16,
      "budget_used_percent": 65.509775744264
    },
    {
      "type": "training",
      "description": "Training step 2759",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:44",
      "total_flops_so_far": 6.553351461639968e+16,
      "budget_used_percent": 65.53351461639969
    },
    {
      "type": "training",
      "description": "Training step 2760",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:45",
      "total_flops_so_far": 6.555725348853536e+16,
      "budget_used_percent": 65.55725348853537
    },
    {
      "type": "training",
      "description": "Training step 2761",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:46",
      "total_flops_so_far": 6.558099236067104e+16,
      "budget_used_percent": 65.58099236067105
    },
    {
      "type": "training",
      "description": "Training step 2762",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:48",
      "total_flops_so_far": 6.560473123280672e+16,
      "budget_used_percent": 65.60473123280673
    },
    {
      "type": "training",
      "description": "Training step 2763",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:49",
      "total_flops_so_far": 6.56284701049424e+16,
      "budget_used_percent": 65.62847010494241
    },
    {
      "type": "training",
      "description": "Training step 2764",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:50",
      "total_flops_so_far": 6.565220897707808e+16,
      "budget_used_percent": 65.65220897707808
    },
    {
      "type": "training",
      "description": "Training step 2765",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:52",
      "total_flops_so_far": 6.567594784921376e+16,
      "budget_used_percent": 65.67594784921376
    },
    {
      "type": "training",
      "description": "Training step 2766",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:53",
      "total_flops_so_far": 6.569968672134944e+16,
      "budget_used_percent": 65.69968672134944
    },
    {
      "type": "training",
      "description": "Training step 2767",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:54",
      "total_flops_so_far": 6.572342559348512e+16,
      "budget_used_percent": 65.72342559348512
    },
    {
      "type": "training",
      "description": "Training step 2768",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:56",
      "total_flops_so_far": 6.57471644656208e+16,
      "budget_used_percent": 65.7471644656208
    },
    {
      "type": "training",
      "description": "Training step 2769",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:57",
      "total_flops_so_far": 6.577090333775648e+16,
      "budget_used_percent": 65.77090333775648
    },
    {
      "type": "training",
      "description": "Training step 2770",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:33:58",
      "total_flops_so_far": 6.579464220989216e+16,
      "budget_used_percent": 65.79464220989216
    },
    {
      "type": "training",
      "description": "Training step 2771",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:00",
      "total_flops_so_far": 6.581838108202784e+16,
      "budget_used_percent": 65.81838108202784
    },
    {
      "type": "training",
      "description": "Training step 2772",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:00",
      "total_flops_so_far": 6.584211995416352e+16,
      "budget_used_percent": 65.84211995416352
    },
    {
      "type": "training",
      "description": "Training step 2773",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:02",
      "total_flops_so_far": 6.58658588262992e+16,
      "budget_used_percent": 65.86585882629919
    },
    {
      "type": "training",
      "description": "Training step 2774",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:03",
      "total_flops_so_far": 6.588959769843488e+16,
      "budget_used_percent": 65.88959769843487
    },
    {
      "type": "training",
      "description": "Training step 2775",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:04",
      "total_flops_so_far": 6.591333657057056e+16,
      "budget_used_percent": 65.91333657057056
    },
    {
      "type": "training",
      "description": "Training step 2776",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:06",
      "total_flops_so_far": 6.593707544270624e+16,
      "budget_used_percent": 65.93707544270625
    },
    {
      "type": "training",
      "description": "Training step 2777",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:07",
      "total_flops_so_far": 6.596081431484192e+16,
      "budget_used_percent": 65.96081431484193
    },
    {
      "type": "training",
      "description": "Training step 2778",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:08",
      "total_flops_so_far": 6.59845531869776e+16,
      "budget_used_percent": 65.9845531869776
    },
    {
      "type": "training",
      "description": "Training step 2779",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:10",
      "total_flops_so_far": 6.600829205911328e+16,
      "budget_used_percent": 66.00829205911329
    },
    {
      "type": "training",
      "description": "Training step 2780",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:11",
      "total_flops_so_far": 6.603203093124896e+16,
      "budget_used_percent": 66.03203093124897
    },
    {
      "type": "training",
      "description": "Training step 2781",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:12",
      "total_flops_so_far": 6.605576980338464e+16,
      "budget_used_percent": 66.05576980338465
    },
    {
      "type": "training",
      "description": "Training step 2782",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:14",
      "total_flops_so_far": 6.607950867552032e+16,
      "budget_used_percent": 66.07950867552032
    },
    {
      "type": "training",
      "description": "Training step 2783",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:15",
      "total_flops_so_far": 6.6103247547656e+16,
      "budget_used_percent": 66.103247547656
    },
    {
      "type": "training",
      "description": "Training step 2784",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:16",
      "total_flops_so_far": 6.612698641979168e+16,
      "budget_used_percent": 66.12698641979168
    },
    {
      "type": "training",
      "description": "Training step 2785",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:18",
      "total_flops_so_far": 6.615072529192736e+16,
      "budget_used_percent": 66.15072529192736
    },
    {
      "type": "training",
      "description": "Training step 2786",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:19",
      "total_flops_so_far": 6.617446416406304e+16,
      "budget_used_percent": 66.17446416406304
    },
    {
      "type": "training",
      "description": "Training step 2787",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:20",
      "total_flops_so_far": 6.619820303619872e+16,
      "budget_used_percent": 66.19820303619872
    },
    {
      "type": "training",
      "description": "Training step 2788",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:22",
      "total_flops_so_far": 6.62219419083344e+16,
      "budget_used_percent": 66.2219419083344
    },
    {
      "type": "training",
      "description": "Training step 2789",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:23",
      "total_flops_so_far": 6.624568078047008e+16,
      "budget_used_percent": 66.24568078047008
    },
    {
      "type": "training",
      "description": "Training step 2790",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:24",
      "total_flops_so_far": 6.626941965260576e+16,
      "budget_used_percent": 66.26941965260576
    },
    {
      "type": "training",
      "description": "Training step 2791",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:26",
      "total_flops_so_far": 6.629315852474144e+16,
      "budget_used_percent": 66.29315852474143
    },
    {
      "type": "training",
      "description": "Training step 2792",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:27",
      "total_flops_so_far": 6.631689739687712e+16,
      "budget_used_percent": 66.31689739687712
    },
    {
      "type": "training",
      "description": "Training step 2793",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:28",
      "total_flops_so_far": 6.63406362690128e+16,
      "budget_used_percent": 66.3406362690128
    },
    {
      "type": "training",
      "description": "Training step 2794",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:30",
      "total_flops_so_far": 6.636437514114848e+16,
      "budget_used_percent": 66.36437514114849
    },
    {
      "type": "training",
      "description": "Training step 2795",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:31",
      "total_flops_so_far": 6.638811401328416e+16,
      "budget_used_percent": 66.38811401328417
    },
    {
      "type": "training",
      "description": "Training step 2796",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:33",
      "total_flops_so_far": 6.641185288541984e+16,
      "budget_used_percent": 66.41185288541985
    },
    {
      "type": "training",
      "description": "Training step 2797",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:34",
      "total_flops_so_far": 6.643559175755552e+16,
      "budget_used_percent": 66.43559175755553
    },
    {
      "type": "training",
      "description": "Training step 2798",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:35",
      "total_flops_so_far": 6.64593306296912e+16,
      "budget_used_percent": 66.45933062969121
    },
    {
      "type": "training",
      "description": "Training step 2799",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:37",
      "total_flops_so_far": 6.648306950182688e+16,
      "budget_used_percent": 66.48306950182688
    },
    {
      "type": "training",
      "description": "Training step 2800",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:38",
      "total_flops_so_far": 6.650680837396256e+16,
      "budget_used_percent": 66.50680837396256
    },
    {
      "type": "training",
      "description": "Training step 2801",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:39",
      "total_flops_so_far": 6.653054724609824e+16,
      "budget_used_percent": 66.53054724609824
    },
    {
      "type": "training",
      "description": "Training step 2802",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:41",
      "total_flops_so_far": 6.655428611823392e+16,
      "budget_used_percent": 66.55428611823392
    },
    {
      "type": "training",
      "description": "Training step 2803",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:42",
      "total_flops_so_far": 6.65780249903696e+16,
      "budget_used_percent": 66.5780249903696
    },
    {
      "type": "training",
      "description": "Training step 2804",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:43",
      "total_flops_so_far": 6.660176386250528e+16,
      "budget_used_percent": 66.60176386250528
    },
    {
      "type": "training",
      "description": "Training step 2805",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:45",
      "total_flops_so_far": 6.662550273464096e+16,
      "budget_used_percent": 66.62550273464096
    },
    {
      "type": "training",
      "description": "Training step 2806",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:46",
      "total_flops_so_far": 6.664924160677664e+16,
      "budget_used_percent": 66.64924160677664
    },
    {
      "type": "training",
      "description": "Training step 2807",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:47",
      "total_flops_so_far": 6.667298047891232e+16,
      "budget_used_percent": 66.67298047891232
    },
    {
      "type": "training",
      "description": "Training step 2808",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:49",
      "total_flops_so_far": 6.6696719351048e+16,
      "budget_used_percent": 66.69671935104799
    },
    {
      "type": "training",
      "description": "Training step 2809",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:50",
      "total_flops_so_far": 6.672045822318368e+16,
      "budget_used_percent": 66.72045822318367
    },
    {
      "type": "training",
      "description": "Training step 2810",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:51",
      "total_flops_so_far": 6.674419709531936e+16,
      "budget_used_percent": 66.74419709531935
    },
    {
      "type": "training",
      "description": "Training step 2811",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:53",
      "total_flops_so_far": 6.676793596745504e+16,
      "budget_used_percent": 66.76793596745505
    },
    {
      "type": "training",
      "description": "Training step 2812",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:54",
      "total_flops_so_far": 6.679167483959072e+16,
      "budget_used_percent": 66.79167483959073
    },
    {
      "type": "training",
      "description": "Training step 2813",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:55",
      "total_flops_so_far": 6.68154137117264e+16,
      "budget_used_percent": 66.8154137117264
    },
    {
      "type": "training",
      "description": "Training step 2814",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:57",
      "total_flops_so_far": 6.683915258386208e+16,
      "budget_used_percent": 66.83915258386209
    },
    {
      "type": "training",
      "description": "Training step 2815",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:58",
      "total_flops_so_far": 6.686289145599776e+16,
      "budget_used_percent": 66.86289145599777
    },
    {
      "type": "training",
      "description": "Training step 2816",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:34:59",
      "total_flops_so_far": 6.688663032813344e+16,
      "budget_used_percent": 66.88663032813345
    },
    {
      "type": "training",
      "description": "Training step 2817",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:01",
      "total_flops_so_far": 6.691036920026912e+16,
      "budget_used_percent": 66.91036920026912
    },
    {
      "type": "training",
      "description": "Training step 2818",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:02",
      "total_flops_so_far": 6.69341080724048e+16,
      "budget_used_percent": 66.9341080724048
    },
    {
      "type": "training",
      "description": "Training step 2819",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:04",
      "total_flops_so_far": 6.695784694454048e+16,
      "budget_used_percent": 66.95784694454048
    },
    {
      "type": "training",
      "description": "Training step 2820",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:06",
      "total_flops_so_far": 6.698158581667616e+16,
      "budget_used_percent": 66.98158581667616
    },
    {
      "type": "training",
      "description": "Training step 2821",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:07",
      "total_flops_so_far": 6.700532468881184e+16,
      "budget_used_percent": 67.00532468881184
    },
    {
      "type": "training",
      "description": "Training step 2822",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:08",
      "total_flops_so_far": 6.702906356094752e+16,
      "budget_used_percent": 67.02906356094752
    },
    {
      "type": "training",
      "description": "Training step 2823",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:10",
      "total_flops_so_far": 6.70528024330832e+16,
      "budget_used_percent": 67.0528024330832
    },
    {
      "type": "training",
      "description": "Training step 2824",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:11",
      "total_flops_so_far": 6.707654130521888e+16,
      "budget_used_percent": 67.07654130521888
    },
    {
      "type": "training",
      "description": "Training step 2825",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:12",
      "total_flops_so_far": 6.710028017735456e+16,
      "budget_used_percent": 67.10028017735456
    },
    {
      "type": "training",
      "description": "Training step 2826",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:14",
      "total_flops_so_far": 6.712401904949024e+16,
      "budget_used_percent": 67.12401904949023
    },
    {
      "type": "training",
      "description": "Training step 2827",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:15",
      "total_flops_so_far": 6.714775792162592e+16,
      "budget_used_percent": 67.14775792162591
    },
    {
      "type": "training",
      "description": "Training step 2828",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:16",
      "total_flops_so_far": 6.71714967937616e+16,
      "budget_used_percent": 67.1714967937616
    },
    {
      "type": "training",
      "description": "Training step 2829",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:18",
      "total_flops_so_far": 6.719523566589728e+16,
      "budget_used_percent": 67.19523566589729
    },
    {
      "type": "training",
      "description": "Training step 2830",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:19",
      "total_flops_so_far": 6.721897453803296e+16,
      "budget_used_percent": 67.21897453803297
    },
    {
      "type": "training",
      "description": "Training step 2831",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:20",
      "total_flops_so_far": 6.724271341016864e+16,
      "budget_used_percent": 67.24271341016865
    },
    {
      "type": "training",
      "description": "Training step 2832",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:22",
      "total_flops_so_far": 6.726645228230432e+16,
      "budget_used_percent": 67.26645228230433
    },
    {
      "type": "training",
      "description": "Training step 2833",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:23",
      "total_flops_so_far": 6.729019115444e+16,
      "budget_used_percent": 67.29019115444001
    },
    {
      "type": "training",
      "description": "Training step 2834",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:24",
      "total_flops_so_far": 6.731393002657568e+16,
      "budget_used_percent": 67.31393002657568
    },
    {
      "type": "training",
      "description": "Training step 2835",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:26",
      "total_flops_so_far": 6.733766889871136e+16,
      "budget_used_percent": 67.33766889871136
    },
    {
      "type": "training",
      "description": "Training step 2836",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:27",
      "total_flops_so_far": 6.736140777084704e+16,
      "budget_used_percent": 67.36140777084704
    },
    {
      "type": "training",
      "description": "Training step 2837",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:28",
      "total_flops_so_far": 6.738514664298272e+16,
      "budget_used_percent": 67.38514664298272
    },
    {
      "type": "training",
      "description": "Training step 2838",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:30",
      "total_flops_so_far": 6.74088855151184e+16,
      "budget_used_percent": 67.4088855151184
    },
    {
      "type": "training",
      "description": "Training step 2839",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:31",
      "total_flops_so_far": 6.743262438725408e+16,
      "budget_used_percent": 67.43262438725408
    },
    {
      "type": "training",
      "description": "Training step 2840",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:32",
      "total_flops_so_far": 6.745636325938976e+16,
      "budget_used_percent": 67.45636325938976
    },
    {
      "type": "training",
      "description": "Training step 2841",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:34",
      "total_flops_so_far": 6.748010213152544e+16,
      "budget_used_percent": 67.48010213152544
    },
    {
      "type": "training",
      "description": "Training step 2842",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:35",
      "total_flops_so_far": 6.750384100366112e+16,
      "budget_used_percent": 67.50384100366112
    },
    {
      "type": "training",
      "description": "Training step 2843",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:36",
      "total_flops_so_far": 6.75275798757968e+16,
      "budget_used_percent": 67.52757987579679
    },
    {
      "type": "training",
      "description": "Training step 2844",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:38",
      "total_flops_so_far": 6.755131874793248e+16,
      "budget_used_percent": 67.55131874793247
    },
    {
      "type": "training",
      "description": "Training step 2845",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:39",
      "total_flops_so_far": 6.757505762006816e+16,
      "budget_used_percent": 67.57505762006815
    },
    {
      "type": "training",
      "description": "Training step 2846",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:41",
      "total_flops_so_far": 6.759879649220384e+16,
      "budget_used_percent": 67.59879649220383
    },
    {
      "type": "training",
      "description": "Training step 2847",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:42",
      "total_flops_so_far": 6.762253536433952e+16,
      "budget_used_percent": 67.62253536433953
    },
    {
      "type": "training",
      "description": "Training step 2848",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:43",
      "total_flops_so_far": 6.76462742364752e+16,
      "budget_used_percent": 67.64627423647521
    },
    {
      "type": "training",
      "description": "Training step 2849",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:45",
      "total_flops_so_far": 6.767001310861088e+16,
      "budget_used_percent": 67.67001310861089
    },
    {
      "type": "training",
      "description": "Training step 2850",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:46",
      "total_flops_so_far": 6.769375198074656e+16,
      "budget_used_percent": 67.69375198074657
    },
    {
      "type": "training",
      "description": "Training step 2851",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:47",
      "total_flops_so_far": 6.771749085288224e+16,
      "budget_used_percent": 67.71749085288225
    },
    {
      "type": "training",
      "description": "Training step 2852",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:49",
      "total_flops_so_far": 6.774122972501792e+16,
      "budget_used_percent": 67.74122972501792
    },
    {
      "type": "training",
      "description": "Training step 2853",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:50",
      "total_flops_so_far": 6.77649685971536e+16,
      "budget_used_percent": 67.7649685971536
    },
    {
      "type": "training",
      "description": "Training step 2854",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:51",
      "total_flops_so_far": 6.778870746928928e+16,
      "budget_used_percent": 67.78870746928928
    },
    {
      "type": "training",
      "description": "Training step 2855",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:53",
      "total_flops_so_far": 6.781244634142496e+16,
      "budget_used_percent": 67.81244634142496
    },
    {
      "type": "training",
      "description": "Training step 2856",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:54",
      "total_flops_so_far": 6.783618521356064e+16,
      "budget_used_percent": 67.83618521356064
    },
    {
      "type": "training",
      "description": "Training step 2857",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:55",
      "total_flops_so_far": 6.785992408569632e+16,
      "budget_used_percent": 67.85992408569632
    },
    {
      "type": "training",
      "description": "Training step 2858",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:57",
      "total_flops_so_far": 6.7883662957832e+16,
      "budget_used_percent": 67.883662957832
    },
    {
      "type": "training",
      "description": "Training step 2859",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:58",
      "total_flops_so_far": 6.790740182996768e+16,
      "budget_used_percent": 67.90740182996768
    },
    {
      "type": "training",
      "description": "Training step 2860",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:35:59",
      "total_flops_so_far": 6.793114070210336e+16,
      "budget_used_percent": 67.93114070210336
    },
    {
      "type": "training",
      "description": "Training step 2861",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:01",
      "total_flops_so_far": 6.795487957423904e+16,
      "budget_used_percent": 67.95487957423903
    },
    {
      "type": "training",
      "description": "Training step 2862",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:02",
      "total_flops_so_far": 6.797861844637472e+16,
      "budget_used_percent": 67.97861844637471
    },
    {
      "type": "training",
      "description": "Training step 2863",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:03",
      "total_flops_so_far": 6.80023573185104e+16,
      "budget_used_percent": 68.00235731851039
    },
    {
      "type": "training",
      "description": "Training step 2864",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:05",
      "total_flops_so_far": 6.802609619064608e+16,
      "budget_used_percent": 68.02609619064609
    },
    {
      "type": "training",
      "description": "Training step 2865",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:06",
      "total_flops_so_far": 6.804983506278176e+16,
      "budget_used_percent": 68.04983506278177
    },
    {
      "type": "training",
      "description": "Training step 2866",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:07",
      "total_flops_so_far": 6.807357393491744e+16,
      "budget_used_percent": 68.07357393491745
    },
    {
      "type": "training",
      "description": "Training step 2867",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:09",
      "total_flops_so_far": 6.809731280705312e+16,
      "budget_used_percent": 68.09731280705313
    },
    {
      "type": "training",
      "description": "Training step 2868",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:10",
      "total_flops_so_far": 6.81210516791888e+16,
      "budget_used_percent": 68.12105167918881
    },
    {
      "type": "training",
      "description": "Training step 2869",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:11",
      "total_flops_so_far": 6.814479055132448e+16,
      "budget_used_percent": 68.14479055132448
    },
    {
      "type": "training",
      "description": "Training step 2870",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:13",
      "total_flops_so_far": 6.816852942346016e+16,
      "budget_used_percent": 68.16852942346016
    },
    {
      "type": "training",
      "description": "Training step 2871",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:14",
      "total_flops_so_far": 6.819226829559584e+16,
      "budget_used_percent": 68.19226829559584
    },
    {
      "type": "training",
      "description": "Training step 2872",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:15",
      "total_flops_so_far": 6.821600716773152e+16,
      "budget_used_percent": 68.21600716773152
    },
    {
      "type": "training",
      "description": "Training step 2873",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:17",
      "total_flops_so_far": 6.82397460398672e+16,
      "budget_used_percent": 68.2397460398672
    },
    {
      "type": "training",
      "description": "Training step 2874",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:18",
      "total_flops_so_far": 6.826348491200288e+16,
      "budget_used_percent": 68.26348491200288
    },
    {
      "type": "training",
      "description": "Training step 2875",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:20",
      "total_flops_so_far": 6.828722378413856e+16,
      "budget_used_percent": 68.28722378413856
    },
    {
      "type": "training",
      "description": "Training step 2876",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:21",
      "total_flops_so_far": 6.831096265627424e+16,
      "budget_used_percent": 68.31096265627424
    },
    {
      "type": "training",
      "description": "Training step 2877",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:22",
      "total_flops_so_far": 6.833470152840992e+16,
      "budget_used_percent": 68.33470152840992
    },
    {
      "type": "training",
      "description": "Training step 2878",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:24",
      "total_flops_so_far": 6.83584404005456e+16,
      "budget_used_percent": 68.35844040054559
    },
    {
      "type": "training",
      "description": "Training step 2879",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:25",
      "total_flops_so_far": 6.838217927268128e+16,
      "budget_used_percent": 68.38217927268127
    },
    {
      "type": "training",
      "description": "Training step 2880",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:26",
      "total_flops_so_far": 6.840591814481696e+16,
      "budget_used_percent": 68.40591814481695
    },
    {
      "type": "training",
      "description": "Training step 2881",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:28",
      "total_flops_so_far": 6.842965701695264e+16,
      "budget_used_percent": 68.42965701695263
    },
    {
      "type": "training",
      "description": "Training step 2882",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:29",
      "total_flops_so_far": 6.845339588908832e+16,
      "budget_used_percent": 68.45339588908831
    },
    {
      "type": "training",
      "description": "Training step 2883",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:31",
      "total_flops_so_far": 6.8477134761224e+16,
      "budget_used_percent": 68.47713476122401
    },
    {
      "type": "training",
      "description": "Training step 2884",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:32",
      "total_flops_so_far": 6.850087363335968e+16,
      "budget_used_percent": 68.50087363335969
    },
    {
      "type": "training",
      "description": "Training step 2885",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:33",
      "total_flops_so_far": 6.852461250549536e+16,
      "budget_used_percent": 68.52461250549537
    },
    {
      "type": "training",
      "description": "Training step 2886",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:35",
      "total_flops_so_far": 6.854835137763104e+16,
      "budget_used_percent": 68.54835137763105
    },
    {
      "type": "training",
      "description": "Training step 2887",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:36",
      "total_flops_so_far": 6.857209024976672e+16,
      "budget_used_percent": 68.57209024976672
    },
    {
      "type": "training",
      "description": "Training step 2888",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:37",
      "total_flops_so_far": 6.85958291219024e+16,
      "budget_used_percent": 68.5958291219024
    },
    {
      "type": "training",
      "description": "Training step 2889",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:39",
      "total_flops_so_far": 6.861956799403808e+16,
      "budget_used_percent": 68.61956799403808
    },
    {
      "type": "training",
      "description": "Training step 2890",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:40",
      "total_flops_so_far": 6.864330686617376e+16,
      "budget_used_percent": 68.64330686617376
    },
    {
      "type": "training",
      "description": "Training step 2891",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:41",
      "total_flops_so_far": 6.866704573830944e+16,
      "budget_used_percent": 68.66704573830944
    },
    {
      "type": "training",
      "description": "Training step 2892",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:43",
      "total_flops_so_far": 6.869078461044512e+16,
      "budget_used_percent": 68.69078461044512
    },
    {
      "type": "training",
      "description": "Training step 2893",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:44",
      "total_flops_so_far": 6.87145234825808e+16,
      "budget_used_percent": 68.7145234825808
    },
    {
      "type": "training",
      "description": "Training step 2894",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:45",
      "total_flops_so_far": 6.873826235471648e+16,
      "budget_used_percent": 68.73826235471648
    },
    {
      "type": "training",
      "description": "Training step 2895",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:47",
      "total_flops_so_far": 6.876200122685216e+16,
      "budget_used_percent": 68.76200122685216
    },
    {
      "type": "training",
      "description": "Training step 2896",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:48",
      "total_flops_so_far": 6.878574009898784e+16,
      "budget_used_percent": 68.78574009898783
    },
    {
      "type": "training",
      "description": "Training step 2897",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:49",
      "total_flops_so_far": 6.880947897112352e+16,
      "budget_used_percent": 68.80947897112351
    },
    {
      "type": "training",
      "description": "Training step 2898",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:51",
      "total_flops_so_far": 6.88332178432592e+16,
      "budget_used_percent": 68.83321784325919
    },
    {
      "type": "training",
      "description": "Training step 2899",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:52",
      "total_flops_so_far": 6.885695671539488e+16,
      "budget_used_percent": 68.85695671539487
    },
    {
      "type": "training",
      "description": "Training step 2900",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:53",
      "total_flops_so_far": 6.888069558753056e+16,
      "budget_used_percent": 68.88069558753057
    },
    {
      "type": "training",
      "description": "Training step 2901",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:55",
      "total_flops_so_far": 6.890443445966624e+16,
      "budget_used_percent": 68.90443445966625
    },
    {
      "type": "training",
      "description": "Training step 2902",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:56",
      "total_flops_so_far": 6.892817333180192e+16,
      "budget_used_percent": 68.92817333180193
    },
    {
      "type": "training",
      "description": "Training step 2903",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:57",
      "total_flops_so_far": 6.89519122039376e+16,
      "budget_used_percent": 68.95191220393761
    },
    {
      "type": "training",
      "description": "Training step 2904",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:58",
      "total_flops_so_far": 6.897565107607328e+16,
      "budget_used_percent": 68.97565107607329
    },
    {
      "type": "training",
      "description": "Training step 2905",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:36:59",
      "total_flops_so_far": 6.899938994820896e+16,
      "budget_used_percent": 68.99938994820896
    },
    {
      "type": "training",
      "description": "Training step 2906",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:01",
      "total_flops_so_far": 6.902312882034464e+16,
      "budget_used_percent": 69.02312882034464
    },
    {
      "type": "training",
      "description": "Training step 2907",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:02",
      "total_flops_so_far": 6.904686769248032e+16,
      "budget_used_percent": 69.04686769248032
    },
    {
      "type": "training",
      "description": "Training step 2908",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:03",
      "total_flops_so_far": 6.9070606564616e+16,
      "budget_used_percent": 69.070606564616
    },
    {
      "type": "training",
      "description": "Training step 2909",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:05",
      "total_flops_so_far": 6.909434543675168e+16,
      "budget_used_percent": 69.09434543675168
    },
    {
      "type": "training",
      "description": "Training step 2910",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:06",
      "total_flops_so_far": 6.911808430888736e+16,
      "budget_used_percent": 69.11808430888736
    },
    {
      "type": "training",
      "description": "Training step 2911",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:07",
      "total_flops_so_far": 6.914182318102304e+16,
      "budget_used_percent": 69.14182318102304
    },
    {
      "type": "training",
      "description": "Training step 2912",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:09",
      "total_flops_so_far": 6.916556205315872e+16,
      "budget_used_percent": 69.16556205315872
    },
    {
      "type": "training",
      "description": "Training step 2913",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:10",
      "total_flops_so_far": 6.91893009252944e+16,
      "budget_used_percent": 69.18930092529439
    },
    {
      "type": "training",
      "description": "Training step 2914",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:11",
      "total_flops_so_far": 6.921303979743008e+16,
      "budget_used_percent": 69.21303979743007
    },
    {
      "type": "training",
      "description": "Training step 2915",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:13",
      "total_flops_so_far": 6.923677866956576e+16,
      "budget_used_percent": 69.23677866956575
    },
    {
      "type": "training",
      "description": "Training step 2916",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:14",
      "total_flops_so_far": 6.926051754170144e+16,
      "budget_used_percent": 69.26051754170143
    },
    {
      "type": "training",
      "description": "Training step 2917",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:15",
      "total_flops_so_far": 6.928425641383712e+16,
      "budget_used_percent": 69.28425641383711
    },
    {
      "type": "training",
      "description": "Training step 2918",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:17",
      "total_flops_so_far": 6.93079952859728e+16,
      "budget_used_percent": 69.3079952859728
    },
    {
      "type": "training",
      "description": "Training step 2919",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:18",
      "total_flops_so_far": 6.933173415810848e+16,
      "budget_used_percent": 69.33173415810849
    },
    {
      "type": "training",
      "description": "Training step 2920",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:19",
      "total_flops_so_far": 6.935547303024416e+16,
      "budget_used_percent": 69.35547303024417
    },
    {
      "type": "training",
      "description": "Training step 2921",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:21",
      "total_flops_so_far": 6.937921190237984e+16,
      "budget_used_percent": 69.37921190237985
    },
    {
      "type": "training",
      "description": "Training step 2922",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:22",
      "total_flops_so_far": 6.940295077451552e+16,
      "budget_used_percent": 69.40295077451552
    },
    {
      "type": "training",
      "description": "Training step 2923",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:23",
      "total_flops_so_far": 6.94266896466512e+16,
      "budget_used_percent": 69.4266896466512
    },
    {
      "type": "training",
      "description": "Training step 2924",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:25",
      "total_flops_so_far": 6.945042851878688e+16,
      "budget_used_percent": 69.45042851878688
    },
    {
      "type": "training",
      "description": "Training step 2925",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:26",
      "total_flops_so_far": 6.947416739092256e+16,
      "budget_used_percent": 69.47416739092256
    },
    {
      "type": "training",
      "description": "Training step 2926",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:27",
      "total_flops_so_far": 6.949790626305824e+16,
      "budget_used_percent": 69.49790626305824
    },
    {
      "type": "training",
      "description": "Training step 2927",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:29",
      "total_flops_so_far": 6.952164513519392e+16,
      "budget_used_percent": 69.52164513519392
    },
    {
      "type": "training",
      "description": "Training step 2928",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:30",
      "total_flops_so_far": 6.95453840073296e+16,
      "budget_used_percent": 69.5453840073296
    },
    {
      "type": "training",
      "description": "Training step 2929",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:31",
      "total_flops_so_far": 6.956912287946528e+16,
      "budget_used_percent": 69.56912287946528
    },
    {
      "type": "training",
      "description": "Training step 2930",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:33",
      "total_flops_so_far": 6.959286175160096e+16,
      "budget_used_percent": 69.59286175160096
    },
    {
      "type": "training",
      "description": "Training step 2931",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:34",
      "total_flops_so_far": 6.961660062373664e+16,
      "budget_used_percent": 69.61660062373663
    },
    {
      "type": "training",
      "description": "Training step 2932",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:35",
      "total_flops_so_far": 6.964033949587232e+16,
      "budget_used_percent": 69.64033949587231
    },
    {
      "type": "training",
      "description": "Training step 2933",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:37",
      "total_flops_so_far": 6.9664078368008e+16,
      "budget_used_percent": 69.66407836800799
    },
    {
      "type": "training",
      "description": "Training step 2934",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:38",
      "total_flops_so_far": 6.968781724014368e+16,
      "budget_used_percent": 69.68781724014367
    },
    {
      "type": "training",
      "description": "Training step 2935",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:40",
      "total_flops_so_far": 6.971155611227936e+16,
      "budget_used_percent": 69.71155611227935
    },
    {
      "type": "training",
      "description": "Training step 2936",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:41",
      "total_flops_so_far": 6.973529498441504e+16,
      "budget_used_percent": 69.73529498441505
    },
    {
      "type": "training",
      "description": "Training step 2937",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:42",
      "total_flops_so_far": 6.975903385655072e+16,
      "budget_used_percent": 69.75903385655073
    },
    {
      "type": "training",
      "description": "Training step 2938",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:44",
      "total_flops_so_far": 6.97827727286864e+16,
      "budget_used_percent": 69.78277272868641
    },
    {
      "type": "training",
      "description": "Training step 2939",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:45",
      "total_flops_so_far": 6.980651160082208e+16,
      "budget_used_percent": 69.80651160082209
    },
    {
      "type": "training",
      "description": "Training step 2940",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:46",
      "total_flops_so_far": 6.983025047295776e+16,
      "budget_used_percent": 69.83025047295776
    },
    {
      "type": "training",
      "description": "Training step 2941",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:48",
      "total_flops_so_far": 6.985398934509344e+16,
      "budget_used_percent": 69.85398934509344
    },
    {
      "type": "training",
      "description": "Training step 2942",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:49",
      "total_flops_so_far": 6.987772821722912e+16,
      "budget_used_percent": 69.87772821722912
    },
    {
      "type": "training",
      "description": "Training step 2943",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:50",
      "total_flops_so_far": 6.99014670893648e+16,
      "budget_used_percent": 69.9014670893648
    },
    {
      "type": "training",
      "description": "Training step 2944",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:52",
      "total_flops_so_far": 6.992520596150048e+16,
      "budget_used_percent": 69.92520596150048
    },
    {
      "type": "training",
      "description": "Training step 2945",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:53",
      "total_flops_so_far": 6.994894483363616e+16,
      "budget_used_percent": 69.94894483363616
    },
    {
      "type": "training",
      "description": "Training step 2946",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:54",
      "total_flops_so_far": 6.997268370577184e+16,
      "budget_used_percent": 69.97268370577184
    },
    {
      "type": "training",
      "description": "Training step 2947",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:56",
      "total_flops_so_far": 6.999642257790752e+16,
      "budget_used_percent": 69.99642257790752
    },
    {
      "type": "training",
      "description": "Training step 2948",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:57",
      "total_flops_so_far": 7.00201614500432e+16,
      "budget_used_percent": 70.02016145004319
    },
    {
      "type": "training",
      "description": "Training step 2949",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:37:59",
      "total_flops_so_far": 7.004390032217888e+16,
      "budget_used_percent": 70.04390032217887
    },
    {
      "type": "training",
      "description": "Training step 2950",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:00",
      "total_flops_so_far": 7.006763919431456e+16,
      "budget_used_percent": 70.06763919431455
    },
    {
      "type": "training",
      "description": "Training step 2951",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:01",
      "total_flops_so_far": 7.009137806645024e+16,
      "budget_used_percent": 70.09137806645023
    },
    {
      "type": "training",
      "description": "Training step 2952",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:03",
      "total_flops_so_far": 7.011511693858592e+16,
      "budget_used_percent": 70.11511693858591
    },
    {
      "type": "training",
      "description": "Training step 2953",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:04",
      "total_flops_so_far": 7.01388558107216e+16,
      "budget_used_percent": 70.1388558107216
    },
    {
      "type": "training",
      "description": "Training step 2954",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:05",
      "total_flops_so_far": 7.016259468285728e+16,
      "budget_used_percent": 70.16259468285728
    },
    {
      "type": "training",
      "description": "Training step 2955",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:07",
      "total_flops_so_far": 7.018633355499296e+16,
      "budget_used_percent": 70.18633355499297
    },
    {
      "type": "training",
      "description": "Training step 2956",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:08",
      "total_flops_so_far": 7.021007242712864e+16,
      "budget_used_percent": 70.21007242712865
    },
    {
      "type": "training",
      "description": "Training step 2957",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:09",
      "total_flops_so_far": 7.023381129926432e+16,
      "budget_used_percent": 70.23381129926432
    },
    {
      "type": "training",
      "description": "Training step 2958",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:11",
      "total_flops_so_far": 7.02575501714e+16,
      "budget_used_percent": 70.2575501714
    },
    {
      "type": "training",
      "description": "Training step 2959",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:12",
      "total_flops_so_far": 7.028128904353568e+16,
      "budget_used_percent": 70.28128904353568
    },
    {
      "type": "training",
      "description": "Training step 2960",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:13",
      "total_flops_so_far": 7.030502791567136e+16,
      "budget_used_percent": 70.30502791567136
    },
    {
      "type": "training",
      "description": "Training step 2961",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:15",
      "total_flops_so_far": 7.032876678780704e+16,
      "budget_used_percent": 70.32876678780704
    },
    {
      "type": "training",
      "description": "Training step 2962",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:16",
      "total_flops_so_far": 7.035250565994272e+16,
      "budget_used_percent": 70.35250565994272
    },
    {
      "type": "training",
      "description": "Training step 2963",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:17",
      "total_flops_so_far": 7.03762445320784e+16,
      "budget_used_percent": 70.3762445320784
    },
    {
      "type": "training",
      "description": "Training step 2964",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:19",
      "total_flops_so_far": 7.039998340421408e+16,
      "budget_used_percent": 70.39998340421408
    },
    {
      "type": "training",
      "description": "Training step 2965",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:20",
      "total_flops_so_far": 7.042372227634976e+16,
      "budget_used_percent": 70.42372227634976
    },
    {
      "type": "training",
      "description": "Training step 2966",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:21",
      "total_flops_so_far": 7.044746114848544e+16,
      "budget_used_percent": 70.44746114848543
    },
    {
      "type": "training",
      "description": "Training step 2967",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:23",
      "total_flops_so_far": 7.047120002062112e+16,
      "budget_used_percent": 70.47120002062111
    },
    {
      "type": "training",
      "description": "Training step 2968",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:24",
      "total_flops_so_far": 7.04949388927568e+16,
      "budget_used_percent": 70.49493889275679
    },
    {
      "type": "training",
      "description": "Training step 2969",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:25",
      "total_flops_so_far": 7.051867776489248e+16,
      "budget_used_percent": 70.51867776489247
    },
    {
      "type": "training",
      "description": "Training step 2970",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:27",
      "total_flops_so_far": 7.054241663702816e+16,
      "budget_used_percent": 70.54241663702815
    },
    {
      "type": "training",
      "description": "Training step 2971",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:28",
      "total_flops_so_far": 7.056615550916384e+16,
      "budget_used_percent": 70.56615550916383
    },
    {
      "type": "training",
      "description": "Training step 2972",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:30",
      "total_flops_so_far": 7.058989438129952e+16,
      "budget_used_percent": 70.58989438129953
    },
    {
      "type": "training",
      "description": "Training step 2973",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:31",
      "total_flops_so_far": 7.06136332534352e+16,
      "budget_used_percent": 70.61363325343521
    },
    {
      "type": "training",
      "description": "Training step 2974",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:32",
      "total_flops_so_far": 7.063737212557088e+16,
      "budget_used_percent": 70.63737212557089
    },
    {
      "type": "training",
      "description": "Training step 2975",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:34",
      "total_flops_so_far": 7.066111099770656e+16,
      "budget_used_percent": 70.66111099770656
    },
    {
      "type": "training",
      "description": "Training step 2976",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:35",
      "total_flops_so_far": 7.068484986984224e+16,
      "budget_used_percent": 70.68484986984224
    },
    {
      "type": "training",
      "description": "Training step 2977",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:36",
      "total_flops_so_far": 7.070858874197792e+16,
      "budget_used_percent": 70.70858874197792
    },
    {
      "type": "training",
      "description": "Training step 2978",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:38",
      "total_flops_so_far": 7.07323276141136e+16,
      "budget_used_percent": 70.7323276141136
    },
    {
      "type": "training",
      "description": "Training step 2979",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:39",
      "total_flops_so_far": 7.075606648624928e+16,
      "budget_used_percent": 70.75606648624928
    },
    {
      "type": "training",
      "description": "Training step 2980",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:40",
      "total_flops_so_far": 7.077980535838496e+16,
      "budget_used_percent": 70.77980535838496
    },
    {
      "type": "training",
      "description": "Training step 2981",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:42",
      "total_flops_so_far": 7.080354423052064e+16,
      "budget_used_percent": 70.80354423052064
    },
    {
      "type": "training",
      "description": "Training step 2982",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:43",
      "total_flops_so_far": 7.082728310265632e+16,
      "budget_used_percent": 70.82728310265632
    },
    {
      "type": "training",
      "description": "Training step 2983",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:44",
      "total_flops_so_far": 7.0851021974792e+16,
      "budget_used_percent": 70.85102197479199
    },
    {
      "type": "training",
      "description": "Training step 2984",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:46",
      "total_flops_so_far": 7.087476084692768e+16,
      "budget_used_percent": 70.87476084692767
    },
    {
      "type": "training",
      "description": "Training step 2985",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:47",
      "total_flops_so_far": 7.089849971906336e+16,
      "budget_used_percent": 70.89849971906335
    },
    {
      "type": "training",
      "description": "Training step 2986",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:48",
      "total_flops_so_far": 7.092223859119904e+16,
      "budget_used_percent": 70.92223859119903
    },
    {
      "type": "training",
      "description": "Training step 2987",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:50",
      "total_flops_so_far": 7.094597746333472e+16,
      "budget_used_percent": 70.94597746333471
    },
    {
      "type": "training",
      "description": "Training step 2988",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:51",
      "total_flops_so_far": 7.09697163354704e+16,
      "budget_used_percent": 70.9697163354704
    },
    {
      "type": "training",
      "description": "Training step 2989",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:52",
      "total_flops_so_far": 7.099345520760608e+16,
      "budget_used_percent": 70.99345520760608
    },
    {
      "type": "training",
      "description": "Training step 2990",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:54",
      "total_flops_so_far": 7.101719407974176e+16,
      "budget_used_percent": 71.01719407974176
    },
    {
      "type": "training",
      "description": "Training step 2991",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:55",
      "total_flops_so_far": 7.104093295187744e+16,
      "budget_used_percent": 71.04093295187745
    },
    {
      "type": "training",
      "description": "Training step 2992",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:56",
      "total_flops_so_far": 7.106467182401312e+16,
      "budget_used_percent": 71.06467182401312
    },
    {
      "type": "training",
      "description": "Training step 2993",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:58",
      "total_flops_so_far": 7.10884106961488e+16,
      "budget_used_percent": 71.0884106961488
    },
    {
      "type": "training",
      "description": "Training step 2994",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:38:59",
      "total_flops_so_far": 7.111214956828448e+16,
      "budget_used_percent": 71.11214956828448
    },
    {
      "type": "training",
      "description": "Training step 2995",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:39:00",
      "total_flops_so_far": 7.113588844042016e+16,
      "budget_used_percent": 71.13588844042016
    },
    {
      "type": "training",
      "description": "Training step 2996",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:39:02",
      "total_flops_so_far": 7.115962731255584e+16,
      "budget_used_percent": 71.15962731255584
    },
    {
      "type": "training",
      "description": "Training step 2997",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:39:03",
      "total_flops_so_far": 7.118336618469152e+16,
      "budget_used_percent": 71.18336618469152
    },
    {
      "type": "training",
      "description": "Training step 2998",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:39:05",
      "total_flops_so_far": 7.12071050568272e+16,
      "budget_used_percent": 71.2071050568272
    },
    {
      "type": "training",
      "description": "Training step 2999",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:39:06",
      "total_flops_so_far": 7.123084392896288e+16,
      "budget_used_percent": 71.23084392896288
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 709803614656.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:39:12",
      "total_flops_so_far": 7.123155373257754e+16,
      "budget_used_percent": 71.23155373257754
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 713504058416.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:39:17",
      "total_flops_so_far": 7.123226723663595e+16,
      "budget_used_percent": 71.23226723663595
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711653476344.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:39:22",
      "total_flops_so_far": 7.1232978890112296e+16,
      "budget_used_percent": 71.23297889011229
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 709803614656.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:39:27",
      "total_flops_so_far": 7.123368869372695e+16,
      "budget_used_percent": 71.23368869372695
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712578677332.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:39:32",
      "total_flops_so_far": 7.123440127240429e+16,
      "budget_used_percent": 71.23440127240428
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 5",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 709803614656.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:39:37",
      "total_flops_so_far": 7.123511107601894e+16,
      "budget_used_percent": 71.23511107601894
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 6",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711653476344.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:39:42",
      "total_flops_so_far": 7.123582272949529e+16,
      "budget_used_percent": 71.2358227294953
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 7",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711653476344.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:39:47",
      "total_flops_so_far": 7.123653438297163e+16,
      "budget_used_percent": 71.23653438297163
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 8",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711653476344.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:39:52",
      "total_flops_so_far": 7.1237246036447976e+16,
      "budget_used_percent": 71.23724603644797
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 9",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711653476344.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:39:57",
      "total_flops_so_far": 7.123795768992432e+16,
      "budget_used_percent": 71.23795768992433
    },
    {
      "type": "training",
      "description": "Training step 3000",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:39:57",
      "total_flops_so_far": 7.126169656206e+16,
      "budget_used_percent": 71.26169656206
    },
    {
      "type": "training",
      "description": "Training step 3001",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:39:59",
      "total_flops_so_far": 7.128543543419568e+16,
      "budget_used_percent": 71.28543543419568
    },
    {
      "type": "training",
      "description": "Training step 3002",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:00",
      "total_flops_so_far": 7.130917430633136e+16,
      "budget_used_percent": 71.30917430633136
    },
    {
      "type": "training",
      "description": "Training step 3003",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:01",
      "total_flops_so_far": 7.133291317846704e+16,
      "budget_used_percent": 71.33291317846704
    },
    {
      "type": "training",
      "description": "Training step 3004",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:03",
      "total_flops_so_far": 7.135665205060272e+16,
      "budget_used_percent": 71.35665205060272
    },
    {
      "type": "training",
      "description": "Training step 3005",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:04",
      "total_flops_so_far": 7.13803909227384e+16,
      "budget_used_percent": 71.3803909227384
    },
    {
      "type": "training",
      "description": "Training step 3006",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:06",
      "total_flops_so_far": 7.140412979487408e+16,
      "budget_used_percent": 71.40412979487408
    },
    {
      "type": "training",
      "description": "Training step 3007",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:07",
      "total_flops_so_far": 7.142786866700976e+16,
      "budget_used_percent": 71.42786866700976
    },
    {
      "type": "training",
      "description": "Training step 3008",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:09",
      "total_flops_so_far": 7.145160753914544e+16,
      "budget_used_percent": 71.45160753914544
    },
    {
      "type": "training",
      "description": "Training step 3009",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:10",
      "total_flops_so_far": 7.147534641128112e+16,
      "budget_used_percent": 71.47534641128111
    },
    {
      "type": "training",
      "description": "Training step 3010",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:12",
      "total_flops_so_far": 7.14990852834168e+16,
      "budget_used_percent": 71.4990852834168
    },
    {
      "type": "training",
      "description": "Training step 3011",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:13",
      "total_flops_so_far": 7.152282415555248e+16,
      "budget_used_percent": 71.52282415555248
    },
    {
      "type": "training",
      "description": "Training step 3012",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:14",
      "total_flops_so_far": 7.154656302768816e+16,
      "budget_used_percent": 71.54656302768817
    },
    {
      "type": "training",
      "description": "Training step 3013",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:16",
      "total_flops_so_far": 7.157030189982384e+16,
      "budget_used_percent": 71.57030189982385
    },
    {
      "type": "training",
      "description": "Training step 3014",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:17",
      "total_flops_so_far": 7.159404077195952e+16,
      "budget_used_percent": 71.59404077195953
    },
    {
      "type": "training",
      "description": "Training step 3015",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:18",
      "total_flops_so_far": 7.16177796440952e+16,
      "budget_used_percent": 71.61777964409521
    },
    {
      "type": "training",
      "description": "Training step 3016",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:20",
      "total_flops_so_far": 7.164151851623088e+16,
      "budget_used_percent": 71.64151851623089
    },
    {
      "type": "training",
      "description": "Training step 3017",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:21",
      "total_flops_so_far": 7.166525738836656e+16,
      "budget_used_percent": 71.66525738836656
    },
    {
      "type": "training",
      "description": "Training step 3018",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:22",
      "total_flops_so_far": 7.168899626050224e+16,
      "budget_used_percent": 71.68899626050224
    },
    {
      "type": "training",
      "description": "Training step 3019",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:24",
      "total_flops_so_far": 7.171273513263792e+16,
      "budget_used_percent": 71.71273513263792
    },
    {
      "type": "training",
      "description": "Training step 3020",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:25",
      "total_flops_so_far": 7.17364740047736e+16,
      "budget_used_percent": 71.7364740047736
    },
    {
      "type": "training",
      "description": "Training step 3021",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:27",
      "total_flops_so_far": 7.176021287690928e+16,
      "budget_used_percent": 71.76021287690928
    },
    {
      "type": "training",
      "description": "Training step 3022",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:28",
      "total_flops_so_far": 7.178395174904496e+16,
      "budget_used_percent": 71.78395174904496
    },
    {
      "type": "training",
      "description": "Training step 3023",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:29",
      "total_flops_so_far": 7.180769062118064e+16,
      "budget_used_percent": 71.80769062118064
    },
    {
      "type": "training",
      "description": "Training step 3024",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:31",
      "total_flops_so_far": 7.183142949331632e+16,
      "budget_used_percent": 71.83142949331632
    },
    {
      "type": "training",
      "description": "Training step 3025",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:32",
      "total_flops_so_far": 7.1855168365452e+16,
      "budget_used_percent": 71.855168365452
    },
    {
      "type": "training",
      "description": "Training step 3026",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:33",
      "total_flops_so_far": 7.187890723758768e+16,
      "budget_used_percent": 71.87890723758767
    },
    {
      "type": "training",
      "description": "Training step 3027",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:35",
      "total_flops_so_far": 7.190264610972336e+16,
      "budget_used_percent": 71.90264610972335
    },
    {
      "type": "training",
      "description": "Training step 3028",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:36",
      "total_flops_so_far": 7.192638498185904e+16,
      "budget_used_percent": 71.92638498185903
    },
    {
      "type": "training",
      "description": "Training step 3029",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:37",
      "total_flops_so_far": 7.195012385399472e+16,
      "budget_used_percent": 71.95012385399473
    },
    {
      "type": "training",
      "description": "Training step 3030",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:39",
      "total_flops_so_far": 7.19738627261304e+16,
      "budget_used_percent": 71.9738627261304
    },
    {
      "type": "training",
      "description": "Training step 3031",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:40",
      "total_flops_so_far": 7.199760159826608e+16,
      "budget_used_percent": 71.99760159826609
    },
    {
      "type": "training",
      "description": "Training step 3032",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:41",
      "total_flops_so_far": 7.202134047040176e+16,
      "budget_used_percent": 72.02134047040177
    },
    {
      "type": "training",
      "description": "Training step 3033",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:43",
      "total_flops_so_far": 7.204507934253744e+16,
      "budget_used_percent": 72.04507934253745
    },
    {
      "type": "training",
      "description": "Training step 3034",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:44",
      "total_flops_so_far": 7.206881821467312e+16,
      "budget_used_percent": 72.06881821467313
    },
    {
      "type": "training",
      "description": "Training step 3035",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:45",
      "total_flops_so_far": 7.20925570868088e+16,
      "budget_used_percent": 72.0925570868088
    },
    {
      "type": "training",
      "description": "Training step 3036",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:46",
      "total_flops_so_far": 7.211629595894448e+16,
      "budget_used_percent": 72.11629595894448
    },
    {
      "type": "training",
      "description": "Training step 3037",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:47",
      "total_flops_so_far": 7.214003483108016e+16,
      "budget_used_percent": 72.14003483108016
    },
    {
      "type": "training",
      "description": "Training step 3038",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:49",
      "total_flops_so_far": 7.216377370321584e+16,
      "budget_used_percent": 72.16377370321584
    },
    {
      "type": "training",
      "description": "Training step 3039",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:50",
      "total_flops_so_far": 7.218751257535152e+16,
      "budget_used_percent": 72.18751257535152
    },
    {
      "type": "training",
      "description": "Training step 3040",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:51",
      "total_flops_so_far": 7.22112514474872e+16,
      "budget_used_percent": 72.2112514474872
    },
    {
      "type": "training",
      "description": "Training step 3041",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:53",
      "total_flops_so_far": 7.223499031962288e+16,
      "budget_used_percent": 72.23499031962288
    },
    {
      "type": "training",
      "description": "Training step 3042",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:54",
      "total_flops_so_far": 7.225872919175856e+16,
      "budget_used_percent": 72.25872919175856
    },
    {
      "type": "training",
      "description": "Training step 3043",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:55",
      "total_flops_so_far": 7.228246806389424e+16,
      "budget_used_percent": 72.28246806389424
    },
    {
      "type": "training",
      "description": "Training step 3044",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:57",
      "total_flops_so_far": 7.230620693602992e+16,
      "budget_used_percent": 72.30620693602991
    },
    {
      "type": "training",
      "description": "Training step 3045",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:58",
      "total_flops_so_far": 7.23299458081656e+16,
      "budget_used_percent": 72.32994580816559
    },
    {
      "type": "training",
      "description": "Training step 3046",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:40:59",
      "total_flops_so_far": 7.235368468030128e+16,
      "budget_used_percent": 72.35368468030128
    },
    {
      "type": "training",
      "description": "Training step 3047",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:01",
      "total_flops_so_far": 7.237742355243696e+16,
      "budget_used_percent": 72.37742355243697
    },
    {
      "type": "training",
      "description": "Training step 3048",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:02",
      "total_flops_so_far": 7.240116242457264e+16,
      "budget_used_percent": 72.40116242457265
    },
    {
      "type": "training",
      "description": "Training step 3049",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:04",
      "total_flops_so_far": 7.242490129670832e+16,
      "budget_used_percent": 72.42490129670833
    },
    {
      "type": "training",
      "description": "Training step 3050",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:05",
      "total_flops_so_far": 7.2448640168844e+16,
      "budget_used_percent": 72.44864016884401
    },
    {
      "type": "training",
      "description": "Training step 3051",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:06",
      "total_flops_so_far": 7.247237904097968e+16,
      "budget_used_percent": 72.47237904097969
    },
    {
      "type": "training",
      "description": "Training step 3052",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:08",
      "total_flops_so_far": 7.249611791311536e+16,
      "budget_used_percent": 72.49611791311537
    },
    {
      "type": "training",
      "description": "Training step 3053",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:09",
      "total_flops_so_far": 7.251985678525104e+16,
      "budget_used_percent": 72.51985678525104
    },
    {
      "type": "training",
      "description": "Training step 3054",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:10",
      "total_flops_so_far": 7.254359565738672e+16,
      "budget_used_percent": 72.54359565738672
    },
    {
      "type": "training",
      "description": "Training step 3055",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:12",
      "total_flops_so_far": 7.25673345295224e+16,
      "budget_used_percent": 72.5673345295224
    },
    {
      "type": "training",
      "description": "Training step 3056",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:13",
      "total_flops_so_far": 7.259107340165808e+16,
      "budget_used_percent": 72.59107340165808
    },
    {
      "type": "training",
      "description": "Training step 3057",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:14",
      "total_flops_so_far": 7.261481227379376e+16,
      "budget_used_percent": 72.61481227379376
    },
    {
      "type": "training",
      "description": "Training step 3058",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:16",
      "total_flops_so_far": 7.263855114592944e+16,
      "budget_used_percent": 72.63855114592944
    },
    {
      "type": "training",
      "description": "Training step 3059",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:17",
      "total_flops_so_far": 7.266229001806512e+16,
      "budget_used_percent": 72.66229001806512
    },
    {
      "type": "training",
      "description": "Training step 3060",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:18",
      "total_flops_so_far": 7.26860288902008e+16,
      "budget_used_percent": 72.6860288902008
    },
    {
      "type": "training",
      "description": "Training step 3061",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:20",
      "total_flops_so_far": 7.270976776233648e+16,
      "budget_used_percent": 72.70976776233647
    },
    {
      "type": "training",
      "description": "Training step 3062",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:21",
      "total_flops_so_far": 7.273350663447216e+16,
      "budget_used_percent": 72.73350663447215
    },
    {
      "type": "training",
      "description": "Training step 3063",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:22",
      "total_flops_so_far": 7.275724550660784e+16,
      "budget_used_percent": 72.75724550660783
    },
    {
      "type": "training",
      "description": "Training step 3064",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:24",
      "total_flops_so_far": 7.278098437874352e+16,
      "budget_used_percent": 72.78098437874351
    },
    {
      "type": "training",
      "description": "Training step 3065",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:25",
      "total_flops_so_far": 7.28047232508792e+16,
      "budget_used_percent": 72.8047232508792
    },
    {
      "type": "training",
      "description": "Training step 3066",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:26",
      "total_flops_so_far": 7.282846212301488e+16,
      "budget_used_percent": 72.82846212301489
    },
    {
      "type": "training",
      "description": "Training step 3067",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:28",
      "total_flops_so_far": 7.285220099515056e+16,
      "budget_used_percent": 72.85220099515057
    },
    {
      "type": "training",
      "description": "Training step 3068",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:29",
      "total_flops_so_far": 7.287593986728624e+16,
      "budget_used_percent": 72.87593986728625
    },
    {
      "type": "training",
      "description": "Training step 3069",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:30",
      "total_flops_so_far": 7.289967873942192e+16,
      "budget_used_percent": 72.89967873942193
    },
    {
      "type": "training",
      "description": "Training step 3070",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:32",
      "total_flops_so_far": 7.29234176115576e+16,
      "budget_used_percent": 72.9234176115576
    },
    {
      "type": "training",
      "description": "Training step 3071",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:33",
      "total_flops_so_far": 7.294715648369328e+16,
      "budget_used_percent": 72.94715648369328
    },
    {
      "type": "training",
      "description": "Training step 3072",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:35",
      "total_flops_so_far": 7.297089535582896e+16,
      "budget_used_percent": 72.97089535582896
    },
    {
      "type": "training",
      "description": "Training step 3073",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:36",
      "total_flops_so_far": 7.299463422796464e+16,
      "budget_used_percent": 72.99463422796464
    },
    {
      "type": "training",
      "description": "Training step 3074",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:37",
      "total_flops_so_far": 7.301837310010032e+16,
      "budget_used_percent": 73.01837310010032
    },
    {
      "type": "training",
      "description": "Training step 3075",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:39",
      "total_flops_so_far": 7.3042111972236e+16,
      "budget_used_percent": 73.042111972236
    },
    {
      "type": "training",
      "description": "Training step 3076",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:40",
      "total_flops_so_far": 7.306585084437168e+16,
      "budget_used_percent": 73.06585084437168
    },
    {
      "type": "training",
      "description": "Training step 3077",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:41",
      "total_flops_so_far": 7.308958971650736e+16,
      "budget_used_percent": 73.08958971650736
    },
    {
      "type": "training",
      "description": "Training step 3078",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:43",
      "total_flops_so_far": 7.311332858864304e+16,
      "budget_used_percent": 73.11332858864304
    },
    {
      "type": "training",
      "description": "Training step 3079",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:44",
      "total_flops_so_far": 7.313706746077872e+16,
      "budget_used_percent": 73.13706746077871
    },
    {
      "type": "training",
      "description": "Training step 3080",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:45",
      "total_flops_so_far": 7.31608063329144e+16,
      "budget_used_percent": 73.16080633291439
    },
    {
      "type": "training",
      "description": "Training step 3081",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:47",
      "total_flops_so_far": 7.318454520505008e+16,
      "budget_used_percent": 73.18454520505007
    },
    {
      "type": "training",
      "description": "Training step 3082",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:48",
      "total_flops_so_far": 7.320828407718576e+16,
      "budget_used_percent": 73.20828407718577
    },
    {
      "type": "training",
      "description": "Training step 3083",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:49",
      "total_flops_so_far": 7.323202294932144e+16,
      "budget_used_percent": 73.23202294932145
    },
    {
      "type": "training",
      "description": "Training step 3084",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:51",
      "total_flops_so_far": 7.325576182145712e+16,
      "budget_used_percent": 73.25576182145713
    },
    {
      "type": "training",
      "description": "Training step 3085",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:52",
      "total_flops_so_far": 7.32795006935928e+16,
      "budget_used_percent": 73.27950069359281
    },
    {
      "type": "training",
      "description": "Training step 3086",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:53",
      "total_flops_so_far": 7.330323956572848e+16,
      "budget_used_percent": 73.30323956572849
    },
    {
      "type": "training",
      "description": "Training step 3087",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:55",
      "total_flops_so_far": 7.332697843786416e+16,
      "budget_used_percent": 73.32697843786417
    },
    {
      "type": "training",
      "description": "Training step 3088",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:56",
      "total_flops_so_far": 7.335071730999984e+16,
      "budget_used_percent": 73.35071730999984
    },
    {
      "type": "training",
      "description": "Training step 3089",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:57",
      "total_flops_so_far": 7.337445618213552e+16,
      "budget_used_percent": 73.37445618213552
    },
    {
      "type": "training",
      "description": "Training step 3090",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:41:59",
      "total_flops_so_far": 7.33981950542712e+16,
      "budget_used_percent": 73.3981950542712
    },
    {
      "type": "training",
      "description": "Training step 3091",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:00",
      "total_flops_so_far": 7.342193392640688e+16,
      "budget_used_percent": 73.42193392640688
    },
    {
      "type": "training",
      "description": "Training step 3092",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:01",
      "total_flops_so_far": 7.344567279854256e+16,
      "budget_used_percent": 73.44567279854256
    },
    {
      "type": "training",
      "description": "Training step 3093",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:03",
      "total_flops_so_far": 7.346941167067824e+16,
      "budget_used_percent": 73.46941167067824
    },
    {
      "type": "training",
      "description": "Training step 3094",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:04",
      "total_flops_so_far": 7.349315054281392e+16,
      "budget_used_percent": 73.49315054281392
    },
    {
      "type": "training",
      "description": "Training step 3095",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:06",
      "total_flops_so_far": 7.35168894149496e+16,
      "budget_used_percent": 73.5168894149496
    },
    {
      "type": "training",
      "description": "Training step 3096",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:07",
      "total_flops_so_far": 7.354062828708528e+16,
      "budget_used_percent": 73.54062828708527
    },
    {
      "type": "training",
      "description": "Training step 3097",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:08",
      "total_flops_so_far": 7.356436715922096e+16,
      "budget_used_percent": 73.56436715922095
    },
    {
      "type": "training",
      "description": "Training step 3098",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:10",
      "total_flops_so_far": 7.358810603135664e+16,
      "budget_used_percent": 73.58810603135663
    },
    {
      "type": "training",
      "description": "Training step 3099",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:11",
      "total_flops_so_far": 7.361184490349232e+16,
      "budget_used_percent": 73.61184490349231
    },
    {
      "type": "training",
      "description": "Training step 3100",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:12",
      "total_flops_so_far": 7.3635583775628e+16,
      "budget_used_percent": 73.63558377562799
    },
    {
      "type": "training",
      "description": "Training step 3101",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:14",
      "total_flops_so_far": 7.365932264776368e+16,
      "budget_used_percent": 73.65932264776369
    },
    {
      "type": "training",
      "description": "Training step 3102",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:15",
      "total_flops_so_far": 7.368306151989936e+16,
      "budget_used_percent": 73.68306151989937
    },
    {
      "type": "training",
      "description": "Training step 3103",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:16",
      "total_flops_so_far": 7.370680039203504e+16,
      "budget_used_percent": 73.70680039203505
    },
    {
      "type": "training",
      "description": "Training step 3104",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:18",
      "total_flops_so_far": 7.373053926417072e+16,
      "budget_used_percent": 73.73053926417073
    },
    {
      "type": "training",
      "description": "Training step 3105",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:19",
      "total_flops_so_far": 7.37542781363064e+16,
      "budget_used_percent": 73.7542781363064
    },
    {
      "type": "training",
      "description": "Training step 3106",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:21",
      "total_flops_so_far": 7.377801700844208e+16,
      "budget_used_percent": 73.77801700844208
    },
    {
      "type": "training",
      "description": "Training step 3107",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:22",
      "total_flops_so_far": 7.380175588057776e+16,
      "budget_used_percent": 73.80175588057776
    },
    {
      "type": "training",
      "description": "Training step 3108",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:23",
      "total_flops_so_far": 7.382549475271344e+16,
      "budget_used_percent": 73.82549475271344
    },
    {
      "type": "training",
      "description": "Training step 3109",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:25",
      "total_flops_so_far": 7.384923362484912e+16,
      "budget_used_percent": 73.84923362484912
    },
    {
      "type": "training",
      "description": "Training step 3110",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:26",
      "total_flops_so_far": 7.38729724969848e+16,
      "budget_used_percent": 73.8729724969848
    },
    {
      "type": "training",
      "description": "Training step 3111",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:27",
      "total_flops_so_far": 7.389671136912048e+16,
      "budget_used_percent": 73.89671136912048
    },
    {
      "type": "training",
      "description": "Training step 3112",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:29",
      "total_flops_so_far": 7.392045024125616e+16,
      "budget_used_percent": 73.92045024125616
    },
    {
      "type": "training",
      "description": "Training step 3113",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:30",
      "total_flops_so_far": 7.394418911339184e+16,
      "budget_used_percent": 73.94418911339184
    },
    {
      "type": "training",
      "description": "Training step 3114",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:31",
      "total_flops_so_far": 7.396792798552752e+16,
      "budget_used_percent": 73.96792798552751
    },
    {
      "type": "training",
      "description": "Training step 3115",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:33",
      "total_flops_so_far": 7.39916668576632e+16,
      "budget_used_percent": 73.99166685766319
    },
    {
      "type": "training",
      "description": "Training step 3116",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:34",
      "total_flops_so_far": 7.401540572979888e+16,
      "budget_used_percent": 74.01540572979887
    },
    {
      "type": "training",
      "description": "Training step 3117",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:35",
      "total_flops_so_far": 7.403914460193456e+16,
      "budget_used_percent": 74.03914460193455
    },
    {
      "type": "training",
      "description": "Training step 3118",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:37",
      "total_flops_so_far": 7.406288347407024e+16,
      "budget_used_percent": 74.06288347407025
    },
    {
      "type": "training",
      "description": "Training step 3119",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:38",
      "total_flops_so_far": 7.408662234620592e+16,
      "budget_used_percent": 74.08662234620593
    },
    {
      "type": "training",
      "description": "Training step 3120",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:39",
      "total_flops_so_far": 7.41103612183416e+16,
      "budget_used_percent": 74.11036121834161
    },
    {
      "type": "training",
      "description": "Training step 3121",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:41",
      "total_flops_so_far": 7.413410009047728e+16,
      "budget_used_percent": 74.13410009047729
    },
    {
      "type": "training",
      "description": "Training step 3122",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:42",
      "total_flops_so_far": 7.415783896261296e+16,
      "budget_used_percent": 74.15783896261297
    },
    {
      "type": "training",
      "description": "Training step 3123",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:43",
      "total_flops_so_far": 7.418157783474864e+16,
      "budget_used_percent": 74.18157783474864
    },
    {
      "type": "training",
      "description": "Training step 3124",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:45",
      "total_flops_so_far": 7.420531670688432e+16,
      "budget_used_percent": 74.20531670688432
    },
    {
      "type": "training",
      "description": "Training step 3125",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:46",
      "total_flops_so_far": 7.422905557902e+16,
      "budget_used_percent": 74.22905557902
    },
    {
      "type": "training",
      "description": "Training step 3126",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:48",
      "total_flops_so_far": 7.425279445115568e+16,
      "budget_used_percent": 74.25279445115568
    },
    {
      "type": "training",
      "description": "Training step 3127",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:49",
      "total_flops_so_far": 7.427653332329136e+16,
      "budget_used_percent": 74.27653332329136
    },
    {
      "type": "training",
      "description": "Training step 3128",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:50",
      "total_flops_so_far": 7.430027219542704e+16,
      "budget_used_percent": 74.30027219542704
    },
    {
      "type": "training",
      "description": "Training step 3129",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:52",
      "total_flops_so_far": 7.432401106756272e+16,
      "budget_used_percent": 74.32401106756272
    },
    {
      "type": "training",
      "description": "Training step 3130",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:53",
      "total_flops_so_far": 7.43477499396984e+16,
      "budget_used_percent": 74.3477499396984
    },
    {
      "type": "training",
      "description": "Training step 3131",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:54",
      "total_flops_so_far": 7.437148881183408e+16,
      "budget_used_percent": 74.37148881183407
    },
    {
      "type": "training",
      "description": "Training step 3132",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:56",
      "total_flops_so_far": 7.439522768396976e+16,
      "budget_used_percent": 74.39522768396975
    },
    {
      "type": "training",
      "description": "Training step 3133",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:57",
      "total_flops_so_far": 7.441896655610544e+16,
      "budget_used_percent": 74.41896655610543
    },
    {
      "type": "training",
      "description": "Training step 3134",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:42:58",
      "total_flops_so_far": 7.444270542824112e+16,
      "budget_used_percent": 74.44270542824111
    },
    {
      "type": "training",
      "description": "Training step 3135",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:00",
      "total_flops_so_far": 7.44664443003768e+16,
      "budget_used_percent": 74.46644430037679
    },
    {
      "type": "training",
      "description": "Training step 3136",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:01",
      "total_flops_so_far": 7.449018317251248e+16,
      "budget_used_percent": 74.49018317251247
    },
    {
      "type": "training",
      "description": "Training step 3137",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:02",
      "total_flops_so_far": 7.451392204464816e+16,
      "budget_used_percent": 74.51392204464817
    },
    {
      "type": "training",
      "description": "Training step 3138",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:04",
      "total_flops_so_far": 7.453766091678384e+16,
      "budget_used_percent": 74.53766091678385
    },
    {
      "type": "training",
      "description": "Training step 3139",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:05",
      "total_flops_so_far": 7.456139978891952e+16,
      "budget_used_percent": 74.56139978891953
    },
    {
      "type": "training",
      "description": "Training step 3140",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:06",
      "total_flops_so_far": 7.45851386610552e+16,
      "budget_used_percent": 74.5851386610552
    },
    {
      "type": "training",
      "description": "Training step 3141",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:08",
      "total_flops_so_far": 7.460887753319088e+16,
      "budget_used_percent": 74.60887753319088
    },
    {
      "type": "training",
      "description": "Training step 3142",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:09",
      "total_flops_so_far": 7.463261640532656e+16,
      "budget_used_percent": 74.63261640532656
    },
    {
      "type": "training",
      "description": "Training step 3143",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:10",
      "total_flops_so_far": 7.465635527746224e+16,
      "budget_used_percent": 74.65635527746224
    },
    {
      "type": "training",
      "description": "Training step 3144",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:12",
      "total_flops_so_far": 7.468009414959792e+16,
      "budget_used_percent": 74.68009414959792
    },
    {
      "type": "training",
      "description": "Training step 3145",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:13",
      "total_flops_so_far": 7.47038330217336e+16,
      "budget_used_percent": 74.7038330217336
    },
    {
      "type": "training",
      "description": "Training step 3146",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:15",
      "total_flops_so_far": 7.472757189386928e+16,
      "budget_used_percent": 74.72757189386928
    },
    {
      "type": "training",
      "description": "Training step 3147",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:16",
      "total_flops_so_far": 7.475131076600496e+16,
      "budget_used_percent": 74.75131076600496
    },
    {
      "type": "training",
      "description": "Training step 3148",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:17",
      "total_flops_so_far": 7.477504963814064e+16,
      "budget_used_percent": 74.77504963814064
    },
    {
      "type": "training",
      "description": "Training step 3149",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:19",
      "total_flops_so_far": 7.479878851027632e+16,
      "budget_used_percent": 74.79878851027631
    },
    {
      "type": "training",
      "description": "Training step 3150",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:20",
      "total_flops_so_far": 7.4822527382412e+16,
      "budget_used_percent": 74.82252738241199
    },
    {
      "type": "training",
      "description": "Training step 3151",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:21",
      "total_flops_so_far": 7.484626625454768e+16,
      "budget_used_percent": 74.84626625454767
    },
    {
      "type": "training",
      "description": "Training step 3152",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:23",
      "total_flops_so_far": 7.487000512668336e+16,
      "budget_used_percent": 74.87000512668335
    },
    {
      "type": "training",
      "description": "Training step 3153",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:24",
      "total_flops_so_far": 7.489374399881904e+16,
      "budget_used_percent": 74.89374399881903
    },
    {
      "type": "training",
      "description": "Training step 3154",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:25",
      "total_flops_so_far": 7.491748287095472e+16,
      "budget_used_percent": 74.91748287095473
    },
    {
      "type": "training",
      "description": "Training step 3155",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:27",
      "total_flops_so_far": 7.49412217430904e+16,
      "budget_used_percent": 74.94122174309041
    },
    {
      "type": "training",
      "description": "Training step 3156",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:28",
      "total_flops_so_far": 7.496496061522608e+16,
      "budget_used_percent": 74.96496061522609
    },
    {
      "type": "training",
      "description": "Training step 3157",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:30",
      "total_flops_so_far": 7.498869948736176e+16,
      "budget_used_percent": 74.98869948736177
    },
    {
      "type": "training",
      "description": "Training step 3158",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:31",
      "total_flops_so_far": 7.501243835949744e+16,
      "budget_used_percent": 75.01243835949744
    },
    {
      "type": "training",
      "description": "Training step 3159",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:32",
      "total_flops_so_far": 7.503617723163312e+16,
      "budget_used_percent": 75.03617723163312
    },
    {
      "type": "training",
      "description": "Training step 3160",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:34",
      "total_flops_so_far": 7.50599161037688e+16,
      "budget_used_percent": 75.0599161037688
    },
    {
      "type": "training",
      "description": "Training step 3161",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:35",
      "total_flops_so_far": 7.508365497590448e+16,
      "budget_used_percent": 75.08365497590448
    },
    {
      "type": "training",
      "description": "Training step 3162",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:36",
      "total_flops_so_far": 7.510739384804016e+16,
      "budget_used_percent": 75.10739384804016
    },
    {
      "type": "training",
      "description": "Training step 3163",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:38",
      "total_flops_so_far": 7.513113272017584e+16,
      "budget_used_percent": 75.13113272017584
    },
    {
      "type": "training",
      "description": "Training step 3164",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:39",
      "total_flops_so_far": 7.515487159231152e+16,
      "budget_used_percent": 75.15487159231152
    },
    {
      "type": "training",
      "description": "Training step 3165",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:40",
      "total_flops_so_far": 7.51786104644472e+16,
      "budget_used_percent": 75.1786104644472
    },
    {
      "type": "training",
      "description": "Training step 3166",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:42",
      "total_flops_so_far": 7.520234933658288e+16,
      "budget_used_percent": 75.20234933658288
    },
    {
      "type": "training",
      "description": "Training step 3167",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:43",
      "total_flops_so_far": 7.522608820871856e+16,
      "budget_used_percent": 75.22608820871855
    },
    {
      "type": "training",
      "description": "Training step 3168",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:44",
      "total_flops_so_far": 7.524982708085424e+16,
      "budget_used_percent": 75.24982708085423
    },
    {
      "type": "training",
      "description": "Training step 3169",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:45",
      "total_flops_so_far": 7.527356595298992e+16,
      "budget_used_percent": 75.27356595298991
    },
    {
      "type": "training",
      "description": "Training step 3170",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:46",
      "total_flops_so_far": 7.52973048251256e+16,
      "budget_used_percent": 75.2973048251256
    },
    {
      "type": "training",
      "description": "Training step 3171",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:48",
      "total_flops_so_far": 7.532104369726128e+16,
      "budget_used_percent": 75.32104369726127
    },
    {
      "type": "training",
      "description": "Training step 3172",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:49",
      "total_flops_so_far": 7.534478256939696e+16,
      "budget_used_percent": 75.34478256939695
    },
    {
      "type": "training",
      "description": "Training step 3173",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:50",
      "total_flops_so_far": 7.536852144153264e+16,
      "budget_used_percent": 75.36852144153265
    },
    {
      "type": "training",
      "description": "Training step 3174",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:52",
      "total_flops_so_far": 7.539226031366832e+16,
      "budget_used_percent": 75.39226031366833
    },
    {
      "type": "training",
      "description": "Training step 3175",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:53",
      "total_flops_so_far": 7.5415999185804e+16,
      "budget_used_percent": 75.415999185804
    },
    {
      "type": "training",
      "description": "Training step 3176",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:54",
      "total_flops_so_far": 7.543973805793968e+16,
      "budget_used_percent": 75.43973805793968
    },
    {
      "type": "training",
      "description": "Training step 3177",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:56",
      "total_flops_so_far": 7.546347693007536e+16,
      "budget_used_percent": 75.46347693007536
    },
    {
      "type": "training",
      "description": "Training step 3178",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:57",
      "total_flops_so_far": 7.548721580221104e+16,
      "budget_used_percent": 75.48721580221104
    },
    {
      "type": "training",
      "description": "Training step 3179",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:43:58",
      "total_flops_so_far": 7.551095467434672e+16,
      "budget_used_percent": 75.51095467434672
    },
    {
      "type": "training",
      "description": "Training step 3180",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:00",
      "total_flops_so_far": 7.55346935464824e+16,
      "budget_used_percent": 75.5346935464824
    },
    {
      "type": "training",
      "description": "Training step 3181",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:01",
      "total_flops_so_far": 7.555843241861808e+16,
      "budget_used_percent": 75.55843241861808
    },
    {
      "type": "training",
      "description": "Training step 3182",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:02",
      "total_flops_so_far": 7.558217129075376e+16,
      "budget_used_percent": 75.58217129075376
    },
    {
      "type": "training",
      "description": "Training step 3183",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:04",
      "total_flops_so_far": 7.560591016288944e+16,
      "budget_used_percent": 75.60591016288944
    },
    {
      "type": "training",
      "description": "Training step 3184",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:05",
      "total_flops_so_far": 7.562964903502512e+16,
      "budget_used_percent": 75.62964903502511
    },
    {
      "type": "training",
      "description": "Training step 3185",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:06",
      "total_flops_so_far": 7.56533879071608e+16,
      "budget_used_percent": 75.65338790716079
    },
    {
      "type": "training",
      "description": "Training step 3186",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:08",
      "total_flops_so_far": 7.567712677929648e+16,
      "budget_used_percent": 75.67712677929647
    },
    {
      "type": "training",
      "description": "Training step 3187",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:09",
      "total_flops_so_far": 7.570086565143216e+16,
      "budget_used_percent": 75.70086565143215
    },
    {
      "type": "training",
      "description": "Training step 3188",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:11",
      "total_flops_so_far": 7.572460452356784e+16,
      "budget_used_percent": 75.72460452356783
    },
    {
      "type": "training",
      "description": "Training step 3189",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:12",
      "total_flops_so_far": 7.574834339570352e+16,
      "budget_used_percent": 75.74834339570351
    },
    {
      "type": "training",
      "description": "Training step 3190",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:13",
      "total_flops_so_far": 7.57720822678392e+16,
      "budget_used_percent": 75.77208226783921
    },
    {
      "type": "training",
      "description": "Training step 3191",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:15",
      "total_flops_so_far": 7.579582113997488e+16,
      "budget_used_percent": 75.79582113997489
    },
    {
      "type": "training",
      "description": "Training step 3192",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:16",
      "total_flops_so_far": 7.581956001211056e+16,
      "budget_used_percent": 75.81956001211057
    },
    {
      "type": "training",
      "description": "Training step 3193",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:17",
      "total_flops_so_far": 7.584329888424624e+16,
      "budget_used_percent": 75.84329888424624
    },
    {
      "type": "training",
      "description": "Training step 3194",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:19",
      "total_flops_so_far": 7.586703775638192e+16,
      "budget_used_percent": 75.86703775638192
    },
    {
      "type": "training",
      "description": "Training step 3195",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:20",
      "total_flops_so_far": 7.58907766285176e+16,
      "budget_used_percent": 75.8907766285176
    },
    {
      "type": "training",
      "description": "Training step 3196",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:21",
      "total_flops_so_far": 7.591451550065328e+16,
      "budget_used_percent": 75.91451550065328
    },
    {
      "type": "training",
      "description": "Training step 3197",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:23",
      "total_flops_so_far": 7.593825437278896e+16,
      "budget_used_percent": 75.93825437278896
    },
    {
      "type": "training",
      "description": "Training step 3198",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:24",
      "total_flops_so_far": 7.596199324492464e+16,
      "budget_used_percent": 75.96199324492464
    },
    {
      "type": "training",
      "description": "Training step 3199",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:25",
      "total_flops_so_far": 7.598573211706032e+16,
      "budget_used_percent": 75.98573211706032
    },
    {
      "type": "training",
      "description": "Training step 3200",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:27",
      "total_flops_so_far": 7.6009470989196e+16,
      "budget_used_percent": 76.009470989196
    },
    {
      "type": "training",
      "description": "Training step 3201",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:28",
      "total_flops_so_far": 7.603320986133168e+16,
      "budget_used_percent": 76.03320986133168
    },
    {
      "type": "training",
      "description": "Training step 3202",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:29",
      "total_flops_so_far": 7.605694873346736e+16,
      "budget_used_percent": 76.05694873346735
    },
    {
      "type": "training",
      "description": "Training step 3203",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:31",
      "total_flops_so_far": 7.608068760560304e+16,
      "budget_used_percent": 76.08068760560303
    },
    {
      "type": "training",
      "description": "Training step 3204",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:32",
      "total_flops_so_far": 7.610442647773872e+16,
      "budget_used_percent": 76.10442647773871
    },
    {
      "type": "training",
      "description": "Training step 3205",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:34",
      "total_flops_so_far": 7.61281653498744e+16,
      "budget_used_percent": 76.1281653498744
    },
    {
      "type": "training",
      "description": "Training step 3206",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:35",
      "total_flops_so_far": 7.615190422201008e+16,
      "budget_used_percent": 76.15190422201007
    },
    {
      "type": "training",
      "description": "Training step 3207",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:36",
      "total_flops_so_far": 7.617564309414576e+16,
      "budget_used_percent": 76.17564309414576
    },
    {
      "type": "training",
      "description": "Training step 3208",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:38",
      "total_flops_so_far": 7.619938196628144e+16,
      "budget_used_percent": 76.19938196628144
    },
    {
      "type": "training",
      "description": "Training step 3209",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:39",
      "total_flops_so_far": 7.622312083841712e+16,
      "budget_used_percent": 76.22312083841713
    },
    {
      "type": "training",
      "description": "Training step 3210",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:40",
      "total_flops_so_far": 7.62468597105528e+16,
      "budget_used_percent": 76.2468597105528
    },
    {
      "type": "training",
      "description": "Training step 3211",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:42",
      "total_flops_so_far": 7.627059858268848e+16,
      "budget_used_percent": 76.27059858268848
    },
    {
      "type": "training",
      "description": "Training step 3212",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:43",
      "total_flops_so_far": 7.629433745482416e+16,
      "budget_used_percent": 76.29433745482416
    },
    {
      "type": "training",
      "description": "Training step 3213",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:44",
      "total_flops_so_far": 7.631807632695984e+16,
      "budget_used_percent": 76.31807632695984
    },
    {
      "type": "training",
      "description": "Training step 3214",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:46",
      "total_flops_so_far": 7.634181519909552e+16,
      "budget_used_percent": 76.34181519909552
    },
    {
      "type": "training",
      "description": "Training step 3215",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:47",
      "total_flops_so_far": 7.63655540712312e+16,
      "budget_used_percent": 76.3655540712312
    },
    {
      "type": "training",
      "description": "Training step 3216",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:49",
      "total_flops_so_far": 7.638929294336688e+16,
      "budget_used_percent": 76.38929294336688
    },
    {
      "type": "training",
      "description": "Training step 3217",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:50",
      "total_flops_so_far": 7.641303181550256e+16,
      "budget_used_percent": 76.41303181550256
    },
    {
      "type": "training",
      "description": "Training step 3218",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:51",
      "total_flops_so_far": 7.643677068763824e+16,
      "budget_used_percent": 76.43677068763824
    },
    {
      "type": "training",
      "description": "Training step 3219",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:53",
      "total_flops_so_far": 7.646050955977392e+16,
      "budget_used_percent": 76.46050955977391
    },
    {
      "type": "training",
      "description": "Training step 3220",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:54",
      "total_flops_so_far": 7.64842484319096e+16,
      "budget_used_percent": 76.48424843190959
    },
    {
      "type": "training",
      "description": "Training step 3221",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:55",
      "total_flops_so_far": 7.650798730404528e+16,
      "budget_used_percent": 76.50798730404527
    },
    {
      "type": "training",
      "description": "Training step 3222",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:57",
      "total_flops_so_far": 7.653172617618096e+16,
      "budget_used_percent": 76.53172617618095
    },
    {
      "type": "training",
      "description": "Training step 3223",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:58",
      "total_flops_so_far": 7.655546504831664e+16,
      "budget_used_percent": 76.55546504831663
    },
    {
      "type": "training",
      "description": "Training step 3224",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:44:59",
      "total_flops_so_far": 7.657920392045232e+16,
      "budget_used_percent": 76.57920392045231
    },
    {
      "type": "training",
      "description": "Training step 3225",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:01",
      "total_flops_so_far": 7.6602942792588e+16,
      "budget_used_percent": 76.602942792588
    },
    {
      "type": "training",
      "description": "Training step 3226",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:02",
      "total_flops_so_far": 7.662668166472368e+16,
      "budget_used_percent": 76.62668166472369
    },
    {
      "type": "training",
      "description": "Training step 3227",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:03",
      "total_flops_so_far": 7.665042053685936e+16,
      "budget_used_percent": 76.65042053685937
    },
    {
      "type": "training",
      "description": "Training step 3228",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:05",
      "total_flops_so_far": 7.667415940899504e+16,
      "budget_used_percent": 76.67415940899504
    },
    {
      "type": "training",
      "description": "Training step 3229",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:06",
      "total_flops_so_far": 7.669789828113072e+16,
      "budget_used_percent": 76.69789828113072
    },
    {
      "type": "training",
      "description": "Training step 3230",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:07",
      "total_flops_so_far": 7.67216371532664e+16,
      "budget_used_percent": 76.7216371532664
    },
    {
      "type": "training",
      "description": "Training step 3231",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:09",
      "total_flops_so_far": 7.674537602540208e+16,
      "budget_used_percent": 76.74537602540208
    },
    {
      "type": "training",
      "description": "Training step 3232",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:10",
      "total_flops_so_far": 7.676911489753776e+16,
      "budget_used_percent": 76.76911489753776
    },
    {
      "type": "training",
      "description": "Training step 3233",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:11",
      "total_flops_so_far": 7.679285376967344e+16,
      "budget_used_percent": 76.79285376967344
    },
    {
      "type": "training",
      "description": "Training step 3234",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:13",
      "total_flops_so_far": 7.681659264180912e+16,
      "budget_used_percent": 76.81659264180912
    },
    {
      "type": "training",
      "description": "Training step 3235",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:14",
      "total_flops_so_far": 7.68403315139448e+16,
      "budget_used_percent": 76.8403315139448
    },
    {
      "type": "training",
      "description": "Training step 3236",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:16",
      "total_flops_so_far": 7.686407038608048e+16,
      "budget_used_percent": 76.86407038608048
    },
    {
      "type": "training",
      "description": "Training step 3237",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:17",
      "total_flops_so_far": 7.688780925821616e+16,
      "budget_used_percent": 76.88780925821615
    },
    {
      "type": "training",
      "description": "Training step 3238",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:18",
      "total_flops_so_far": 7.691154813035184e+16,
      "budget_used_percent": 76.91154813035183
    },
    {
      "type": "training",
      "description": "Training step 3239",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:20",
      "total_flops_so_far": 7.693528700248752e+16,
      "budget_used_percent": 76.93528700248751
    },
    {
      "type": "training",
      "description": "Training step 3240",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:21",
      "total_flops_so_far": 7.69590258746232e+16,
      "budget_used_percent": 76.9590258746232
    },
    {
      "type": "training",
      "description": "Training step 3241",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:22",
      "total_flops_so_far": 7.698276474675888e+16,
      "budget_used_percent": 76.98276474675887
    },
    {
      "type": "training",
      "description": "Training step 3242",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:24",
      "total_flops_so_far": 7.700650361889456e+16,
      "budget_used_percent": 77.00650361889456
    },
    {
      "type": "training",
      "description": "Training step 3243",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:25",
      "total_flops_so_far": 7.703024249103024e+16,
      "budget_used_percent": 77.03024249103024
    },
    {
      "type": "training",
      "description": "Training step 3244",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:26",
      "total_flops_so_far": 7.705398136316592e+16,
      "budget_used_percent": 77.05398136316592
    },
    {
      "type": "training",
      "description": "Training step 3245",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:28",
      "total_flops_so_far": 7.70777202353016e+16,
      "budget_used_percent": 77.07772023530161
    },
    {
      "type": "training",
      "description": "Training step 3246",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:29",
      "total_flops_so_far": 7.710145910743728e+16,
      "budget_used_percent": 77.10145910743728
    },
    {
      "type": "training",
      "description": "Training step 3247",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:31",
      "total_flops_so_far": 7.712519797957296e+16,
      "budget_used_percent": 77.12519797957296
    },
    {
      "type": "training",
      "description": "Training step 3248",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:32",
      "total_flops_so_far": 7.714893685170864e+16,
      "budget_used_percent": 77.14893685170864
    },
    {
      "type": "training",
      "description": "Training step 3249",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:33",
      "total_flops_so_far": 7.717267572384432e+16,
      "budget_used_percent": 77.17267572384432
    },
    {
      "type": "training",
      "description": "Training step 3250",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:35",
      "total_flops_so_far": 7.719641459598e+16,
      "budget_used_percent": 77.19641459598
    },
    {
      "type": "training",
      "description": "Training step 3251",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:36",
      "total_flops_so_far": 7.722015346811568e+16,
      "budget_used_percent": 77.22015346811568
    },
    {
      "type": "training",
      "description": "Training step 3252",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:37",
      "total_flops_so_far": 7.724389234025136e+16,
      "budget_used_percent": 77.24389234025136
    },
    {
      "type": "training",
      "description": "Training step 3253",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:39",
      "total_flops_so_far": 7.726763121238704e+16,
      "budget_used_percent": 77.26763121238704
    },
    {
      "type": "training",
      "description": "Training step 3254",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:40",
      "total_flops_so_far": 7.729137008452272e+16,
      "budget_used_percent": 77.29137008452271
    },
    {
      "type": "training",
      "description": "Training step 3255",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:41",
      "total_flops_so_far": 7.73151089566584e+16,
      "budget_used_percent": 77.31510895665839
    },
    {
      "type": "training",
      "description": "Training step 3256",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:43",
      "total_flops_so_far": 7.733884782879408e+16,
      "budget_used_percent": 77.33884782879407
    },
    {
      "type": "training",
      "description": "Training step 3257",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:44",
      "total_flops_so_far": 7.736258670092976e+16,
      "budget_used_percent": 77.36258670092975
    },
    {
      "type": "training",
      "description": "Training step 3258",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:45",
      "total_flops_so_far": 7.738632557306544e+16,
      "budget_used_percent": 77.38632557306543
    },
    {
      "type": "training",
      "description": "Training step 3259",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:47",
      "total_flops_so_far": 7.741006444520112e+16,
      "budget_used_percent": 77.41006444520112
    },
    {
      "type": "training",
      "description": "Training step 3260",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:48",
      "total_flops_so_far": 7.74338033173368e+16,
      "budget_used_percent": 77.4338033173368
    },
    {
      "type": "training",
      "description": "Training step 3261",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:49",
      "total_flops_so_far": 7.745754218947248e+16,
      "budget_used_percent": 77.45754218947248
    },
    {
      "type": "training",
      "description": "Training step 3262",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:51",
      "total_flops_so_far": 7.748128106160816e+16,
      "budget_used_percent": 77.48128106160817
    },
    {
      "type": "training",
      "description": "Training step 3263",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:52",
      "total_flops_so_far": 7.750501993374384e+16,
      "budget_used_percent": 77.50501993374384
    },
    {
      "type": "training",
      "description": "Training step 3264",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:54",
      "total_flops_so_far": 7.752875880587952e+16,
      "budget_used_percent": 77.52875880587952
    },
    {
      "type": "training",
      "description": "Training step 3265",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:55",
      "total_flops_so_far": 7.75524976780152e+16,
      "budget_used_percent": 77.5524976780152
    },
    {
      "type": "training",
      "description": "Training step 3266",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:56",
      "total_flops_so_far": 7.757623655015088e+16,
      "budget_used_percent": 77.57623655015088
    },
    {
      "type": "training",
      "description": "Training step 3267",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:58",
      "total_flops_so_far": 7.759997542228656e+16,
      "budget_used_percent": 77.59997542228656
    },
    {
      "type": "training",
      "description": "Training step 3268",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:45:59",
      "total_flops_so_far": 7.762371429442224e+16,
      "budget_used_percent": 77.62371429442224
    },
    {
      "type": "training",
      "description": "Training step 3269",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:00",
      "total_flops_so_far": 7.764745316655792e+16,
      "budget_used_percent": 77.64745316655792
    },
    {
      "type": "training",
      "description": "Training step 3270",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:02",
      "total_flops_so_far": 7.76711920386936e+16,
      "budget_used_percent": 77.6711920386936
    },
    {
      "type": "training",
      "description": "Training step 3271",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:03",
      "total_flops_so_far": 7.769493091082928e+16,
      "budget_used_percent": 77.69493091082929
    },
    {
      "type": "training",
      "description": "Training step 3272",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:04",
      "total_flops_so_far": 7.771866978296496e+16,
      "budget_used_percent": 77.71866978296495
    },
    {
      "type": "training",
      "description": "Training step 3273",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:06",
      "total_flops_so_far": 7.774240865510064e+16,
      "budget_used_percent": 77.74240865510063
    },
    {
      "type": "training",
      "description": "Training step 3274",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:07",
      "total_flops_so_far": 7.776614752723632e+16,
      "budget_used_percent": 77.76614752723631
    },
    {
      "type": "training",
      "description": "Training step 3275",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:08",
      "total_flops_so_far": 7.7789886399372e+16,
      "budget_used_percent": 77.789886399372
    },
    {
      "type": "training",
      "description": "Training step 3276",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:10",
      "total_flops_so_far": 7.781362527150768e+16,
      "budget_used_percent": 77.81362527150768
    },
    {
      "type": "training",
      "description": "Training step 3277",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:11",
      "total_flops_so_far": 7.783736414364336e+16,
      "budget_used_percent": 77.83736414364336
    },
    {
      "type": "training",
      "description": "Training step 3278",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:12",
      "total_flops_so_far": 7.786110301577904e+16,
      "budget_used_percent": 77.86110301577904
    },
    {
      "type": "training",
      "description": "Training step 3279",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:14",
      "total_flops_so_far": 7.788484188791472e+16,
      "budget_used_percent": 77.88484188791472
    },
    {
      "type": "training",
      "description": "Training step 3280",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:15",
      "total_flops_so_far": 7.79085807600504e+16,
      "budget_used_percent": 77.9085807600504
    },
    {
      "type": "training",
      "description": "Training step 3281",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:17",
      "total_flops_so_far": 7.793231963218608e+16,
      "budget_used_percent": 77.93231963218608
    },
    {
      "type": "training",
      "description": "Training step 3282",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:18",
      "total_flops_so_far": 7.795605850432176e+16,
      "budget_used_percent": 77.95605850432176
    },
    {
      "type": "training",
      "description": "Training step 3283",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:19",
      "total_flops_so_far": 7.797979737645744e+16,
      "budget_used_percent": 77.97979737645744
    },
    {
      "type": "training",
      "description": "Training step 3284",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:21",
      "total_flops_so_far": 7.800353624859312e+16,
      "budget_used_percent": 78.00353624859312
    },
    {
      "type": "training",
      "description": "Training step 3285",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:22",
      "total_flops_so_far": 7.80272751207288e+16,
      "budget_used_percent": 78.0272751207288
    },
    {
      "type": "training",
      "description": "Training step 3286",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:23",
      "total_flops_so_far": 7.805101399286448e+16,
      "budget_used_percent": 78.05101399286448
    },
    {
      "type": "training",
      "description": "Training step 3287",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:25",
      "total_flops_so_far": 7.807475286500016e+16,
      "budget_used_percent": 78.07475286500016
    },
    {
      "type": "training",
      "description": "Training step 3288",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:26",
      "total_flops_so_far": 7.809849173713584e+16,
      "budget_used_percent": 78.09849173713584
    },
    {
      "type": "training",
      "description": "Training step 3289",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:27",
      "total_flops_so_far": 7.812223060927152e+16,
      "budget_used_percent": 78.12223060927151
    },
    {
      "type": "training",
      "description": "Training step 3290",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:29",
      "total_flops_so_far": 7.81459694814072e+16,
      "budget_used_percent": 78.14596948140719
    },
    {
      "type": "training",
      "description": "Training step 3291",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:30",
      "total_flops_so_far": 7.816970835354288e+16,
      "budget_used_percent": 78.16970835354287
    },
    {
      "type": "training",
      "description": "Training step 3292",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:31",
      "total_flops_so_far": 7.819344722567856e+16,
      "budget_used_percent": 78.19344722567855
    },
    {
      "type": "training",
      "description": "Training step 3293",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:33",
      "total_flops_so_far": 7.821718609781424e+16,
      "budget_used_percent": 78.21718609781423
    },
    {
      "type": "training",
      "description": "Training step 3294",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:34",
      "total_flops_so_far": 7.824092496994992e+16,
      "budget_used_percent": 78.24092496994992
    },
    {
      "type": "training",
      "description": "Training step 3295",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:35",
      "total_flops_so_far": 7.82646638420856e+16,
      "budget_used_percent": 78.2646638420856
    },
    {
      "type": "training",
      "description": "Training step 3296",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:37",
      "total_flops_so_far": 7.828840271422128e+16,
      "budget_used_percent": 78.28840271422128
    },
    {
      "type": "training",
      "description": "Training step 3297",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:38",
      "total_flops_so_far": 7.831214158635696e+16,
      "budget_used_percent": 78.31214158635696
    },
    {
      "type": "training",
      "description": "Training step 3298",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:40",
      "total_flops_so_far": 7.833588045849264e+16,
      "budget_used_percent": 78.33588045849264
    },
    {
      "type": "training",
      "description": "Training step 3299",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:41",
      "total_flops_so_far": 7.835961933062832e+16,
      "budget_used_percent": 78.35961933062832
    },
    {
      "type": "training",
      "description": "Training step 3300",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:41",
      "total_flops_so_far": 7.8383358202764e+16,
      "budget_used_percent": 78.383358202764
    },
    {
      "type": "training",
      "description": "Training step 3301",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:43",
      "total_flops_so_far": 7.840709707489968e+16,
      "budget_used_percent": 78.40709707489968
    },
    {
      "type": "training",
      "description": "Training step 3302",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:44",
      "total_flops_so_far": 7.843083594703536e+16,
      "budget_used_percent": 78.43083594703536
    },
    {
      "type": "training",
      "description": "Training step 3303",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:46",
      "total_flops_so_far": 7.845457481917104e+16,
      "budget_used_percent": 78.45457481917104
    },
    {
      "type": "training",
      "description": "Training step 3304",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:47",
      "total_flops_so_far": 7.847831369130672e+16,
      "budget_used_percent": 78.47831369130672
    },
    {
      "type": "training",
      "description": "Training step 3305",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:48",
      "total_flops_so_far": 7.85020525634424e+16,
      "budget_used_percent": 78.5020525634424
    },
    {
      "type": "training",
      "description": "Training step 3306",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:50",
      "total_flops_so_far": 7.852579143557808e+16,
      "budget_used_percent": 78.52579143557809
    },
    {
      "type": "training",
      "description": "Training step 3307",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:51",
      "total_flops_so_far": 7.854953030771376e+16,
      "budget_used_percent": 78.54953030771375
    },
    {
      "type": "training",
      "description": "Training step 3308",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:52",
      "total_flops_so_far": 7.857326917984944e+16,
      "budget_used_percent": 78.57326917984943
    },
    {
      "type": "training",
      "description": "Training step 3309",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:54",
      "total_flops_so_far": 7.859700805198512e+16,
      "budget_used_percent": 78.59700805198511
    },
    {
      "type": "training",
      "description": "Training step 3310",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:55",
      "total_flops_so_far": 7.86207469241208e+16,
      "budget_used_percent": 78.6207469241208
    },
    {
      "type": "training",
      "description": "Training step 3311",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:56",
      "total_flops_so_far": 7.864448579625648e+16,
      "budget_used_percent": 78.64448579625648
    },
    {
      "type": "training",
      "description": "Training step 3312",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:58",
      "total_flops_so_far": 7.866822466839216e+16,
      "budget_used_percent": 78.66822466839216
    },
    {
      "type": "training",
      "description": "Training step 3313",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:46:59",
      "total_flops_so_far": 7.869196354052784e+16,
      "budget_used_percent": 78.69196354052784
    },
    {
      "type": "training",
      "description": "Training step 3314",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:00",
      "total_flops_so_far": 7.871570241266352e+16,
      "budget_used_percent": 78.71570241266352
    },
    {
      "type": "training",
      "description": "Training step 3315",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:02",
      "total_flops_so_far": 7.87394412847992e+16,
      "budget_used_percent": 78.7394412847992
    },
    {
      "type": "training",
      "description": "Training step 3316",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:03",
      "total_flops_so_far": 7.876318015693488e+16,
      "budget_used_percent": 78.76318015693488
    },
    {
      "type": "training",
      "description": "Training step 3317",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:04",
      "total_flops_so_far": 7.878691902907056e+16,
      "budget_used_percent": 78.78691902907056
    },
    {
      "type": "training",
      "description": "Training step 3318",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:06",
      "total_flops_so_far": 7.881065790120624e+16,
      "budget_used_percent": 78.81065790120624
    },
    {
      "type": "training",
      "description": "Training step 3319",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:07",
      "total_flops_so_far": 7.883439677334192e+16,
      "budget_used_percent": 78.83439677334192
    },
    {
      "type": "training",
      "description": "Training step 3320",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:09",
      "total_flops_so_far": 7.88581356454776e+16,
      "budget_used_percent": 78.8581356454776
    },
    {
      "type": "training",
      "description": "Training step 3321",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:10",
      "total_flops_so_far": 7.888187451761328e+16,
      "budget_used_percent": 78.88187451761328
    },
    {
      "type": "training",
      "description": "Training step 3322",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:11",
      "total_flops_so_far": 7.890561338974896e+16,
      "budget_used_percent": 78.90561338974896
    },
    {
      "type": "training",
      "description": "Training step 3323",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:13",
      "total_flops_so_far": 7.892935226188464e+16,
      "budget_used_percent": 78.92935226188465
    },
    {
      "type": "training",
      "description": "Training step 3324",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:14",
      "total_flops_so_far": 7.895309113402032e+16,
      "budget_used_percent": 78.95309113402031
    },
    {
      "type": "training",
      "description": "Training step 3325",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:15",
      "total_flops_so_far": 7.8976830006156e+16,
      "budget_used_percent": 78.97683000615599
    },
    {
      "type": "training",
      "description": "Training step 3326",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:17",
      "total_flops_so_far": 7.900056887829168e+16,
      "budget_used_percent": 79.00056887829167
    },
    {
      "type": "training",
      "description": "Training step 3327",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:18",
      "total_flops_so_far": 7.902430775042736e+16,
      "budget_used_percent": 79.02430775042735
    },
    {
      "type": "training",
      "description": "Training step 3328",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:19",
      "total_flops_so_far": 7.904804662256304e+16,
      "budget_used_percent": 79.04804662256304
    },
    {
      "type": "training",
      "description": "Training step 3329",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:21",
      "total_flops_so_far": 7.907178549469872e+16,
      "budget_used_percent": 79.07178549469872
    },
    {
      "type": "training",
      "description": "Training step 3330",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:22",
      "total_flops_so_far": 7.90955243668344e+16,
      "budget_used_percent": 79.0955243668344
    },
    {
      "type": "training",
      "description": "Training step 3331",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:23",
      "total_flops_so_far": 7.911926323897008e+16,
      "budget_used_percent": 79.11926323897008
    },
    {
      "type": "training",
      "description": "Training step 3332",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:25",
      "total_flops_so_far": 7.914300211110576e+16,
      "budget_used_percent": 79.14300211110576
    },
    {
      "type": "training",
      "description": "Training step 3333",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:26",
      "total_flops_so_far": 7.916674098324144e+16,
      "budget_used_percent": 79.16674098324144
    },
    {
      "type": "training",
      "description": "Training step 3334",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:28",
      "total_flops_so_far": 7.919047985537712e+16,
      "budget_used_percent": 79.19047985537712
    },
    {
      "type": "training",
      "description": "Training step 3335",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:29",
      "total_flops_so_far": 7.92142187275128e+16,
      "budget_used_percent": 79.2142187275128
    },
    {
      "type": "training",
      "description": "Training step 3336",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:30",
      "total_flops_so_far": 7.923795759964848e+16,
      "budget_used_percent": 79.23795759964848
    },
    {
      "type": "training",
      "description": "Training step 3337",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:32",
      "total_flops_so_far": 7.926169647178416e+16,
      "budget_used_percent": 79.26169647178416
    },
    {
      "type": "training",
      "description": "Training step 3338",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:33",
      "total_flops_so_far": 7.928543534391984e+16,
      "budget_used_percent": 79.28543534391984
    },
    {
      "type": "training",
      "description": "Training step 3339",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:34",
      "total_flops_so_far": 7.930917421605552e+16,
      "budget_used_percent": 79.30917421605552
    },
    {
      "type": "training",
      "description": "Training step 3340",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:36",
      "total_flops_so_far": 7.93329130881912e+16,
      "budget_used_percent": 79.3329130881912
    },
    {
      "type": "training",
      "description": "Training step 3341",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:37",
      "total_flops_so_far": 7.935665196032688e+16,
      "budget_used_percent": 79.35665196032689
    },
    {
      "type": "training",
      "description": "Training step 3342",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:38",
      "total_flops_so_far": 7.938039083246256e+16,
      "budget_used_percent": 79.38039083246255
    },
    {
      "type": "training",
      "description": "Training step 3343",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:40",
      "total_flops_so_far": 7.940412970459824e+16,
      "budget_used_percent": 79.40412970459823
    },
    {
      "type": "training",
      "description": "Training step 3344",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:41",
      "total_flops_so_far": 7.942786857673392e+16,
      "budget_used_percent": 79.42786857673391
    },
    {
      "type": "training",
      "description": "Training step 3345",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:42",
      "total_flops_so_far": 7.94516074488696e+16,
      "budget_used_percent": 79.4516074488696
    },
    {
      "type": "training",
      "description": "Training step 3346",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:44",
      "total_flops_so_far": 7.947534632100528e+16,
      "budget_used_percent": 79.47534632100528
    },
    {
      "type": "training",
      "description": "Training step 3347",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:45",
      "total_flops_so_far": 7.949908519314096e+16,
      "budget_used_percent": 79.49908519314096
    },
    {
      "type": "training",
      "description": "Training step 3348",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:47",
      "total_flops_so_far": 7.952282406527664e+16,
      "budget_used_percent": 79.52282406527664
    },
    {
      "type": "training",
      "description": "Training step 3349",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:48",
      "total_flops_so_far": 7.954656293741232e+16,
      "budget_used_percent": 79.54656293741232
    },
    {
      "type": "training",
      "description": "Training step 3350",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:49",
      "total_flops_so_far": 7.9570301809548e+16,
      "budget_used_percent": 79.570301809548
    },
    {
      "type": "training",
      "description": "Training step 3351",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:51",
      "total_flops_so_far": 7.959404068168368e+16,
      "budget_used_percent": 79.59404068168368
    },
    {
      "type": "training",
      "description": "Training step 3352",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:52",
      "total_flops_so_far": 7.961777955381936e+16,
      "budget_used_percent": 79.61777955381936
    },
    {
      "type": "training",
      "description": "Training step 3353",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:53",
      "total_flops_so_far": 7.964151842595504e+16,
      "budget_used_percent": 79.64151842595504
    },
    {
      "type": "training",
      "description": "Training step 3354",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:55",
      "total_flops_so_far": 7.966525729809072e+16,
      "budget_used_percent": 79.66525729809072
    },
    {
      "type": "training",
      "description": "Training step 3355",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:56",
      "total_flops_so_far": 7.96889961702264e+16,
      "budget_used_percent": 79.6889961702264
    },
    {
      "type": "training",
      "description": "Training step 3356",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:58",
      "total_flops_so_far": 7.971273504236208e+16,
      "budget_used_percent": 79.71273504236208
    },
    {
      "type": "training",
      "description": "Training step 3357",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:47:59",
      "total_flops_so_far": 7.973647391449776e+16,
      "budget_used_percent": 79.73647391449776
    },
    {
      "type": "training",
      "description": "Training step 3358",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:00",
      "total_flops_so_far": 7.976021278663344e+16,
      "budget_used_percent": 79.76021278663345
    },
    {
      "type": "training",
      "description": "Training step 3359",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:02",
      "total_flops_so_far": 7.978395165876912e+16,
      "budget_used_percent": 79.78395165876913
    },
    {
      "type": "training",
      "description": "Training step 3360",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:03",
      "total_flops_so_far": 7.98076905309048e+16,
      "budget_used_percent": 79.8076905309048
    },
    {
      "type": "training",
      "description": "Training step 3361",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:04",
      "total_flops_so_far": 7.983142940304048e+16,
      "budget_used_percent": 79.83142940304047
    },
    {
      "type": "training",
      "description": "Training step 3362",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:06",
      "total_flops_so_far": 7.985516827517616e+16,
      "budget_used_percent": 79.85516827517615
    },
    {
      "type": "training",
      "description": "Training step 3363",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:07",
      "total_flops_so_far": 7.987890714731184e+16,
      "budget_used_percent": 79.87890714731184
    },
    {
      "type": "training",
      "description": "Training step 3364",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:08",
      "total_flops_so_far": 7.990264601944752e+16,
      "budget_used_percent": 79.90264601944752
    },
    {
      "type": "training",
      "description": "Training step 3365",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:10",
      "total_flops_so_far": 7.99263848915832e+16,
      "budget_used_percent": 79.9263848915832
    },
    {
      "type": "training",
      "description": "Training step 3366",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:11",
      "total_flops_so_far": 7.995012376371888e+16,
      "budget_used_percent": 79.95012376371888
    },
    {
      "type": "training",
      "description": "Training step 3367",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:12",
      "total_flops_so_far": 7.997386263585456e+16,
      "budget_used_percent": 79.97386263585456
    },
    {
      "type": "training",
      "description": "Training step 3368",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:14",
      "total_flops_so_far": 7.999760150799024e+16,
      "budget_used_percent": 79.99760150799024
    },
    {
      "type": "training",
      "description": "Training step 3369",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:15",
      "total_flops_so_far": 8.002134038012592e+16,
      "budget_used_percent": 80.02134038012592
    },
    {
      "type": "training",
      "description": "Training step 3370",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:17",
      "total_flops_so_far": 8.00450792522616e+16,
      "budget_used_percent": 80.0450792522616
    },
    {
      "type": "training",
      "description": "Training step 3371",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:18",
      "total_flops_so_far": 8.006881812439728e+16,
      "budget_used_percent": 80.06881812439728
    },
    {
      "type": "training",
      "description": "Training step 3372",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:19",
      "total_flops_so_far": 8.009255699653296e+16,
      "budget_used_percent": 80.09255699653296
    },
    {
      "type": "training",
      "description": "Training step 3373",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:21",
      "total_flops_so_far": 8.011629586866864e+16,
      "budget_used_percent": 80.11629586866864
    },
    {
      "type": "training",
      "description": "Training step 3374",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:22",
      "total_flops_so_far": 8.014003474080432e+16,
      "budget_used_percent": 80.14003474080432
    },
    {
      "type": "training",
      "description": "Training step 3375",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:23",
      "total_flops_so_far": 8.016377361294e+16,
      "budget_used_percent": 80.16377361294
    },
    {
      "type": "training",
      "description": "Training step 3376",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:25",
      "total_flops_so_far": 8.018751248507568e+16,
      "budget_used_percent": 80.18751248507569
    },
    {
      "type": "training",
      "description": "Training step 3377",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:26",
      "total_flops_so_far": 8.021125135721136e+16,
      "budget_used_percent": 80.21125135721135
    },
    {
      "type": "training",
      "description": "Training step 3378",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:27",
      "total_flops_so_far": 8.023499022934704e+16,
      "budget_used_percent": 80.23499022934703
    },
    {
      "type": "training",
      "description": "Training step 3379",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:29",
      "total_flops_so_far": 8.025872910148272e+16,
      "budget_used_percent": 80.25872910148271
    },
    {
      "type": "training",
      "description": "Training step 3380",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:30",
      "total_flops_so_far": 8.02824679736184e+16,
      "budget_used_percent": 80.2824679736184
    },
    {
      "type": "training",
      "description": "Training step 3381",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:31",
      "total_flops_so_far": 8.030620684575408e+16,
      "budget_used_percent": 80.30620684575408
    },
    {
      "type": "training",
      "description": "Training step 3382",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:33",
      "total_flops_so_far": 8.032994571788976e+16,
      "budget_used_percent": 80.32994571788976
    },
    {
      "type": "training",
      "description": "Training step 3383",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:34",
      "total_flops_so_far": 8.035368459002544e+16,
      "budget_used_percent": 80.35368459002544
    },
    {
      "type": "training",
      "description": "Training step 3384",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:35",
      "total_flops_so_far": 8.037742346216112e+16,
      "budget_used_percent": 80.37742346216112
    },
    {
      "type": "training",
      "description": "Training step 3385",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:37",
      "total_flops_so_far": 8.04011623342968e+16,
      "budget_used_percent": 80.4011623342968
    },
    {
      "type": "training",
      "description": "Training step 3386",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:38",
      "total_flops_so_far": 8.042490120643248e+16,
      "budget_used_percent": 80.42490120643248
    },
    {
      "type": "training",
      "description": "Training step 3387",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:40",
      "total_flops_so_far": 8.044864007856816e+16,
      "budget_used_percent": 80.44864007856816
    },
    {
      "type": "training",
      "description": "Training step 3388",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:41",
      "total_flops_so_far": 8.047237895070384e+16,
      "budget_used_percent": 80.47237895070384
    },
    {
      "type": "training",
      "description": "Training step 3389",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:42",
      "total_flops_so_far": 8.049611782283952e+16,
      "budget_used_percent": 80.49611782283952
    },
    {
      "type": "training",
      "description": "Training step 3390",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:44",
      "total_flops_so_far": 8.05198566949752e+16,
      "budget_used_percent": 80.5198566949752
    },
    {
      "type": "training",
      "description": "Training step 3391",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:45",
      "total_flops_so_far": 8.054359556711088e+16,
      "budget_used_percent": 80.54359556711088
    },
    {
      "type": "training",
      "description": "Training step 3392",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:46",
      "total_flops_so_far": 8.056733443924656e+16,
      "budget_used_percent": 80.56733443924657
    },
    {
      "type": "training",
      "description": "Training step 3393",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:48",
      "total_flops_so_far": 8.059107331138224e+16,
      "budget_used_percent": 80.59107331138225
    },
    {
      "type": "training",
      "description": "Training step 3394",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:49",
      "total_flops_so_far": 8.061481218351792e+16,
      "budget_used_percent": 80.61481218351793
    },
    {
      "type": "training",
      "description": "Training step 3395",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:51",
      "total_flops_so_far": 8.06385510556536e+16,
      "budget_used_percent": 80.6385510556536
    },
    {
      "type": "training",
      "description": "Training step 3396",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:52",
      "total_flops_so_far": 8.066228992778928e+16,
      "budget_used_percent": 80.66228992778927
    },
    {
      "type": "training",
      "description": "Training step 3397",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:53",
      "total_flops_so_far": 8.068602879992496e+16,
      "budget_used_percent": 80.68602879992496
    },
    {
      "type": "training",
      "description": "Training step 3398",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:55",
      "total_flops_so_far": 8.070976767206064e+16,
      "budget_used_percent": 80.70976767206064
    },
    {
      "type": "training",
      "description": "Training step 3399",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:56",
      "total_flops_so_far": 8.073350654419632e+16,
      "budget_used_percent": 80.73350654419632
    },
    {
      "type": "training",
      "description": "Training step 3400",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:57",
      "total_flops_so_far": 8.0757245416332e+16,
      "budget_used_percent": 80.757245416332
    },
    {
      "type": "training",
      "description": "Training step 3401",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:48:59",
      "total_flops_so_far": 8.078098428846768e+16,
      "budget_used_percent": 80.78098428846768
    },
    {
      "type": "training",
      "description": "Training step 3402",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:00",
      "total_flops_so_far": 8.080472316060336e+16,
      "budget_used_percent": 80.80472316060336
    },
    {
      "type": "training",
      "description": "Training step 3403",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:01",
      "total_flops_so_far": 8.082846203273904e+16,
      "budget_used_percent": 80.82846203273904
    },
    {
      "type": "training",
      "description": "Training step 3404",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:03",
      "total_flops_so_far": 8.085220090487472e+16,
      "budget_used_percent": 80.85220090487472
    },
    {
      "type": "training",
      "description": "Training step 3405",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:04",
      "total_flops_so_far": 8.08759397770104e+16,
      "budget_used_percent": 80.8759397770104
    },
    {
      "type": "training",
      "description": "Training step 3406",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:06",
      "total_flops_so_far": 8.089967864914608e+16,
      "budget_used_percent": 80.89967864914608
    },
    {
      "type": "training",
      "description": "Training step 3407",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:07",
      "total_flops_so_far": 8.092341752128176e+16,
      "budget_used_percent": 80.92341752128176
    },
    {
      "type": "training",
      "description": "Training step 3408",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:08",
      "total_flops_so_far": 8.094715639341744e+16,
      "budget_used_percent": 80.94715639341744
    },
    {
      "type": "training",
      "description": "Training step 3409",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:10",
      "total_flops_so_far": 8.097089526555312e+16,
      "budget_used_percent": 80.97089526555312
    },
    {
      "type": "training",
      "description": "Training step 3410",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:11",
      "total_flops_so_far": 8.09946341376888e+16,
      "budget_used_percent": 80.9946341376888
    },
    {
      "type": "training",
      "description": "Training step 3411",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:12",
      "total_flops_so_far": 8.101837300982448e+16,
      "budget_used_percent": 81.01837300982449
    },
    {
      "type": "training",
      "description": "Training step 3412",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:14",
      "total_flops_so_far": 8.104211188196016e+16,
      "budget_used_percent": 81.04211188196015
    },
    {
      "type": "training",
      "description": "Training step 3413",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:15",
      "total_flops_so_far": 8.106585075409584e+16,
      "budget_used_percent": 81.06585075409583
    },
    {
      "type": "training",
      "description": "Training step 3414",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:16",
      "total_flops_so_far": 8.108958962623152e+16,
      "budget_used_percent": 81.08958962623151
    },
    {
      "type": "training",
      "description": "Training step 3415",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:18",
      "total_flops_so_far": 8.11133284983672e+16,
      "budget_used_percent": 81.1133284983672
    },
    {
      "type": "training",
      "description": "Training step 3416",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:19",
      "total_flops_so_far": 8.113706737050288e+16,
      "budget_used_percent": 81.13706737050288
    },
    {
      "type": "training",
      "description": "Training step 3417",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:20",
      "total_flops_so_far": 8.116080624263856e+16,
      "budget_used_percent": 81.16080624263856
    },
    {
      "type": "training",
      "description": "Training step 3418",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:22",
      "total_flops_so_far": 8.118454511477424e+16,
      "budget_used_percent": 81.18454511477424
    },
    {
      "type": "training",
      "description": "Training step 3419",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:23",
      "total_flops_so_far": 8.120828398690992e+16,
      "budget_used_percent": 81.20828398690992
    },
    {
      "type": "training",
      "description": "Training step 3420",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:24",
      "total_flops_so_far": 8.12320228590456e+16,
      "budget_used_percent": 81.2320228590456
    },
    {
      "type": "training",
      "description": "Training step 3421",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:26",
      "total_flops_so_far": 8.125576173118128e+16,
      "budget_used_percent": 81.25576173118128
    },
    {
      "type": "training",
      "description": "Training step 3422",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:27",
      "total_flops_so_far": 8.127950060331696e+16,
      "budget_used_percent": 81.27950060331696
    },
    {
      "type": "training",
      "description": "Training step 3423",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:29",
      "total_flops_so_far": 8.130323947545264e+16,
      "budget_used_percent": 81.30323947545264
    },
    {
      "type": "training",
      "description": "Training step 3424",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:30",
      "total_flops_so_far": 8.132697834758832e+16,
      "budget_used_percent": 81.32697834758832
    },
    {
      "type": "training",
      "description": "Training step 3425",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:31",
      "total_flops_so_far": 8.1350717219724e+16,
      "budget_used_percent": 81.350717219724
    },
    {
      "type": "training",
      "description": "Training step 3426",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:33",
      "total_flops_so_far": 8.137445609185968e+16,
      "budget_used_percent": 81.37445609185968
    },
    {
      "type": "training",
      "description": "Training step 3427",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:34",
      "total_flops_so_far": 8.139819496399536e+16,
      "budget_used_percent": 81.39819496399537
    },
    {
      "type": "training",
      "description": "Training step 3428",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:35",
      "total_flops_so_far": 8.142193383613104e+16,
      "budget_used_percent": 81.42193383613105
    },
    {
      "type": "training",
      "description": "Training step 3429",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:37",
      "total_flops_so_far": 8.144567270826672e+16,
      "budget_used_percent": 81.44567270826673
    },
    {
      "type": "training",
      "description": "Training step 3430",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:38",
      "total_flops_so_far": 8.14694115804024e+16,
      "budget_used_percent": 81.4694115804024
    },
    {
      "type": "training",
      "description": "Training step 3431",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:39",
      "total_flops_so_far": 8.149315045253808e+16,
      "budget_used_percent": 81.49315045253807
    },
    {
      "type": "training",
      "description": "Training step 3432",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:40",
      "total_flops_so_far": 8.151688932467376e+16,
      "budget_used_percent": 81.51688932467376
    },
    {
      "type": "training",
      "description": "Training step 3433",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:41",
      "total_flops_so_far": 8.154062819680944e+16,
      "budget_used_percent": 81.54062819680944
    },
    {
      "type": "training",
      "description": "Training step 3434",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:43",
      "total_flops_so_far": 8.156436706894512e+16,
      "budget_used_percent": 81.56436706894512
    },
    {
      "type": "training",
      "description": "Training step 3435",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:44",
      "total_flops_so_far": 8.15881059410808e+16,
      "budget_used_percent": 81.5881059410808
    },
    {
      "type": "training",
      "description": "Training step 3436",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:45",
      "total_flops_so_far": 8.161184481321648e+16,
      "budget_used_percent": 81.61184481321648
    },
    {
      "type": "training",
      "description": "Training step 3437",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:47",
      "total_flops_so_far": 8.163558368535216e+16,
      "budget_used_percent": 81.63558368535216
    },
    {
      "type": "training",
      "description": "Training step 3438",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:48",
      "total_flops_so_far": 8.165932255748784e+16,
      "budget_used_percent": 81.65932255748784
    },
    {
      "type": "training",
      "description": "Training step 3439",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:49",
      "total_flops_so_far": 8.168306142962352e+16,
      "budget_used_percent": 81.68306142962352
    },
    {
      "type": "training",
      "description": "Training step 3440",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:51",
      "total_flops_so_far": 8.17068003017592e+16,
      "budget_used_percent": 81.7068003017592
    },
    {
      "type": "training",
      "description": "Training step 3441",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:52",
      "total_flops_so_far": 8.173053917389488e+16,
      "budget_used_percent": 81.73053917389488
    },
    {
      "type": "training",
      "description": "Training step 3442",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:53",
      "total_flops_so_far": 8.175427804603056e+16,
      "budget_used_percent": 81.75427804603056
    },
    {
      "type": "training",
      "description": "Training step 3443",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:55",
      "total_flops_so_far": 8.177801691816624e+16,
      "budget_used_percent": 81.77801691816624
    },
    {
      "type": "training",
      "description": "Training step 3444",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:56",
      "total_flops_so_far": 8.180175579030192e+16,
      "budget_used_percent": 81.80175579030193
    },
    {
      "type": "training",
      "description": "Training step 3445",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:58",
      "total_flops_so_far": 8.18254946624376e+16,
      "budget_used_percent": 81.8254946624376
    },
    {
      "type": "training",
      "description": "Training step 3446",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:49:59",
      "total_flops_so_far": 8.184923353457328e+16,
      "budget_used_percent": 81.84923353457329
    },
    {
      "type": "training",
      "description": "Training step 3447",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:00",
      "total_flops_so_far": 8.187297240670896e+16,
      "budget_used_percent": 81.87297240670895
    },
    {
      "type": "training",
      "description": "Training step 3448",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:02",
      "total_flops_so_far": 8.189671127884464e+16,
      "budget_used_percent": 81.89671127884463
    },
    {
      "type": "training",
      "description": "Training step 3449",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:03",
      "total_flops_so_far": 8.192045015098032e+16,
      "budget_used_percent": 81.92045015098032
    },
    {
      "type": "training",
      "description": "Training step 3450",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:04",
      "total_flops_so_far": 8.1944189023116e+16,
      "budget_used_percent": 81.944189023116
    },
    {
      "type": "training",
      "description": "Training step 3451",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:06",
      "total_flops_so_far": 8.196792789525168e+16,
      "budget_used_percent": 81.96792789525168
    },
    {
      "type": "training",
      "description": "Training step 3452",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:07",
      "total_flops_so_far": 8.199166676738736e+16,
      "budget_used_percent": 81.99166676738736
    },
    {
      "type": "training",
      "description": "Training step 3453",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:09",
      "total_flops_so_far": 8.201540563952304e+16,
      "budget_used_percent": 82.01540563952304
    },
    {
      "type": "training",
      "description": "Training step 3454",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:10",
      "total_flops_so_far": 8.203914451165872e+16,
      "budget_used_percent": 82.03914451165872
    },
    {
      "type": "training",
      "description": "Training step 3455",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:11",
      "total_flops_so_far": 8.20628833837944e+16,
      "budget_used_percent": 82.0628833837944
    },
    {
      "type": "training",
      "description": "Training step 3456",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:13",
      "total_flops_so_far": 8.208662225593008e+16,
      "budget_used_percent": 82.08662225593008
    },
    {
      "type": "training",
      "description": "Training step 3457",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:15",
      "total_flops_so_far": 8.211036112806576e+16,
      "budget_used_percent": 82.11036112806576
    },
    {
      "type": "training",
      "description": "Training step 3458",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:16",
      "total_flops_so_far": 8.213410000020144e+16,
      "budget_used_percent": 82.13410000020144
    },
    {
      "type": "training",
      "description": "Training step 3459",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:18",
      "total_flops_so_far": 8.215783887233712e+16,
      "budget_used_percent": 82.15783887233712
    },
    {
      "type": "training",
      "description": "Training step 3460",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:19",
      "total_flops_so_far": 8.21815777444728e+16,
      "budget_used_percent": 82.1815777444728
    },
    {
      "type": "training",
      "description": "Training step 3461",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:20",
      "total_flops_so_far": 8.220531661660848e+16,
      "budget_used_percent": 82.20531661660848
    },
    {
      "type": "training",
      "description": "Training step 3462",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:22",
      "total_flops_so_far": 8.222905548874416e+16,
      "budget_used_percent": 82.22905548874417
    },
    {
      "type": "training",
      "description": "Training step 3463",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:23",
      "total_flops_so_far": 8.225279436087984e+16,
      "budget_used_percent": 82.25279436087985
    },
    {
      "type": "training",
      "description": "Training step 3464",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:24",
      "total_flops_so_far": 8.227653323301552e+16,
      "budget_used_percent": 82.27653323301553
    },
    {
      "type": "training",
      "description": "Training step 3465",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:26",
      "total_flops_so_far": 8.23002721051512e+16,
      "budget_used_percent": 82.3002721051512
    },
    {
      "type": "training",
      "description": "Training step 3466",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:27",
      "total_flops_so_far": 8.232401097728688e+16,
      "budget_used_percent": 82.32401097728687
    },
    {
      "type": "training",
      "description": "Training step 3467",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:28",
      "total_flops_so_far": 8.234774984942256e+16,
      "budget_used_percent": 82.34774984942256
    },
    {
      "type": "training",
      "description": "Training step 3468",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:30",
      "total_flops_so_far": 8.237148872155824e+16,
      "budget_used_percent": 82.37148872155824
    },
    {
      "type": "training",
      "description": "Training step 3469",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:31",
      "total_flops_so_far": 8.239522759369392e+16,
      "budget_used_percent": 82.39522759369392
    },
    {
      "type": "training",
      "description": "Training step 3470",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:32",
      "total_flops_so_far": 8.24189664658296e+16,
      "budget_used_percent": 82.4189664658296
    },
    {
      "type": "training",
      "description": "Training step 3471",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:34",
      "total_flops_so_far": 8.244270533796528e+16,
      "budget_used_percent": 82.44270533796528
    },
    {
      "type": "training",
      "description": "Training step 3472",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:35",
      "total_flops_so_far": 8.246644421010096e+16,
      "budget_used_percent": 82.46644421010096
    },
    {
      "type": "training",
      "description": "Training step 3473",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:37",
      "total_flops_so_far": 8.249018308223664e+16,
      "budget_used_percent": 82.49018308223664
    },
    {
      "type": "training",
      "description": "Training step 3474",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:38",
      "total_flops_so_far": 8.251392195437232e+16,
      "budget_used_percent": 82.51392195437232
    },
    {
      "type": "training",
      "description": "Training step 3475",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:39",
      "total_flops_so_far": 8.2537660826508e+16,
      "budget_used_percent": 82.537660826508
    },
    {
      "type": "training",
      "description": "Training step 3476",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:41",
      "total_flops_so_far": 8.256139969864368e+16,
      "budget_used_percent": 82.56139969864368
    },
    {
      "type": "training",
      "description": "Training step 3477",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:42",
      "total_flops_so_far": 8.258513857077936e+16,
      "budget_used_percent": 82.58513857077936
    },
    {
      "type": "training",
      "description": "Training step 3478",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:43",
      "total_flops_so_far": 8.260887744291504e+16,
      "budget_used_percent": 82.60887744291504
    },
    {
      "type": "training",
      "description": "Training step 3479",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:45",
      "total_flops_so_far": 8.263261631505072e+16,
      "budget_used_percent": 82.63261631505073
    },
    {
      "type": "training",
      "description": "Training step 3480",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:46",
      "total_flops_so_far": 8.26563551871864e+16,
      "budget_used_percent": 82.6563551871864
    },
    {
      "type": "training",
      "description": "Training step 3481",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:47",
      "total_flops_so_far": 8.268009405932208e+16,
      "budget_used_percent": 82.68009405932209
    },
    {
      "type": "training",
      "description": "Training step 3482",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:49",
      "total_flops_so_far": 8.270383293145776e+16,
      "budget_used_percent": 82.70383293145775
    },
    {
      "type": "training",
      "description": "Training step 3483",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:50",
      "total_flops_so_far": 8.272757180359344e+16,
      "budget_used_percent": 82.72757180359343
    },
    {
      "type": "training",
      "description": "Training step 3484",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:52",
      "total_flops_so_far": 8.275131067572912e+16,
      "budget_used_percent": 82.75131067572912
    },
    {
      "type": "training",
      "description": "Training step 3485",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:53",
      "total_flops_so_far": 8.27750495478648e+16,
      "budget_used_percent": 82.7750495478648
    },
    {
      "type": "training",
      "description": "Training step 3486",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:54",
      "total_flops_so_far": 8.279878842000048e+16,
      "budget_used_percent": 82.79878842000048
    },
    {
      "type": "training",
      "description": "Training step 3487",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:56",
      "total_flops_so_far": 8.282252729213616e+16,
      "budget_used_percent": 82.82252729213616
    },
    {
      "type": "training",
      "description": "Training step 3488",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:57",
      "total_flops_so_far": 8.284626616427184e+16,
      "budget_used_percent": 82.84626616427184
    },
    {
      "type": "training",
      "description": "Training step 3489",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:50:58",
      "total_flops_so_far": 8.287000503640752e+16,
      "budget_used_percent": 82.87000503640752
    },
    {
      "type": "training",
      "description": "Training step 3490",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:00",
      "total_flops_so_far": 8.28937439085432e+16,
      "budget_used_percent": 82.8937439085432
    },
    {
      "type": "training",
      "description": "Training step 3491",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:01",
      "total_flops_so_far": 8.291748278067888e+16,
      "budget_used_percent": 82.91748278067888
    },
    {
      "type": "training",
      "description": "Training step 3492",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:03",
      "total_flops_so_far": 8.294122165281456e+16,
      "budget_used_percent": 82.94122165281456
    },
    {
      "type": "training",
      "description": "Training step 3493",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:04",
      "total_flops_so_far": 8.296496052495024e+16,
      "budget_used_percent": 82.96496052495024
    },
    {
      "type": "training",
      "description": "Training step 3494",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:05",
      "total_flops_so_far": 8.298869939708592e+16,
      "budget_used_percent": 82.98869939708592
    },
    {
      "type": "training",
      "description": "Training step 3495",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:07",
      "total_flops_so_far": 8.30124382692216e+16,
      "budget_used_percent": 83.0124382692216
    },
    {
      "type": "training",
      "description": "Training step 3496",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:08",
      "total_flops_so_far": 8.303617714135728e+16,
      "budget_used_percent": 83.03617714135729
    },
    {
      "type": "training",
      "description": "Training step 3497",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:09",
      "total_flops_so_far": 8.305991601349296e+16,
      "budget_used_percent": 83.05991601349297
    },
    {
      "type": "training",
      "description": "Training step 3498",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:11",
      "total_flops_so_far": 8.308365488562864e+16,
      "budget_used_percent": 83.08365488562865
    },
    {
      "type": "training",
      "description": "Training step 3499",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:12",
      "total_flops_so_far": 8.310739375776432e+16,
      "budget_used_percent": 83.10739375776433
    },
    {
      "type": "training",
      "description": "Training step 3500",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:13",
      "total_flops_so_far": 8.31311326299e+16,
      "budget_used_percent": 83.1311326299
    },
    {
      "type": "training",
      "description": "Training step 3501",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:15",
      "total_flops_so_far": 8.315487150203568e+16,
      "budget_used_percent": 83.15487150203568
    },
    {
      "type": "training",
      "description": "Training step 3502",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:16",
      "total_flops_so_far": 8.317861037417136e+16,
      "budget_used_percent": 83.17861037417136
    },
    {
      "type": "training",
      "description": "Training step 3503",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:17",
      "total_flops_so_far": 8.320234924630704e+16,
      "budget_used_percent": 83.20234924630704
    },
    {
      "type": "training",
      "description": "Training step 3504",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:19",
      "total_flops_so_far": 8.322608811844272e+16,
      "budget_used_percent": 83.22608811844272
    },
    {
      "type": "training",
      "description": "Training step 3505",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:20",
      "total_flops_so_far": 8.32498269905784e+16,
      "budget_used_percent": 83.2498269905784
    },
    {
      "type": "training",
      "description": "Training step 3506",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:22",
      "total_flops_so_far": 8.327356586271408e+16,
      "budget_used_percent": 83.27356586271408
    },
    {
      "type": "training",
      "description": "Training step 3507",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:23",
      "total_flops_so_far": 8.329730473484976e+16,
      "budget_used_percent": 83.29730473484976
    },
    {
      "type": "training",
      "description": "Training step 3508",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:24",
      "total_flops_so_far": 8.332104360698544e+16,
      "budget_used_percent": 83.32104360698544
    },
    {
      "type": "training",
      "description": "Training step 3509",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:26",
      "total_flops_so_far": 8.334478247912112e+16,
      "budget_used_percent": 83.34478247912112
    },
    {
      "type": "training",
      "description": "Training step 3510",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:27",
      "total_flops_so_far": 8.33685213512568e+16,
      "budget_used_percent": 83.3685213512568
    },
    {
      "type": "training",
      "description": "Training step 3511",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:28",
      "total_flops_so_far": 8.339226022339248e+16,
      "budget_used_percent": 83.39226022339248
    },
    {
      "type": "training",
      "description": "Training step 3512",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:30",
      "total_flops_so_far": 8.341599909552816e+16,
      "budget_used_percent": 83.41599909552816
    },
    {
      "type": "training",
      "description": "Training step 3513",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:31",
      "total_flops_so_far": 8.343973796766384e+16,
      "budget_used_percent": 83.43973796766385
    },
    {
      "type": "training",
      "description": "Training step 3514",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:32",
      "total_flops_so_far": 8.346347683979952e+16,
      "budget_used_percent": 83.46347683979953
    },
    {
      "type": "training",
      "description": "Training step 3515",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:34",
      "total_flops_so_far": 8.34872157119352e+16,
      "budget_used_percent": 83.4872157119352
    },
    {
      "type": "training",
      "description": "Training step 3516",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:35",
      "total_flops_so_far": 8.351095458407088e+16,
      "budget_used_percent": 83.51095458407089
    },
    {
      "type": "training",
      "description": "Training step 3517",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:37",
      "total_flops_so_far": 8.353469345620656e+16,
      "budget_used_percent": 83.53469345620655
    },
    {
      "type": "training",
      "description": "Training step 3518",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:38",
      "total_flops_so_far": 8.355843232834224e+16,
      "budget_used_percent": 83.55843232834223
    },
    {
      "type": "training",
      "description": "Training step 3519",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:39",
      "total_flops_so_far": 8.358217120047792e+16,
      "budget_used_percent": 83.58217120047792
    },
    {
      "type": "training",
      "description": "Training step 3520",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:41",
      "total_flops_so_far": 8.36059100726136e+16,
      "budget_used_percent": 83.6059100726136
    },
    {
      "type": "training",
      "description": "Training step 3521",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:42",
      "total_flops_so_far": 8.362964894474928e+16,
      "budget_used_percent": 83.62964894474928
    },
    {
      "type": "training",
      "description": "Training step 3522",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:43",
      "total_flops_so_far": 8.365338781688496e+16,
      "budget_used_percent": 83.65338781688496
    },
    {
      "type": "training",
      "description": "Training step 3523",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:45",
      "total_flops_so_far": 8.367712668902064e+16,
      "budget_used_percent": 83.67712668902064
    },
    {
      "type": "training",
      "description": "Training step 3524",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:46",
      "total_flops_so_far": 8.370086556115632e+16,
      "budget_used_percent": 83.70086556115632
    },
    {
      "type": "training",
      "description": "Training step 3525",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:47",
      "total_flops_so_far": 8.3724604433292e+16,
      "budget_used_percent": 83.724604433292
    },
    {
      "type": "training",
      "description": "Training step 3526",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:49",
      "total_flops_so_far": 8.374834330542768e+16,
      "budget_used_percent": 83.74834330542768
    },
    {
      "type": "training",
      "description": "Training step 3527",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:50",
      "total_flops_so_far": 8.377208217756336e+16,
      "budget_used_percent": 83.77208217756336
    },
    {
      "type": "training",
      "description": "Training step 3528",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:52",
      "total_flops_so_far": 8.379582104969904e+16,
      "budget_used_percent": 83.79582104969904
    },
    {
      "type": "training",
      "description": "Training step 3529",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:53",
      "total_flops_so_far": 8.381955992183472e+16,
      "budget_used_percent": 83.81955992183472
    },
    {
      "type": "training",
      "description": "Training step 3530",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:54",
      "total_flops_so_far": 8.38432987939704e+16,
      "budget_used_percent": 83.8432987939704
    },
    {
      "type": "training",
      "description": "Training step 3531",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:56",
      "total_flops_so_far": 8.386703766610608e+16,
      "budget_used_percent": 83.86703766610609
    },
    {
      "type": "training",
      "description": "Training step 3532",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:57",
      "total_flops_so_far": 8.389077653824176e+16,
      "budget_used_percent": 83.89077653824177
    },
    {
      "type": "training",
      "description": "Training step 3533",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:51:58",
      "total_flops_so_far": 8.391451541037744e+16,
      "budget_used_percent": 83.91451541037745
    },
    {
      "type": "training",
      "description": "Training step 3534",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:00",
      "total_flops_so_far": 8.393825428251312e+16,
      "budget_used_percent": 83.93825428251313
    },
    {
      "type": "training",
      "description": "Training step 3535",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:01",
      "total_flops_so_far": 8.39619931546488e+16,
      "budget_used_percent": 83.9619931546488
    },
    {
      "type": "training",
      "description": "Training step 3536",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:03",
      "total_flops_so_far": 8.398573202678448e+16,
      "budget_used_percent": 83.98573202678448
    },
    {
      "type": "training",
      "description": "Training step 3537",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:04",
      "total_flops_so_far": 8.400947089892016e+16,
      "budget_used_percent": 84.00947089892016
    },
    {
      "type": "training",
      "description": "Training step 3538",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:05",
      "total_flops_so_far": 8.403320977105584e+16,
      "budget_used_percent": 84.03320977105584
    },
    {
      "type": "training",
      "description": "Training step 3539",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:07",
      "total_flops_so_far": 8.405694864319152e+16,
      "budget_used_percent": 84.05694864319152
    },
    {
      "type": "training",
      "description": "Training step 3540",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:08",
      "total_flops_so_far": 8.40806875153272e+16,
      "budget_used_percent": 84.0806875153272
    },
    {
      "type": "training",
      "description": "Training step 3541",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:09",
      "total_flops_so_far": 8.410442638746288e+16,
      "budget_used_percent": 84.10442638746288
    },
    {
      "type": "training",
      "description": "Training step 3542",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:11",
      "total_flops_so_far": 8.412816525959856e+16,
      "budget_used_percent": 84.12816525959856
    },
    {
      "type": "training",
      "description": "Training step 3543",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:12",
      "total_flops_so_far": 8.415190413173424e+16,
      "budget_used_percent": 84.15190413173424
    },
    {
      "type": "training",
      "description": "Training step 3544",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:13",
      "total_flops_so_far": 8.417564300386992e+16,
      "budget_used_percent": 84.17564300386992
    },
    {
      "type": "training",
      "description": "Training step 3545",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:15",
      "total_flops_so_far": 8.41993818760056e+16,
      "budget_used_percent": 84.1993818760056
    },
    {
      "type": "training",
      "description": "Training step 3546",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:16",
      "total_flops_so_far": 8.422312074814128e+16,
      "budget_used_percent": 84.22312074814128
    },
    {
      "type": "training",
      "description": "Training step 3547",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:18",
      "total_flops_so_far": 8.424685962027696e+16,
      "budget_used_percent": 84.24685962027696
    },
    {
      "type": "training",
      "description": "Training step 3548",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:19",
      "total_flops_so_far": 8.427059849241264e+16,
      "budget_used_percent": 84.27059849241265
    },
    {
      "type": "training",
      "description": "Training step 3549",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:20",
      "total_flops_so_far": 8.429433736454832e+16,
      "budget_used_percent": 84.29433736454833
    },
    {
      "type": "training",
      "description": "Training step 3550",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:22",
      "total_flops_so_far": 8.4318076236684e+16,
      "budget_used_percent": 84.318076236684
    },
    {
      "type": "training",
      "description": "Training step 3551",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:23",
      "total_flops_so_far": 8.434181510881968e+16,
      "budget_used_percent": 84.34181510881969
    },
    {
      "type": "training",
      "description": "Training step 3552",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:24",
      "total_flops_so_far": 8.436555398095536e+16,
      "budget_used_percent": 84.36555398095535
    },
    {
      "type": "training",
      "description": "Training step 3553",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:26",
      "total_flops_so_far": 8.438929285309104e+16,
      "budget_used_percent": 84.38929285309104
    },
    {
      "type": "training",
      "description": "Training step 3554",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:27",
      "total_flops_so_far": 8.441303172522672e+16,
      "budget_used_percent": 84.41303172522672
    },
    {
      "type": "training",
      "description": "Training step 3555",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:28",
      "total_flops_so_far": 8.44367705973624e+16,
      "budget_used_percent": 84.4367705973624
    },
    {
      "type": "training",
      "description": "Training step 3556",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:30",
      "total_flops_so_far": 8.446050946949808e+16,
      "budget_used_percent": 84.46050946949808
    },
    {
      "type": "training",
      "description": "Training step 3557",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:31",
      "total_flops_so_far": 8.448424834163376e+16,
      "budget_used_percent": 84.48424834163376
    },
    {
      "type": "training",
      "description": "Training step 3558",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:32",
      "total_flops_so_far": 8.450798721376944e+16,
      "budget_used_percent": 84.50798721376944
    },
    {
      "type": "training",
      "description": "Training step 3559",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:34",
      "total_flops_so_far": 8.453172608590512e+16,
      "budget_used_percent": 84.53172608590512
    },
    {
      "type": "training",
      "description": "Training step 3560",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:35",
      "total_flops_so_far": 8.45554649580408e+16,
      "budget_used_percent": 84.5554649580408
    },
    {
      "type": "training",
      "description": "Training step 3561",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:37",
      "total_flops_so_far": 8.457920383017648e+16,
      "budget_used_percent": 84.57920383017648
    },
    {
      "type": "training",
      "description": "Training step 3562",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:38",
      "total_flops_so_far": 8.460294270231216e+16,
      "budget_used_percent": 84.60294270231216
    },
    {
      "type": "training",
      "description": "Training step 3563",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:39",
      "total_flops_so_far": 8.462668157444784e+16,
      "budget_used_percent": 84.62668157444784
    },
    {
      "type": "training",
      "description": "Training step 3564",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:40",
      "total_flops_so_far": 8.465042044658352e+16,
      "budget_used_percent": 84.65042044658352
    },
    {
      "type": "training",
      "description": "Training step 3565",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:41",
      "total_flops_so_far": 8.46741593187192e+16,
      "budget_used_percent": 84.6741593187192
    },
    {
      "type": "training",
      "description": "Training step 3566",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:43",
      "total_flops_so_far": 8.469789819085488e+16,
      "budget_used_percent": 84.69789819085489
    },
    {
      "type": "training",
      "description": "Training step 3567",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:44",
      "total_flops_so_far": 8.472163706299056e+16,
      "budget_used_percent": 84.72163706299057
    },
    {
      "type": "training",
      "description": "Training step 3568",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:45",
      "total_flops_so_far": 8.474537593512624e+16,
      "budget_used_percent": 84.74537593512625
    },
    {
      "type": "training",
      "description": "Training step 3569",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:47",
      "total_flops_so_far": 8.476911480726192e+16,
      "budget_used_percent": 84.76911480726193
    },
    {
      "type": "training",
      "description": "Training step 3570",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:48",
      "total_flops_so_far": 8.47928536793976e+16,
      "budget_used_percent": 84.7928536793976
    },
    {
      "type": "training",
      "description": "Training step 3571",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:49",
      "total_flops_so_far": 8.481659255153328e+16,
      "budget_used_percent": 84.81659255153328
    },
    {
      "type": "training",
      "description": "Training step 3572",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:51",
      "total_flops_so_far": 8.484033142366896e+16,
      "budget_used_percent": 84.84033142366896
    },
    {
      "type": "training",
      "description": "Training step 3573",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:52",
      "total_flops_so_far": 8.486407029580464e+16,
      "budget_used_percent": 84.86407029580464
    },
    {
      "type": "training",
      "description": "Training step 3574",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:53",
      "total_flops_so_far": 8.488780916794032e+16,
      "budget_used_percent": 84.88780916794032
    },
    {
      "type": "training",
      "description": "Training step 3575",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:55",
      "total_flops_so_far": 8.4911548040076e+16,
      "budget_used_percent": 84.911548040076
    },
    {
      "type": "training",
      "description": "Training step 3576",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:56",
      "total_flops_so_far": 8.493528691221168e+16,
      "budget_used_percent": 84.93528691221168
    },
    {
      "type": "training",
      "description": "Training step 3577",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:58",
      "total_flops_so_far": 8.495902578434736e+16,
      "budget_used_percent": 84.95902578434736
    },
    {
      "type": "training",
      "description": "Training step 3578",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:52:59",
      "total_flops_so_far": 8.498276465648304e+16,
      "budget_used_percent": 84.98276465648304
    },
    {
      "type": "training",
      "description": "Training step 3579",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:00",
      "total_flops_so_far": 8.500650352861872e+16,
      "budget_used_percent": 85.00650352861871
    },
    {
      "type": "training",
      "description": "Training step 3580",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:02",
      "total_flops_so_far": 8.50302424007544e+16,
      "budget_used_percent": 85.0302424007544
    },
    {
      "type": "training",
      "description": "Training step 3581",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:03",
      "total_flops_so_far": 8.505398127289008e+16,
      "budget_used_percent": 85.05398127289008
    },
    {
      "type": "training",
      "description": "Training step 3582",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:04",
      "total_flops_so_far": 8.507772014502576e+16,
      "budget_used_percent": 85.07772014502576
    },
    {
      "type": "training",
      "description": "Training step 3583",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:06",
      "total_flops_so_far": 8.510145901716144e+16,
      "budget_used_percent": 85.10145901716145
    },
    {
      "type": "training",
      "description": "Training step 3584",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:07",
      "total_flops_so_far": 8.512519788929712e+16,
      "budget_used_percent": 85.12519788929713
    },
    {
      "type": "training",
      "description": "Training step 3585",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:08",
      "total_flops_so_far": 8.51489367614328e+16,
      "budget_used_percent": 85.14893676143281
    },
    {
      "type": "training",
      "description": "Training step 3586",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:10",
      "total_flops_so_far": 8.517267563356848e+16,
      "budget_used_percent": 85.17267563356849
    },
    {
      "type": "training",
      "description": "Training step 3587",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:11",
      "total_flops_so_far": 8.519641450570416e+16,
      "budget_used_percent": 85.19641450570417
    },
    {
      "type": "training",
      "description": "Training step 3588",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:12",
      "total_flops_so_far": 8.522015337783984e+16,
      "budget_used_percent": 85.22015337783984
    },
    {
      "type": "training",
      "description": "Training step 3589",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:14",
      "total_flops_so_far": 8.524389224997552e+16,
      "budget_used_percent": 85.24389224997552
    },
    {
      "type": "training",
      "description": "Training step 3590",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:15",
      "total_flops_so_far": 8.52676311221112e+16,
      "budget_used_percent": 85.2676311221112
    },
    {
      "type": "training",
      "description": "Training step 3591",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:17",
      "total_flops_so_far": 8.529136999424688e+16,
      "budget_used_percent": 85.29136999424688
    },
    {
      "type": "training",
      "description": "Training step 3592",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:18",
      "total_flops_so_far": 8.531510886638256e+16,
      "budget_used_percent": 85.31510886638256
    },
    {
      "type": "training",
      "description": "Training step 3593",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:19",
      "total_flops_so_far": 8.533884773851824e+16,
      "budget_used_percent": 85.33884773851824
    },
    {
      "type": "training",
      "description": "Training step 3594",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:21",
      "total_flops_so_far": 8.536258661065392e+16,
      "budget_used_percent": 85.36258661065392
    },
    {
      "type": "training",
      "description": "Training step 3595",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:22",
      "total_flops_so_far": 8.53863254827896e+16,
      "budget_used_percent": 85.3863254827896
    },
    {
      "type": "training",
      "description": "Training step 3596",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:23",
      "total_flops_so_far": 8.541006435492528e+16,
      "budget_used_percent": 85.41006435492527
    },
    {
      "type": "training",
      "description": "Training step 3597",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:25",
      "total_flops_so_far": 8.543380322706096e+16,
      "budget_used_percent": 85.43380322706096
    },
    {
      "type": "training",
      "description": "Training step 3598",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:26",
      "total_flops_so_far": 8.545754209919664e+16,
      "budget_used_percent": 85.45754209919664
    },
    {
      "type": "training",
      "description": "Training step 3599",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:27",
      "total_flops_so_far": 8.548128097133232e+16,
      "budget_used_percent": 85.48128097133232
    },
    {
      "type": "training",
      "description": "Training step 3600",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:29",
      "total_flops_so_far": 8.5505019843468e+16,
      "budget_used_percent": 85.505019843468
    },
    {
      "type": "training",
      "description": "Training step 3601",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:30",
      "total_flops_so_far": 8.552875871560368e+16,
      "budget_used_percent": 85.52875871560369
    },
    {
      "type": "training",
      "description": "Training step 3602",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:32",
      "total_flops_so_far": 8.555249758773936e+16,
      "budget_used_percent": 85.55249758773937
    },
    {
      "type": "training",
      "description": "Training step 3603",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:33",
      "total_flops_so_far": 8.557623645987504e+16,
      "budget_used_percent": 85.57623645987505
    },
    {
      "type": "training",
      "description": "Training step 3604",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:34",
      "total_flops_so_far": 8.559997533201072e+16,
      "budget_used_percent": 85.59997533201073
    },
    {
      "type": "training",
      "description": "Training step 3605",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:36",
      "total_flops_so_far": 8.56237142041464e+16,
      "budget_used_percent": 85.6237142041464
    },
    {
      "type": "training",
      "description": "Training step 3606",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:37",
      "total_flops_so_far": 8.564745307628208e+16,
      "budget_used_percent": 85.64745307628208
    },
    {
      "type": "training",
      "description": "Training step 3607",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:39",
      "total_flops_so_far": 8.567119194841776e+16,
      "budget_used_percent": 85.67119194841776
    },
    {
      "type": "training",
      "description": "Training step 3608",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:40",
      "total_flops_so_far": 8.569493082055344e+16,
      "budget_used_percent": 85.69493082055344
    },
    {
      "type": "training",
      "description": "Training step 3609",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:41",
      "total_flops_so_far": 8.571866969268912e+16,
      "budget_used_percent": 85.71866969268912
    },
    {
      "type": "training",
      "description": "Training step 3610",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:43",
      "total_flops_so_far": 8.57424085648248e+16,
      "budget_used_percent": 85.7424085648248
    },
    {
      "type": "training",
      "description": "Training step 3611",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:44",
      "total_flops_so_far": 8.576614743696048e+16,
      "budget_used_percent": 85.76614743696048
    },
    {
      "type": "training",
      "description": "Training step 3612",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:45",
      "total_flops_so_far": 8.578988630909616e+16,
      "budget_used_percent": 85.78988630909616
    },
    {
      "type": "training",
      "description": "Training step 3613",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:47",
      "total_flops_so_far": 8.581362518123184e+16,
      "budget_used_percent": 85.81362518123184
    },
    {
      "type": "training",
      "description": "Training step 3614",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:48",
      "total_flops_so_far": 8.583736405336752e+16,
      "budget_used_percent": 85.83736405336751
    },
    {
      "type": "training",
      "description": "Training step 3615",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:49",
      "total_flops_so_far": 8.58611029255032e+16,
      "budget_used_percent": 85.86110292550319
    },
    {
      "type": "training",
      "description": "Training step 3616",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:51",
      "total_flops_so_far": 8.588484179763888e+16,
      "budget_used_percent": 85.88484179763888
    },
    {
      "type": "training",
      "description": "Training step 3617",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:52",
      "total_flops_so_far": 8.590858066977456e+16,
      "budget_used_percent": 85.90858066977457
    },
    {
      "type": "training",
      "description": "Training step 3618",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:53",
      "total_flops_so_far": 8.593231954191024e+16,
      "budget_used_percent": 85.93231954191025
    },
    {
      "type": "training",
      "description": "Training step 3619",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:55",
      "total_flops_so_far": 8.595605841404592e+16,
      "budget_used_percent": 85.95605841404593
    },
    {
      "type": "training",
      "description": "Training step 3620",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:56",
      "total_flops_so_far": 8.59797972861816e+16,
      "budget_used_percent": 85.97979728618161
    },
    {
      "type": "training",
      "description": "Training step 3621",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:58",
      "total_flops_so_far": 8.600353615831728e+16,
      "budget_used_percent": 86.00353615831729
    },
    {
      "type": "training",
      "description": "Training step 3622",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:53:59",
      "total_flops_so_far": 8.602727503045296e+16,
      "budget_used_percent": 86.02727503045297
    },
    {
      "type": "training",
      "description": "Training step 3623",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:00",
      "total_flops_so_far": 8.605101390258864e+16,
      "budget_used_percent": 86.05101390258864
    },
    {
      "type": "training",
      "description": "Training step 3624",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:02",
      "total_flops_so_far": 8.607475277472432e+16,
      "budget_used_percent": 86.07475277472432
    },
    {
      "type": "training",
      "description": "Training step 3625",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:03",
      "total_flops_so_far": 8.609849164686e+16,
      "budget_used_percent": 86.09849164686
    },
    {
      "type": "training",
      "description": "Training step 3626",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:04",
      "total_flops_so_far": 8.612223051899568e+16,
      "budget_used_percent": 86.12223051899568
    },
    {
      "type": "training",
      "description": "Training step 3627",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:06",
      "total_flops_so_far": 8.614596939113136e+16,
      "budget_used_percent": 86.14596939113136
    },
    {
      "type": "training",
      "description": "Training step 3628",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:07",
      "total_flops_so_far": 8.616970826326704e+16,
      "budget_used_percent": 86.16970826326704
    },
    {
      "type": "training",
      "description": "Training step 3629",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:08",
      "total_flops_so_far": 8.619344713540272e+16,
      "budget_used_percent": 86.19344713540272
    },
    {
      "type": "training",
      "description": "Training step 3630",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:10",
      "total_flops_so_far": 8.62171860075384e+16,
      "budget_used_percent": 86.2171860075384
    },
    {
      "type": "training",
      "description": "Training step 3631",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:11",
      "total_flops_so_far": 8.624092487967408e+16,
      "budget_used_percent": 86.24092487967407
    },
    {
      "type": "training",
      "description": "Training step 3632",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:13",
      "total_flops_so_far": 8.626466375180976e+16,
      "budget_used_percent": 86.26466375180975
    },
    {
      "type": "training",
      "description": "Training step 3633",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:14",
      "total_flops_so_far": 8.628840262394544e+16,
      "budget_used_percent": 86.28840262394544
    },
    {
      "type": "training",
      "description": "Training step 3634",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:15",
      "total_flops_so_far": 8.631214149608112e+16,
      "budget_used_percent": 86.31214149608113
    },
    {
      "type": "training",
      "description": "Training step 3635",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:17",
      "total_flops_so_far": 8.63358803682168e+16,
      "budget_used_percent": 86.3358803682168
    },
    {
      "type": "training",
      "description": "Training step 3636",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:18",
      "total_flops_so_far": 8.635961924035248e+16,
      "budget_used_percent": 86.35961924035249
    },
    {
      "type": "training",
      "description": "Training step 3637",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:19",
      "total_flops_so_far": 8.638335811248816e+16,
      "budget_used_percent": 86.38335811248817
    },
    {
      "type": "training",
      "description": "Training step 3638",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:21",
      "total_flops_so_far": 8.640709698462384e+16,
      "budget_used_percent": 86.40709698462385
    },
    {
      "type": "training",
      "description": "Training step 3639",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:22",
      "total_flops_so_far": 8.643083585675952e+16,
      "budget_used_percent": 86.43083585675953
    },
    {
      "type": "training",
      "description": "Training step 3640",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:23",
      "total_flops_so_far": 8.64545747288952e+16,
      "budget_used_percent": 86.4545747288952
    },
    {
      "type": "training",
      "description": "Training step 3641",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:25",
      "total_flops_so_far": 8.647831360103088e+16,
      "budget_used_percent": 86.47831360103088
    },
    {
      "type": "training",
      "description": "Training step 3642",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:26",
      "total_flops_so_far": 8.650205247316656e+16,
      "budget_used_percent": 86.50205247316656
    },
    {
      "type": "training",
      "description": "Training step 3643",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:28",
      "total_flops_so_far": 8.652579134530224e+16,
      "budget_used_percent": 86.52579134530224
    },
    {
      "type": "training",
      "description": "Training step 3644",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:29",
      "total_flops_so_far": 8.654953021743792e+16,
      "budget_used_percent": 86.54953021743792
    },
    {
      "type": "training",
      "description": "Training step 3645",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:30",
      "total_flops_so_far": 8.65732690895736e+16,
      "budget_used_percent": 86.5732690895736
    },
    {
      "type": "training",
      "description": "Training step 3646",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:32",
      "total_flops_so_far": 8.659700796170928e+16,
      "budget_used_percent": 86.59700796170928
    },
    {
      "type": "training",
      "description": "Training step 3647",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:33",
      "total_flops_so_far": 8.662074683384496e+16,
      "budget_used_percent": 86.62074683384496
    },
    {
      "type": "training",
      "description": "Training step 3648",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:34",
      "total_flops_so_far": 8.664448570598064e+16,
      "budget_used_percent": 86.64448570598064
    },
    {
      "type": "training",
      "description": "Training step 3649",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:36",
      "total_flops_so_far": 8.666822457811632e+16,
      "budget_used_percent": 86.66822457811631
    },
    {
      "type": "training",
      "description": "Training step 3650",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:37",
      "total_flops_so_far": 8.6691963450252e+16,
      "budget_used_percent": 86.69196345025199
    },
    {
      "type": "training",
      "description": "Training step 3651",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:39",
      "total_flops_so_far": 8.671570232238768e+16,
      "budget_used_percent": 86.71570232238767
    },
    {
      "type": "training",
      "description": "Training step 3652",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:40",
      "total_flops_so_far": 8.673944119452336e+16,
      "budget_used_percent": 86.73944119452337
    },
    {
      "type": "training",
      "description": "Training step 3653",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:41",
      "total_flops_so_far": 8.676318006665904e+16,
      "budget_used_percent": 86.76318006665905
    },
    {
      "type": "training",
      "description": "Training step 3654",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:43",
      "total_flops_so_far": 8.678691893879472e+16,
      "budget_used_percent": 86.78691893879473
    },
    {
      "type": "training",
      "description": "Training step 3655",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:44",
      "total_flops_so_far": 8.68106578109304e+16,
      "budget_used_percent": 86.81065781093041
    },
    {
      "type": "training",
      "description": "Training step 3656",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:45",
      "total_flops_so_far": 8.683439668306608e+16,
      "budget_used_percent": 86.83439668306609
    },
    {
      "type": "training",
      "description": "Training step 3657",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:47",
      "total_flops_so_far": 8.685813555520176e+16,
      "budget_used_percent": 86.85813555520177
    },
    {
      "type": "training",
      "description": "Training step 3658",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:48",
      "total_flops_so_far": 8.688187442733744e+16,
      "budget_used_percent": 86.88187442733744
    },
    {
      "type": "training",
      "description": "Training step 3659",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:49",
      "total_flops_so_far": 8.690561329947312e+16,
      "budget_used_percent": 86.90561329947312
    },
    {
      "type": "training",
      "description": "Training step 3660",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:51",
      "total_flops_so_far": 8.69293521716088e+16,
      "budget_used_percent": 86.9293521716088
    },
    {
      "type": "training",
      "description": "Training step 3661",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:52",
      "total_flops_so_far": 8.695309104374448e+16,
      "budget_used_percent": 86.95309104374448
    },
    {
      "type": "training",
      "description": "Training step 3662",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:53",
      "total_flops_so_far": 8.697682991588016e+16,
      "budget_used_percent": 86.97682991588016
    },
    {
      "type": "training",
      "description": "Training step 3663",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:55",
      "total_flops_so_far": 8.700056878801584e+16,
      "budget_used_percent": 87.00056878801584
    },
    {
      "type": "training",
      "description": "Training step 3664",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:56",
      "total_flops_so_far": 8.702430766015152e+16,
      "budget_used_percent": 87.02430766015152
    },
    {
      "type": "training",
      "description": "Training step 3665",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:58",
      "total_flops_so_far": 8.70480465322872e+16,
      "budget_used_percent": 87.0480465322872
    },
    {
      "type": "training",
      "description": "Training step 3666",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:54:59",
      "total_flops_so_far": 8.707178540442288e+16,
      "budget_used_percent": 87.07178540442287
    },
    {
      "type": "training",
      "description": "Training step 3667",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:00",
      "total_flops_so_far": 8.709552427655856e+16,
      "budget_used_percent": 87.09552427655855
    },
    {
      "type": "training",
      "description": "Training step 3668",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:02",
      "total_flops_so_far": 8.711926314869424e+16,
      "budget_used_percent": 87.11926314869423
    },
    {
      "type": "training",
      "description": "Training step 3669",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:03",
      "total_flops_so_far": 8.714300202082992e+16,
      "budget_used_percent": 87.14300202082993
    },
    {
      "type": "training",
      "description": "Training step 3670",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:04",
      "total_flops_so_far": 8.71667408929656e+16,
      "budget_used_percent": 87.1667408929656
    },
    {
      "type": "training",
      "description": "Training step 3671",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:06",
      "total_flops_so_far": 8.719047976510128e+16,
      "budget_used_percent": 87.19047976510129
    },
    {
      "type": "training",
      "description": "Training step 3672",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:07",
      "total_flops_so_far": 8.721421863723696e+16,
      "budget_used_percent": 87.21421863723697
    },
    {
      "type": "training",
      "description": "Training step 3673",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:09",
      "total_flops_so_far": 8.723795750937264e+16,
      "budget_used_percent": 87.23795750937265
    },
    {
      "type": "training",
      "description": "Training step 3674",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:10",
      "total_flops_so_far": 8.726169638150832e+16,
      "budget_used_percent": 87.26169638150833
    },
    {
      "type": "training",
      "description": "Training step 3675",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:11",
      "total_flops_so_far": 8.7285435253644e+16,
      "budget_used_percent": 87.285435253644
    },
    {
      "type": "training",
      "description": "Training step 3676",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:13",
      "total_flops_so_far": 8.730917412577968e+16,
      "budget_used_percent": 87.30917412577968
    },
    {
      "type": "training",
      "description": "Training step 3677",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:14",
      "total_flops_so_far": 8.733291299791536e+16,
      "budget_used_percent": 87.33291299791536
    },
    {
      "type": "training",
      "description": "Training step 3678",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:15",
      "total_flops_so_far": 8.735665187005104e+16,
      "budget_used_percent": 87.35665187005104
    },
    {
      "type": "training",
      "description": "Training step 3679",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:17",
      "total_flops_so_far": 8.738039074218672e+16,
      "budget_used_percent": 87.38039074218672
    },
    {
      "type": "training",
      "description": "Training step 3680",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:18",
      "total_flops_so_far": 8.74041296143224e+16,
      "budget_used_percent": 87.4041296143224
    },
    {
      "type": "training",
      "description": "Training step 3681",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:19",
      "total_flops_so_far": 8.742786848645808e+16,
      "budget_used_percent": 87.42786848645808
    },
    {
      "type": "training",
      "description": "Training step 3682",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:21",
      "total_flops_so_far": 8.745160735859376e+16,
      "budget_used_percent": 87.45160735859376
    },
    {
      "type": "training",
      "description": "Training step 3683",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:22",
      "total_flops_so_far": 8.747534623072944e+16,
      "budget_used_percent": 87.47534623072944
    },
    {
      "type": "training",
      "description": "Training step 3684",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:24",
      "total_flops_so_far": 8.749908510286512e+16,
      "budget_used_percent": 87.49908510286511
    },
    {
      "type": "training",
      "description": "Training step 3685",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:25",
      "total_flops_so_far": 8.75228239750008e+16,
      "budget_used_percent": 87.52282397500079
    },
    {
      "type": "training",
      "description": "Training step 3686",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:26",
      "total_flops_so_far": 8.754656284713648e+16,
      "budget_used_percent": 87.54656284713647
    },
    {
      "type": "training",
      "description": "Training step 3687",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:28",
      "total_flops_so_far": 8.757030171927216e+16,
      "budget_used_percent": 87.57030171927215
    },
    {
      "type": "training",
      "description": "Training step 3688",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:29",
      "total_flops_so_far": 8.759404059140784e+16,
      "budget_used_percent": 87.59404059140785
    },
    {
      "type": "training",
      "description": "Training step 3689",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:30",
      "total_flops_so_far": 8.761777946354352e+16,
      "budget_used_percent": 87.61777946354353
    },
    {
      "type": "training",
      "description": "Training step 3690",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:32",
      "total_flops_so_far": 8.76415183356792e+16,
      "budget_used_percent": 87.64151833567921
    },
    {
      "type": "training",
      "description": "Training step 3691",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:33",
      "total_flops_so_far": 8.766525720781488e+16,
      "budget_used_percent": 87.66525720781489
    },
    {
      "type": "training",
      "description": "Training step 3692",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:35",
      "total_flops_so_far": 8.768899607995056e+16,
      "budget_used_percent": 87.68899607995057
    },
    {
      "type": "training",
      "description": "Training step 3693",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:36",
      "total_flops_so_far": 8.771273495208624e+16,
      "budget_used_percent": 87.71273495208624
    },
    {
      "type": "training",
      "description": "Training step 3694",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:37",
      "total_flops_so_far": 8.773647382422192e+16,
      "budget_used_percent": 87.73647382422192
    },
    {
      "type": "training",
      "description": "Training step 3695",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:39",
      "total_flops_so_far": 8.77602126963576e+16,
      "budget_used_percent": 87.7602126963576
    },
    {
      "type": "training",
      "description": "Training step 3696",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:39",
      "total_flops_so_far": 8.778395156849328e+16,
      "budget_used_percent": 87.78395156849328
    },
    {
      "type": "training",
      "description": "Training step 3697",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:40",
      "total_flops_so_far": 8.780769044062896e+16,
      "budget_used_percent": 87.80769044062896
    },
    {
      "type": "training",
      "description": "Training step 3698",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:42",
      "total_flops_so_far": 8.783142931276464e+16,
      "budget_used_percent": 87.83142931276464
    },
    {
      "type": "training",
      "description": "Training step 3699",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:43",
      "total_flops_so_far": 8.785516818490032e+16,
      "budget_used_percent": 87.85516818490032
    },
    {
      "type": "training",
      "description": "Training step 3700",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:45",
      "total_flops_so_far": 8.7878907057036e+16,
      "budget_used_percent": 87.878907057036
    },
    {
      "type": "training",
      "description": "Training step 3701",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:46",
      "total_flops_so_far": 8.790264592917168e+16,
      "budget_used_percent": 87.90264592917168
    },
    {
      "type": "training",
      "description": "Training step 3702",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:47",
      "total_flops_so_far": 8.792638480130736e+16,
      "budget_used_percent": 87.92638480130735
    },
    {
      "type": "training",
      "description": "Training step 3703",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:49",
      "total_flops_so_far": 8.795012367344304e+16,
      "budget_used_percent": 87.95012367344303
    },
    {
      "type": "training",
      "description": "Training step 3704",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:50",
      "total_flops_so_far": 8.797386254557872e+16,
      "budget_used_percent": 87.97386254557871
    },
    {
      "type": "training",
      "description": "Training step 3705",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:51",
      "total_flops_so_far": 8.79976014177144e+16,
      "budget_used_percent": 87.9976014177144
    },
    {
      "type": "training",
      "description": "Training step 3706",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:53",
      "total_flops_so_far": 8.802134028985008e+16,
      "budget_used_percent": 88.02134028985009
    },
    {
      "type": "training",
      "description": "Training step 3707",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:54",
      "total_flops_so_far": 8.804507916198576e+16,
      "budget_used_percent": 88.04507916198577
    },
    {
      "type": "training",
      "description": "Training step 3708",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:55",
      "total_flops_so_far": 8.806881803412144e+16,
      "budget_used_percent": 88.06881803412145
    },
    {
      "type": "training",
      "description": "Training step 3709",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:57",
      "total_flops_so_far": 8.809255690625712e+16,
      "budget_used_percent": 88.09255690625713
    },
    {
      "type": "training",
      "description": "Training step 3710",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:55:58",
      "total_flops_so_far": 8.81162957783928e+16,
      "budget_used_percent": 88.1162957783928
    },
    {
      "type": "training",
      "description": "Training step 3711",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:00",
      "total_flops_so_far": 8.814003465052848e+16,
      "budget_used_percent": 88.14003465052848
    },
    {
      "type": "training",
      "description": "Training step 3712",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:01",
      "total_flops_so_far": 8.816377352266416e+16,
      "budget_used_percent": 88.16377352266416
    },
    {
      "type": "training",
      "description": "Training step 3713",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:02",
      "total_flops_so_far": 8.818751239479984e+16,
      "budget_used_percent": 88.18751239479984
    },
    {
      "type": "training",
      "description": "Training step 3714",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:04",
      "total_flops_so_far": 8.821125126693552e+16,
      "budget_used_percent": 88.21125126693552
    },
    {
      "type": "training",
      "description": "Training step 3715",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:05",
      "total_flops_so_far": 8.82349901390712e+16,
      "budget_used_percent": 88.2349901390712
    },
    {
      "type": "training",
      "description": "Training step 3716",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:06",
      "total_flops_so_far": 8.825872901120688e+16,
      "budget_used_percent": 88.25872901120688
    },
    {
      "type": "training",
      "description": "Training step 3717",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:08",
      "total_flops_so_far": 8.828246788334256e+16,
      "budget_used_percent": 88.28246788334256
    },
    {
      "type": "training",
      "description": "Training step 3718",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:09",
      "total_flops_so_far": 8.830620675547824e+16,
      "budget_used_percent": 88.30620675547824
    },
    {
      "type": "training",
      "description": "Training step 3719",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:11",
      "total_flops_so_far": 8.832994562761392e+16,
      "budget_used_percent": 88.32994562761391
    },
    {
      "type": "training",
      "description": "Training step 3720",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:12",
      "total_flops_so_far": 8.83536844997496e+16,
      "budget_used_percent": 88.35368449974959
    },
    {
      "type": "training",
      "description": "Training step 3721",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:13",
      "total_flops_so_far": 8.837742337188528e+16,
      "budget_used_percent": 88.37742337188527
    },
    {
      "type": "training",
      "description": "Training step 3722",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:15",
      "total_flops_so_far": 8.840116224402096e+16,
      "budget_used_percent": 88.40116224402095
    },
    {
      "type": "training",
      "description": "Training step 3723",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:16",
      "total_flops_so_far": 8.842490111615664e+16,
      "budget_used_percent": 88.42490111615663
    },
    {
      "type": "training",
      "description": "Training step 3724",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:17",
      "total_flops_so_far": 8.844863998829232e+16,
      "budget_used_percent": 88.44863998829233
    },
    {
      "type": "training",
      "description": "Training step 3725",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:19",
      "total_flops_so_far": 8.8472378860428e+16,
      "budget_used_percent": 88.47237886042801
    },
    {
      "type": "training",
      "description": "Training step 3726",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:20",
      "total_flops_so_far": 8.849611773256368e+16,
      "budget_used_percent": 88.49611773256369
    },
    {
      "type": "training",
      "description": "Training step 3727",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:22",
      "total_flops_so_far": 8.851985660469936e+16,
      "budget_used_percent": 88.51985660469937
    },
    {
      "type": "training",
      "description": "Training step 3728",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:23",
      "total_flops_so_far": 8.854359547683504e+16,
      "budget_used_percent": 88.54359547683504
    },
    {
      "type": "training",
      "description": "Training step 3729",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:24",
      "total_flops_so_far": 8.856733434897072e+16,
      "budget_used_percent": 88.56733434897072
    },
    {
      "type": "training",
      "description": "Training step 3730",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:26",
      "total_flops_so_far": 8.85910732211064e+16,
      "budget_used_percent": 88.5910732211064
    },
    {
      "type": "training",
      "description": "Training step 3731",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:27",
      "total_flops_so_far": 8.861481209324208e+16,
      "budget_used_percent": 88.61481209324208
    },
    {
      "type": "training",
      "description": "Training step 3732",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:28",
      "total_flops_so_far": 8.863855096537776e+16,
      "budget_used_percent": 88.63855096537776
    },
    {
      "type": "training",
      "description": "Training step 3733",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:30",
      "total_flops_so_far": 8.866228983751344e+16,
      "budget_used_percent": 88.66228983751344
    },
    {
      "type": "training",
      "description": "Training step 3734",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:31",
      "total_flops_so_far": 8.868602870964912e+16,
      "budget_used_percent": 88.68602870964912
    },
    {
      "type": "training",
      "description": "Training step 3735",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:32",
      "total_flops_so_far": 8.87097675817848e+16,
      "budget_used_percent": 88.7097675817848
    },
    {
      "type": "training",
      "description": "Training step 3736",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:34",
      "total_flops_so_far": 8.873350645392048e+16,
      "budget_used_percent": 88.73350645392048
    },
    {
      "type": "training",
      "description": "Training step 3737",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:35",
      "total_flops_so_far": 8.875724532605616e+16,
      "budget_used_percent": 88.75724532605615
    },
    {
      "type": "training",
      "description": "Training step 3738",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:37",
      "total_flops_so_far": 8.878098419819184e+16,
      "budget_used_percent": 88.78098419819183
    },
    {
      "type": "training",
      "description": "Training step 3739",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:38",
      "total_flops_so_far": 8.880472307032752e+16,
      "budget_used_percent": 88.80472307032751
    },
    {
      "type": "training",
      "description": "Training step 3740",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:39",
      "total_flops_so_far": 8.88284619424632e+16,
      "budget_used_percent": 88.82846194246319
    },
    {
      "type": "training",
      "description": "Training step 3741",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:41",
      "total_flops_so_far": 8.885220081459888e+16,
      "budget_used_percent": 88.85220081459889
    },
    {
      "type": "training",
      "description": "Training step 3742",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:42",
      "total_flops_so_far": 8.887593968673456e+16,
      "budget_used_percent": 88.87593968673457
    },
    {
      "type": "training",
      "description": "Training step 3743",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:43",
      "total_flops_so_far": 8.889967855887024e+16,
      "budget_used_percent": 88.89967855887025
    },
    {
      "type": "training",
      "description": "Training step 3744",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:45",
      "total_flops_so_far": 8.892341743100592e+16,
      "budget_used_percent": 88.92341743100593
    },
    {
      "type": "training",
      "description": "Training step 3745",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:46",
      "total_flops_so_far": 8.89471563031416e+16,
      "budget_used_percent": 88.9471563031416
    },
    {
      "type": "training",
      "description": "Training step 3746",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:47",
      "total_flops_so_far": 8.897089517527728e+16,
      "budget_used_percent": 88.97089517527728
    },
    {
      "type": "training",
      "description": "Training step 3747",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:49",
      "total_flops_so_far": 8.899463404741296e+16,
      "budget_used_percent": 88.99463404741296
    },
    {
      "type": "training",
      "description": "Training step 3748",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:50",
      "total_flops_so_far": 8.901837291954864e+16,
      "budget_used_percent": 89.01837291954864
    },
    {
      "type": "training",
      "description": "Training step 3749",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:52",
      "total_flops_so_far": 8.904211179168432e+16,
      "budget_used_percent": 89.04211179168432
    },
    {
      "type": "training",
      "description": "Training step 3750",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:53",
      "total_flops_so_far": 8.906585066382e+16,
      "budget_used_percent": 89.06585066382
    },
    {
      "type": "training",
      "description": "Training step 3751",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:54",
      "total_flops_so_far": 8.908958953595568e+16,
      "budget_used_percent": 89.08958953595568
    },
    {
      "type": "training",
      "description": "Training step 3752",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:56",
      "total_flops_so_far": 8.911332840809136e+16,
      "budget_used_percent": 89.11332840809136
    },
    {
      "type": "training",
      "description": "Training step 3753",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:57",
      "total_flops_so_far": 8.913706728022704e+16,
      "budget_used_percent": 89.13706728022704
    },
    {
      "type": "training",
      "description": "Training step 3754",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:56:58",
      "total_flops_so_far": 8.916080615236272e+16,
      "budget_used_percent": 89.16080615236271
    },
    {
      "type": "training",
      "description": "Training step 3755",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:00",
      "total_flops_so_far": 8.91845450244984e+16,
      "budget_used_percent": 89.18454502449839
    },
    {
      "type": "training",
      "description": "Training step 3756",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:01",
      "total_flops_so_far": 8.920828389663408e+16,
      "budget_used_percent": 89.20828389663407
    },
    {
      "type": "training",
      "description": "Training step 3757",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:02",
      "total_flops_so_far": 8.923202276876976e+16,
      "budget_used_percent": 89.23202276876975
    },
    {
      "type": "training",
      "description": "Training step 3758",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:04",
      "total_flops_so_far": 8.925576164090544e+16,
      "budget_used_percent": 89.25576164090543
    },
    {
      "type": "training",
      "description": "Training step 3759",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:05",
      "total_flops_so_far": 8.927950051304112e+16,
      "budget_used_percent": 89.27950051304111
    },
    {
      "type": "training",
      "description": "Training step 3760",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:07",
      "total_flops_so_far": 8.93032393851768e+16,
      "budget_used_percent": 89.30323938517681
    },
    {
      "type": "training",
      "description": "Training step 3761",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:08",
      "total_flops_so_far": 8.932697825731248e+16,
      "budget_used_percent": 89.32697825731249
    },
    {
      "type": "training",
      "description": "Training step 3762",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:09",
      "total_flops_so_far": 8.935071712944816e+16,
      "budget_used_percent": 89.35071712944817
    },
    {
      "type": "training",
      "description": "Training step 3763",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:11",
      "total_flops_so_far": 8.937445600158384e+16,
      "budget_used_percent": 89.37445600158384
    },
    {
      "type": "training",
      "description": "Training step 3764",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:12",
      "total_flops_so_far": 8.939819487371952e+16,
      "budget_used_percent": 89.39819487371952
    },
    {
      "type": "training",
      "description": "Training step 3765",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:14",
      "total_flops_so_far": 8.94219337458552e+16,
      "budget_used_percent": 89.4219337458552
    },
    {
      "type": "training",
      "description": "Training step 3766",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:15",
      "total_flops_so_far": 8.944567261799088e+16,
      "budget_used_percent": 89.44567261799088
    },
    {
      "type": "training",
      "description": "Training step 3767",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:17",
      "total_flops_so_far": 8.946941149012656e+16,
      "budget_used_percent": 89.46941149012656
    },
    {
      "type": "training",
      "description": "Training step 3768",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:18",
      "total_flops_so_far": 8.949315036226224e+16,
      "budget_used_percent": 89.49315036226224
    },
    {
      "type": "training",
      "description": "Training step 3769",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:19",
      "total_flops_so_far": 8.951688923439792e+16,
      "budget_used_percent": 89.51688923439792
    },
    {
      "type": "training",
      "description": "Training step 3770",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:21",
      "total_flops_so_far": 8.95406281065336e+16,
      "budget_used_percent": 89.5406281065336
    },
    {
      "type": "training",
      "description": "Training step 3771",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:22",
      "total_flops_so_far": 8.956436697866928e+16,
      "budget_used_percent": 89.56436697866928
    },
    {
      "type": "training",
      "description": "Training step 3772",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:24",
      "total_flops_so_far": 8.958810585080496e+16,
      "budget_used_percent": 89.58810585080495
    },
    {
      "type": "training",
      "description": "Training step 3773",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:25",
      "total_flops_so_far": 8.961184472294064e+16,
      "budget_used_percent": 89.61184472294063
    },
    {
      "type": "training",
      "description": "Training step 3774",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:26",
      "total_flops_so_far": 8.963558359507632e+16,
      "budget_used_percent": 89.63558359507631
    },
    {
      "type": "training",
      "description": "Training step 3775",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:28",
      "total_flops_so_far": 8.9659322467212e+16,
      "budget_used_percent": 89.659322467212
    },
    {
      "type": "training",
      "description": "Training step 3776",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:29",
      "total_flops_so_far": 8.968306133934768e+16,
      "budget_used_percent": 89.68306133934767
    },
    {
      "type": "training",
      "description": "Training step 3777",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:30",
      "total_flops_so_far": 8.970680021148336e+16,
      "budget_used_percent": 89.70680021148337
    },
    {
      "type": "training",
      "description": "Training step 3778",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:32",
      "total_flops_so_far": 8.973053908361904e+16,
      "budget_used_percent": 89.73053908361905
    },
    {
      "type": "training",
      "description": "Training step 3779",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:33",
      "total_flops_so_far": 8.975427795575472e+16,
      "budget_used_percent": 89.75427795575473
    },
    {
      "type": "training",
      "description": "Training step 3780",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:35",
      "total_flops_so_far": 8.97780168278904e+16,
      "budget_used_percent": 89.7780168278904
    },
    {
      "type": "training",
      "description": "Training step 3781",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:36",
      "total_flops_so_far": 8.980175570002608e+16,
      "budget_used_percent": 89.80175570002608
    },
    {
      "type": "training",
      "description": "Training step 3782",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:37",
      "total_flops_so_far": 8.982549457216176e+16,
      "budget_used_percent": 89.82549457216176
    },
    {
      "type": "training",
      "description": "Training step 3783",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:39",
      "total_flops_so_far": 8.984923344429744e+16,
      "budget_used_percent": 89.84923344429744
    },
    {
      "type": "training",
      "description": "Training step 3784",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:40",
      "total_flops_so_far": 8.987297231643312e+16,
      "budget_used_percent": 89.87297231643312
    },
    {
      "type": "training",
      "description": "Training step 3785",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:41",
      "total_flops_so_far": 8.98967111885688e+16,
      "budget_used_percent": 89.8967111885688
    },
    {
      "type": "training",
      "description": "Training step 3786",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:43",
      "total_flops_so_far": 8.992045006070448e+16,
      "budget_used_percent": 89.92045006070448
    },
    {
      "type": "training",
      "description": "Training step 3787",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:44",
      "total_flops_so_far": 8.994418893284016e+16,
      "budget_used_percent": 89.94418893284016
    },
    {
      "type": "training",
      "description": "Training step 3788",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:45",
      "total_flops_so_far": 8.996792780497584e+16,
      "budget_used_percent": 89.96792780497584
    },
    {
      "type": "training",
      "description": "Training step 3789",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:47",
      "total_flops_so_far": 8.999166667711152e+16,
      "budget_used_percent": 89.99166667711151
    },
    {
      "type": "training",
      "description": "Training step 3790",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7912957378560.0,
      "backward_flops": 15825914757120.0,
      "flops": 23738872135680.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:48",
      "total_flops_so_far": 9.00154055492472e+16,
      "budget_used_percent": 90.01540554924719
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 709803614656.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:54",
      "total_flops_so_far": 9.001611535286186e+16,
      "budget_used_percent": 90.01611535286186
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 713504058416.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:57:59",
      "total_flops_so_far": 9.001682885692027e+16,
      "budget_used_percent": 90.01682885692027
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711653476344.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:58:04",
      "total_flops_so_far": 9.00175405103966e+16,
      "budget_used_percent": 90.01754051039661
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 709803614656.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:58:10",
      "total_flops_so_far": 9.001825031401126e+16,
      "budget_used_percent": 90.01825031401125
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712578677332.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:58:14",
      "total_flops_so_far": 9.001896289268859e+16,
      "budget_used_percent": 90.01896289268859
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 5",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 709803614656.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:58:19",
      "total_flops_so_far": 9.001967269630325e+16,
      "budget_used_percent": 90.01967269630325
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 6",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711653476344.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:58:24",
      "total_flops_so_far": 9.002038434977958e+16,
      "budget_used_percent": 90.02038434977958
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 7",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711653476344.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:58:29",
      "total_flops_so_far": 9.002109600325594e+16,
      "budget_used_percent": 90.02109600325593
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 8",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711653476344.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:58:35",
      "total_flops_so_far": 9.002180765673229e+16,
      "budget_used_percent": 90.02180765673229
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 9",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 711653476344.0,
      "lora_r": 2,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-19 20:58:40",
      "total_flops_so_far": 9.002251931020864e+16,
      "budget_used_percent": 90.02251931020864
    }
  ],
  "total_flops": 9.002251931020864e+16,
  "budget_used_percent": 90.02251931020864
}