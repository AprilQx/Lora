{
  "experiment_name": "lr1e-04_rank8",
  "model_config": {
    "hidden_size": 896,
    "num_attention_heads": 14,
    "num_hidden_layers": 24,
    "intermediate_size": 4864,
    "head_dim": 64,
    "vocab_size": 151936,
    "lora_r": 8,
    "lora_target_modules": [
      "q_proj",
      "v_proj"
    ]
  },
  "max_budget": 1e+17,
  "start_time": "2025-03-20 07:10:06",
  "operations": [
    {
      "type": "training",
      "description": "Training step 0",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:09",
      "total_flops_so_far": 23764232208384.0,
      "budget_used_percent": 0.023764232208384
    },
    {
      "type": "training",
      "description": "Training step 1",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:10",
      "total_flops_so_far": 47528464416768.0,
      "budget_used_percent": 0.047528464416768
    },
    {
      "type": "training",
      "description": "Training step 2",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:12",
      "total_flops_so_far": 71292696625152.0,
      "budget_used_percent": 0.071292696625152
    },
    {
      "type": "training",
      "description": "Training step 3",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:14",
      "total_flops_so_far": 95056928833536.0,
      "budget_used_percent": 0.095056928833536
    },
    {
      "type": "training",
      "description": "Training step 4",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:15",
      "total_flops_so_far": 118821161041920.0,
      "budget_used_percent": 0.11882116104192
    },
    {
      "type": "training",
      "description": "Training step 5",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:16",
      "total_flops_so_far": 142585393250304.0,
      "budget_used_percent": 0.142585393250304
    },
    {
      "type": "training",
      "description": "Training step 6",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:17",
      "total_flops_so_far": 166349625458688.0,
      "budget_used_percent": 0.166349625458688
    },
    {
      "type": "training",
      "description": "Training step 7",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:19",
      "total_flops_so_far": 190113857667072.0,
      "budget_used_percent": 0.190113857667072
    },
    {
      "type": "training",
      "description": "Training step 8",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:20",
      "total_flops_so_far": 213878089875456.0,
      "budget_used_percent": 0.21387808987545603
    },
    {
      "type": "training",
      "description": "Training step 9",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:21",
      "total_flops_so_far": 237642322083840.0,
      "budget_used_percent": 0.23764232208384
    },
    {
      "type": "training",
      "description": "Training step 10",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:23",
      "total_flops_so_far": 261406554292224.0,
      "budget_used_percent": 0.26140655429222404
    },
    {
      "type": "training",
      "description": "Training step 11",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:24",
      "total_flops_so_far": 285170786500608.0,
      "budget_used_percent": 0.285170786500608
    },
    {
      "type": "training",
      "description": "Training step 12",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:25",
      "total_flops_so_far": 308935018708992.0,
      "budget_used_percent": 0.30893501870899204
    },
    {
      "type": "training",
      "description": "Training step 13",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:26",
      "total_flops_so_far": 332699250917376.0,
      "budget_used_percent": 0.332699250917376
    },
    {
      "type": "training",
      "description": "Training step 14",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:28",
      "total_flops_so_far": 356463483125760.0,
      "budget_used_percent": 0.35646348312576004
    },
    {
      "type": "training",
      "description": "Training step 15",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:29",
      "total_flops_so_far": 380227715334144.0,
      "budget_used_percent": 0.380227715334144
    },
    {
      "type": "training",
      "description": "Training step 16",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:30",
      "total_flops_so_far": 403991947542528.0,
      "budget_used_percent": 0.40399194754252804
    },
    {
      "type": "training",
      "description": "Training step 17",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:32",
      "total_flops_so_far": 427756179750912.0,
      "budget_used_percent": 0.42775617975091207
    },
    {
      "type": "training",
      "description": "Training step 18",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:33",
      "total_flops_so_far": 451520411959296.0,
      "budget_used_percent": 0.451520411959296
    },
    {
      "type": "training",
      "description": "Training step 19",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:34",
      "total_flops_so_far": 475284644167680.0,
      "budget_used_percent": 0.47528464416768
    },
    {
      "type": "training",
      "description": "Training step 20",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:35",
      "total_flops_so_far": 499048876376064.0,
      "budget_used_percent": 0.49904887637606404
    },
    {
      "type": "training",
      "description": "Training step 21",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:37",
      "total_flops_so_far": 522813108584448.0,
      "budget_used_percent": 0.5228131085844481
    },
    {
      "type": "training",
      "description": "Training step 22",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:38",
      "total_flops_so_far": 546577340792832.0,
      "budget_used_percent": 0.5465773407928319
    },
    {
      "type": "training",
      "description": "Training step 23",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:39",
      "total_flops_so_far": 570341573001216.0,
      "budget_used_percent": 0.570341573001216
    },
    {
      "type": "training",
      "description": "Training step 24",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:41",
      "total_flops_so_far": 594105805209600.0,
      "budget_used_percent": 0.5941058052096
    },
    {
      "type": "training",
      "description": "Training step 25",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:42",
      "total_flops_so_far": 617870037417984.0,
      "budget_used_percent": 0.6178700374179841
    },
    {
      "type": "training",
      "description": "Training step 26",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:43",
      "total_flops_so_far": 641634269626368.0,
      "budget_used_percent": 0.6416342696263679
    },
    {
      "type": "training",
      "description": "Training step 27",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:44",
      "total_flops_so_far": 665398501834752.0,
      "budget_used_percent": 0.665398501834752
    },
    {
      "type": "training",
      "description": "Training step 28",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:46",
      "total_flops_so_far": 689162734043136.0,
      "budget_used_percent": 0.689162734043136
    },
    {
      "type": "training",
      "description": "Training step 29",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:47",
      "total_flops_so_far": 712926966251520.0,
      "budget_used_percent": 0.7129269662515201
    },
    {
      "type": "training",
      "description": "Training step 30",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:48",
      "total_flops_so_far": 736691198459904.0,
      "budget_used_percent": 0.7366911984599039
    },
    {
      "type": "training",
      "description": "Training step 31",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:49",
      "total_flops_so_far": 760455430668288.0,
      "budget_used_percent": 0.760455430668288
    },
    {
      "type": "training",
      "description": "Training step 32",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:51",
      "total_flops_so_far": 784219662876672.0,
      "budget_used_percent": 0.784219662876672
    },
    {
      "type": "training",
      "description": "Training step 33",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:52",
      "total_flops_so_far": 807983895085056.0,
      "budget_used_percent": 0.8079838950850561
    },
    {
      "type": "training",
      "description": "Training step 34",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:53",
      "total_flops_so_far": 831748127293440.0,
      "budget_used_percent": 0.83174812729344
    },
    {
      "type": "training",
      "description": "Training step 35",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:55",
      "total_flops_so_far": 855512359501824.0,
      "budget_used_percent": 0.8555123595018241
    },
    {
      "type": "training",
      "description": "Training step 36",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:56",
      "total_flops_so_far": 879276591710208.0,
      "budget_used_percent": 0.8792765917102079
    },
    {
      "type": "training",
      "description": "Training step 37",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:57",
      "total_flops_so_far": 903040823918592.0,
      "budget_used_percent": 0.903040823918592
    },
    {
      "type": "training",
      "description": "Training step 38",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:10:58",
      "total_flops_so_far": 926805056126976.0,
      "budget_used_percent": 0.926805056126976
    },
    {
      "type": "training",
      "description": "Training step 39",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:00",
      "total_flops_so_far": 950569288335360.0,
      "budget_used_percent": 0.95056928833536
    },
    {
      "type": "training",
      "description": "Training step 40",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:01",
      "total_flops_so_far": 974333520543744.0,
      "budget_used_percent": 0.974333520543744
    },
    {
      "type": "training",
      "description": "Training step 41",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:02",
      "total_flops_so_far": 998097752752128.0,
      "budget_used_percent": 0.9980977527521281
    },
    {
      "type": "training",
      "description": "Training step 42",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:04",
      "total_flops_so_far": 1021861984960512.0,
      "budget_used_percent": 1.0218619849605122
    },
    {
      "type": "training",
      "description": "Training step 43",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:05",
      "total_flops_so_far": 1045626217168896.0,
      "budget_used_percent": 1.0456262171688961
    },
    {
      "type": "training",
      "description": "Training step 44",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:06",
      "total_flops_so_far": 1069390449377280.0,
      "budget_used_percent": 1.06939044937728
    },
    {
      "type": "training",
      "description": "Training step 45",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:07",
      "total_flops_so_far": 1093154681585664.0,
      "budget_used_percent": 1.0931546815856639
    },
    {
      "type": "training",
      "description": "Training step 46",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:09",
      "total_flops_so_far": 1116918913794048.0,
      "budget_used_percent": 1.116918913794048
    },
    {
      "type": "training",
      "description": "Training step 47",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:10",
      "total_flops_so_far": 1140683146002432.0,
      "budget_used_percent": 1.140683146002432
    },
    {
      "type": "training",
      "description": "Training step 48",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:11",
      "total_flops_so_far": 1164447378210816.0,
      "budget_used_percent": 1.164447378210816
    },
    {
      "type": "training",
      "description": "Training step 49",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:13",
      "total_flops_so_far": 1188211610419200.0,
      "budget_used_percent": 1.1882116104192
    },
    {
      "type": "training",
      "description": "Training step 50",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:14",
      "total_flops_so_far": 1211975842627584.0,
      "budget_used_percent": 1.211975842627584
    },
    {
      "type": "training",
      "description": "Training step 51",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:15",
      "total_flops_so_far": 1235740074835968.0,
      "budget_used_percent": 1.2357400748359681
    },
    {
      "type": "training",
      "description": "Training step 52",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:16",
      "total_flops_so_far": 1259504307044352.0,
      "budget_used_percent": 1.259504307044352
    },
    {
      "type": "training",
      "description": "Training step 53",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:18",
      "total_flops_so_far": 1283268539252736.0,
      "budget_used_percent": 1.2832685392527359
    },
    {
      "type": "training",
      "description": "Training step 54",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:19",
      "total_flops_so_far": 1307032771461120.0,
      "budget_used_percent": 1.30703277146112
    },
    {
      "type": "training",
      "description": "Training step 55",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:20",
      "total_flops_so_far": 1330797003669504.0,
      "budget_used_percent": 1.330797003669504
    },
    {
      "type": "training",
      "description": "Training step 56",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:21",
      "total_flops_so_far": 1354561235877888.0,
      "budget_used_percent": 1.354561235877888
    },
    {
      "type": "training",
      "description": "Training step 57",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:23",
      "total_flops_so_far": 1378325468086272.0,
      "budget_used_percent": 1.378325468086272
    },
    {
      "type": "training",
      "description": "Training step 58",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:24",
      "total_flops_so_far": 1402089700294656.0,
      "budget_used_percent": 1.402089700294656
    },
    {
      "type": "training",
      "description": "Training step 59",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:25",
      "total_flops_so_far": 1425853932503040.0,
      "budget_used_percent": 1.4258539325030402
    },
    {
      "type": "training",
      "description": "Training step 60",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:27",
      "total_flops_so_far": 1449618164711424.0,
      "budget_used_percent": 1.449618164711424
    },
    {
      "type": "training",
      "description": "Training step 61",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:28",
      "total_flops_so_far": 1473382396919808.0,
      "budget_used_percent": 1.4733823969198079
    },
    {
      "type": "training",
      "description": "Training step 62",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:29",
      "total_flops_so_far": 1497146629128192.0,
      "budget_used_percent": 1.497146629128192
    },
    {
      "type": "training",
      "description": "Training step 63",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:30",
      "total_flops_so_far": 1520910861336576.0,
      "budget_used_percent": 1.520910861336576
    },
    {
      "type": "training",
      "description": "Training step 64",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:32",
      "total_flops_so_far": 1544675093544960.0,
      "budget_used_percent": 1.54467509354496
    },
    {
      "type": "training",
      "description": "Training step 65",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:33",
      "total_flops_so_far": 1568439325753344.0,
      "budget_used_percent": 1.568439325753344
    },
    {
      "type": "training",
      "description": "Training step 66",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:34",
      "total_flops_so_far": 1592203557961728.0,
      "budget_used_percent": 1.592203557961728
    },
    {
      "type": "training",
      "description": "Training step 67",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:36",
      "total_flops_so_far": 1615967790170112.0,
      "budget_used_percent": 1.6159677901701122
    },
    {
      "type": "training",
      "description": "Training step 68",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:37",
      "total_flops_so_far": 1639732022378496.0,
      "budget_used_percent": 1.639732022378496
    },
    {
      "type": "training",
      "description": "Training step 69",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:38",
      "total_flops_so_far": 1663496254586880.0,
      "budget_used_percent": 1.66349625458688
    },
    {
      "type": "training",
      "description": "Training step 70",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:39",
      "total_flops_so_far": 1687260486795264.0,
      "budget_used_percent": 1.6872604867952639
    },
    {
      "type": "training",
      "description": "Training step 71",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:41",
      "total_flops_so_far": 1711024719003648.0,
      "budget_used_percent": 1.7110247190036483
    },
    {
      "type": "training",
      "description": "Training step 72",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:42",
      "total_flops_so_far": 1734788951212032.0,
      "budget_used_percent": 1.734788951212032
    },
    {
      "type": "training",
      "description": "Training step 73",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:43",
      "total_flops_so_far": 1758553183420416.0,
      "budget_used_percent": 1.7585531834204158
    },
    {
      "type": "training",
      "description": "Training step 74",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:45",
      "total_flops_so_far": 1782317415628800.0,
      "budget_used_percent": 1.7823174156288
    },
    {
      "type": "training",
      "description": "Training step 75",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:46",
      "total_flops_so_far": 1806081647837184.0,
      "budget_used_percent": 1.806081647837184
    },
    {
      "type": "training",
      "description": "Training step 76",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:47",
      "total_flops_so_far": 1829845880045568.0,
      "budget_used_percent": 1.8298458800455681
    },
    {
      "type": "training",
      "description": "Training step 77",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:48",
      "total_flops_so_far": 1853610112253952.0,
      "budget_used_percent": 1.853610112253952
    },
    {
      "type": "training",
      "description": "Training step 78",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:50",
      "total_flops_so_far": 1877374344462336.0,
      "budget_used_percent": 1.877374344462336
    },
    {
      "type": "training",
      "description": "Training step 79",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:51",
      "total_flops_so_far": 1901138576670720.0,
      "budget_used_percent": 1.90113857667072
    },
    {
      "type": "training",
      "description": "Training step 80",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:52",
      "total_flops_so_far": 1924902808879104.0,
      "budget_used_percent": 1.9249028088791038
    },
    {
      "type": "training",
      "description": "Training step 81",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:54",
      "total_flops_so_far": 1948667041087488.0,
      "budget_used_percent": 1.948667041087488
    },
    {
      "type": "training",
      "description": "Training step 82",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:55",
      "total_flops_so_far": 1972431273295872.0,
      "budget_used_percent": 1.972431273295872
    },
    {
      "type": "training",
      "description": "Training step 83",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:56",
      "total_flops_so_far": 1996195505504256.0,
      "budget_used_percent": 1.9961955055042562
    },
    {
      "type": "training",
      "description": "Training step 84",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:57",
      "total_flops_so_far": 2019959737712640.0,
      "budget_used_percent": 2.01995973771264
    },
    {
      "type": "training",
      "description": "Training step 85",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:11:59",
      "total_flops_so_far": 2043723969921024.0,
      "budget_used_percent": 2.0437239699210243
    },
    {
      "type": "training",
      "description": "Training step 86",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:00",
      "total_flops_so_far": 2067488202129408.0,
      "budget_used_percent": 2.067488202129408
    },
    {
      "type": "training",
      "description": "Training step 87",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:01",
      "total_flops_so_far": 2091252434337792.0,
      "budget_used_percent": 2.0912524343377923
    },
    {
      "type": "training",
      "description": "Training step 88",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:02",
      "total_flops_so_far": 2115016666546176.0,
      "budget_used_percent": 2.115016666546176
    },
    {
      "type": "training",
      "description": "Training step 89",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:04",
      "total_flops_so_far": 2138780898754560.0,
      "budget_used_percent": 2.13878089875456
    },
    {
      "type": "training",
      "description": "Training step 90",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:05",
      "total_flops_so_far": 2162545130962944.0,
      "budget_used_percent": 2.162545130962944
    },
    {
      "type": "training",
      "description": "Training step 91",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:06",
      "total_flops_so_far": 2186309363171328.0,
      "budget_used_percent": 2.1863093631713277
    },
    {
      "type": "training",
      "description": "Training step 92",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:08",
      "total_flops_so_far": 2210073595379712.0,
      "budget_used_percent": 2.210073595379712
    },
    {
      "type": "training",
      "description": "Training step 93",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:09",
      "total_flops_so_far": 2233837827588096.0,
      "budget_used_percent": 2.233837827588096
    },
    {
      "type": "training",
      "description": "Training step 94",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:10",
      "total_flops_so_far": 2257602059796480.0,
      "budget_used_percent": 2.25760205979648
    },
    {
      "type": "training",
      "description": "Training step 95",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:11",
      "total_flops_so_far": 2281366292004864.0,
      "budget_used_percent": 2.281366292004864
    },
    {
      "type": "training",
      "description": "Training step 96",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:13",
      "total_flops_so_far": 2305130524213248.0,
      "budget_used_percent": 2.305130524213248
    },
    {
      "type": "training",
      "description": "Training step 97",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:14",
      "total_flops_so_far": 2328894756421632.0,
      "budget_used_percent": 2.328894756421632
    },
    {
      "type": "training",
      "description": "Training step 98",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:15",
      "total_flops_so_far": 2352658988630016.0,
      "budget_used_percent": 2.352658988630016
    },
    {
      "type": "training",
      "description": "Training step 99",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:17",
      "total_flops_so_far": 2376423220838400.0,
      "budget_used_percent": 2.3764232208384
    },
    {
      "type": "training",
      "description": "Training step 100",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:18",
      "total_flops_so_far": 2400187453046784.0,
      "budget_used_percent": 2.400187453046784
    },
    {
      "type": "training",
      "description": "Training step 101",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:19",
      "total_flops_so_far": 2423951685255168.0,
      "budget_used_percent": 2.423951685255168
    },
    {
      "type": "training",
      "description": "Training step 102",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:20",
      "total_flops_so_far": 2447715917463552.0,
      "budget_used_percent": 2.447715917463552
    },
    {
      "type": "training",
      "description": "Training step 103",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:22",
      "total_flops_so_far": 2471480149671936.0,
      "budget_used_percent": 2.4714801496719363
    },
    {
      "type": "training",
      "description": "Training step 104",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:23",
      "total_flops_so_far": 2495244381880320.0,
      "budget_used_percent": 2.49524438188032
    },
    {
      "type": "training",
      "description": "Training step 105",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:24",
      "total_flops_so_far": 2519008614088704.0,
      "budget_used_percent": 2.519008614088704
    },
    {
      "type": "training",
      "description": "Training step 106",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:26",
      "total_flops_so_far": 2542772846297088.0,
      "budget_used_percent": 2.542772846297088
    },
    {
      "type": "training",
      "description": "Training step 107",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:27",
      "total_flops_so_far": 2566537078505472.0,
      "budget_used_percent": 2.5665370785054717
    },
    {
      "type": "training",
      "description": "Training step 108",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:28",
      "total_flops_so_far": 2590301310713856.0,
      "budget_used_percent": 2.590301310713856
    },
    {
      "type": "training",
      "description": "Training step 109",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:29",
      "total_flops_so_far": 2614065542922240.0,
      "budget_used_percent": 2.61406554292224
    },
    {
      "type": "training",
      "description": "Training step 110",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:31",
      "total_flops_so_far": 2637829775130624.0,
      "budget_used_percent": 2.637829775130624
    },
    {
      "type": "training",
      "description": "Training step 111",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:32",
      "total_flops_so_far": 2661594007339008.0,
      "budget_used_percent": 2.661594007339008
    },
    {
      "type": "training",
      "description": "Training step 112",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:33",
      "total_flops_so_far": 2685358239547392.0,
      "budget_used_percent": 2.6853582395473916
    },
    {
      "type": "training",
      "description": "Training step 113",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:35",
      "total_flops_so_far": 2709122471755776.0,
      "budget_used_percent": 2.709122471755776
    },
    {
      "type": "training",
      "description": "Training step 114",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:36",
      "total_flops_so_far": 2732886703964160.0,
      "budget_used_percent": 2.73288670396416
    },
    {
      "type": "training",
      "description": "Training step 115",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:37",
      "total_flops_so_far": 2756650936172544.0,
      "budget_used_percent": 2.756650936172544
    },
    {
      "type": "training",
      "description": "Training step 116",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:38",
      "total_flops_so_far": 2780415168380928.0,
      "budget_used_percent": 2.780415168380928
    },
    {
      "type": "training",
      "description": "Training step 117",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:40",
      "total_flops_so_far": 2804179400589312.0,
      "budget_used_percent": 2.804179400589312
    },
    {
      "type": "training",
      "description": "Training step 118",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:41",
      "total_flops_so_far": 2827943632797696.0,
      "budget_used_percent": 2.827943632797696
    },
    {
      "type": "training",
      "description": "Training step 119",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:42",
      "total_flops_so_far": 2851707865006080.0,
      "budget_used_percent": 2.8517078650060803
    },
    {
      "type": "training",
      "description": "Training step 120",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:43",
      "total_flops_so_far": 2875472097214464.0,
      "budget_used_percent": 2.875472097214464
    },
    {
      "type": "training",
      "description": "Training step 121",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:45",
      "total_flops_so_far": 2899236329422848.0,
      "budget_used_percent": 2.899236329422848
    },
    {
      "type": "training",
      "description": "Training step 122",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:46",
      "total_flops_so_far": 2923000561631232.0,
      "budget_used_percent": 2.9230005616312322
    },
    {
      "type": "training",
      "description": "Training step 123",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:47",
      "total_flops_so_far": 2946764793839616.0,
      "budget_used_percent": 2.9467647938396158
    },
    {
      "type": "training",
      "description": "Training step 124",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:49",
      "total_flops_so_far": 2970529026048000.0,
      "budget_used_percent": 2.970529026048
    },
    {
      "type": "training",
      "description": "Training step 125",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:50",
      "total_flops_so_far": 2994293258256384.0,
      "budget_used_percent": 2.994293258256384
    },
    {
      "type": "training",
      "description": "Training step 126",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:51",
      "total_flops_so_far": 3018057490464768.0,
      "budget_used_percent": 3.018057490464768
    },
    {
      "type": "training",
      "description": "Training step 127",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:52",
      "total_flops_so_far": 3041821722673152.0,
      "budget_used_percent": 3.041821722673152
    },
    {
      "type": "training",
      "description": "Training step 128",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:54",
      "total_flops_so_far": 3065585954881536.0,
      "budget_used_percent": 3.0655859548815356
    },
    {
      "type": "training",
      "description": "Training step 129",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:55",
      "total_flops_so_far": 3089350187089920.0,
      "budget_used_percent": 3.08935018708992
    },
    {
      "type": "training",
      "description": "Training step 130",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:56",
      "total_flops_so_far": 3113114419298304.0,
      "budget_used_percent": 3.113114419298304
    },
    {
      "type": "training",
      "description": "Training step 131",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:58",
      "total_flops_so_far": 3136878651506688.0,
      "budget_used_percent": 3.136878651506688
    },
    {
      "type": "training",
      "description": "Training step 132",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:58",
      "total_flops_so_far": 3160642883715072.0,
      "budget_used_percent": 3.1606428837150724
    },
    {
      "type": "training",
      "description": "Training step 133",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:12:59",
      "total_flops_so_far": 3184407115923456.0,
      "budget_used_percent": 3.184407115923456
    },
    {
      "type": "training",
      "description": "Training step 134",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:01",
      "total_flops_so_far": 3208171348131840.0,
      "budget_used_percent": 3.20817134813184
    },
    {
      "type": "training",
      "description": "Training step 135",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:02",
      "total_flops_so_far": 3231935580340224.0,
      "budget_used_percent": 3.2319355803402243
    },
    {
      "type": "training",
      "description": "Training step 136",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:03",
      "total_flops_so_far": 3255699812548608.0,
      "budget_used_percent": 3.255699812548608
    },
    {
      "type": "training",
      "description": "Training step 137",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:04",
      "total_flops_so_far": 3279464044756992.0,
      "budget_used_percent": 3.279464044756992
    },
    {
      "type": "training",
      "description": "Training step 138",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:06",
      "total_flops_so_far": 3303228276965376.0,
      "budget_used_percent": 3.3032282769653762
    },
    {
      "type": "training",
      "description": "Training step 139",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:07",
      "total_flops_so_far": 3326992509173760.0,
      "budget_used_percent": 3.32699250917376
    },
    {
      "type": "training",
      "description": "Training step 140",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:08",
      "total_flops_so_far": 3350756741382144.0,
      "budget_used_percent": 3.3507567413821437
    },
    {
      "type": "training",
      "description": "Training step 141",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:09",
      "total_flops_so_far": 3374520973590528.0,
      "budget_used_percent": 3.3745209735905277
    },
    {
      "type": "training",
      "description": "Training step 142",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:11",
      "total_flops_so_far": 3398285205798912.0,
      "budget_used_percent": 3.398285205798912
    },
    {
      "type": "training",
      "description": "Training step 143",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:12",
      "total_flops_so_far": 3422049438007296.0,
      "budget_used_percent": 3.4220494380072966
    },
    {
      "type": "training",
      "description": "Training step 144",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:13",
      "total_flops_so_far": 3445813670215680.0,
      "budget_used_percent": 3.4458136702156796
    },
    {
      "type": "training",
      "description": "Training step 145",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:15",
      "total_flops_so_far": 3469577902424064.0,
      "budget_used_percent": 3.469577902424064
    },
    {
      "type": "training",
      "description": "Training step 146",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:16",
      "total_flops_so_far": 3493342134632448.0,
      "budget_used_percent": 3.493342134632448
    },
    {
      "type": "training",
      "description": "Training step 147",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:17",
      "total_flops_so_far": 3517106366840832.0,
      "budget_used_percent": 3.5171063668408316
    },
    {
      "type": "training",
      "description": "Training step 148",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:18",
      "total_flops_so_far": 3540870599049216.0,
      "budget_used_percent": 3.540870599049216
    },
    {
      "type": "training",
      "description": "Training step 149",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:20",
      "total_flops_so_far": 3564634831257600.0,
      "budget_used_percent": 3.5646348312576
    },
    {
      "type": "training",
      "description": "Training step 150",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:21",
      "total_flops_so_far": 3588399063465984.0,
      "budget_used_percent": 3.5883990634659844
    },
    {
      "type": "training",
      "description": "Training step 151",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:22",
      "total_flops_so_far": 3612163295674368.0,
      "budget_used_percent": 3.612163295674368
    },
    {
      "type": "training",
      "description": "Training step 152",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:24",
      "total_flops_so_far": 3635927527882752.0,
      "budget_used_percent": 3.635927527882752
    },
    {
      "type": "training",
      "description": "Training step 153",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:25",
      "total_flops_so_far": 3659691760091136.0,
      "budget_used_percent": 3.6596917600911363
    },
    {
      "type": "training",
      "description": "Training step 154",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:26",
      "total_flops_so_far": 3683455992299520.0,
      "budget_used_percent": 3.68345599229952
    },
    {
      "type": "training",
      "description": "Training step 155",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:27",
      "total_flops_so_far": 3707220224507904.0,
      "budget_used_percent": 3.707220224507904
    },
    {
      "type": "training",
      "description": "Training step 156",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:29",
      "total_flops_so_far": 3730984456716288.0,
      "budget_used_percent": 3.730984456716288
    },
    {
      "type": "training",
      "description": "Training step 157",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:30",
      "total_flops_so_far": 3754748688924672.0,
      "budget_used_percent": 3.754748688924672
    },
    {
      "type": "training",
      "description": "Training step 158",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:31",
      "total_flops_so_far": 3778512921133056.0,
      "budget_used_percent": 3.7785129211330557
    },
    {
      "type": "training",
      "description": "Training step 159",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:33",
      "total_flops_so_far": 3802277153341440.0,
      "budget_used_percent": 3.80227715334144
    },
    {
      "type": "training",
      "description": "Training step 160",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:34",
      "total_flops_so_far": 3826041385549824.0,
      "budget_used_percent": 3.826041385549824
    },
    {
      "type": "training",
      "description": "Training step 161",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:35",
      "total_flops_so_far": 3849805617758208.0,
      "budget_used_percent": 3.8498056177582076
    },
    {
      "type": "training",
      "description": "Training step 162",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:36",
      "total_flops_so_far": 3873569849966592.0,
      "budget_used_percent": 3.873569849966592
    },
    {
      "type": "training",
      "description": "Training step 163",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:38",
      "total_flops_so_far": 3897334082174976.0,
      "budget_used_percent": 3.897334082174976
    },
    {
      "type": "training",
      "description": "Training step 164",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:39",
      "total_flops_so_far": 3921098314383360.0,
      "budget_used_percent": 3.9210983143833604
    },
    {
      "type": "training",
      "description": "Training step 165",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:40",
      "total_flops_so_far": 3944862546591744.0,
      "budget_used_percent": 3.944862546591744
    },
    {
      "type": "training",
      "description": "Training step 166",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:42",
      "total_flops_so_far": 3968626778800128.0,
      "budget_used_percent": 3.968626778800128
    },
    {
      "type": "training",
      "description": "Training step 167",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:43",
      "total_flops_so_far": 3992391011008512.0,
      "budget_used_percent": 3.9923910110085123
    },
    {
      "type": "training",
      "description": "Training step 168",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:44",
      "total_flops_so_far": 4016155243216896.0,
      "budget_used_percent": 4.016155243216896
    },
    {
      "type": "training",
      "description": "Training step 169",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:45",
      "total_flops_so_far": 4039919475425280.0,
      "budget_used_percent": 4.03991947542528
    },
    {
      "type": "training",
      "description": "Training step 170",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:47",
      "total_flops_so_far": 4063683707633664.0,
      "budget_used_percent": 4.063683707633664
    },
    {
      "type": "training",
      "description": "Training step 171",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:48",
      "total_flops_so_far": 4087447939842048.0,
      "budget_used_percent": 4.087447939842049
    },
    {
      "type": "training",
      "description": "Training step 172",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:49",
      "total_flops_so_far": 4111212172050432.0,
      "budget_used_percent": 4.111212172050432
    },
    {
      "type": "training",
      "description": "Training step 173",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:51",
      "total_flops_so_far": 4134976404258816.0,
      "budget_used_percent": 4.134976404258816
    },
    {
      "type": "training",
      "description": "Training step 174",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:52",
      "total_flops_so_far": 4158740636467200.0,
      "budget_used_percent": 4.1587406364672
    },
    {
      "type": "training",
      "description": "Training step 175",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:53",
      "total_flops_so_far": 4182504868675584.0,
      "budget_used_percent": 4.182504868675585
    },
    {
      "type": "training",
      "description": "Training step 176",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:54",
      "total_flops_so_far": 4206269100883968.0,
      "budget_used_percent": 4.206269100883968
    },
    {
      "type": "training",
      "description": "Training step 177",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:56",
      "total_flops_so_far": 4230033333092352.0,
      "budget_used_percent": 4.230033333092352
    },
    {
      "type": "training",
      "description": "Training step 178",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:57",
      "total_flops_so_far": 4253797565300736.0,
      "budget_used_percent": 4.2537975653007365
    },
    {
      "type": "training",
      "description": "Training step 179",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:13:58",
      "total_flops_so_far": 4277561797509120.0,
      "budget_used_percent": 4.27756179750912
    },
    {
      "type": "training",
      "description": "Training step 180",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:00",
      "total_flops_so_far": 4301326029717504.0,
      "budget_used_percent": 4.3013260297175036
    },
    {
      "type": "training",
      "description": "Training step 181",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:01",
      "total_flops_so_far": 4325090261925888.0,
      "budget_used_percent": 4.325090261925888
    },
    {
      "type": "training",
      "description": "Training step 182",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:02",
      "total_flops_so_far": 4348854494134272.0,
      "budget_used_percent": 4.348854494134272
    },
    {
      "type": "training",
      "description": "Training step 183",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:03",
      "total_flops_so_far": 4372618726342656.0,
      "budget_used_percent": 4.3726187263426555
    },
    {
      "type": "training",
      "description": "Training step 184",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:05",
      "total_flops_so_far": 4396382958551040.0,
      "budget_used_percent": 4.39638295855104
    },
    {
      "type": "training",
      "description": "Training step 185",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:06",
      "total_flops_so_far": 4420147190759424.0,
      "budget_used_percent": 4.420147190759424
    },
    {
      "type": "training",
      "description": "Training step 186",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:07",
      "total_flops_so_far": 4443911422967808.0,
      "budget_used_percent": 4.443911422967807
    },
    {
      "type": "training",
      "description": "Training step 187",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:09",
      "total_flops_so_far": 4467675655176192.0,
      "budget_used_percent": 4.467675655176192
    },
    {
      "type": "training",
      "description": "Training step 188",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:10",
      "total_flops_so_far": 4491439887384576.0,
      "budget_used_percent": 4.491439887384576
    },
    {
      "type": "training",
      "description": "Training step 189",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:11",
      "total_flops_so_far": 4515204119592960.0,
      "budget_used_percent": 4.51520411959296
    },
    {
      "type": "training",
      "description": "Training step 190",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:12",
      "total_flops_so_far": 4538968351801344.0,
      "budget_used_percent": 4.538968351801344
    },
    {
      "type": "training",
      "description": "Training step 191",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:14",
      "total_flops_so_far": 4562732584009728.0,
      "budget_used_percent": 4.562732584009728
    },
    {
      "type": "training",
      "description": "Training step 192",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:15",
      "total_flops_so_far": 4586496816218112.0,
      "budget_used_percent": 4.586496816218112
    },
    {
      "type": "training",
      "description": "Training step 193",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:16",
      "total_flops_so_far": 4610261048426496.0,
      "budget_used_percent": 4.610261048426496
    },
    {
      "type": "training",
      "description": "Training step 194",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:18",
      "total_flops_so_far": 4634025280634880.0,
      "budget_used_percent": 4.63402528063488
    },
    {
      "type": "training",
      "description": "Training step 195",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:19",
      "total_flops_so_far": 4657789512843264.0,
      "budget_used_percent": 4.657789512843264
    },
    {
      "type": "training",
      "description": "Training step 196",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:20",
      "total_flops_so_far": 4681553745051648.0,
      "budget_used_percent": 4.681553745051648
    },
    {
      "type": "training",
      "description": "Training step 197",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:21",
      "total_flops_so_far": 4705317977260032.0,
      "budget_used_percent": 4.705317977260032
    },
    {
      "type": "training",
      "description": "Training step 198",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:23",
      "total_flops_so_far": 4729082209468416.0,
      "budget_used_percent": 4.729082209468416
    },
    {
      "type": "training",
      "description": "Training step 199",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:24",
      "total_flops_so_far": 4752846441676800.0,
      "budget_used_percent": 4.7528464416768
    },
    {
      "type": "training",
      "description": "Training step 200",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:25",
      "total_flops_so_far": 4776610673885184.0,
      "budget_used_percent": 4.776610673885184
    },
    {
      "type": "training",
      "description": "Training step 201",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:26",
      "total_flops_so_far": 4800374906093568.0,
      "budget_used_percent": 4.800374906093568
    },
    {
      "type": "training",
      "description": "Training step 202",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:28",
      "total_flops_so_far": 4824139138301952.0,
      "budget_used_percent": 4.824139138301952
    },
    {
      "type": "training",
      "description": "Training step 203",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:29",
      "total_flops_so_far": 4847903370510336.0,
      "budget_used_percent": 4.847903370510336
    },
    {
      "type": "training",
      "description": "Training step 204",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:30",
      "total_flops_so_far": 4871667602718720.0,
      "budget_used_percent": 4.87166760271872
    },
    {
      "type": "training",
      "description": "Training step 205",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:32",
      "total_flops_so_far": 4895431834927104.0,
      "budget_used_percent": 4.895431834927104
    },
    {
      "type": "training",
      "description": "Training step 206",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:33",
      "total_flops_so_far": 4919196067135488.0,
      "budget_used_percent": 4.919196067135488
    },
    {
      "type": "training",
      "description": "Training step 207",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:34",
      "total_flops_so_far": 4942960299343872.0,
      "budget_used_percent": 4.942960299343873
    },
    {
      "type": "training",
      "description": "Training step 208",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:35",
      "total_flops_so_far": 4966724531552256.0,
      "budget_used_percent": 4.966724531552256
    },
    {
      "type": "training",
      "description": "Training step 209",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:37",
      "total_flops_so_far": 4990488763760640.0,
      "budget_used_percent": 4.99048876376064
    },
    {
      "type": "training",
      "description": "Training step 210",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:38",
      "total_flops_so_far": 5014252995969024.0,
      "budget_used_percent": 5.0142529959690245
    },
    {
      "type": "training",
      "description": "Training step 211",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:39",
      "total_flops_so_far": 5038017228177408.0,
      "budget_used_percent": 5.038017228177408
    },
    {
      "type": "training",
      "description": "Training step 212",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:41",
      "total_flops_so_far": 5061781460385792.0,
      "budget_used_percent": 5.061781460385792
    },
    {
      "type": "training",
      "description": "Training step 213",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:42",
      "total_flops_so_far": 5085545692594176.0,
      "budget_used_percent": 5.085545692594176
    },
    {
      "type": "training",
      "description": "Training step 214",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:43",
      "total_flops_so_far": 5109309924802560.0,
      "budget_used_percent": 5.10930992480256
    },
    {
      "type": "training",
      "description": "Training step 215",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:44",
      "total_flops_so_far": 5133074157010944.0,
      "budget_used_percent": 5.1330741570109435
    },
    {
      "type": "training",
      "description": "Training step 216",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:46",
      "total_flops_so_far": 5156838389219328.0,
      "budget_used_percent": 5.156838389219328
    },
    {
      "type": "training",
      "description": "Training step 217",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:47",
      "total_flops_so_far": 5180602621427712.0,
      "budget_used_percent": 5.180602621427712
    },
    {
      "type": "training",
      "description": "Training step 218",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:48",
      "total_flops_so_far": 5204366853636096.0,
      "budget_used_percent": 5.204366853636095
    },
    {
      "type": "training",
      "description": "Training step 219",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:50",
      "total_flops_so_far": 5228131085844480.0,
      "budget_used_percent": 5.22813108584448
    },
    {
      "type": "training",
      "description": "Training step 220",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:51",
      "total_flops_so_far": 5251895318052864.0,
      "budget_used_percent": 5.251895318052864
    },
    {
      "type": "training",
      "description": "Training step 221",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:52",
      "total_flops_so_far": 5275659550261248.0,
      "budget_used_percent": 5.275659550261248
    },
    {
      "type": "training",
      "description": "Training step 222",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:53",
      "total_flops_so_far": 5299423782469632.0,
      "budget_used_percent": 5.299423782469632
    },
    {
      "type": "training",
      "description": "Training step 223",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:55",
      "total_flops_so_far": 5323188014678016.0,
      "budget_used_percent": 5.323188014678016
    },
    {
      "type": "training",
      "description": "Training step 224",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:56",
      "total_flops_so_far": 5346952246886400.0,
      "budget_used_percent": 5.3469522468864
    },
    {
      "type": "training",
      "description": "Training step 225",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:57",
      "total_flops_so_far": 5370716479094784.0,
      "budget_used_percent": 5.370716479094783
    },
    {
      "type": "training",
      "description": "Training step 226",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:14:59",
      "total_flops_so_far": 5394480711303168.0,
      "budget_used_percent": 5.394480711303168
    },
    {
      "type": "training",
      "description": "Training step 227",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:00",
      "total_flops_so_far": 5418244943511552.0,
      "budget_used_percent": 5.418244943511552
    },
    {
      "type": "training",
      "description": "Training step 228",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:01",
      "total_flops_so_far": 5442009175719936.0,
      "budget_used_percent": 5.442009175719936
    },
    {
      "type": "training",
      "description": "Training step 229",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:02",
      "total_flops_so_far": 5465773407928320.0,
      "budget_used_percent": 5.46577340792832
    },
    {
      "type": "training",
      "description": "Training step 230",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:04",
      "total_flops_so_far": 5489537640136704.0,
      "budget_used_percent": 5.489537640136704
    },
    {
      "type": "training",
      "description": "Training step 231",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:05",
      "total_flops_so_far": 5513301872345088.0,
      "budget_used_percent": 5.513301872345088
    },
    {
      "type": "training",
      "description": "Training step 232",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:06",
      "total_flops_so_far": 5537066104553472.0,
      "budget_used_percent": 5.537066104553472
    },
    {
      "type": "training",
      "description": "Training step 233",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:08",
      "total_flops_so_far": 5560830336761856.0,
      "budget_used_percent": 5.560830336761856
    },
    {
      "type": "training",
      "description": "Training step 234",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:09",
      "total_flops_so_far": 5584594568970240.0,
      "budget_used_percent": 5.58459456897024
    },
    {
      "type": "training",
      "description": "Training step 235",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:10",
      "total_flops_so_far": 5608358801178624.0,
      "budget_used_percent": 5.608358801178624
    },
    {
      "type": "training",
      "description": "Training step 236",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:11",
      "total_flops_so_far": 5632123033387008.0,
      "budget_used_percent": 5.632123033387008
    },
    {
      "type": "training",
      "description": "Training step 237",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:13",
      "total_flops_so_far": 5655887265595392.0,
      "budget_used_percent": 5.655887265595392
    },
    {
      "type": "training",
      "description": "Training step 238",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:14",
      "total_flops_so_far": 5679651497803776.0,
      "budget_used_percent": 5.679651497803776
    },
    {
      "type": "training",
      "description": "Training step 239",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:15",
      "total_flops_so_far": 5703415730012160.0,
      "budget_used_percent": 5.703415730012161
    },
    {
      "type": "training",
      "description": "Training step 240",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:17",
      "total_flops_so_far": 5727179962220544.0,
      "budget_used_percent": 5.727179962220544
    },
    {
      "type": "training",
      "description": "Training step 241",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:18",
      "total_flops_so_far": 5750944194428928.0,
      "budget_used_percent": 5.750944194428928
    },
    {
      "type": "training",
      "description": "Training step 242",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:19",
      "total_flops_so_far": 5774708426637312.0,
      "budget_used_percent": 5.7747084266373125
    },
    {
      "type": "training",
      "description": "Training step 243",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:20",
      "total_flops_so_far": 5798472658845696.0,
      "budget_used_percent": 5.798472658845696
    },
    {
      "type": "training",
      "description": "Training step 244",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:22",
      "total_flops_so_far": 5822236891054080.0,
      "budget_used_percent": 5.82223689105408
    },
    {
      "type": "training",
      "description": "Training step 245",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:23",
      "total_flops_so_far": 5846001123262464.0,
      "budget_used_percent": 5.8460011232624645
    },
    {
      "type": "training",
      "description": "Training step 246",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:24",
      "total_flops_so_far": 5869765355470848.0,
      "budget_used_percent": 5.869765355470848
    },
    {
      "type": "training",
      "description": "Training step 247",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:26",
      "total_flops_so_far": 5893529587679232.0,
      "budget_used_percent": 5.8935295876792315
    },
    {
      "type": "training",
      "description": "Training step 248",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:27",
      "total_flops_so_far": 5917293819887616.0,
      "budget_used_percent": 5.917293819887616
    },
    {
      "type": "training",
      "description": "Training step 249",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:28",
      "total_flops_so_far": 5941058052096000.0,
      "budget_used_percent": 5.941058052096
    },
    {
      "type": "training",
      "description": "Training step 250",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:29",
      "total_flops_so_far": 5964822284304384.0,
      "budget_used_percent": 5.964822284304383
    },
    {
      "type": "training",
      "description": "Training step 251",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:31",
      "total_flops_so_far": 5988586516512768.0,
      "budget_used_percent": 5.988586516512768
    },
    {
      "type": "training",
      "description": "Training step 252",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:32",
      "total_flops_so_far": 6012350748721152.0,
      "budget_used_percent": 6.012350748721152
    },
    {
      "type": "training",
      "description": "Training step 253",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:33",
      "total_flops_so_far": 6036114980929536.0,
      "budget_used_percent": 6.036114980929536
    },
    {
      "type": "training",
      "description": "Training step 254",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:35",
      "total_flops_so_far": 6059879213137920.0,
      "budget_used_percent": 6.05987921313792
    },
    {
      "type": "training",
      "description": "Training step 255",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:36",
      "total_flops_so_far": 6083643445346304.0,
      "budget_used_percent": 6.083643445346304
    },
    {
      "type": "training",
      "description": "Training step 256",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:37",
      "total_flops_so_far": 6107407677554688.0,
      "budget_used_percent": 6.107407677554688
    },
    {
      "type": "training",
      "description": "Training step 257",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:38",
      "total_flops_so_far": 6131171909763072.0,
      "budget_used_percent": 6.131171909763071
    },
    {
      "type": "training",
      "description": "Training step 258",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:40",
      "total_flops_so_far": 6154936141971456.0,
      "budget_used_percent": 6.154936141971456
    },
    {
      "type": "training",
      "description": "Training step 259",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:41",
      "total_flops_so_far": 6178700374179840.0,
      "budget_used_percent": 6.17870037417984
    },
    {
      "type": "training",
      "description": "Training step 260",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:42",
      "total_flops_so_far": 6202464606388224.0,
      "budget_used_percent": 6.202464606388224
    },
    {
      "type": "training",
      "description": "Training step 261",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:44",
      "total_flops_so_far": 6226228838596608.0,
      "budget_used_percent": 6.226228838596608
    },
    {
      "type": "training",
      "description": "Training step 262",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:45",
      "total_flops_so_far": 6249993070804992.0,
      "budget_used_percent": 6.249993070804992
    },
    {
      "type": "training",
      "description": "Training step 263",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:46",
      "total_flops_so_far": 6273757303013376.0,
      "budget_used_percent": 6.273757303013376
    },
    {
      "type": "training",
      "description": "Training step 264",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:47",
      "total_flops_so_far": 6297521535221760.0,
      "budget_used_percent": 6.29752153522176
    },
    {
      "type": "training",
      "description": "Training step 265",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:48",
      "total_flops_so_far": 6321285767430144.0,
      "budget_used_percent": 6.321285767430145
    },
    {
      "type": "training",
      "description": "Training step 266",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:49",
      "total_flops_so_far": 6345049999638528.0,
      "budget_used_percent": 6.345049999638527
    },
    {
      "type": "training",
      "description": "Training step 267",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:50",
      "total_flops_so_far": 6368814231846912.0,
      "budget_used_percent": 6.368814231846912
    },
    {
      "type": "training",
      "description": "Training step 268",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:52",
      "total_flops_so_far": 6392578464055296.0,
      "budget_used_percent": 6.392578464055296
    },
    {
      "type": "training",
      "description": "Training step 269",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:53",
      "total_flops_so_far": 6416342696263680.0,
      "budget_used_percent": 6.41634269626368
    },
    {
      "type": "training",
      "description": "Training step 270",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:54",
      "total_flops_so_far": 6440106928472064.0,
      "budget_used_percent": 6.440106928472064
    },
    {
      "type": "training",
      "description": "Training step 271",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:56",
      "total_flops_so_far": 6463871160680448.0,
      "budget_used_percent": 6.463871160680449
    },
    {
      "type": "training",
      "description": "Training step 272",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:57",
      "total_flops_so_far": 6487635392888832.0,
      "budget_used_percent": 6.487635392888833
    },
    {
      "type": "training",
      "description": "Training step 273",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:58",
      "total_flops_so_far": 6511399625097216.0,
      "budget_used_percent": 6.511399625097216
    },
    {
      "type": "training",
      "description": "Training step 274",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:15:59",
      "total_flops_so_far": 6535163857305600.0,
      "budget_used_percent": 6.5351638573056
    },
    {
      "type": "training",
      "description": "Training step 275",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:01",
      "total_flops_so_far": 6558928089513984.0,
      "budget_used_percent": 6.558928089513984
    },
    {
      "type": "training",
      "description": "Training step 276",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:02",
      "total_flops_so_far": 6582692321722368.0,
      "budget_used_percent": 6.582692321722368
    },
    {
      "type": "training",
      "description": "Training step 277",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:03",
      "total_flops_so_far": 6606456553930752.0,
      "budget_used_percent": 6.6064565539307525
    },
    {
      "type": "training",
      "description": "Training step 278",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:05",
      "total_flops_so_far": 6630220786139136.0,
      "budget_used_percent": 6.6302207861391365
    },
    {
      "type": "training",
      "description": "Training step 279",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:06",
      "total_flops_so_far": 6653985018347520.0,
      "budget_used_percent": 6.65398501834752
    },
    {
      "type": "training",
      "description": "Training step 280",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:07",
      "total_flops_so_far": 6677749250555904.0,
      "budget_used_percent": 6.677749250555904
    },
    {
      "type": "training",
      "description": "Training step 281",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:08",
      "total_flops_so_far": 6701513482764288.0,
      "budget_used_percent": 6.7015134827642875
    },
    {
      "type": "training",
      "description": "Training step 282",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:10",
      "total_flops_so_far": 6725277714972672.0,
      "budget_used_percent": 6.7252777149726715
    },
    {
      "type": "training",
      "description": "Training step 283",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:11",
      "total_flops_so_far": 6749041947181056.0,
      "budget_used_percent": 6.749041947181055
    },
    {
      "type": "training",
      "description": "Training step 284",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:12",
      "total_flops_so_far": 6772806179389440.0,
      "budget_used_percent": 6.77280617938944
    },
    {
      "type": "training",
      "description": "Training step 285",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:14",
      "total_flops_so_far": 6796570411597824.0,
      "budget_used_percent": 6.796570411597824
    },
    {
      "type": "training",
      "description": "Training step 286",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:15",
      "total_flops_so_far": 6820334643806208.0,
      "budget_used_percent": 6.820334643806208
    },
    {
      "type": "training",
      "description": "Training step 287",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:16",
      "total_flops_so_far": 6844098876014592.0,
      "budget_used_percent": 6.844098876014593
    },
    {
      "type": "training",
      "description": "Training step 288",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:17",
      "total_flops_so_far": 6867863108222976.0,
      "budget_used_percent": 6.867863108222975
    },
    {
      "type": "training",
      "description": "Training step 289",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:19",
      "total_flops_so_far": 6891627340431360.0,
      "budget_used_percent": 6.891627340431359
    },
    {
      "type": "training",
      "description": "Training step 290",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:20",
      "total_flops_so_far": 6915391572639744.0,
      "budget_used_percent": 6.915391572639744
    },
    {
      "type": "training",
      "description": "Training step 291",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:21",
      "total_flops_so_far": 6939155804848128.0,
      "budget_used_percent": 6.939155804848128
    },
    {
      "type": "training",
      "description": "Training step 292",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:23",
      "total_flops_so_far": 6962920037056512.0,
      "budget_used_percent": 6.962920037056512
    },
    {
      "type": "training",
      "description": "Training step 293",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:24",
      "total_flops_so_far": 6986684269264896.0,
      "budget_used_percent": 6.986684269264896
    },
    {
      "type": "training",
      "description": "Training step 294",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:25",
      "total_flops_so_far": 7010448501473280.0,
      "budget_used_percent": 7.010448501473281
    },
    {
      "type": "training",
      "description": "Training step 295",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:26",
      "total_flops_so_far": 7034212733681664.0,
      "budget_used_percent": 7.034212733681663
    },
    {
      "type": "training",
      "description": "Training step 296",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:28",
      "total_flops_so_far": 7057976965890048.0,
      "budget_used_percent": 7.057976965890048
    },
    {
      "type": "training",
      "description": "Training step 297",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:29",
      "total_flops_so_far": 7081741198098432.0,
      "budget_used_percent": 7.081741198098432
    },
    {
      "type": "training",
      "description": "Training step 298",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:30",
      "total_flops_so_far": 7105505430306816.0,
      "budget_used_percent": 7.105505430306816
    },
    {
      "type": "training",
      "description": "Training step 299",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:32",
      "total_flops_so_far": 7129269662515200.0,
      "budget_used_percent": 7.1292696625152
    },
    {
      "type": "training",
      "description": "Training step 300",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:33",
      "total_flops_so_far": 7153033894723584.0,
      "budget_used_percent": 7.153033894723585
    },
    {
      "type": "training",
      "description": "Training step 301",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:34",
      "total_flops_so_far": 7176798126931968.0,
      "budget_used_percent": 7.176798126931969
    },
    {
      "type": "training",
      "description": "Training step 302",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:35",
      "total_flops_so_far": 7200562359140352.0,
      "budget_used_percent": 7.200562359140352
    },
    {
      "type": "training",
      "description": "Training step 303",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:37",
      "total_flops_so_far": 7224326591348736.0,
      "budget_used_percent": 7.224326591348736
    },
    {
      "type": "training",
      "description": "Training step 304",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:38",
      "total_flops_so_far": 7248090823557120.0,
      "budget_used_percent": 7.24809082355712
    },
    {
      "type": "training",
      "description": "Training step 305",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:39",
      "total_flops_so_far": 7271855055765504.0,
      "budget_used_percent": 7.271855055765504
    },
    {
      "type": "training",
      "description": "Training step 306",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:41",
      "total_flops_so_far": 7295619287973888.0,
      "budget_used_percent": 7.295619287973889
    },
    {
      "type": "training",
      "description": "Training step 307",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:42",
      "total_flops_so_far": 7319383520182272.0,
      "budget_used_percent": 7.319383520182273
    },
    {
      "type": "training",
      "description": "Training step 308",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:43",
      "total_flops_so_far": 7343147752390656.0,
      "budget_used_percent": 7.3431477523906565
    },
    {
      "type": "training",
      "description": "Training step 309",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:44",
      "total_flops_so_far": 7366911984599040.0,
      "budget_used_percent": 7.36691198459904
    },
    {
      "type": "training",
      "description": "Training step 310",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:46",
      "total_flops_so_far": 7390676216807424.0,
      "budget_used_percent": 7.390676216807424
    },
    {
      "type": "training",
      "description": "Training step 311",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:47",
      "total_flops_so_far": 7414440449015808.0,
      "budget_used_percent": 7.414440449015808
    },
    {
      "type": "training",
      "description": "Training step 312",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:48",
      "total_flops_so_far": 7438204681224192.0,
      "budget_used_percent": 7.438204681224192
    },
    {
      "type": "training",
      "description": "Training step 313",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:50",
      "total_flops_so_far": 7461968913432576.0,
      "budget_used_percent": 7.461968913432576
    },
    {
      "type": "training",
      "description": "Training step 314",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:51",
      "total_flops_so_far": 7485733145640960.0,
      "budget_used_percent": 7.48573314564096
    },
    {
      "type": "training",
      "description": "Training step 315",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:52",
      "total_flops_so_far": 7509497377849344.0,
      "budget_used_percent": 7.509497377849344
    },
    {
      "type": "training",
      "description": "Training step 316",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:53",
      "total_flops_so_far": 7533261610057728.0,
      "budget_used_percent": 7.533261610057727
    },
    {
      "type": "training",
      "description": "Training step 317",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:55",
      "total_flops_so_far": 7557025842266112.0,
      "budget_used_percent": 7.557025842266111
    },
    {
      "type": "training",
      "description": "Training step 318",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:56",
      "total_flops_so_far": 7580790074474496.0,
      "budget_used_percent": 7.580790074474495
    },
    {
      "type": "training",
      "description": "Training step 319",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:57",
      "total_flops_so_far": 7604554306682880.0,
      "budget_used_percent": 7.60455430668288
    },
    {
      "type": "training",
      "description": "Training step 320",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:16:59",
      "total_flops_so_far": 7628318538891264.0,
      "budget_used_percent": 7.628318538891264
    },
    {
      "type": "training",
      "description": "Training step 321",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:00",
      "total_flops_so_far": 7652082771099648.0,
      "budget_used_percent": 7.652082771099648
    },
    {
      "type": "training",
      "description": "Training step 322",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:01",
      "total_flops_so_far": 7675847003308032.0,
      "budget_used_percent": 7.675847003308032
    },
    {
      "type": "training",
      "description": "Training step 323",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:02",
      "total_flops_so_far": 7699611235516416.0,
      "budget_used_percent": 7.699611235516415
    },
    {
      "type": "training",
      "description": "Training step 324",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:04",
      "total_flops_so_far": 7723375467724800.0,
      "budget_used_percent": 7.723375467724799
    },
    {
      "type": "training",
      "description": "Training step 325",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:05",
      "total_flops_so_far": 7747139699933184.0,
      "budget_used_percent": 7.747139699933184
    },
    {
      "type": "training",
      "description": "Training step 326",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:06",
      "total_flops_so_far": 7770903932141568.0,
      "budget_used_percent": 7.770903932141568
    },
    {
      "type": "training",
      "description": "Training step 327",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:08",
      "total_flops_so_far": 7794668164349952.0,
      "budget_used_percent": 7.794668164349952
    },
    {
      "type": "training",
      "description": "Training step 328",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:09",
      "total_flops_so_far": 7818432396558336.0,
      "budget_used_percent": 7.818432396558336
    },
    {
      "type": "training",
      "description": "Training step 329",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:10",
      "total_flops_so_far": 7842196628766720.0,
      "budget_used_percent": 7.842196628766721
    },
    {
      "type": "training",
      "description": "Training step 330",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:11",
      "total_flops_so_far": 7865960860975104.0,
      "budget_used_percent": 7.865960860975103
    },
    {
      "type": "training",
      "description": "Training step 331",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:13",
      "total_flops_so_far": 7889725093183488.0,
      "budget_used_percent": 7.889725093183488
    },
    {
      "type": "training",
      "description": "Training step 332",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:14",
      "total_flops_so_far": 7913489325391872.0,
      "budget_used_percent": 7.913489325391872
    },
    {
      "type": "training",
      "description": "Training step 333",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:15",
      "total_flops_so_far": 7937253557600256.0,
      "budget_used_percent": 7.937253557600256
    },
    {
      "type": "training",
      "description": "Training step 334",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:17",
      "total_flops_so_far": 7961017789808640.0,
      "budget_used_percent": 7.96101778980864
    },
    {
      "type": "training",
      "description": "Training step 335",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:18",
      "total_flops_so_far": 7984782022017024.0,
      "budget_used_percent": 7.984782022017025
    },
    {
      "type": "training",
      "description": "Training step 336",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:19",
      "total_flops_so_far": 8008546254225408.0,
      "budget_used_percent": 8.008546254225408
    },
    {
      "type": "training",
      "description": "Training step 337",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:20",
      "total_flops_so_far": 8032310486433792.0,
      "budget_used_percent": 8.032310486433792
    },
    {
      "type": "training",
      "description": "Training step 338",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:22",
      "total_flops_so_far": 8056074718642176.0,
      "budget_used_percent": 8.056074718642176
    },
    {
      "type": "training",
      "description": "Training step 339",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:23",
      "total_flops_so_far": 8079838950850560.0,
      "budget_used_percent": 8.07983895085056
    },
    {
      "type": "training",
      "description": "Training step 340",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:24",
      "total_flops_so_far": 8103603183058944.0,
      "budget_used_percent": 8.103603183058944
    },
    {
      "type": "training",
      "description": "Training step 341",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:26",
      "total_flops_so_far": 8127367415267328.0,
      "budget_used_percent": 8.127367415267328
    },
    {
      "type": "training",
      "description": "Training step 342",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:27",
      "total_flops_so_far": 8151131647475712.0,
      "budget_used_percent": 8.151131647475712
    },
    {
      "type": "training",
      "description": "Training step 343",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:28",
      "total_flops_so_far": 8174895879684096.0,
      "budget_used_percent": 8.174895879684097
    },
    {
      "type": "training",
      "description": "Training step 344",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:30",
      "total_flops_so_far": 8198660111892480.0,
      "budget_used_percent": 8.198660111892481
    },
    {
      "type": "training",
      "description": "Training step 345",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:31",
      "total_flops_so_far": 8222424344100864.0,
      "budget_used_percent": 8.222424344100864
    },
    {
      "type": "training",
      "description": "Training step 346",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:32",
      "total_flops_so_far": 8246188576309248.0,
      "budget_used_percent": 8.246188576309248
    },
    {
      "type": "training",
      "description": "Training step 347",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:33",
      "total_flops_so_far": 8269952808517632.0,
      "budget_used_percent": 8.269952808517631
    },
    {
      "type": "training",
      "description": "Training step 348",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:35",
      "total_flops_so_far": 8293717040726016.0,
      "budget_used_percent": 8.293717040726015
    },
    {
      "type": "training",
      "description": "Training step 349",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:36",
      "total_flops_so_far": 8317481272934400.0,
      "budget_used_percent": 8.3174812729344
    },
    {
      "type": "training",
      "description": "Training step 350",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:37",
      "total_flops_so_far": 8341245505142784.0,
      "budget_used_percent": 8.341245505142785
    },
    {
      "type": "training",
      "description": "Training step 351",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:39",
      "total_flops_so_far": 8365009737351168.0,
      "budget_used_percent": 8.36500973735117
    },
    {
      "type": "training",
      "description": "Training step 352",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:40",
      "total_flops_so_far": 8388773969559552.0,
      "budget_used_percent": 8.388773969559551
    },
    {
      "type": "training",
      "description": "Training step 353",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:41",
      "total_flops_so_far": 8412538201767936.0,
      "budget_used_percent": 8.412538201767935
    },
    {
      "type": "training",
      "description": "Training step 354",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:42",
      "total_flops_so_far": 8436302433976320.0,
      "budget_used_percent": 8.43630243397632
    },
    {
      "type": "training",
      "description": "Training step 355",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:44",
      "total_flops_so_far": 8460066666184704.0,
      "budget_used_percent": 8.460066666184703
    },
    {
      "type": "training",
      "description": "Training step 356",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:45",
      "total_flops_so_far": 8483830898393088.0,
      "budget_used_percent": 8.483830898393089
    },
    {
      "type": "training",
      "description": "Training step 357",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:46",
      "total_flops_so_far": 8507595130601472.0,
      "budget_used_percent": 8.507595130601473
    },
    {
      "type": "training",
      "description": "Training step 358",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:48",
      "total_flops_so_far": 8531359362809856.0,
      "budget_used_percent": 8.531359362809857
    },
    {
      "type": "training",
      "description": "Training step 359",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:49",
      "total_flops_so_far": 8555123595018240.0,
      "budget_used_percent": 8.55512359501824
    },
    {
      "type": "training",
      "description": "Training step 360",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:50",
      "total_flops_so_far": 8578887827226624.0,
      "budget_used_percent": 8.578887827226623
    },
    {
      "type": "training",
      "description": "Training step 361",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:51",
      "total_flops_so_far": 8602652059435008.0,
      "budget_used_percent": 8.602652059435007
    },
    {
      "type": "training",
      "description": "Training step 362",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:53",
      "total_flops_so_far": 8626416291643392.0,
      "budget_used_percent": 8.626416291643393
    },
    {
      "type": "training",
      "description": "Training step 363",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:54",
      "total_flops_so_far": 8650180523851776.0,
      "budget_used_percent": 8.650180523851777
    },
    {
      "type": "training",
      "description": "Training step 364",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:55",
      "total_flops_so_far": 8673944756060160.0,
      "budget_used_percent": 8.67394475606016
    },
    {
      "type": "training",
      "description": "Training step 365",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:57",
      "total_flops_so_far": 8697708988268544.0,
      "budget_used_percent": 8.697708988268545
    },
    {
      "type": "training",
      "description": "Training step 366",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:58",
      "total_flops_so_far": 8721473220476928.0,
      "budget_used_percent": 8.721473220476927
    },
    {
      "type": "training",
      "description": "Training step 367",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:17:59",
      "total_flops_so_far": 8745237452685312.0,
      "budget_used_percent": 8.745237452685311
    },
    {
      "type": "training",
      "description": "Training step 368",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:00",
      "total_flops_so_far": 8769001684893696.0,
      "budget_used_percent": 8.769001684893695
    },
    {
      "type": "training",
      "description": "Training step 369",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:02",
      "total_flops_so_far": 8792765917102080.0,
      "budget_used_percent": 8.79276591710208
    },
    {
      "type": "training",
      "description": "Training step 370",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:03",
      "total_flops_so_far": 8816530149310464.0,
      "budget_used_percent": 8.816530149310465
    },
    {
      "type": "training",
      "description": "Training step 371",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:04",
      "total_flops_so_far": 8840294381518848.0,
      "budget_used_percent": 8.840294381518849
    },
    {
      "type": "training",
      "description": "Training step 372",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:06",
      "total_flops_so_far": 8864058613727232.0,
      "budget_used_percent": 8.864058613727233
    },
    {
      "type": "training",
      "description": "Training step 373",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:07",
      "total_flops_so_far": 8887822845935616.0,
      "budget_used_percent": 8.887822845935615
    },
    {
      "type": "training",
      "description": "Training step 374",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:08",
      "total_flops_so_far": 8911587078144000.0,
      "budget_used_percent": 8.911587078143999
    },
    {
      "type": "training",
      "description": "Training step 375",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:09",
      "total_flops_so_far": 8935351310352384.0,
      "budget_used_percent": 8.935351310352385
    },
    {
      "type": "training",
      "description": "Training step 376",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:11",
      "total_flops_so_far": 8959115542560768.0,
      "budget_used_percent": 8.959115542560768
    },
    {
      "type": "training",
      "description": "Training step 377",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:12",
      "total_flops_so_far": 8982879774769152.0,
      "budget_used_percent": 8.982879774769152
    },
    {
      "type": "training",
      "description": "Training step 378",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:13",
      "total_flops_so_far": 9006644006977536.0,
      "budget_used_percent": 9.006644006977536
    },
    {
      "type": "training",
      "description": "Training step 379",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:15",
      "total_flops_so_far": 9030408239185920.0,
      "budget_used_percent": 9.03040823918592
    },
    {
      "type": "training",
      "description": "Training step 380",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:16",
      "total_flops_so_far": 9054172471394304.0,
      "budget_used_percent": 9.054172471394303
    },
    {
      "type": "training",
      "description": "Training step 381",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:17",
      "total_flops_so_far": 9077936703602688.0,
      "budget_used_percent": 9.077936703602688
    },
    {
      "type": "training",
      "description": "Training step 382",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:18",
      "total_flops_so_far": 9101700935811072.0,
      "budget_used_percent": 9.101700935811072
    },
    {
      "type": "training",
      "description": "Training step 383",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:20",
      "total_flops_so_far": 9125465168019456.0,
      "budget_used_percent": 9.125465168019456
    },
    {
      "type": "training",
      "description": "Training step 384",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:21",
      "total_flops_so_far": 9149229400227840.0,
      "budget_used_percent": 9.14922940022784
    },
    {
      "type": "training",
      "description": "Training step 385",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:22",
      "total_flops_so_far": 9172993632436224.0,
      "budget_used_percent": 9.172993632436224
    },
    {
      "type": "training",
      "description": "Training step 386",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:24",
      "total_flops_so_far": 9196757864644608.0,
      "budget_used_percent": 9.196757864644608
    },
    {
      "type": "training",
      "description": "Training step 387",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:25",
      "total_flops_so_far": 9220522096852992.0,
      "budget_used_percent": 9.220522096852992
    },
    {
      "type": "training",
      "description": "Training step 388",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:26",
      "total_flops_so_far": 9244286329061376.0,
      "budget_used_percent": 9.244286329061376
    },
    {
      "type": "training",
      "description": "Training step 389",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:28",
      "total_flops_so_far": 9268050561269760.0,
      "budget_used_percent": 9.26805056126976
    },
    {
      "type": "training",
      "description": "Training step 390",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:29",
      "total_flops_so_far": 9291814793478144.0,
      "budget_used_percent": 9.291814793478144
    },
    {
      "type": "training",
      "description": "Training step 391",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:30",
      "total_flops_so_far": 9315579025686528.0,
      "budget_used_percent": 9.315579025686528
    },
    {
      "type": "training",
      "description": "Training step 392",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:31",
      "total_flops_so_far": 9339343257894912.0,
      "budget_used_percent": 9.339343257894912
    },
    {
      "type": "training",
      "description": "Training step 393",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:33",
      "total_flops_so_far": 9363107490103296.0,
      "budget_used_percent": 9.363107490103296
    },
    {
      "type": "training",
      "description": "Training step 394",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:34",
      "total_flops_so_far": 9386871722311680.0,
      "budget_used_percent": 9.38687172231168
    },
    {
      "type": "training",
      "description": "Training step 395",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:35",
      "total_flops_so_far": 9410635954520064.0,
      "budget_used_percent": 9.410635954520064
    },
    {
      "type": "training",
      "description": "Training step 396",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:36",
      "total_flops_so_far": 9434400186728448.0,
      "budget_used_percent": 9.434400186728448
    },
    {
      "type": "training",
      "description": "Training step 397",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:37",
      "total_flops_so_far": 9458164418936832.0,
      "budget_used_percent": 9.458164418936832
    },
    {
      "type": "training",
      "description": "Training step 398",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:38",
      "total_flops_so_far": 9481928651145216.0,
      "budget_used_percent": 9.481928651145216
    },
    {
      "type": "training",
      "description": "Training step 399",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:39",
      "total_flops_so_far": 9505692883353600.0,
      "budget_used_percent": 9.5056928833536
    },
    {
      "type": "training",
      "description": "Training step 400",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:41",
      "total_flops_so_far": 9529457115561984.0,
      "budget_used_percent": 9.529457115561984
    },
    {
      "type": "training",
      "description": "Training step 401",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:42",
      "total_flops_so_far": 9553221347770368.0,
      "budget_used_percent": 9.553221347770368
    },
    {
      "type": "training",
      "description": "Training step 402",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:43",
      "total_flops_so_far": 9576985579978752.0,
      "budget_used_percent": 9.576985579978752
    },
    {
      "type": "training",
      "description": "Training step 403",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:45",
      "total_flops_so_far": 9600749812187136.0,
      "budget_used_percent": 9.600749812187136
    },
    {
      "type": "training",
      "description": "Training step 404",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:46",
      "total_flops_so_far": 9624514044395520.0,
      "budget_used_percent": 9.62451404439552
    },
    {
      "type": "training",
      "description": "Training step 405",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:47",
      "total_flops_so_far": 9648278276603904.0,
      "budget_used_percent": 9.648278276603904
    },
    {
      "type": "training",
      "description": "Training step 406",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:48",
      "total_flops_so_far": 9672042508812288.0,
      "budget_used_percent": 9.672042508812288
    },
    {
      "type": "training",
      "description": "Training step 407",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:50",
      "total_flops_so_far": 9695806741020672.0,
      "budget_used_percent": 9.695806741020672
    },
    {
      "type": "training",
      "description": "Training step 408",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:51",
      "total_flops_so_far": 9719570973229056.0,
      "budget_used_percent": 9.719570973229057
    },
    {
      "type": "training",
      "description": "Training step 409",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:52",
      "total_flops_so_far": 9743335205437440.0,
      "budget_used_percent": 9.74333520543744
    },
    {
      "type": "training",
      "description": "Training step 410",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:54",
      "total_flops_so_far": 9767099437645824.0,
      "budget_used_percent": 9.767099437645824
    },
    {
      "type": "training",
      "description": "Training step 411",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:55",
      "total_flops_so_far": 9790863669854208.0,
      "budget_used_percent": 9.790863669854208
    },
    {
      "type": "training",
      "description": "Training step 412",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:56",
      "total_flops_so_far": 9814627902062592.0,
      "budget_used_percent": 9.814627902062592
    },
    {
      "type": "training",
      "description": "Training step 413",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:58",
      "total_flops_so_far": 9838392134270976.0,
      "budget_used_percent": 9.838392134270975
    },
    {
      "type": "training",
      "description": "Training step 414",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:18:59",
      "total_flops_so_far": 9862156366479360.0,
      "budget_used_percent": 9.862156366479361
    },
    {
      "type": "training",
      "description": "Training step 415",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:00",
      "total_flops_so_far": 9885920598687744.0,
      "budget_used_percent": 9.885920598687745
    },
    {
      "type": "training",
      "description": "Training step 416",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:01",
      "total_flops_so_far": 9909684830896128.0,
      "budget_used_percent": 9.909684830896127
    },
    {
      "type": "training",
      "description": "Training step 417",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:03",
      "total_flops_so_far": 9933449063104512.0,
      "budget_used_percent": 9.933449063104511
    },
    {
      "type": "training",
      "description": "Training step 418",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:04",
      "total_flops_so_far": 9957213295312896.0,
      "budget_used_percent": 9.957213295312895
    },
    {
      "type": "training",
      "description": "Training step 419",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:05",
      "total_flops_so_far": 9980977527521280.0,
      "budget_used_percent": 9.98097752752128
    },
    {
      "type": "training",
      "description": "Training step 420",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:07",
      "total_flops_so_far": 1.0004741759729664e+16,
      "budget_used_percent": 10.004741759729665
    },
    {
      "type": "training",
      "description": "Training step 421",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:08",
      "total_flops_so_far": 1.0028505991938048e+16,
      "budget_used_percent": 10.028505991938049
    },
    {
      "type": "training",
      "description": "Training step 422",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:09",
      "total_flops_so_far": 1.0052270224146432e+16,
      "budget_used_percent": 10.052270224146433
    },
    {
      "type": "training",
      "description": "Training step 423",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:10",
      "total_flops_so_far": 1.0076034456354816e+16,
      "budget_used_percent": 10.076034456354815
    },
    {
      "type": "training",
      "description": "Training step 424",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:12",
      "total_flops_so_far": 1.00997986885632e+16,
      "budget_used_percent": 10.0997986885632
    },
    {
      "type": "training",
      "description": "Training step 425",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:13",
      "total_flops_so_far": 1.0123562920771584e+16,
      "budget_used_percent": 10.123562920771583
    },
    {
      "type": "training",
      "description": "Training step 426",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:14",
      "total_flops_so_far": 1.0147327152979968e+16,
      "budget_used_percent": 10.147327152979969
    },
    {
      "type": "training",
      "description": "Training step 427",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:16",
      "total_flops_so_far": 1.0171091385188352e+16,
      "budget_used_percent": 10.171091385188353
    },
    {
      "type": "training",
      "description": "Training step 428",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:17",
      "total_flops_so_far": 1.0194855617396736e+16,
      "budget_used_percent": 10.194855617396737
    },
    {
      "type": "training",
      "description": "Training step 429",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:18",
      "total_flops_so_far": 1.021861984960512e+16,
      "budget_used_percent": 10.21861984960512
    },
    {
      "type": "training",
      "description": "Training step 430",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:19",
      "total_flops_so_far": 1.0242384081813504e+16,
      "budget_used_percent": 10.242384081813503
    },
    {
      "type": "training",
      "description": "Training step 431",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:21",
      "total_flops_so_far": 1.0266148314021888e+16,
      "budget_used_percent": 10.266148314021887
    },
    {
      "type": "training",
      "description": "Training step 432",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:22",
      "total_flops_so_far": 1.0289912546230272e+16,
      "budget_used_percent": 10.289912546230271
    },
    {
      "type": "training",
      "description": "Training step 433",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:23",
      "total_flops_so_far": 1.0313676778438656e+16,
      "budget_used_percent": 10.313676778438657
    },
    {
      "type": "training",
      "description": "Training step 434",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:25",
      "total_flops_so_far": 1.033744101064704e+16,
      "budget_used_percent": 10.33744101064704
    },
    {
      "type": "training",
      "description": "Training step 435",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:26",
      "total_flops_so_far": 1.0361205242855424e+16,
      "budget_used_percent": 10.361205242855425
    },
    {
      "type": "training",
      "description": "Training step 436",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:27",
      "total_flops_so_far": 1.0384969475063808e+16,
      "budget_used_percent": 10.384969475063809
    },
    {
      "type": "training",
      "description": "Training step 437",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:28",
      "total_flops_so_far": 1.0408733707272192e+16,
      "budget_used_percent": 10.40873370727219
    },
    {
      "type": "training",
      "description": "Training step 438",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:30",
      "total_flops_so_far": 1.0432497939480576e+16,
      "budget_used_percent": 10.432497939480575
    },
    {
      "type": "training",
      "description": "Training step 439",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:31",
      "total_flops_so_far": 1.045626217168896e+16,
      "budget_used_percent": 10.45626217168896
    },
    {
      "type": "training",
      "description": "Training step 440",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:32",
      "total_flops_so_far": 1.0480026403897344e+16,
      "budget_used_percent": 10.480026403897345
    },
    {
      "type": "training",
      "description": "Training step 441",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:34",
      "total_flops_so_far": 1.0503790636105728e+16,
      "budget_used_percent": 10.503790636105728
    },
    {
      "type": "training",
      "description": "Training step 442",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:35",
      "total_flops_so_far": 1.0527554868314112e+16,
      "budget_used_percent": 10.527554868314112
    },
    {
      "type": "training",
      "description": "Training step 443",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:36",
      "total_flops_so_far": 1.0551319100522496e+16,
      "budget_used_percent": 10.551319100522496
    },
    {
      "type": "training",
      "description": "Training step 444",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:38",
      "total_flops_so_far": 1.057508333273088e+16,
      "budget_used_percent": 10.575083332730879
    },
    {
      "type": "training",
      "description": "Training step 445",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:39",
      "total_flops_so_far": 1.0598847564939264e+16,
      "budget_used_percent": 10.598847564939264
    },
    {
      "type": "training",
      "description": "Training step 446",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:40",
      "total_flops_so_far": 1.0622611797147648e+16,
      "budget_used_percent": 10.622611797147648
    },
    {
      "type": "training",
      "description": "Training step 447",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:41",
      "total_flops_so_far": 1.0646376029356032e+16,
      "budget_used_percent": 10.646376029356032
    },
    {
      "type": "training",
      "description": "Training step 448",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:43",
      "total_flops_so_far": 1.0670140261564416e+16,
      "budget_used_percent": 10.670140261564416
    },
    {
      "type": "training",
      "description": "Training step 449",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:44",
      "total_flops_so_far": 1.06939044937728e+16,
      "budget_used_percent": 10.6939044937728
    },
    {
      "type": "training",
      "description": "Training step 450",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:45",
      "total_flops_so_far": 1.0717668725981184e+16,
      "budget_used_percent": 10.717668725981184
    },
    {
      "type": "training",
      "description": "Training step 451",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:47",
      "total_flops_so_far": 1.0741432958189568e+16,
      "budget_used_percent": 10.741432958189566
    },
    {
      "type": "training",
      "description": "Training step 452",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:48",
      "total_flops_so_far": 1.0765197190397952e+16,
      "budget_used_percent": 10.765197190397952
    },
    {
      "type": "training",
      "description": "Training step 453",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:49",
      "total_flops_so_far": 1.0788961422606336e+16,
      "budget_used_percent": 10.788961422606336
    },
    {
      "type": "training",
      "description": "Training step 454",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:50",
      "total_flops_so_far": 1.081272565481472e+16,
      "budget_used_percent": 10.81272565481472
    },
    {
      "type": "training",
      "description": "Training step 455",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:52",
      "total_flops_so_far": 1.0836489887023104e+16,
      "budget_used_percent": 10.836489887023104
    },
    {
      "type": "training",
      "description": "Training step 456",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:53",
      "total_flops_so_far": 1.0860254119231488e+16,
      "budget_used_percent": 10.860254119231488
    },
    {
      "type": "training",
      "description": "Training step 457",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:54",
      "total_flops_so_far": 1.0884018351439872e+16,
      "budget_used_percent": 10.884018351439872
    },
    {
      "type": "training",
      "description": "Training step 458",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:56",
      "total_flops_so_far": 1.0907782583648256e+16,
      "budget_used_percent": 10.907782583648256
    },
    {
      "type": "training",
      "description": "Training step 459",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:57",
      "total_flops_so_far": 1.093154681585664e+16,
      "budget_used_percent": 10.93154681585664
    },
    {
      "type": "training",
      "description": "Training step 460",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:19:59",
      "total_flops_so_far": 1.0955311048065024e+16,
      "budget_used_percent": 10.955311048065024
    },
    {
      "type": "training",
      "description": "Training step 461",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:01",
      "total_flops_so_far": 1.0979075280273408e+16,
      "budget_used_percent": 10.979075280273408
    },
    {
      "type": "training",
      "description": "Training step 462",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:02",
      "total_flops_so_far": 1.1002839512481792e+16,
      "budget_used_percent": 11.002839512481792
    },
    {
      "type": "training",
      "description": "Training step 463",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:03",
      "total_flops_so_far": 1.1026603744690176e+16,
      "budget_used_percent": 11.026603744690176
    },
    {
      "type": "training",
      "description": "Training step 464",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:05",
      "total_flops_so_far": 1.105036797689856e+16,
      "budget_used_percent": 11.05036797689856
    },
    {
      "type": "training",
      "description": "Training step 465",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:06",
      "total_flops_so_far": 1.1074132209106944e+16,
      "budget_used_percent": 11.074132209106944
    },
    {
      "type": "training",
      "description": "Training step 466",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:07",
      "total_flops_so_far": 1.1097896441315328e+16,
      "budget_used_percent": 11.097896441315328
    },
    {
      "type": "training",
      "description": "Training step 467",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:10",
      "total_flops_so_far": 1.1121660673523712e+16,
      "budget_used_percent": 11.121660673523712
    },
    {
      "type": "training",
      "description": "Training step 468",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:11",
      "total_flops_so_far": 1.1145424905732096e+16,
      "budget_used_percent": 11.145424905732096
    },
    {
      "type": "training",
      "description": "Training step 469",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:13",
      "total_flops_so_far": 1.116918913794048e+16,
      "budget_used_percent": 11.16918913794048
    },
    {
      "type": "training",
      "description": "Training step 470",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:14",
      "total_flops_so_far": 1.1192953370148864e+16,
      "budget_used_percent": 11.192953370148864
    },
    {
      "type": "training",
      "description": "Training step 471",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:15",
      "total_flops_so_far": 1.1216717602357248e+16,
      "budget_used_percent": 11.216717602357248
    },
    {
      "type": "training",
      "description": "Training step 472",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:16",
      "total_flops_so_far": 1.1240481834565632e+16,
      "budget_used_percent": 11.240481834565633
    },
    {
      "type": "training",
      "description": "Training step 473",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:18",
      "total_flops_so_far": 1.1264246066774016e+16,
      "budget_used_percent": 11.264246066774016
    },
    {
      "type": "training",
      "description": "Training step 474",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:19",
      "total_flops_so_far": 1.12880102989824e+16,
      "budget_used_percent": 11.2880102989824
    },
    {
      "type": "training",
      "description": "Training step 475",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:20",
      "total_flops_so_far": 1.1311774531190784e+16,
      "budget_used_percent": 11.311774531190784
    },
    {
      "type": "training",
      "description": "Training step 476",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:22",
      "total_flops_so_far": 1.1335538763399168e+16,
      "budget_used_percent": 11.335538763399168
    },
    {
      "type": "training",
      "description": "Training step 477",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:23",
      "total_flops_so_far": 1.1359302995607552e+16,
      "budget_used_percent": 11.359302995607552
    },
    {
      "type": "training",
      "description": "Training step 478",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:24",
      "total_flops_so_far": 1.1383067227815936e+16,
      "budget_used_percent": 11.383067227815937
    },
    {
      "type": "training",
      "description": "Training step 479",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:25",
      "total_flops_so_far": 1.140683146002432e+16,
      "budget_used_percent": 11.406831460024321
    },
    {
      "type": "training",
      "description": "Training step 480",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:27",
      "total_flops_so_far": 1.1430595692232704e+16,
      "budget_used_percent": 11.430595692232703
    },
    {
      "type": "training",
      "description": "Training step 481",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:28",
      "total_flops_so_far": 1.1454359924441088e+16,
      "budget_used_percent": 11.454359924441087
    },
    {
      "type": "training",
      "description": "Training step 482",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:29",
      "total_flops_so_far": 1.1478124156649472e+16,
      "budget_used_percent": 11.478124156649471
    },
    {
      "type": "training",
      "description": "Training step 483",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:31",
      "total_flops_so_far": 1.1501888388857856e+16,
      "budget_used_percent": 11.501888388857855
    },
    {
      "type": "training",
      "description": "Training step 484",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:32",
      "total_flops_so_far": 1.152565262106624e+16,
      "budget_used_percent": 11.52565262106624
    },
    {
      "type": "training",
      "description": "Training step 485",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:33",
      "total_flops_so_far": 1.1549416853274624e+16,
      "budget_used_percent": 11.549416853274625
    },
    {
      "type": "training",
      "description": "Training step 486",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:34",
      "total_flops_so_far": 1.1573181085483008e+16,
      "budget_used_percent": 11.573181085483009
    },
    {
      "type": "training",
      "description": "Training step 487",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:36",
      "total_flops_so_far": 1.1596945317691392e+16,
      "budget_used_percent": 11.596945317691391
    },
    {
      "type": "training",
      "description": "Training step 488",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:37",
      "total_flops_so_far": 1.1620709549899776e+16,
      "budget_used_percent": 11.620709549899775
    },
    {
      "type": "training",
      "description": "Training step 489",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:38",
      "total_flops_so_far": 1.164447378210816e+16,
      "budget_used_percent": 11.64447378210816
    },
    {
      "type": "training",
      "description": "Training step 490",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:40",
      "total_flops_so_far": 1.1668238014316544e+16,
      "budget_used_percent": 11.668238014316543
    },
    {
      "type": "training",
      "description": "Training step 491",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:41",
      "total_flops_so_far": 1.1692002246524928e+16,
      "budget_used_percent": 11.692002246524929
    },
    {
      "type": "training",
      "description": "Training step 492",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:42",
      "total_flops_so_far": 1.1715766478733312e+16,
      "budget_used_percent": 11.715766478733313
    },
    {
      "type": "training",
      "description": "Training step 493",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:44",
      "total_flops_so_far": 1.1739530710941696e+16,
      "budget_used_percent": 11.739530710941697
    },
    {
      "type": "training",
      "description": "Training step 494",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:45",
      "total_flops_so_far": 1.176329494315008e+16,
      "budget_used_percent": 11.763294943150079
    },
    {
      "type": "training",
      "description": "Training step 495",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:46",
      "total_flops_so_far": 1.1787059175358464e+16,
      "budget_used_percent": 11.787059175358463
    },
    {
      "type": "training",
      "description": "Training step 496",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:47",
      "total_flops_so_far": 1.1810823407566848e+16,
      "budget_used_percent": 11.810823407566847
    },
    {
      "type": "training",
      "description": "Training step 497",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:49",
      "total_flops_so_far": 1.1834587639775232e+16,
      "budget_used_percent": 11.834587639775233
    },
    {
      "type": "training",
      "description": "Training step 498",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:50",
      "total_flops_so_far": 1.1858351871983616e+16,
      "budget_used_percent": 11.858351871983617
    },
    {
      "type": "training",
      "description": "Training step 499",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:51",
      "total_flops_so_far": 1.1882116104192e+16,
      "budget_used_percent": 11.882116104192
    },
    {
      "type": "training",
      "description": "Training step 500",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:53",
      "total_flops_so_far": 1.1905880336400384e+16,
      "budget_used_percent": 11.905880336400385
    },
    {
      "type": "training",
      "description": "Training step 501",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:54",
      "total_flops_so_far": 1.1929644568608768e+16,
      "budget_used_percent": 11.929644568608767
    },
    {
      "type": "training",
      "description": "Training step 502",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:55",
      "total_flops_so_far": 1.1953408800817152e+16,
      "budget_used_percent": 11.95340880081715
    },
    {
      "type": "training",
      "description": "Training step 503",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:56",
      "total_flops_so_far": 1.1977173033025536e+16,
      "budget_used_percent": 11.977173033025537
    },
    {
      "type": "training",
      "description": "Training step 504",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:58",
      "total_flops_so_far": 1.200093726523392e+16,
      "budget_used_percent": 12.00093726523392
    },
    {
      "type": "training",
      "description": "Training step 505",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:20:59",
      "total_flops_so_far": 1.2024701497442304e+16,
      "budget_used_percent": 12.024701497442305
    },
    {
      "type": "training",
      "description": "Training step 506",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:00",
      "total_flops_so_far": 1.2048465729650688e+16,
      "budget_used_percent": 12.048465729650689
    },
    {
      "type": "training",
      "description": "Training step 507",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:02",
      "total_flops_so_far": 1.2072229961859072e+16,
      "budget_used_percent": 12.072229961859072
    },
    {
      "type": "training",
      "description": "Training step 508",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:03",
      "total_flops_so_far": 1.2095994194067456e+16,
      "budget_used_percent": 12.095994194067455
    },
    {
      "type": "training",
      "description": "Training step 509",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:04",
      "total_flops_so_far": 1.211975842627584e+16,
      "budget_used_percent": 12.11975842627584
    },
    {
      "type": "training",
      "description": "Training step 510",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:05",
      "total_flops_so_far": 1.2143522658484224e+16,
      "budget_used_percent": 12.143522658484224
    },
    {
      "type": "training",
      "description": "Training step 511",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:07",
      "total_flops_so_far": 1.2167286890692608e+16,
      "budget_used_percent": 12.167286890692608
    },
    {
      "type": "training",
      "description": "Training step 512",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:08",
      "total_flops_so_far": 1.2191051122900992e+16,
      "budget_used_percent": 12.191051122900992
    },
    {
      "type": "training",
      "description": "Training step 513",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:09",
      "total_flops_so_far": 1.2214815355109376e+16,
      "budget_used_percent": 12.214815355109376
    },
    {
      "type": "training",
      "description": "Training step 514",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:11",
      "total_flops_so_far": 1.223857958731776e+16,
      "budget_used_percent": 12.23857958731776
    },
    {
      "type": "training",
      "description": "Training step 515",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:12",
      "total_flops_so_far": 1.2262343819526144e+16,
      "budget_used_percent": 12.262343819526142
    },
    {
      "type": "training",
      "description": "Training step 516",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:13",
      "total_flops_so_far": 1.2286108051734528e+16,
      "budget_used_percent": 12.286108051734528
    },
    {
      "type": "training",
      "description": "Training step 517",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:15",
      "total_flops_so_far": 1.2309872283942912e+16,
      "budget_used_percent": 12.309872283942912
    },
    {
      "type": "training",
      "description": "Training step 518",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:16",
      "total_flops_so_far": 1.2333636516151296e+16,
      "budget_used_percent": 12.333636516151296
    },
    {
      "type": "training",
      "description": "Training step 519",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:17",
      "total_flops_so_far": 1.235740074835968e+16,
      "budget_used_percent": 12.35740074835968
    },
    {
      "type": "training",
      "description": "Training step 520",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:18",
      "total_flops_so_far": 1.2381164980568064e+16,
      "budget_used_percent": 12.381164980568064
    },
    {
      "type": "training",
      "description": "Training step 521",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:20",
      "total_flops_so_far": 1.2404929212776448e+16,
      "budget_used_percent": 12.404929212776448
    },
    {
      "type": "training",
      "description": "Training step 522",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:21",
      "total_flops_so_far": 1.2428693444984832e+16,
      "budget_used_percent": 12.428693444984832
    },
    {
      "type": "training",
      "description": "Training step 523",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:22",
      "total_flops_so_far": 1.2452457677193216e+16,
      "budget_used_percent": 12.452457677193216
    },
    {
      "type": "training",
      "description": "Training step 524",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:24",
      "total_flops_so_far": 1.24762219094016e+16,
      "budget_used_percent": 12.4762219094016
    },
    {
      "type": "training",
      "description": "Training step 525",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:25",
      "total_flops_so_far": 1.2499986141609984e+16,
      "budget_used_percent": 12.499986141609984
    },
    {
      "type": "training",
      "description": "Training step 526",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:26",
      "total_flops_so_far": 1.2523750373818368e+16,
      "budget_used_percent": 12.523750373818368
    },
    {
      "type": "training",
      "description": "Training step 527",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:27",
      "total_flops_so_far": 1.2547514606026752e+16,
      "budget_used_percent": 12.547514606026752
    },
    {
      "type": "training",
      "description": "Training step 528",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:28",
      "total_flops_so_far": 1.2571278838235136e+16,
      "budget_used_percent": 12.571278838235136
    },
    {
      "type": "training",
      "description": "Training step 529",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:29",
      "total_flops_so_far": 1.259504307044352e+16,
      "budget_used_percent": 12.59504307044352
    },
    {
      "type": "training",
      "description": "Training step 530",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:30",
      "total_flops_so_far": 1.2618807302651904e+16,
      "budget_used_percent": 12.618807302651906
    },
    {
      "type": "training",
      "description": "Training step 531",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:32",
      "total_flops_so_far": 1.2642571534860288e+16,
      "budget_used_percent": 12.64257153486029
    },
    {
      "type": "training",
      "description": "Training step 532",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:33",
      "total_flops_so_far": 1.2666335767068672e+16,
      "budget_used_percent": 12.666335767068674
    },
    {
      "type": "training",
      "description": "Training step 533",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:34",
      "total_flops_so_far": 1.2690099999277056e+16,
      "budget_used_percent": 12.690099999277054
    },
    {
      "type": "training",
      "description": "Training step 534",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:36",
      "total_flops_so_far": 1.271386423148544e+16,
      "budget_used_percent": 12.713864231485438
    },
    {
      "type": "training",
      "description": "Training step 535",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:37",
      "total_flops_so_far": 1.2737628463693824e+16,
      "budget_used_percent": 12.737628463693824
    },
    {
      "type": "training",
      "description": "Training step 536",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:38",
      "total_flops_so_far": 1.2761392695902208e+16,
      "budget_used_percent": 12.761392695902208
    },
    {
      "type": "training",
      "description": "Training step 537",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:39",
      "total_flops_so_far": 1.2785156928110592e+16,
      "budget_used_percent": 12.785156928110592
    },
    {
      "type": "training",
      "description": "Training step 538",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:41",
      "total_flops_so_far": 1.2808921160318976e+16,
      "budget_used_percent": 12.808921160318976
    },
    {
      "type": "training",
      "description": "Training step 539",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:42",
      "total_flops_so_far": 1.283268539252736e+16,
      "budget_used_percent": 12.83268539252736
    },
    {
      "type": "training",
      "description": "Training step 540",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:43",
      "total_flops_so_far": 1.2856449624735744e+16,
      "budget_used_percent": 12.856449624735744
    },
    {
      "type": "training",
      "description": "Training step 541",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:45",
      "total_flops_so_far": 1.2880213856944128e+16,
      "budget_used_percent": 12.880213856944128
    },
    {
      "type": "training",
      "description": "Training step 542",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:46",
      "total_flops_so_far": 1.2903978089152512e+16,
      "budget_used_percent": 12.903978089152513
    },
    {
      "type": "training",
      "description": "Training step 543",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:47",
      "total_flops_so_far": 1.2927742321360896e+16,
      "budget_used_percent": 12.927742321360897
    },
    {
      "type": "training",
      "description": "Training step 544",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:48",
      "total_flops_so_far": 1.295150655356928e+16,
      "budget_used_percent": 12.951506553569281
    },
    {
      "type": "training",
      "description": "Training step 545",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:50",
      "total_flops_so_far": 1.2975270785777664e+16,
      "budget_used_percent": 12.975270785777665
    },
    {
      "type": "training",
      "description": "Training step 546",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:51",
      "total_flops_so_far": 1.2999035017986048e+16,
      "budget_used_percent": 12.99903501798605
    },
    {
      "type": "training",
      "description": "Training step 547",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:52",
      "total_flops_so_far": 1.3022799250194432e+16,
      "budget_used_percent": 13.022799250194431
    },
    {
      "type": "training",
      "description": "Training step 548",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:54",
      "total_flops_so_far": 1.3046563482402816e+16,
      "budget_used_percent": 13.046563482402815
    },
    {
      "type": "training",
      "description": "Training step 549",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:55",
      "total_flops_so_far": 1.30703277146112e+16,
      "budget_used_percent": 13.0703277146112
    },
    {
      "type": "training",
      "description": "Training step 550",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:56",
      "total_flops_so_far": 1.3094091946819584e+16,
      "budget_used_percent": 13.094091946819583
    },
    {
      "type": "training",
      "description": "Training step 551",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:58",
      "total_flops_so_far": 1.3117856179027968e+16,
      "budget_used_percent": 13.117856179027967
    },
    {
      "type": "training",
      "description": "Training step 552",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:21:59",
      "total_flops_so_far": 1.3141620411236352e+16,
      "budget_used_percent": 13.141620411236351
    },
    {
      "type": "training",
      "description": "Training step 553",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:00",
      "total_flops_so_far": 1.3165384643444736e+16,
      "budget_used_percent": 13.165384643444735
    },
    {
      "type": "training",
      "description": "Training step 554",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:01",
      "total_flops_so_far": 1.318914887565312e+16,
      "budget_used_percent": 13.18914887565312
    },
    {
      "type": "training",
      "description": "Training step 555",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:03",
      "total_flops_so_far": 1.3212913107861504e+16,
      "budget_used_percent": 13.212913107861505
    },
    {
      "type": "training",
      "description": "Training step 556",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:04",
      "total_flops_so_far": 1.3236677340069888e+16,
      "budget_used_percent": 13.236677340069889
    },
    {
      "type": "training",
      "description": "Training step 557",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:05",
      "total_flops_so_far": 1.3260441572278272e+16,
      "budget_used_percent": 13.260441572278273
    },
    {
      "type": "training",
      "description": "Training step 558",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:07",
      "total_flops_so_far": 1.3284205804486656e+16,
      "budget_used_percent": 13.284205804486657
    },
    {
      "type": "training",
      "description": "Training step 559",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:08",
      "total_flops_so_far": 1.330797003669504e+16,
      "budget_used_percent": 13.30797003669504
    },
    {
      "type": "training",
      "description": "Training step 560",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:09",
      "total_flops_so_far": 1.3331734268903424e+16,
      "budget_used_percent": 13.331734268903425
    },
    {
      "type": "training",
      "description": "Training step 561",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:10",
      "total_flops_so_far": 1.3355498501111808e+16,
      "budget_used_percent": 13.355498501111809
    },
    {
      "type": "training",
      "description": "Training step 562",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:12",
      "total_flops_so_far": 1.3379262733320192e+16,
      "budget_used_percent": 13.379262733320191
    },
    {
      "type": "training",
      "description": "Training step 563",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:13",
      "total_flops_so_far": 1.3403026965528576e+16,
      "budget_used_percent": 13.403026965528575
    },
    {
      "type": "training",
      "description": "Training step 564",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:14",
      "total_flops_so_far": 1.342679119773696e+16,
      "budget_used_percent": 13.426791197736959
    },
    {
      "type": "training",
      "description": "Training step 565",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:16",
      "total_flops_so_far": 1.3450555429945344e+16,
      "budget_used_percent": 13.450555429945343
    },
    {
      "type": "training",
      "description": "Training step 566",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:17",
      "total_flops_so_far": 1.3474319662153728e+16,
      "budget_used_percent": 13.474319662153727
    },
    {
      "type": "training",
      "description": "Training step 567",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:18",
      "total_flops_so_far": 1.3498083894362112e+16,
      "budget_used_percent": 13.49808389436211
    },
    {
      "type": "training",
      "description": "Training step 568",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:20",
      "total_flops_so_far": 1.3521848126570496e+16,
      "budget_used_percent": 13.521848126570497
    },
    {
      "type": "training",
      "description": "Training step 569",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:21",
      "total_flops_so_far": 1.354561235877888e+16,
      "budget_used_percent": 13.54561235877888
    },
    {
      "type": "training",
      "description": "Training step 570",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:22",
      "total_flops_so_far": 1.3569376590987264e+16,
      "budget_used_percent": 13.569376590987265
    },
    {
      "type": "training",
      "description": "Training step 571",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:23",
      "total_flops_so_far": 1.3593140823195648e+16,
      "budget_used_percent": 13.593140823195649
    },
    {
      "type": "training",
      "description": "Training step 572",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:25",
      "total_flops_so_far": 1.3616905055404032e+16,
      "budget_used_percent": 13.616905055404033
    },
    {
      "type": "training",
      "description": "Training step 573",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:26",
      "total_flops_so_far": 1.3640669287612416e+16,
      "budget_used_percent": 13.640669287612416
    },
    {
      "type": "training",
      "description": "Training step 574",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:27",
      "total_flops_so_far": 1.36644335198208e+16,
      "budget_used_percent": 13.6644335198208
    },
    {
      "type": "training",
      "description": "Training step 575",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:29",
      "total_flops_so_far": 1.3688197752029184e+16,
      "budget_used_percent": 13.688197752029186
    },
    {
      "type": "training",
      "description": "Training step 576",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:30",
      "total_flops_so_far": 1.3711961984237568e+16,
      "budget_used_percent": 13.711961984237567
    },
    {
      "type": "training",
      "description": "Training step 577",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:31",
      "total_flops_so_far": 1.3735726216445952e+16,
      "budget_used_percent": 13.73572621644595
    },
    {
      "type": "training",
      "description": "Training step 578",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:32",
      "total_flops_so_far": 1.3759490448654336e+16,
      "budget_used_percent": 13.759490448654335
    },
    {
      "type": "training",
      "description": "Training step 579",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:34",
      "total_flops_so_far": 1.378325468086272e+16,
      "budget_used_percent": 13.783254680862719
    },
    {
      "type": "training",
      "description": "Training step 580",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:35",
      "total_flops_so_far": 1.3807018913071104e+16,
      "budget_used_percent": 13.807018913071104
    },
    {
      "type": "training",
      "description": "Training step 581",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:36",
      "total_flops_so_far": 1.3830783145279488e+16,
      "budget_used_percent": 13.830783145279488
    },
    {
      "type": "training",
      "description": "Training step 582",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:38",
      "total_flops_so_far": 1.3854547377487872e+16,
      "budget_used_percent": 13.854547377487872
    },
    {
      "type": "training",
      "description": "Training step 583",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:39",
      "total_flops_so_far": 1.3878311609696256e+16,
      "budget_used_percent": 13.878311609696256
    },
    {
      "type": "training",
      "description": "Training step 584",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:40",
      "total_flops_so_far": 1.390207584190464e+16,
      "budget_used_percent": 13.90207584190464
    },
    {
      "type": "training",
      "description": "Training step 585",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:41",
      "total_flops_so_far": 1.3925840074113024e+16,
      "budget_used_percent": 13.925840074113024
    },
    {
      "type": "training",
      "description": "Training step 586",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:43",
      "total_flops_so_far": 1.3949604306321408e+16,
      "budget_used_percent": 13.949604306321408
    },
    {
      "type": "training",
      "description": "Training step 587",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:44",
      "total_flops_so_far": 1.3973368538529792e+16,
      "budget_used_percent": 13.973368538529792
    },
    {
      "type": "training",
      "description": "Training step 588",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:45",
      "total_flops_so_far": 1.3997132770738176e+16,
      "budget_used_percent": 13.997132770738178
    },
    {
      "type": "training",
      "description": "Training step 589",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:47",
      "total_flops_so_far": 1.402089700294656e+16,
      "budget_used_percent": 14.020897002946562
    },
    {
      "type": "training",
      "description": "Training step 590",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:48",
      "total_flops_so_far": 1.4044661235154944e+16,
      "budget_used_percent": 14.044661235154942
    },
    {
      "type": "training",
      "description": "Training step 591",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:49",
      "total_flops_so_far": 1.4068425467363328e+16,
      "budget_used_percent": 14.068425467363326
    },
    {
      "type": "training",
      "description": "Training step 592",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:51",
      "total_flops_so_far": 1.4092189699571712e+16,
      "budget_used_percent": 14.092189699571712
    },
    {
      "type": "training",
      "description": "Training step 593",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:52",
      "total_flops_so_far": 1.4115953931780096e+16,
      "budget_used_percent": 14.115953931780096
    },
    {
      "type": "training",
      "description": "Training step 594",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:53",
      "total_flops_so_far": 1.413971816398848e+16,
      "budget_used_percent": 14.13971816398848
    },
    {
      "type": "training",
      "description": "Training step 595",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:54",
      "total_flops_so_far": 1.4163482396196864e+16,
      "budget_used_percent": 14.163482396196864
    },
    {
      "type": "training",
      "description": "Training step 596",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:56",
      "total_flops_so_far": 1.4187246628405248e+16,
      "budget_used_percent": 14.187246628405248
    },
    {
      "type": "training",
      "description": "Training step 597",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:57",
      "total_flops_so_far": 1.4211010860613632e+16,
      "budget_used_percent": 14.211010860613632
    },
    {
      "type": "training",
      "description": "Training step 598",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:22:58",
      "total_flops_so_far": 1.4234775092822016e+16,
      "budget_used_percent": 14.234775092822016
    },
    {
      "type": "training",
      "description": "Training step 599",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:00",
      "total_flops_so_far": 1.42585393250304e+16,
      "budget_used_percent": 14.2585393250304
    },
    {
      "type": "training",
      "description": "Training step 600",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:01",
      "total_flops_so_far": 1.4282303557238784e+16,
      "budget_used_percent": 14.282303557238784
    },
    {
      "type": "training",
      "description": "Training step 601",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:02",
      "total_flops_so_far": 1.4306067789447168e+16,
      "budget_used_percent": 14.30606778944717
    },
    {
      "type": "training",
      "description": "Training step 602",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:03",
      "total_flops_so_far": 1.4329832021655552e+16,
      "budget_used_percent": 14.329832021655553
    },
    {
      "type": "training",
      "description": "Training step 603",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:05",
      "total_flops_so_far": 1.4353596253863936e+16,
      "budget_used_percent": 14.353596253863937
    },
    {
      "type": "training",
      "description": "Training step 604",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:06",
      "total_flops_so_far": 1.437736048607232e+16,
      "budget_used_percent": 14.377360486072318
    },
    {
      "type": "training",
      "description": "Training step 605",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:07",
      "total_flops_so_far": 1.4401124718280704e+16,
      "budget_used_percent": 14.401124718280704
    },
    {
      "type": "training",
      "description": "Training step 606",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:09",
      "total_flops_so_far": 1.4424888950489088e+16,
      "budget_used_percent": 14.424888950489088
    },
    {
      "type": "training",
      "description": "Training step 607",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:10",
      "total_flops_so_far": 1.4448653182697472e+16,
      "budget_used_percent": 14.448653182697472
    },
    {
      "type": "training",
      "description": "Training step 608",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:11",
      "total_flops_so_far": 1.4472417414905856e+16,
      "budget_used_percent": 14.472417414905856
    },
    {
      "type": "training",
      "description": "Training step 609",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:13",
      "total_flops_so_far": 1.449618164711424e+16,
      "budget_used_percent": 14.49618164711424
    },
    {
      "type": "training",
      "description": "Training step 610",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:14",
      "total_flops_so_far": 1.4519945879322624e+16,
      "budget_used_percent": 14.519945879322623
    },
    {
      "type": "training",
      "description": "Training step 611",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:15",
      "total_flops_so_far": 1.4543710111531008e+16,
      "budget_used_percent": 14.543710111531007
    },
    {
      "type": "training",
      "description": "Training step 612",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:16",
      "total_flops_so_far": 1.4567474343739392e+16,
      "budget_used_percent": 14.567474343739391
    },
    {
      "type": "training",
      "description": "Training step 613",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:18",
      "total_flops_so_far": 1.4591238575947776e+16,
      "budget_used_percent": 14.591238575947777
    },
    {
      "type": "training",
      "description": "Training step 614",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:19",
      "total_flops_so_far": 1.461500280815616e+16,
      "budget_used_percent": 14.615002808156161
    },
    {
      "type": "training",
      "description": "Training step 615",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:20",
      "total_flops_so_far": 1.4638767040364544e+16,
      "budget_used_percent": 14.638767040364545
    },
    {
      "type": "training",
      "description": "Training step 616",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:22",
      "total_flops_so_far": 1.4662531272572928e+16,
      "budget_used_percent": 14.662531272572929
    },
    {
      "type": "training",
      "description": "Training step 617",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:23",
      "total_flops_so_far": 1.4686295504781312e+16,
      "budget_used_percent": 14.686295504781313
    },
    {
      "type": "training",
      "description": "Training step 618",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:24",
      "total_flops_so_far": 1.4710059736989696e+16,
      "budget_used_percent": 14.710059736989697
    },
    {
      "type": "training",
      "description": "Training step 619",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:25",
      "total_flops_so_far": 1.473382396919808e+16,
      "budget_used_percent": 14.73382396919808
    },
    {
      "type": "training",
      "description": "Training step 620",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:27",
      "total_flops_so_far": 1.4757588201406464e+16,
      "budget_used_percent": 14.757588201406463
    },
    {
      "type": "training",
      "description": "Training step 621",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:28",
      "total_flops_so_far": 1.4781352433614848e+16,
      "budget_used_percent": 14.781352433614847
    },
    {
      "type": "training",
      "description": "Training step 622",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:29",
      "total_flops_so_far": 1.4805116665823232e+16,
      "budget_used_percent": 14.805116665823231
    },
    {
      "type": "training",
      "description": "Training step 623",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:31",
      "total_flops_so_far": 1.4828880898031616e+16,
      "budget_used_percent": 14.828880898031615
    },
    {
      "type": "training",
      "description": "Training step 624",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:32",
      "total_flops_so_far": 1.485264513024e+16,
      "budget_used_percent": 14.852645130239999
    },
    {
      "type": "training",
      "description": "Training step 625",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:33",
      "total_flops_so_far": 1.4876409362448384e+16,
      "budget_used_percent": 14.876409362448385
    },
    {
      "type": "training",
      "description": "Training step 626",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:35",
      "total_flops_so_far": 1.4900173594656768e+16,
      "budget_used_percent": 14.900173594656769
    },
    {
      "type": "training",
      "description": "Training step 627",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:36",
      "total_flops_so_far": 1.4923937826865152e+16,
      "budget_used_percent": 14.923937826865153
    },
    {
      "type": "training",
      "description": "Training step 628",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:37",
      "total_flops_so_far": 1.4947702059073536e+16,
      "budget_used_percent": 14.947702059073537
    },
    {
      "type": "training",
      "description": "Training step 629",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:38",
      "total_flops_so_far": 1.497146629128192e+16,
      "budget_used_percent": 14.97146629128192
    },
    {
      "type": "training",
      "description": "Training step 630",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:40",
      "total_flops_so_far": 1.4995230523490304e+16,
      "budget_used_percent": 14.995230523490305
    },
    {
      "type": "training",
      "description": "Training step 631",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:41",
      "total_flops_so_far": 1.5018994755698688e+16,
      "budget_used_percent": 15.018994755698689
    },
    {
      "type": "training",
      "description": "Training step 632",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:42",
      "total_flops_so_far": 1.5042758987907072e+16,
      "budget_used_percent": 15.042758987907073
    },
    {
      "type": "training",
      "description": "Training step 633",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:44",
      "total_flops_so_far": 1.5066523220115456e+16,
      "budget_used_percent": 15.066523220115455
    },
    {
      "type": "training",
      "description": "Training step 634",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:45",
      "total_flops_so_far": 1.509028745232384e+16,
      "budget_used_percent": 15.090287452323839
    },
    {
      "type": "training",
      "description": "Training step 635",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:46",
      "total_flops_so_far": 1.5114051684532224e+16,
      "budget_used_percent": 15.114051684532223
    },
    {
      "type": "training",
      "description": "Training step 636",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:48",
      "total_flops_so_far": 1.5137815916740608e+16,
      "budget_used_percent": 15.137815916740607
    },
    {
      "type": "training",
      "description": "Training step 637",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:49",
      "total_flops_so_far": 1.5161580148948992e+16,
      "budget_used_percent": 15.16158014894899
    },
    {
      "type": "training",
      "description": "Training step 638",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:50",
      "total_flops_so_far": 1.5185344381157376e+16,
      "budget_used_percent": 15.185344381157377
    },
    {
      "type": "training",
      "description": "Training step 639",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:51",
      "total_flops_so_far": 1.520910861336576e+16,
      "budget_used_percent": 15.20910861336576
    },
    {
      "type": "training",
      "description": "Training step 640",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:53",
      "total_flops_so_far": 1.5232872845574144e+16,
      "budget_used_percent": 15.232872845574144
    },
    {
      "type": "training",
      "description": "Training step 641",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:54",
      "total_flops_so_far": 1.5256637077782528e+16,
      "budget_used_percent": 15.256637077782528
    },
    {
      "type": "training",
      "description": "Training step 642",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:55",
      "total_flops_so_far": 1.5280401309990912e+16,
      "budget_used_percent": 15.280401309990912
    },
    {
      "type": "training",
      "description": "Training step 643",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:57",
      "total_flops_so_far": 1.5304165542199296e+16,
      "budget_used_percent": 15.304165542199296
    },
    {
      "type": "training",
      "description": "Training step 644",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:58",
      "total_flops_so_far": 1.532792977440768e+16,
      "budget_used_percent": 15.32792977440768
    },
    {
      "type": "training",
      "description": "Training step 645",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:23:59",
      "total_flops_so_far": 1.5351694006616064e+16,
      "budget_used_percent": 15.351694006616064
    },
    {
      "type": "training",
      "description": "Training step 646",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:00",
      "total_flops_so_far": 1.5375458238824448e+16,
      "budget_used_percent": 15.37545823882445
    },
    {
      "type": "training",
      "description": "Training step 647",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:02",
      "total_flops_so_far": 1.5399222471032832e+16,
      "budget_used_percent": 15.39922247103283
    },
    {
      "type": "training",
      "description": "Training step 648",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:03",
      "total_flops_so_far": 1.5422986703241216e+16,
      "budget_used_percent": 15.422986703241214
    },
    {
      "type": "training",
      "description": "Training step 649",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:04",
      "total_flops_so_far": 1.54467509354496e+16,
      "budget_used_percent": 15.446750935449598
    },
    {
      "type": "training",
      "description": "Training step 650",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:06",
      "total_flops_so_far": 1.5470515167657984e+16,
      "budget_used_percent": 15.470515167657982
    },
    {
      "type": "training",
      "description": "Training step 651",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:07",
      "total_flops_so_far": 1.5494279399866368e+16,
      "budget_used_percent": 15.494279399866368
    },
    {
      "type": "training",
      "description": "Training step 652",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:08",
      "total_flops_so_far": 1.5518043632074752e+16,
      "budget_used_percent": 15.518043632074752
    },
    {
      "type": "training",
      "description": "Training step 653",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:10",
      "total_flops_so_far": 1.5541807864283136e+16,
      "budget_used_percent": 15.541807864283136
    },
    {
      "type": "training",
      "description": "Training step 654",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:11",
      "total_flops_so_far": 1.556557209649152e+16,
      "budget_used_percent": 15.56557209649152
    },
    {
      "type": "training",
      "description": "Training step 655",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:12",
      "total_flops_so_far": 1.5589336328699904e+16,
      "budget_used_percent": 15.589336328699904
    },
    {
      "type": "training",
      "description": "Training step 656",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:13",
      "total_flops_so_far": 1.5613100560908288e+16,
      "budget_used_percent": 15.613100560908288
    },
    {
      "type": "training",
      "description": "Training step 657",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:15",
      "total_flops_so_far": 1.5636864793116672e+16,
      "budget_used_percent": 15.636864793116672
    },
    {
      "type": "training",
      "description": "Training step 658",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:16",
      "total_flops_so_far": 1.5660629025325056e+16,
      "budget_used_percent": 15.660629025325058
    },
    {
      "type": "training",
      "description": "Training step 659",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:17",
      "total_flops_so_far": 1.568439325753344e+16,
      "budget_used_percent": 15.684393257533442
    },
    {
      "type": "training",
      "description": "Training step 660",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:18",
      "total_flops_so_far": 1.5708157489741824e+16,
      "budget_used_percent": 15.708157489741826
    },
    {
      "type": "training",
      "description": "Training step 661",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:19",
      "total_flops_so_far": 1.5731921721950208e+16,
      "budget_used_percent": 15.731921721950206
    },
    {
      "type": "training",
      "description": "Training step 662",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:20",
      "total_flops_so_far": 1.5755685954158592e+16,
      "budget_used_percent": 15.75568595415859
    },
    {
      "type": "training",
      "description": "Training step 663",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:22",
      "total_flops_so_far": 1.5779450186366976e+16,
      "budget_used_percent": 15.779450186366976
    },
    {
      "type": "training",
      "description": "Training step 664",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:23",
      "total_flops_so_far": 1.580321441857536e+16,
      "budget_used_percent": 15.80321441857536
    },
    {
      "type": "training",
      "description": "Training step 665",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:24",
      "total_flops_so_far": 1.5826978650783744e+16,
      "budget_used_percent": 15.826978650783744
    },
    {
      "type": "training",
      "description": "Training step 666",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:25",
      "total_flops_so_far": 1.5850742882992128e+16,
      "budget_used_percent": 15.850742882992128
    },
    {
      "type": "training",
      "description": "Training step 667",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:27",
      "total_flops_so_far": 1.5874507115200512e+16,
      "budget_used_percent": 15.874507115200512
    },
    {
      "type": "training",
      "description": "Training step 668",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:28",
      "total_flops_so_far": 1.5898271347408896e+16,
      "budget_used_percent": 15.898271347408896
    },
    {
      "type": "training",
      "description": "Training step 669",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:29",
      "total_flops_so_far": 1.592203557961728e+16,
      "budget_used_percent": 15.92203557961728
    },
    {
      "type": "training",
      "description": "Training step 670",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:31",
      "total_flops_so_far": 1.5945799811825664e+16,
      "budget_used_percent": 15.945799811825664
    },
    {
      "type": "training",
      "description": "Training step 671",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:32",
      "total_flops_so_far": 1.5969564044034048e+16,
      "budget_used_percent": 15.96956404403405
    },
    {
      "type": "training",
      "description": "Training step 672",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:33",
      "total_flops_so_far": 1.5993328276242432e+16,
      "budget_used_percent": 15.993328276242433
    },
    {
      "type": "training",
      "description": "Training step 673",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:35",
      "total_flops_so_far": 1.6017092508450816e+16,
      "budget_used_percent": 16.017092508450816
    },
    {
      "type": "training",
      "description": "Training step 674",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:36",
      "total_flops_so_far": 1.60408567406592e+16,
      "budget_used_percent": 16.0408567406592
    },
    {
      "type": "training",
      "description": "Training step 675",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:37",
      "total_flops_so_far": 1.6064620972867584e+16,
      "budget_used_percent": 16.064620972867584
    },
    {
      "type": "training",
      "description": "Training step 676",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:38",
      "total_flops_so_far": 1.6088385205075968e+16,
      "budget_used_percent": 16.088385205075966
    },
    {
      "type": "training",
      "description": "Training step 677",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:40",
      "total_flops_so_far": 1.6112149437284352e+16,
      "budget_used_percent": 16.11214943728435
    },
    {
      "type": "training",
      "description": "Training step 678",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:41",
      "total_flops_so_far": 1.6135913669492736e+16,
      "budget_used_percent": 16.135913669492734
    },
    {
      "type": "training",
      "description": "Training step 679",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:42",
      "total_flops_so_far": 1.615967790170112e+16,
      "budget_used_percent": 16.15967790170112
    },
    {
      "type": "training",
      "description": "Training step 680",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:44",
      "total_flops_so_far": 1.6183442133909504e+16,
      "budget_used_percent": 16.183442133909505
    },
    {
      "type": "training",
      "description": "Training step 681",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:45",
      "total_flops_so_far": 1.6207206366117888e+16,
      "budget_used_percent": 16.207206366117887
    },
    {
      "type": "training",
      "description": "Training step 682",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:46",
      "total_flops_so_far": 1.6230970598326272e+16,
      "budget_used_percent": 16.230970598326273
    },
    {
      "type": "training",
      "description": "Training step 683",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:48",
      "total_flops_so_far": 1.6254734830534656e+16,
      "budget_used_percent": 16.254734830534655
    },
    {
      "type": "training",
      "description": "Training step 684",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:49",
      "total_flops_so_far": 1.627849906274304e+16,
      "budget_used_percent": 16.27849906274304
    },
    {
      "type": "training",
      "description": "Training step 685",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:50",
      "total_flops_so_far": 1.6302263294951424e+16,
      "budget_used_percent": 16.302263294951423
    },
    {
      "type": "training",
      "description": "Training step 686",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:51",
      "total_flops_so_far": 1.6326027527159808e+16,
      "budget_used_percent": 16.32602752715981
    },
    {
      "type": "training",
      "description": "Training step 687",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:53",
      "total_flops_so_far": 1.6349791759368192e+16,
      "budget_used_percent": 16.349791759368195
    },
    {
      "type": "training",
      "description": "Training step 688",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:54",
      "total_flops_so_far": 1.6373555991576576e+16,
      "budget_used_percent": 16.373555991576577
    },
    {
      "type": "training",
      "description": "Training step 689",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:55",
      "total_flops_so_far": 1.639732022378496e+16,
      "budget_used_percent": 16.397320223784963
    },
    {
      "type": "training",
      "description": "Training step 690",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:57",
      "total_flops_so_far": 1.6421084455993344e+16,
      "budget_used_percent": 16.42108445599334
    },
    {
      "type": "training",
      "description": "Training step 691",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:58",
      "total_flops_so_far": 1.6444848688201728e+16,
      "budget_used_percent": 16.444848688201727
    },
    {
      "type": "training",
      "description": "Training step 692",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:24:59",
      "total_flops_so_far": 1.6468612920410112e+16,
      "budget_used_percent": 16.468612920410113
    },
    {
      "type": "training",
      "description": "Training step 693",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:00",
      "total_flops_so_far": 1.6492377152618496e+16,
      "budget_used_percent": 16.492377152618495
    },
    {
      "type": "training",
      "description": "Training step 694",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:02",
      "total_flops_so_far": 1.651614138482688e+16,
      "budget_used_percent": 16.51614138482688
    },
    {
      "type": "training",
      "description": "Training step 695",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:03",
      "total_flops_so_far": 1.6539905617035264e+16,
      "budget_used_percent": 16.539905617035263
    },
    {
      "type": "training",
      "description": "Training step 696",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:04",
      "total_flops_so_far": 1.6563669849243648e+16,
      "budget_used_percent": 16.56366984924365
    },
    {
      "type": "training",
      "description": "Training step 697",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:06",
      "total_flops_so_far": 1.6587434081452032e+16,
      "budget_used_percent": 16.58743408145203
    },
    {
      "type": "training",
      "description": "Training step 698",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:07",
      "total_flops_so_far": 1.6611198313660416e+16,
      "budget_used_percent": 16.611198313660417
    },
    {
      "type": "training",
      "description": "Training step 699",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:08",
      "total_flops_so_far": 1.66349625458688e+16,
      "budget_used_percent": 16.6349625458688
    },
    {
      "type": "training",
      "description": "Training step 700",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:10",
      "total_flops_so_far": 1.6658726778077184e+16,
      "budget_used_percent": 16.658726778077185
    },
    {
      "type": "training",
      "description": "Training step 701",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:11",
      "total_flops_so_far": 1.6682491010285568e+16,
      "budget_used_percent": 16.68249101028557
    },
    {
      "type": "training",
      "description": "Training step 702",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:12",
      "total_flops_so_far": 1.6706255242493952e+16,
      "budget_used_percent": 16.706255242493953
    },
    {
      "type": "training",
      "description": "Training step 703",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:14",
      "total_flops_so_far": 1.6730019474702336e+16,
      "budget_used_percent": 16.73001947470234
    },
    {
      "type": "training",
      "description": "Training step 704",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:15",
      "total_flops_so_far": 1.675378370691072e+16,
      "budget_used_percent": 16.753783706910717
    },
    {
      "type": "training",
      "description": "Training step 705",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:16",
      "total_flops_so_far": 1.6777547939119104e+16,
      "budget_used_percent": 16.777547939119103
    },
    {
      "type": "training",
      "description": "Training step 706",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:18",
      "total_flops_so_far": 1.6801312171327488e+16,
      "budget_used_percent": 16.80131217132749
    },
    {
      "type": "training",
      "description": "Training step 707",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:19",
      "total_flops_so_far": 1.6825076403535872e+16,
      "budget_used_percent": 16.82507640353587
    },
    {
      "type": "training",
      "description": "Training step 708",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:20",
      "total_flops_so_far": 1.6848840635744256e+16,
      "budget_used_percent": 16.848840635744256
    },
    {
      "type": "training",
      "description": "Training step 709",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:21",
      "total_flops_so_far": 1.687260486795264e+16,
      "budget_used_percent": 16.87260486795264
    },
    {
      "type": "training",
      "description": "Training step 710",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:23",
      "total_flops_so_far": 1.6896369100161024e+16,
      "budget_used_percent": 16.896369100161024
    },
    {
      "type": "training",
      "description": "Training step 711",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:24",
      "total_flops_so_far": 1.6920133332369408e+16,
      "budget_used_percent": 16.920133332369407
    },
    {
      "type": "training",
      "description": "Training step 712",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:25",
      "total_flops_so_far": 1.6943897564577792e+16,
      "budget_used_percent": 16.943897564577792
    },
    {
      "type": "training",
      "description": "Training step 713",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:27",
      "total_flops_so_far": 1.6967661796786176e+16,
      "budget_used_percent": 16.967661796786178
    },
    {
      "type": "training",
      "description": "Training step 714",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:28",
      "total_flops_so_far": 1.699142602899456e+16,
      "budget_used_percent": 16.99142602899456
    },
    {
      "type": "training",
      "description": "Training step 715",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:29",
      "total_flops_so_far": 1.7015190261202944e+16,
      "budget_used_percent": 17.015190261202946
    },
    {
      "type": "training",
      "description": "Training step 716",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:30",
      "total_flops_so_far": 1.7038954493411328e+16,
      "budget_used_percent": 17.038954493411328
    },
    {
      "type": "training",
      "description": "Training step 717",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:32",
      "total_flops_so_far": 1.7062718725619712e+16,
      "budget_used_percent": 17.062718725619714
    },
    {
      "type": "training",
      "description": "Training step 718",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:33",
      "total_flops_so_far": 1.7086482957828096e+16,
      "budget_used_percent": 17.086482957828096
    },
    {
      "type": "training",
      "description": "Training step 719",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:34",
      "total_flops_so_far": 1.711024719003648e+16,
      "budget_used_percent": 17.11024719003648
    },
    {
      "type": "training",
      "description": "Training step 720",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:36",
      "total_flops_so_far": 1.7134011422244864e+16,
      "budget_used_percent": 17.134011422244864
    },
    {
      "type": "training",
      "description": "Training step 721",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:37",
      "total_flops_so_far": 1.7157775654453248e+16,
      "budget_used_percent": 17.157775654453246
    },
    {
      "type": "training",
      "description": "Training step 722",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:38",
      "total_flops_so_far": 1.7181539886661632e+16,
      "budget_used_percent": 17.181539886661632
    },
    {
      "type": "training",
      "description": "Training step 723",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:40",
      "total_flops_so_far": 1.7205304118870016e+16,
      "budget_used_percent": 17.205304118870014
    },
    {
      "type": "training",
      "description": "Training step 724",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:41",
      "total_flops_so_far": 1.72290683510784e+16,
      "budget_used_percent": 17.2290683510784
    },
    {
      "type": "training",
      "description": "Training step 725",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:42",
      "total_flops_so_far": 1.7252832583286784e+16,
      "budget_used_percent": 17.252832583286786
    },
    {
      "type": "training",
      "description": "Training step 726",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:43",
      "total_flops_so_far": 1.7276596815495168e+16,
      "budget_used_percent": 17.276596815495168
    },
    {
      "type": "training",
      "description": "Training step 727",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:45",
      "total_flops_so_far": 1.7300361047703552e+16,
      "budget_used_percent": 17.300361047703554
    },
    {
      "type": "training",
      "description": "Training step 728",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:46",
      "total_flops_so_far": 1.7324125279911936e+16,
      "budget_used_percent": 17.324125279911936
    },
    {
      "type": "training",
      "description": "Training step 729",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:47",
      "total_flops_so_far": 1.734788951212032e+16,
      "budget_used_percent": 17.34788951212032
    },
    {
      "type": "training",
      "description": "Training step 730",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:49",
      "total_flops_so_far": 1.7371653744328704e+16,
      "budget_used_percent": 17.371653744328704
    },
    {
      "type": "training",
      "description": "Training step 731",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:50",
      "total_flops_so_far": 1.7395417976537088e+16,
      "budget_used_percent": 17.39541797653709
    },
    {
      "type": "training",
      "description": "Training step 732",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:51",
      "total_flops_so_far": 1.7419182208745472e+16,
      "budget_used_percent": 17.41918220874547
    },
    {
      "type": "training",
      "description": "Training step 733",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:53",
      "total_flops_so_far": 1.7442946440953856e+16,
      "budget_used_percent": 17.442946440953854
    },
    {
      "type": "training",
      "description": "Training step 734",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:54",
      "total_flops_so_far": 1.746671067316224e+16,
      "budget_used_percent": 17.46671067316224
    },
    {
      "type": "training",
      "description": "Training step 735",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:55",
      "total_flops_so_far": 1.7490474905370624e+16,
      "budget_used_percent": 17.490474905370622
    },
    {
      "type": "training",
      "description": "Training step 736",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:56",
      "total_flops_so_far": 1.7514239137579008e+16,
      "budget_used_percent": 17.514239137579008
    },
    {
      "type": "training",
      "description": "Training step 737",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:58",
      "total_flops_so_far": 1.7538003369787392e+16,
      "budget_used_percent": 17.53800336978739
    },
    {
      "type": "training",
      "description": "Training step 738",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:25:59",
      "total_flops_so_far": 1.7561767601995776e+16,
      "budget_used_percent": 17.561767601995776
    },
    {
      "type": "training",
      "description": "Training step 739",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:00",
      "total_flops_so_far": 1.758553183420416e+16,
      "budget_used_percent": 17.58553183420416
    },
    {
      "type": "training",
      "description": "Training step 740",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:02",
      "total_flops_so_far": 1.7609296066412544e+16,
      "budget_used_percent": 17.609296066412544
    },
    {
      "type": "training",
      "description": "Training step 741",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:03",
      "total_flops_so_far": 1.7633060298620928e+16,
      "budget_used_percent": 17.63306029862093
    },
    {
      "type": "training",
      "description": "Training step 742",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:04",
      "total_flops_so_far": 1.7656824530829312e+16,
      "budget_used_percent": 17.65682453082931
    },
    {
      "type": "training",
      "description": "Training step 743",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:06",
      "total_flops_so_far": 1.7680588763037696e+16,
      "budget_used_percent": 17.680588763037697
    },
    {
      "type": "training",
      "description": "Training step 744",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:07",
      "total_flops_so_far": 1.770435299524608e+16,
      "budget_used_percent": 17.70435299524608
    },
    {
      "type": "training",
      "description": "Training step 745",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:08",
      "total_flops_so_far": 1.7728117227454464e+16,
      "budget_used_percent": 17.728117227454465
    },
    {
      "type": "training",
      "description": "Training step 746",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:09",
      "total_flops_so_far": 1.7751881459662848e+16,
      "budget_used_percent": 17.75188145966285
    },
    {
      "type": "training",
      "description": "Training step 747",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:11",
      "total_flops_so_far": 1.7775645691871232e+16,
      "budget_used_percent": 17.77564569187123
    },
    {
      "type": "training",
      "description": "Training step 748",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:12",
      "total_flops_so_far": 1.7799409924079616e+16,
      "budget_used_percent": 17.799409924079615
    },
    {
      "type": "training",
      "description": "Training step 749",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:13",
      "total_flops_so_far": 1.7823174156288e+16,
      "budget_used_percent": 17.823174156287998
    },
    {
      "type": "training",
      "description": "Training step 750",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:15",
      "total_flops_so_far": 1.7846938388496384e+16,
      "budget_used_percent": 17.846938388496383
    },
    {
      "type": "training",
      "description": "Training step 751",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:16",
      "total_flops_so_far": 1.7870702620704768e+16,
      "budget_used_percent": 17.87070262070477
    },
    {
      "type": "training",
      "description": "Training step 752",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:17",
      "total_flops_so_far": 1.7894466852913152e+16,
      "budget_used_percent": 17.89446685291315
    },
    {
      "type": "training",
      "description": "Training step 753",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:19",
      "total_flops_so_far": 1.7918231085121536e+16,
      "budget_used_percent": 17.918231085121537
    },
    {
      "type": "training",
      "description": "Training step 754",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:20",
      "total_flops_so_far": 1.794199531732992e+16,
      "budget_used_percent": 17.94199531732992
    },
    {
      "type": "training",
      "description": "Training step 755",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:21",
      "total_flops_so_far": 1.7965759549538304e+16,
      "budget_used_percent": 17.965759549538305
    },
    {
      "type": "training",
      "description": "Training step 756",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:22",
      "total_flops_so_far": 1.7989523781746688e+16,
      "budget_used_percent": 17.989523781746687
    },
    {
      "type": "training",
      "description": "Training step 757",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:24",
      "total_flops_so_far": 1.8013288013955072e+16,
      "budget_used_percent": 18.013288013955073
    },
    {
      "type": "training",
      "description": "Training step 758",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:25",
      "total_flops_so_far": 1.8037052246163456e+16,
      "budget_used_percent": 18.03705224616346
    },
    {
      "type": "training",
      "description": "Training step 759",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:26",
      "total_flops_so_far": 1.806081647837184e+16,
      "budget_used_percent": 18.06081647837184
    },
    {
      "type": "training",
      "description": "Training step 760",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:28",
      "total_flops_so_far": 1.8084580710580224e+16,
      "budget_used_percent": 18.084580710580227
    },
    {
      "type": "training",
      "description": "Training step 761",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:29",
      "total_flops_so_far": 1.810834494278861e+16,
      "budget_used_percent": 18.108344942788605
    },
    {
      "type": "training",
      "description": "Training step 762",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:30",
      "total_flops_so_far": 1.813210917499699e+16,
      "budget_used_percent": 18.13210917499699
    },
    {
      "type": "training",
      "description": "Training step 763",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:31",
      "total_flops_so_far": 1.8155873407205376e+16,
      "budget_used_percent": 18.155873407205377
    },
    {
      "type": "training",
      "description": "Training step 764",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:33",
      "total_flops_so_far": 1.817963763941376e+16,
      "budget_used_percent": 18.17963763941376
    },
    {
      "type": "training",
      "description": "Training step 765",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:34",
      "total_flops_so_far": 1.8203401871622144e+16,
      "budget_used_percent": 18.203401871622145
    },
    {
      "type": "training",
      "description": "Training step 766",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:35",
      "total_flops_so_far": 1.822716610383053e+16,
      "budget_used_percent": 18.227166103830527
    },
    {
      "type": "training",
      "description": "Training step 767",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:37",
      "total_flops_so_far": 1.825093033603891e+16,
      "budget_used_percent": 18.250930336038913
    },
    {
      "type": "training",
      "description": "Training step 768",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:38",
      "total_flops_so_far": 1.8274694568247296e+16,
      "budget_used_percent": 18.274694568247295
    },
    {
      "type": "training",
      "description": "Training step 769",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:39",
      "total_flops_so_far": 1.829845880045568e+16,
      "budget_used_percent": 18.29845880045568
    },
    {
      "type": "training",
      "description": "Training step 770",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:41",
      "total_flops_so_far": 1.8322223032664064e+16,
      "budget_used_percent": 18.322223032664063
    },
    {
      "type": "training",
      "description": "Training step 771",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:42",
      "total_flops_so_far": 1.834598726487245e+16,
      "budget_used_percent": 18.34598726487245
    },
    {
      "type": "training",
      "description": "Training step 772",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:43",
      "total_flops_so_far": 1.836975149708083e+16,
      "budget_used_percent": 18.369751497080834
    },
    {
      "type": "training",
      "description": "Training step 773",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:44",
      "total_flops_so_far": 1.8393515729289216e+16,
      "budget_used_percent": 18.393515729289216
    },
    {
      "type": "training",
      "description": "Training step 774",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:46",
      "total_flops_so_far": 1.84172799614976e+16,
      "budget_used_percent": 18.417279961497602
    },
    {
      "type": "training",
      "description": "Training step 775",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:47",
      "total_flops_so_far": 1.8441044193705984e+16,
      "budget_used_percent": 18.441044193705984
    },
    {
      "type": "training",
      "description": "Training step 776",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:48",
      "total_flops_so_far": 1.846480842591437e+16,
      "budget_used_percent": 18.464808425914367
    },
    {
      "type": "training",
      "description": "Training step 777",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:50",
      "total_flops_so_far": 1.848857265812275e+16,
      "budget_used_percent": 18.488572658122752
    },
    {
      "type": "training",
      "description": "Training step 778",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:51",
      "total_flops_so_far": 1.8512336890331136e+16,
      "budget_used_percent": 18.512336890331134
    },
    {
      "type": "training",
      "description": "Training step 779",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:52",
      "total_flops_so_far": 1.853610112253952e+16,
      "budget_used_percent": 18.53610112253952
    },
    {
      "type": "training",
      "description": "Training step 780",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:54",
      "total_flops_so_far": 1.8559865354747904e+16,
      "budget_used_percent": 18.559865354747902
    },
    {
      "type": "training",
      "description": "Training step 781",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:55",
      "total_flops_so_far": 1.858362958695629e+16,
      "budget_used_percent": 18.583629586956288
    },
    {
      "type": "training",
      "description": "Training step 782",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:56",
      "total_flops_so_far": 1.860739381916467e+16,
      "budget_used_percent": 18.60739381916467
    },
    {
      "type": "training",
      "description": "Training step 783",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:57",
      "total_flops_so_far": 1.8631158051373056e+16,
      "budget_used_percent": 18.631158051373056
    },
    {
      "type": "training",
      "description": "Training step 784",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:26:59",
      "total_flops_so_far": 1.865492228358144e+16,
      "budget_used_percent": 18.654922283581442
    },
    {
      "type": "training",
      "description": "Training step 785",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:00",
      "total_flops_so_far": 1.8678686515789824e+16,
      "budget_used_percent": 18.678686515789824
    },
    {
      "type": "training",
      "description": "Training step 786",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:01",
      "total_flops_so_far": 1.870245074799821e+16,
      "budget_used_percent": 18.70245074799821
    },
    {
      "type": "training",
      "description": "Training step 787",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:03",
      "total_flops_so_far": 1.872621498020659e+16,
      "budget_used_percent": 18.726214980206592
    },
    {
      "type": "training",
      "description": "Training step 788",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:04",
      "total_flops_so_far": 1.8749979212414976e+16,
      "budget_used_percent": 18.749979212414978
    },
    {
      "type": "training",
      "description": "Training step 789",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:05",
      "total_flops_so_far": 1.877374344462336e+16,
      "budget_used_percent": 18.77374344462336
    },
    {
      "type": "training",
      "description": "Training step 790",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:07",
      "total_flops_so_far": 1.8797507676831744e+16,
      "budget_used_percent": 18.797507676831742
    },
    {
      "type": "training",
      "description": "Training step 791",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:08",
      "total_flops_so_far": 1.882127190904013e+16,
      "budget_used_percent": 18.821271909040128
    },
    {
      "type": "training",
      "description": "Training step 792",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:08",
      "total_flops_so_far": 1.884503614124851e+16,
      "budget_used_percent": 18.84503614124851
    },
    {
      "type": "training",
      "description": "Training step 793",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:10",
      "total_flops_so_far": 1.8868800373456896e+16,
      "budget_used_percent": 18.868800373456896
    },
    {
      "type": "training",
      "description": "Training step 794",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:11",
      "total_flops_so_far": 1.889256460566528e+16,
      "budget_used_percent": 18.892564605665278
    },
    {
      "type": "training",
      "description": "Training step 795",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:12",
      "total_flops_so_far": 1.8916328837873664e+16,
      "budget_used_percent": 18.916328837873664
    },
    {
      "type": "training",
      "description": "Training step 796",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:13",
      "total_flops_so_far": 1.894009307008205e+16,
      "budget_used_percent": 18.94009307008205
    },
    {
      "type": "training",
      "description": "Training step 797",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:15",
      "total_flops_so_far": 1.896385730229043e+16,
      "budget_used_percent": 18.96385730229043
    },
    {
      "type": "training",
      "description": "Training step 798",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:16",
      "total_flops_so_far": 1.8987621534498816e+16,
      "budget_used_percent": 18.987621534498818
    },
    {
      "type": "training",
      "description": "Training step 799",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:17",
      "total_flops_so_far": 1.90113857667072e+16,
      "budget_used_percent": 19.0113857667072
    },
    {
      "type": "training",
      "description": "Training step 800",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:19",
      "total_flops_so_far": 1.9035149998915584e+16,
      "budget_used_percent": 19.035149998915585
    },
    {
      "type": "training",
      "description": "Training step 801",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:20",
      "total_flops_so_far": 1.905891423112397e+16,
      "budget_used_percent": 19.058914231123968
    },
    {
      "type": "training",
      "description": "Training step 802",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:21",
      "total_flops_so_far": 1.908267846333235e+16,
      "budget_used_percent": 19.082678463332353
    },
    {
      "type": "training",
      "description": "Training step 803",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:23",
      "total_flops_so_far": 1.9106442695540736e+16,
      "budget_used_percent": 19.106442695540736
    },
    {
      "type": "training",
      "description": "Training step 804",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:24",
      "total_flops_so_far": 1.913020692774912e+16,
      "budget_used_percent": 19.130206927749118
    },
    {
      "type": "training",
      "description": "Training step 805",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:25",
      "total_flops_so_far": 1.9153971159957504e+16,
      "budget_used_percent": 19.153971159957504
    },
    {
      "type": "training",
      "description": "Training step 806",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:26",
      "total_flops_so_far": 1.917773539216589e+16,
      "budget_used_percent": 19.177735392165886
    },
    {
      "type": "training",
      "description": "Training step 807",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:28",
      "total_flops_so_far": 1.920149962437427e+16,
      "budget_used_percent": 19.20149962437427
    },
    {
      "type": "training",
      "description": "Training step 808",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:29",
      "total_flops_so_far": 1.9225263856582656e+16,
      "budget_used_percent": 19.225263856582657
    },
    {
      "type": "training",
      "description": "Training step 809",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:30",
      "total_flops_so_far": 1.924902808879104e+16,
      "budget_used_percent": 19.24902808879104
    },
    {
      "type": "training",
      "description": "Training step 810",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:32",
      "total_flops_so_far": 1.9272792320999424e+16,
      "budget_used_percent": 19.272792320999425
    },
    {
      "type": "training",
      "description": "Training step 811",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:33",
      "total_flops_so_far": 1.929655655320781e+16,
      "budget_used_percent": 19.296556553207807
    },
    {
      "type": "training",
      "description": "Training step 812",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:34",
      "total_flops_so_far": 1.932032078541619e+16,
      "budget_used_percent": 19.320320785416193
    },
    {
      "type": "training",
      "description": "Training step 813",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:36",
      "total_flops_so_far": 1.9344085017624576e+16,
      "budget_used_percent": 19.344085017624575
    },
    {
      "type": "training",
      "description": "Training step 814",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:37",
      "total_flops_so_far": 1.936784924983296e+16,
      "budget_used_percent": 19.36784924983296
    },
    {
      "type": "training",
      "description": "Training step 815",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:38",
      "total_flops_so_far": 1.9391613482041344e+16,
      "budget_used_percent": 19.391613482041343
    },
    {
      "type": "training",
      "description": "Training step 816",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:39",
      "total_flops_so_far": 1.941537771424973e+16,
      "budget_used_percent": 19.41537771424973
    },
    {
      "type": "training",
      "description": "Training step 817",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:41",
      "total_flops_so_far": 1.943914194645811e+16,
      "budget_used_percent": 19.439141946458115
    },
    {
      "type": "training",
      "description": "Training step 818",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:42",
      "total_flops_so_far": 1.9462906178666496e+16,
      "budget_used_percent": 19.462906178666493
    },
    {
      "type": "training",
      "description": "Training step 819",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:43",
      "total_flops_so_far": 1.948667041087488e+16,
      "budget_used_percent": 19.48667041087488
    },
    {
      "type": "training",
      "description": "Training step 820",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:45",
      "total_flops_so_far": 1.9510434643083264e+16,
      "budget_used_percent": 19.510434643083265
    },
    {
      "type": "training",
      "description": "Training step 821",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:46",
      "total_flops_so_far": 1.953419887529165e+16,
      "budget_used_percent": 19.534198875291647
    },
    {
      "type": "training",
      "description": "Training step 822",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:47",
      "total_flops_so_far": 1.955796310750003e+16,
      "budget_used_percent": 19.557963107500033
    },
    {
      "type": "training",
      "description": "Training step 823",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:49",
      "total_flops_so_far": 1.9581727339708416e+16,
      "budget_used_percent": 19.581727339708415
    },
    {
      "type": "training",
      "description": "Training step 824",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:50",
      "total_flops_so_far": 1.96054915719168e+16,
      "budget_used_percent": 19.6054915719168
    },
    {
      "type": "training",
      "description": "Training step 825",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:51",
      "total_flops_so_far": 1.9629255804125184e+16,
      "budget_used_percent": 19.629255804125183
    },
    {
      "type": "training",
      "description": "Training step 826",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:52",
      "total_flops_so_far": 1.965302003633357e+16,
      "budget_used_percent": 19.65302003633357
    },
    {
      "type": "training",
      "description": "Training step 827",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:54",
      "total_flops_so_far": 1.967678426854195e+16,
      "budget_used_percent": 19.67678426854195
    },
    {
      "type": "training",
      "description": "Training step 828",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:55",
      "total_flops_so_far": 1.9700548500750336e+16,
      "budget_used_percent": 19.700548500750337
    },
    {
      "type": "training",
      "description": "Training step 829",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:56",
      "total_flops_so_far": 1.972431273295872e+16,
      "budget_used_percent": 19.724312732958722
    },
    {
      "type": "training",
      "description": "Training step 830",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:58",
      "total_flops_so_far": 1.9748076965167104e+16,
      "budget_used_percent": 19.748076965167105
    },
    {
      "type": "training",
      "description": "Training step 831",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:27:59",
      "total_flops_so_far": 1.977184119737549e+16,
      "budget_used_percent": 19.77184119737549
    },
    {
      "type": "training",
      "description": "Training step 832",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:00",
      "total_flops_so_far": 1.979560542958387e+16,
      "budget_used_percent": 19.79560542958387
    },
    {
      "type": "training",
      "description": "Training step 833",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:02",
      "total_flops_so_far": 1.9819369661792256e+16,
      "budget_used_percent": 19.819369661792255
    },
    {
      "type": "training",
      "description": "Training step 834",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:03",
      "total_flops_so_far": 1.984313389400064e+16,
      "budget_used_percent": 19.84313389400064
    },
    {
      "type": "training",
      "description": "Training step 835",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:04",
      "total_flops_so_far": 1.9866898126209024e+16,
      "budget_used_percent": 19.866898126209023
    },
    {
      "type": "training",
      "description": "Training step 836",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:05",
      "total_flops_so_far": 1.989066235841741e+16,
      "budget_used_percent": 19.89066235841741
    },
    {
      "type": "training",
      "description": "Training step 837",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:07",
      "total_flops_so_far": 1.991442659062579e+16,
      "budget_used_percent": 19.91442659062579
    },
    {
      "type": "training",
      "description": "Training step 838",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:08",
      "total_flops_so_far": 1.9938190822834176e+16,
      "budget_used_percent": 19.938190822834176
    },
    {
      "type": "training",
      "description": "Training step 839",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:09",
      "total_flops_so_far": 1.996195505504256e+16,
      "budget_used_percent": 19.96195505504256
    },
    {
      "type": "training",
      "description": "Training step 840",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:11",
      "total_flops_so_far": 1.9985719287250944e+16,
      "budget_used_percent": 19.985719287250944
    },
    {
      "type": "training",
      "description": "Training step 841",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:12",
      "total_flops_so_far": 2.000948351945933e+16,
      "budget_used_percent": 20.00948351945933
    },
    {
      "type": "training",
      "description": "Training step 842",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:13",
      "total_flops_so_far": 2.003324775166771e+16,
      "budget_used_percent": 20.033247751667712
    },
    {
      "type": "training",
      "description": "Training step 843",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:14",
      "total_flops_so_far": 2.0057011983876096e+16,
      "budget_used_percent": 20.057011983876098
    },
    {
      "type": "training",
      "description": "Training step 844",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:16",
      "total_flops_so_far": 2.008077621608448e+16,
      "budget_used_percent": 20.08077621608448
    },
    {
      "type": "training",
      "description": "Training step 845",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:17",
      "total_flops_so_far": 2.0104540448292864e+16,
      "budget_used_percent": 20.104540448292866
    },
    {
      "type": "training",
      "description": "Training step 846",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:18",
      "total_flops_so_far": 2.012830468050125e+16,
      "budget_used_percent": 20.128304680501248
    },
    {
      "type": "training",
      "description": "Training step 847",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:20",
      "total_flops_so_far": 2.015206891270963e+16,
      "budget_used_percent": 20.15206891270963
    },
    {
      "type": "training",
      "description": "Training step 848",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:21",
      "total_flops_so_far": 2.0175833144918016e+16,
      "budget_used_percent": 20.175833144918016
    },
    {
      "type": "training",
      "description": "Training step 849",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:22",
      "total_flops_so_far": 2.01995973771264e+16,
      "budget_used_percent": 20.1995973771264
    },
    {
      "type": "training",
      "description": "Training step 850",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:24",
      "total_flops_so_far": 2.0223361609334784e+16,
      "budget_used_percent": 20.223361609334784
    },
    {
      "type": "training",
      "description": "Training step 851",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:25",
      "total_flops_so_far": 2.024712584154317e+16,
      "budget_used_percent": 20.247125841543166
    },
    {
      "type": "training",
      "description": "Training step 852",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:26",
      "total_flops_so_far": 2.027089007375155e+16,
      "budget_used_percent": 20.270890073751552
    },
    {
      "type": "training",
      "description": "Training step 853",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:27",
      "total_flops_so_far": 2.0294654305959936e+16,
      "budget_used_percent": 20.294654305959938
    },
    {
      "type": "training",
      "description": "Training step 854",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:29",
      "total_flops_so_far": 2.031841853816832e+16,
      "budget_used_percent": 20.31841853816832
    },
    {
      "type": "training",
      "description": "Training step 855",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:30",
      "total_flops_so_far": 2.0342182770376704e+16,
      "budget_used_percent": 20.342182770376706
    },
    {
      "type": "training",
      "description": "Training step 856",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:31",
      "total_flops_so_far": 2.036594700258509e+16,
      "budget_used_percent": 20.365947002585088
    },
    {
      "type": "training",
      "description": "Training step 857",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:33",
      "total_flops_so_far": 2.038971123479347e+16,
      "budget_used_percent": 20.389711234793474
    },
    {
      "type": "training",
      "description": "Training step 858",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:34",
      "total_flops_so_far": 2.0413475467001856e+16,
      "budget_used_percent": 20.413475467001856
    },
    {
      "type": "training",
      "description": "Training step 859",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:35",
      "total_flops_so_far": 2.043723969921024e+16,
      "budget_used_percent": 20.43723969921024
    },
    {
      "type": "training",
      "description": "Training step 860",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:37",
      "total_flops_so_far": 2.0461003931418624e+16,
      "budget_used_percent": 20.461003931418624
    },
    {
      "type": "training",
      "description": "Training step 861",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:38",
      "total_flops_so_far": 2.048476816362701e+16,
      "budget_used_percent": 20.484768163627006
    },
    {
      "type": "training",
      "description": "Training step 862",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:39",
      "total_flops_so_far": 2.050853239583539e+16,
      "budget_used_percent": 20.508532395835392
    },
    {
      "type": "training",
      "description": "Training step 863",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:41",
      "total_flops_so_far": 2.0532296628043776e+16,
      "budget_used_percent": 20.532296628043774
    },
    {
      "type": "training",
      "description": "Training step 864",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:42",
      "total_flops_so_far": 2.055606086025216e+16,
      "budget_used_percent": 20.55606086025216
    },
    {
      "type": "training",
      "description": "Training step 865",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:43",
      "total_flops_so_far": 2.0579825092460544e+16,
      "budget_used_percent": 20.579825092460542
    },
    {
      "type": "training",
      "description": "Training step 866",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:44",
      "total_flops_so_far": 2.060358932466893e+16,
      "budget_used_percent": 20.603589324668928
    },
    {
      "type": "training",
      "description": "Training step 867",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:46",
      "total_flops_so_far": 2.062735355687731e+16,
      "budget_used_percent": 20.627353556877313
    },
    {
      "type": "training",
      "description": "Training step 868",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:47",
      "total_flops_so_far": 2.0651117789085696e+16,
      "budget_used_percent": 20.651117789085696
    },
    {
      "type": "training",
      "description": "Training step 869",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:48",
      "total_flops_so_far": 2.067488202129408e+16,
      "budget_used_percent": 20.67488202129408
    },
    {
      "type": "training",
      "description": "Training step 870",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:50",
      "total_flops_so_far": 2.0698646253502464e+16,
      "budget_used_percent": 20.698646253502464
    },
    {
      "type": "training",
      "description": "Training step 871",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:51",
      "total_flops_so_far": 2.072241048571085e+16,
      "budget_used_percent": 20.72241048571085
    },
    {
      "type": "training",
      "description": "Training step 872",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:52",
      "total_flops_so_far": 2.074617471791923e+16,
      "budget_used_percent": 20.74617471791923
    },
    {
      "type": "training",
      "description": "Training step 873",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:54",
      "total_flops_so_far": 2.0769938950127616e+16,
      "budget_used_percent": 20.769938950127617
    },
    {
      "type": "training",
      "description": "Training step 874",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:55",
      "total_flops_so_far": 2.0793703182336e+16,
      "budget_used_percent": 20.793703182336003
    },
    {
      "type": "training",
      "description": "Training step 875",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:56",
      "total_flops_so_far": 2.0817467414544384e+16,
      "budget_used_percent": 20.81746741454438
    },
    {
      "type": "training",
      "description": "Training step 876",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:57",
      "total_flops_so_far": 2.084123164675277e+16,
      "budget_used_percent": 20.841231646752767
    },
    {
      "type": "training",
      "description": "Training step 877",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:28:59",
      "total_flops_so_far": 2.086499587896115e+16,
      "budget_used_percent": 20.86499587896115
    },
    {
      "type": "training",
      "description": "Training step 878",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:00",
      "total_flops_so_far": 2.0888760111169536e+16,
      "budget_used_percent": 20.888760111169535
    },
    {
      "type": "training",
      "description": "Training step 879",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:01",
      "total_flops_so_far": 2.091252434337792e+16,
      "budget_used_percent": 20.91252434337792
    },
    {
      "type": "training",
      "description": "Training step 880",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:03",
      "total_flops_so_far": 2.0936288575586304e+16,
      "budget_used_percent": 20.936288575586303
    },
    {
      "type": "training",
      "description": "Training step 881",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:04",
      "total_flops_so_far": 2.096005280779469e+16,
      "budget_used_percent": 20.96005280779469
    },
    {
      "type": "training",
      "description": "Training step 882",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:05",
      "total_flops_so_far": 2.098381704000307e+16,
      "budget_used_percent": 20.98381704000307
    },
    {
      "type": "training",
      "description": "Training step 883",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:07",
      "total_flops_so_far": 2.1007581272211456e+16,
      "budget_used_percent": 21.007581272211457
    },
    {
      "type": "training",
      "description": "Training step 884",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:08",
      "total_flops_so_far": 2.103134550441984e+16,
      "budget_used_percent": 21.03134550441984
    },
    {
      "type": "training",
      "description": "Training step 885",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:09",
      "total_flops_so_far": 2.1055109736628224e+16,
      "budget_used_percent": 21.055109736628225
    },
    {
      "type": "training",
      "description": "Training step 886",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:10",
      "total_flops_so_far": 2.107887396883661e+16,
      "budget_used_percent": 21.07887396883661
    },
    {
      "type": "training",
      "description": "Training step 887",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:12",
      "total_flops_so_far": 2.110263820104499e+16,
      "budget_used_percent": 21.102638201044993
    },
    {
      "type": "training",
      "description": "Training step 888",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:13",
      "total_flops_so_far": 2.1126402433253376e+16,
      "budget_used_percent": 21.12640243325338
    },
    {
      "type": "training",
      "description": "Training step 889",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:14",
      "total_flops_so_far": 2.115016666546176e+16,
      "budget_used_percent": 21.150166665461757
    },
    {
      "type": "training",
      "description": "Training step 890",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:16",
      "total_flops_so_far": 2.1173930897670144e+16,
      "budget_used_percent": 21.173930897670143
    },
    {
      "type": "training",
      "description": "Training step 891",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:17",
      "total_flops_so_far": 2.119769512987853e+16,
      "budget_used_percent": 21.19769512987853
    },
    {
      "type": "training",
      "description": "Training step 892",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:18",
      "total_flops_so_far": 2.122145936208691e+16,
      "budget_used_percent": 21.22145936208691
    },
    {
      "type": "training",
      "description": "Training step 893",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:20",
      "total_flops_so_far": 2.1245223594295296e+16,
      "budget_used_percent": 21.245223594295297
    },
    {
      "type": "training",
      "description": "Training step 894",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:21",
      "total_flops_so_far": 2.126898782650368e+16,
      "budget_used_percent": 21.26898782650368
    },
    {
      "type": "training",
      "description": "Training step 895",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:22",
      "total_flops_so_far": 2.1292752058712064e+16,
      "budget_used_percent": 21.292752058712065
    },
    {
      "type": "training",
      "description": "Training step 896",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:23",
      "total_flops_so_far": 2.131651629092045e+16,
      "budget_used_percent": 21.316516290920447
    },
    {
      "type": "training",
      "description": "Training step 897",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:25",
      "total_flops_so_far": 2.134028052312883e+16,
      "budget_used_percent": 21.340280523128833
    },
    {
      "type": "training",
      "description": "Training step 898",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:26",
      "total_flops_so_far": 2.1364044755337216e+16,
      "budget_used_percent": 21.364044755337215
    },
    {
      "type": "training",
      "description": "Training step 899",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:27",
      "total_flops_so_far": 2.13878089875456e+16,
      "budget_used_percent": 21.3878089875456
    },
    {
      "type": "training",
      "description": "Training step 900",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:29",
      "total_flops_so_far": 2.1411573219753984e+16,
      "budget_used_percent": 21.411573219753986
    },
    {
      "type": "training",
      "description": "Training step 901",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:30",
      "total_flops_so_far": 2.143533745196237e+16,
      "budget_used_percent": 21.43533745196237
    },
    {
      "type": "training",
      "description": "Training step 902",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:31",
      "total_flops_so_far": 2.145910168417075e+16,
      "budget_used_percent": 21.459101684170754
    },
    {
      "type": "training",
      "description": "Training step 903",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:33",
      "total_flops_so_far": 2.1482865916379136e+16,
      "budget_used_percent": 21.482865916379133
    },
    {
      "type": "training",
      "description": "Training step 904",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:34",
      "total_flops_so_far": 2.150663014858752e+16,
      "budget_used_percent": 21.50663014858752
    },
    {
      "type": "training",
      "description": "Training step 905",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:35",
      "total_flops_so_far": 2.1530394380795904e+16,
      "budget_used_percent": 21.530394380795904
    },
    {
      "type": "training",
      "description": "Training step 906",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:36",
      "total_flops_so_far": 2.155415861300429e+16,
      "budget_used_percent": 21.554158613004287
    },
    {
      "type": "training",
      "description": "Training step 907",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:38",
      "total_flops_so_far": 2.157792284521267e+16,
      "budget_used_percent": 21.577922845212672
    },
    {
      "type": "training",
      "description": "Training step 908",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:39",
      "total_flops_so_far": 2.1601687077421056e+16,
      "budget_used_percent": 21.601687077421055
    },
    {
      "type": "training",
      "description": "Training step 909",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:40",
      "total_flops_so_far": 2.162545130962944e+16,
      "budget_used_percent": 21.62545130962944
    },
    {
      "type": "training",
      "description": "Training step 910",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:42",
      "total_flops_so_far": 2.1649215541837824e+16,
      "budget_used_percent": 21.649215541837822
    },
    {
      "type": "training",
      "description": "Training step 911",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:43",
      "total_flops_so_far": 2.167297977404621e+16,
      "budget_used_percent": 21.67297977404621
    },
    {
      "type": "training",
      "description": "Training step 912",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:44",
      "total_flops_so_far": 2.169674400625459e+16,
      "budget_used_percent": 21.696744006254594
    },
    {
      "type": "training",
      "description": "Training step 913",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:46",
      "total_flops_so_far": 2.1720508238462976e+16,
      "budget_used_percent": 21.720508238462976
    },
    {
      "type": "training",
      "description": "Training step 914",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:47",
      "total_flops_so_far": 2.174427247067136e+16,
      "budget_used_percent": 21.744272470671362
    },
    {
      "type": "training",
      "description": "Training step 915",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:48",
      "total_flops_so_far": 2.1768036702879744e+16,
      "budget_used_percent": 21.768036702879744
    },
    {
      "type": "training",
      "description": "Training step 916",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:49",
      "total_flops_so_far": 2.179180093508813e+16,
      "budget_used_percent": 21.79180093508813
    },
    {
      "type": "training",
      "description": "Training step 917",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:51",
      "total_flops_so_far": 2.181556516729651e+16,
      "budget_used_percent": 21.815565167296512
    },
    {
      "type": "training",
      "description": "Training step 918",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:52",
      "total_flops_so_far": 2.1839329399504896e+16,
      "budget_used_percent": 21.839329399504894
    },
    {
      "type": "training",
      "description": "Training step 919",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:53",
      "total_flops_so_far": 2.186309363171328e+16,
      "budget_used_percent": 21.86309363171328
    },
    {
      "type": "training",
      "description": "Training step 920",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:55",
      "total_flops_so_far": 2.1886857863921664e+16,
      "budget_used_percent": 21.886857863921662
    },
    {
      "type": "training",
      "description": "Training step 921",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:56",
      "total_flops_so_far": 2.191062209613005e+16,
      "budget_used_percent": 21.910622096130048
    },
    {
      "type": "training",
      "description": "Training step 922",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:57",
      "total_flops_so_far": 2.193438632833843e+16,
      "budget_used_percent": 21.93438632833843
    },
    {
      "type": "training",
      "description": "Training step 923",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:59",
      "total_flops_so_far": 2.1958150560546816e+16,
      "budget_used_percent": 21.958150560546816
    },
    {
      "type": "training",
      "description": "Training step 924",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:29:59",
      "total_flops_so_far": 2.19819147927552e+16,
      "budget_used_percent": 21.9819147927552
    },
    {
      "type": "training",
      "description": "Training step 925",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:00",
      "total_flops_so_far": 2.2005679024963584e+16,
      "budget_used_percent": 22.005679024963584
    },
    {
      "type": "training",
      "description": "Training step 926",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:02",
      "total_flops_so_far": 2.202944325717197e+16,
      "budget_used_percent": 22.02944325717197
    },
    {
      "type": "training",
      "description": "Training step 927",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:03",
      "total_flops_so_far": 2.205320748938035e+16,
      "budget_used_percent": 22.053207489380352
    },
    {
      "type": "training",
      "description": "Training step 928",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:05",
      "total_flops_so_far": 2.2076971721588736e+16,
      "budget_used_percent": 22.076971721588738
    },
    {
      "type": "training",
      "description": "Training step 929",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:06",
      "total_flops_so_far": 2.210073595379712e+16,
      "budget_used_percent": 22.10073595379712
    },
    {
      "type": "training",
      "description": "Training step 930",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:07",
      "total_flops_so_far": 2.2124500186005504e+16,
      "budget_used_percent": 22.124500186005505
    },
    {
      "type": "training",
      "description": "Training step 931",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:09",
      "total_flops_so_far": 2.214826441821389e+16,
      "budget_used_percent": 22.148264418213888
    },
    {
      "type": "training",
      "description": "Training step 932",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:11",
      "total_flops_so_far": 2.217202865042227e+16,
      "budget_used_percent": 22.17202865042227
    },
    {
      "type": "training",
      "description": "Training step 933",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:12",
      "total_flops_so_far": 2.2195792882630656e+16,
      "budget_used_percent": 22.195792882630656
    },
    {
      "type": "training",
      "description": "Training step 934",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:17",
      "total_flops_so_far": 2.221955711483904e+16,
      "budget_used_percent": 22.219557114839038
    },
    {
      "type": "training",
      "description": "Training step 935",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:18",
      "total_flops_so_far": 2.2243321347047424e+16,
      "budget_used_percent": 22.243321347047424
    },
    {
      "type": "training",
      "description": "Training step 936",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:20",
      "total_flops_so_far": 2.226708557925581e+16,
      "budget_used_percent": 22.267085579255806
    },
    {
      "type": "training",
      "description": "Training step 937",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:21",
      "total_flops_so_far": 2.229084981146419e+16,
      "budget_used_percent": 22.29084981146419
    },
    {
      "type": "training",
      "description": "Training step 938",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:22",
      "total_flops_so_far": 2.2314614043672576e+16,
      "budget_used_percent": 22.314614043672577
    },
    {
      "type": "training",
      "description": "Training step 939",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:23",
      "total_flops_so_far": 2.233837827588096e+16,
      "budget_used_percent": 22.33837827588096
    },
    {
      "type": "training",
      "description": "Training step 940",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:25",
      "total_flops_so_far": 2.2362142508089344e+16,
      "budget_used_percent": 22.362142508089345
    },
    {
      "type": "training",
      "description": "Training step 941",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:26",
      "total_flops_so_far": 2.238590674029773e+16,
      "budget_used_percent": 22.385906740297727
    },
    {
      "type": "training",
      "description": "Training step 942",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:27",
      "total_flops_so_far": 2.240967097250611e+16,
      "budget_used_percent": 22.409670972506113
    },
    {
      "type": "training",
      "description": "Training step 943",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:29",
      "total_flops_so_far": 2.2433435204714496e+16,
      "budget_used_percent": 22.433435204714495
    },
    {
      "type": "training",
      "description": "Training step 944",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:30",
      "total_flops_so_far": 2.245719943692288e+16,
      "budget_used_percent": 22.45719943692288
    },
    {
      "type": "training",
      "description": "Training step 945",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:31",
      "total_flops_so_far": 2.2480963669131264e+16,
      "budget_used_percent": 22.480963669131267
    },
    {
      "type": "training",
      "description": "Training step 946",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:33",
      "total_flops_so_far": 2.250472790133965e+16,
      "budget_used_percent": 22.504727901339646
    },
    {
      "type": "training",
      "description": "Training step 947",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:34",
      "total_flops_so_far": 2.252849213354803e+16,
      "budget_used_percent": 22.52849213354803
    },
    {
      "type": "training",
      "description": "Training step 948",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:35",
      "total_flops_so_far": 2.2552256365756416e+16,
      "budget_used_percent": 22.552256365756413
    },
    {
      "type": "training",
      "description": "Training step 949",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:36",
      "total_flops_so_far": 2.25760205979648e+16,
      "budget_used_percent": 22.5760205979648
    },
    {
      "type": "training",
      "description": "Training step 950",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:38",
      "total_flops_so_far": 2.2599784830173184e+16,
      "budget_used_percent": 22.599784830173185
    },
    {
      "type": "training",
      "description": "Training step 951",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:39",
      "total_flops_so_far": 2.262354906238157e+16,
      "budget_used_percent": 22.623549062381567
    },
    {
      "type": "training",
      "description": "Training step 952",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:40",
      "total_flops_so_far": 2.264731329458995e+16,
      "budget_used_percent": 22.647313294589953
    },
    {
      "type": "training",
      "description": "Training step 953",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:42",
      "total_flops_so_far": 2.2671077526798336e+16,
      "budget_used_percent": 22.671077526798335
    },
    {
      "type": "training",
      "description": "Training step 954",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:43",
      "total_flops_so_far": 2.269484175900672e+16,
      "budget_used_percent": 22.69484175900672
    },
    {
      "type": "training",
      "description": "Training step 955",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:44",
      "total_flops_so_far": 2.2718605991215104e+16,
      "budget_used_percent": 22.718605991215103
    },
    {
      "type": "training",
      "description": "Training step 956",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:46",
      "total_flops_so_far": 2.274237022342349e+16,
      "budget_used_percent": 22.74237022342349
    },
    {
      "type": "training",
      "description": "Training step 957",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:47",
      "total_flops_so_far": 2.276613445563187e+16,
      "budget_used_percent": 22.766134455631875
    },
    {
      "type": "training",
      "description": "Training step 958",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:48",
      "total_flops_so_far": 2.2789898687840256e+16,
      "budget_used_percent": 22.789898687840257
    },
    {
      "type": "training",
      "description": "Training step 959",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:50",
      "total_flops_so_far": 2.281366292004864e+16,
      "budget_used_percent": 22.813662920048642
    },
    {
      "type": "training",
      "description": "Training step 960",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:51",
      "total_flops_so_far": 2.2837427152257024e+16,
      "budget_used_percent": 22.83742715225702
    },
    {
      "type": "training",
      "description": "Training step 961",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:52",
      "total_flops_so_far": 2.286119138446541e+16,
      "budget_used_percent": 22.861191384465407
    },
    {
      "type": "training",
      "description": "Training step 962",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:53",
      "total_flops_so_far": 2.288495561667379e+16,
      "budget_used_percent": 22.884955616673793
    },
    {
      "type": "training",
      "description": "Training step 963",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:55",
      "total_flops_so_far": 2.2908719848882176e+16,
      "budget_used_percent": 22.908719848882175
    },
    {
      "type": "training",
      "description": "Training step 964",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:56",
      "total_flops_so_far": 2.293248408109056e+16,
      "budget_used_percent": 22.93248408109056
    },
    {
      "type": "training",
      "description": "Training step 965",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:57",
      "total_flops_so_far": 2.2956248313298944e+16,
      "budget_used_percent": 22.956248313298943
    },
    {
      "type": "training",
      "description": "Training step 966",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:30:59",
      "total_flops_so_far": 2.298001254550733e+16,
      "budget_used_percent": 22.98001254550733
    },
    {
      "type": "training",
      "description": "Training step 967",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:00",
      "total_flops_so_far": 2.300377677771571e+16,
      "budget_used_percent": 23.00377677771571
    },
    {
      "type": "training",
      "description": "Training step 968",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:01",
      "total_flops_so_far": 2.3027541009924096e+16,
      "budget_used_percent": 23.027541009924096
    },
    {
      "type": "training",
      "description": "Training step 969",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:03",
      "total_flops_so_far": 2.305130524213248e+16,
      "budget_used_percent": 23.05130524213248
    },
    {
      "type": "training",
      "description": "Training step 970",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:04",
      "total_flops_so_far": 2.3075069474340864e+16,
      "budget_used_percent": 23.075069474340864
    },
    {
      "type": "training",
      "description": "Training step 971",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:05",
      "total_flops_so_far": 2.309883370654925e+16,
      "budget_used_percent": 23.09883370654925
    },
    {
      "type": "training",
      "description": "Training step 972",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:07",
      "total_flops_so_far": 2.312259793875763e+16,
      "budget_used_percent": 23.122597938757632
    },
    {
      "type": "training",
      "description": "Training step 973",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:08",
      "total_flops_so_far": 2.3146362170966016e+16,
      "budget_used_percent": 23.146362170966018
    },
    {
      "type": "training",
      "description": "Training step 974",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:09",
      "total_flops_so_far": 2.31701264031744e+16,
      "budget_used_percent": 23.1701264031744
    },
    {
      "type": "training",
      "description": "Training step 975",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:10",
      "total_flops_so_far": 2.3193890635382784e+16,
      "budget_used_percent": 23.193890635382783
    },
    {
      "type": "training",
      "description": "Training step 976",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:12",
      "total_flops_so_far": 2.321765486759117e+16,
      "budget_used_percent": 23.21765486759117
    },
    {
      "type": "training",
      "description": "Training step 977",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:13",
      "total_flops_so_far": 2.324141909979955e+16,
      "budget_used_percent": 23.24141909979955
    },
    {
      "type": "training",
      "description": "Training step 978",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:14",
      "total_flops_so_far": 2.3265183332007936e+16,
      "budget_used_percent": 23.265183332007936
    },
    {
      "type": "training",
      "description": "Training step 979",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:16",
      "total_flops_so_far": 2.328894756421632e+16,
      "budget_used_percent": 23.28894756421632
    },
    {
      "type": "training",
      "description": "Training step 980",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:17",
      "total_flops_so_far": 2.3312711796424704e+16,
      "budget_used_percent": 23.312711796424704
    },
    {
      "type": "training",
      "description": "Training step 981",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:18",
      "total_flops_so_far": 2.333647602863309e+16,
      "budget_used_percent": 23.336476028633086
    },
    {
      "type": "training",
      "description": "Training step 982",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:20",
      "total_flops_so_far": 2.336024026084147e+16,
      "budget_used_percent": 23.360240260841472
    },
    {
      "type": "training",
      "description": "Training step 983",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:21",
      "total_flops_so_far": 2.3384004493049856e+16,
      "budget_used_percent": 23.384004493049858
    },
    {
      "type": "training",
      "description": "Training step 984",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:22",
      "total_flops_so_far": 2.340776872525824e+16,
      "budget_used_percent": 23.40776872525824
    },
    {
      "type": "training",
      "description": "Training step 985",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:23",
      "total_flops_so_far": 2.3431532957466624e+16,
      "budget_used_percent": 23.431532957466626
    },
    {
      "type": "training",
      "description": "Training step 986",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:25",
      "total_flops_so_far": 2.345529718967501e+16,
      "budget_used_percent": 23.455297189675008
    },
    {
      "type": "training",
      "description": "Training step 987",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:26",
      "total_flops_so_far": 2.347906142188339e+16,
      "budget_used_percent": 23.479061421883394
    },
    {
      "type": "training",
      "description": "Training step 988",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:27",
      "total_flops_so_far": 2.3502825654091776e+16,
      "budget_used_percent": 23.502825654091776
    },
    {
      "type": "training",
      "description": "Training step 989",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:29",
      "total_flops_so_far": 2.352658988630016e+16,
      "budget_used_percent": 23.526589886300158
    },
    {
      "type": "training",
      "description": "Training step 990",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:30",
      "total_flops_so_far": 2.3550354118508544e+16,
      "budget_used_percent": 23.550354118508544
    },
    {
      "type": "training",
      "description": "Training step 991",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:31",
      "total_flops_so_far": 2.357411835071693e+16,
      "budget_used_percent": 23.574118350716926
    },
    {
      "type": "training",
      "description": "Training step 992",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:33",
      "total_flops_so_far": 2.359788258292531e+16,
      "budget_used_percent": 23.597882582925312
    },
    {
      "type": "training",
      "description": "Training step 993",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:34",
      "total_flops_so_far": 2.3621646815133696e+16,
      "budget_used_percent": 23.621646815133694
    },
    {
      "type": "training",
      "description": "Training step 994",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:35",
      "total_flops_so_far": 2.364541104734208e+16,
      "budget_used_percent": 23.64541104734208
    },
    {
      "type": "training",
      "description": "Training step 995",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:36",
      "total_flops_so_far": 2.3669175279550464e+16,
      "budget_used_percent": 23.669175279550466
    },
    {
      "type": "training",
      "description": "Training step 996",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:38",
      "total_flops_so_far": 2.369293951175885e+16,
      "budget_used_percent": 23.692939511758848
    },
    {
      "type": "training",
      "description": "Training step 997",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:39",
      "total_flops_so_far": 2.371670374396723e+16,
      "budget_used_percent": 23.716703743967233
    },
    {
      "type": "training",
      "description": "Training step 998",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:40",
      "total_flops_so_far": 2.3740467976175616e+16,
      "budget_used_percent": 23.740467976175616
    },
    {
      "type": "training",
      "description": "Training step 999",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:42",
      "total_flops_so_far": 2.3764232208384e+16,
      "budget_used_percent": 23.764232208384
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:48",
      "total_flops_so_far": 2.3764942837521856e+16,
      "budget_used_percent": 23.764942837521854
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 714333709232.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:53",
      "total_flops_so_far": 2.376565717123109e+16,
      "budget_used_percent": 23.765657171231087
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:31:58",
      "total_flops_so_far": 2.376636965229444e+16,
      "budget_used_percent": 23.76636965229444
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:03",
      "total_flops_so_far": 2.3767080281432296e+16,
      "budget_used_percent": 23.767080281432296
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 713407296244.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:08",
      "total_flops_so_far": 2.376779368872854e+16,
      "budget_used_percent": 23.76779368872854
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 5",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:13",
      "total_flops_so_far": 2.3768504317866396e+16,
      "budget_used_percent": 23.768504317866395
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 6",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:18",
      "total_flops_so_far": 2.3769216798929748e+16,
      "budget_used_percent": 23.769216798929747
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 7",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:23",
      "total_flops_so_far": 2.37699292799931e+16,
      "budget_used_percent": 23.7699292799931
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 8",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:28",
      "total_flops_so_far": 2.3770641761056452e+16,
      "budget_used_percent": 23.770641761056453
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 9",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:33",
      "total_flops_so_far": 2.3771354242119804e+16,
      "budget_used_percent": 23.771354242119806
    },
    {
      "type": "training",
      "description": "Training step 1000",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:33",
      "total_flops_so_far": 2.3795118474328188e+16,
      "budget_used_percent": 23.795118474328188
    },
    {
      "type": "training",
      "description": "Training step 1001",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:35",
      "total_flops_so_far": 2.3818882706536572e+16,
      "budget_used_percent": 23.818882706536574
    },
    {
      "type": "training",
      "description": "Training step 1002",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:36",
      "total_flops_so_far": 2.3842646938744956e+16,
      "budget_used_percent": 23.842646938744956
    },
    {
      "type": "training",
      "description": "Training step 1003",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:37",
      "total_flops_so_far": 2.386641117095334e+16,
      "budget_used_percent": 23.866411170953338
    },
    {
      "type": "training",
      "description": "Training step 1004",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:39",
      "total_flops_so_far": 2.3890175403161724e+16,
      "budget_used_percent": 23.890175403161724
    },
    {
      "type": "training",
      "description": "Training step 1005",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:40",
      "total_flops_so_far": 2.3913939635370108e+16,
      "budget_used_percent": 23.913939635370106
    },
    {
      "type": "training",
      "description": "Training step 1006",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:41",
      "total_flops_so_far": 2.3937703867578492e+16,
      "budget_used_percent": 23.93770386757849
    },
    {
      "type": "training",
      "description": "Training step 1007",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:43",
      "total_flops_so_far": 2.3961468099786876e+16,
      "budget_used_percent": 23.961468099786877
    },
    {
      "type": "training",
      "description": "Training step 1008",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:44",
      "total_flops_so_far": 2.398523233199526e+16,
      "budget_used_percent": 23.98523233199526
    },
    {
      "type": "training",
      "description": "Training step 1009",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:45",
      "total_flops_so_far": 2.4008996564203644e+16,
      "budget_used_percent": 24.008996564203645
    },
    {
      "type": "training",
      "description": "Training step 1010",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:46",
      "total_flops_so_far": 2.4032760796412028e+16,
      "budget_used_percent": 24.032760796412028
    },
    {
      "type": "training",
      "description": "Training step 1011",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:48",
      "total_flops_so_far": 2.4056525028620412e+16,
      "budget_used_percent": 24.056525028620413
    },
    {
      "type": "training",
      "description": "Training step 1012",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:49",
      "total_flops_so_far": 2.4080289260828796e+16,
      "budget_used_percent": 24.080289260828796
    },
    {
      "type": "training",
      "description": "Training step 1013",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:50",
      "total_flops_so_far": 2.410405349303718e+16,
      "budget_used_percent": 24.10405349303718
    },
    {
      "type": "training",
      "description": "Training step 1014",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:52",
      "total_flops_so_far": 2.4127817725245564e+16,
      "budget_used_percent": 24.127817725245563
    },
    {
      "type": "training",
      "description": "Training step 1015",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:53",
      "total_flops_so_far": 2.4151581957453948e+16,
      "budget_used_percent": 24.15158195745395
    },
    {
      "type": "training",
      "description": "Training step 1016",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:54",
      "total_flops_so_far": 2.4175346189662332e+16,
      "budget_used_percent": 24.17534618966233
    },
    {
      "type": "training",
      "description": "Training step 1017",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:56",
      "total_flops_so_far": 2.4199110421870716e+16,
      "budget_used_percent": 24.199110421870714
    },
    {
      "type": "training",
      "description": "Training step 1018",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:57",
      "total_flops_so_far": 2.42228746540791e+16,
      "budget_used_percent": 24.2228746540791
    },
    {
      "type": "training",
      "description": "Training step 1019",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:58",
      "total_flops_so_far": 2.4246638886287484e+16,
      "budget_used_percent": 24.24663888628748
    },
    {
      "type": "training",
      "description": "Training step 1020",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:32:59",
      "total_flops_so_far": 2.4270403118495868e+16,
      "budget_used_percent": 24.270403118495867
    },
    {
      "type": "training",
      "description": "Training step 1021",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:01",
      "total_flops_so_far": 2.4294167350704252e+16,
      "budget_used_percent": 24.294167350704253
    },
    {
      "type": "training",
      "description": "Training step 1022",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:02",
      "total_flops_so_far": 2.4317931582912636e+16,
      "budget_used_percent": 24.317931582912635
    },
    {
      "type": "training",
      "description": "Training step 1023",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:03",
      "total_flops_so_far": 2.434169581512102e+16,
      "budget_used_percent": 24.34169581512102
    },
    {
      "type": "training",
      "description": "Training step 1024",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:05",
      "total_flops_so_far": 2.4365460047329404e+16,
      "budget_used_percent": 24.365460047329403
    },
    {
      "type": "training",
      "description": "Training step 1025",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:06",
      "total_flops_so_far": 2.4389224279537788e+16,
      "budget_used_percent": 24.38922427953779
    },
    {
      "type": "training",
      "description": "Training step 1026",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:07",
      "total_flops_so_far": 2.4412988511746172e+16,
      "budget_used_percent": 24.41298851174617
    },
    {
      "type": "training",
      "description": "Training step 1027",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:09",
      "total_flops_so_far": 2.4436752743954556e+16,
      "budget_used_percent": 24.436752743954557
    },
    {
      "type": "training",
      "description": "Training step 1028",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:10",
      "total_flops_so_far": 2.446051697616294e+16,
      "budget_used_percent": 24.460516976162943
    },
    {
      "type": "training",
      "description": "Training step 1029",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:11",
      "total_flops_so_far": 2.4484281208371324e+16,
      "budget_used_percent": 24.484281208371325
    },
    {
      "type": "training",
      "description": "Training step 1030",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:13",
      "total_flops_so_far": 2.4508045440579708e+16,
      "budget_used_percent": 24.50804544057971
    },
    {
      "type": "training",
      "description": "Training step 1031",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:14",
      "total_flops_so_far": 2.4531809672788092e+16,
      "budget_used_percent": 24.53180967278809
    },
    {
      "type": "training",
      "description": "Training step 1032",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:15",
      "total_flops_so_far": 2.4555573904996476e+16,
      "budget_used_percent": 24.555573904996475
    },
    {
      "type": "training",
      "description": "Training step 1033",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:16",
      "total_flops_so_far": 2.457933813720486e+16,
      "budget_used_percent": 24.57933813720486
    },
    {
      "type": "training",
      "description": "Training step 1034",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:18",
      "total_flops_so_far": 2.4603102369413244e+16,
      "budget_used_percent": 24.603102369413243
    },
    {
      "type": "training",
      "description": "Training step 1035",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:19",
      "total_flops_so_far": 2.4626866601621628e+16,
      "budget_used_percent": 24.62686660162163
    },
    {
      "type": "training",
      "description": "Training step 1036",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:21",
      "total_flops_so_far": 2.4650630833830012e+16,
      "budget_used_percent": 24.65063083383001
    },
    {
      "type": "training",
      "description": "Training step 1037",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:22",
      "total_flops_so_far": 2.4674395066038396e+16,
      "budget_used_percent": 24.674395066038397
    },
    {
      "type": "training",
      "description": "Training step 1038",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:23",
      "total_flops_so_far": 2.469815929824678e+16,
      "budget_used_percent": 24.69815929824678
    },
    {
      "type": "training",
      "description": "Training step 1039",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:24",
      "total_flops_so_far": 2.4721923530455164e+16,
      "budget_used_percent": 24.721923530455165
    },
    {
      "type": "training",
      "description": "Training step 1040",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:26",
      "total_flops_so_far": 2.4745687762663548e+16,
      "budget_used_percent": 24.74568776266355
    },
    {
      "type": "training",
      "description": "Training step 1041",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:27",
      "total_flops_so_far": 2.4769451994871932e+16,
      "budget_used_percent": 24.769451994871932
    },
    {
      "type": "training",
      "description": "Training step 1042",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:28",
      "total_flops_so_far": 2.4793216227080316e+16,
      "budget_used_percent": 24.79321622708032
    },
    {
      "type": "training",
      "description": "Training step 1043",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:30",
      "total_flops_so_far": 2.48169804592887e+16,
      "budget_used_percent": 24.8169804592887
    },
    {
      "type": "training",
      "description": "Training step 1044",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:31",
      "total_flops_so_far": 2.4840744691497084e+16,
      "budget_used_percent": 24.840744691497086
    },
    {
      "type": "training",
      "description": "Training step 1045",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:32",
      "total_flops_so_far": 2.4864508923705468e+16,
      "budget_used_percent": 24.86450892370547
    },
    {
      "type": "training",
      "description": "Training step 1046",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:34",
      "total_flops_so_far": 2.4888273155913852e+16,
      "budget_used_percent": 24.88827315591385
    },
    {
      "type": "training",
      "description": "Training step 1047",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:35",
      "total_flops_so_far": 2.4912037388122236e+16,
      "budget_used_percent": 24.912037388122236
    },
    {
      "type": "training",
      "description": "Training step 1048",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:36",
      "total_flops_so_far": 2.493580162033062e+16,
      "budget_used_percent": 24.93580162033062
    },
    {
      "type": "training",
      "description": "Training step 1049",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:37",
      "total_flops_so_far": 2.4959565852539004e+16,
      "budget_used_percent": 24.959565852539004
    },
    {
      "type": "training",
      "description": "Training step 1050",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:39",
      "total_flops_so_far": 2.4983330084747388e+16,
      "budget_used_percent": 24.983330084747386
    },
    {
      "type": "training",
      "description": "Training step 1051",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:40",
      "total_flops_so_far": 2.5007094316955772e+16,
      "budget_used_percent": 25.007094316955776
    },
    {
      "type": "training",
      "description": "Training step 1052",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:41",
      "total_flops_so_far": 2.5030858549164156e+16,
      "budget_used_percent": 25.030858549164154
    },
    {
      "type": "training",
      "description": "Training step 1053",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:43",
      "total_flops_so_far": 2.505462278137254e+16,
      "budget_used_percent": 25.054622781372537
    },
    {
      "type": "training",
      "description": "Training step 1054",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:44",
      "total_flops_so_far": 2.5078387013580924e+16,
      "budget_used_percent": 25.078387013580922
    },
    {
      "type": "training",
      "description": "Training step 1055",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:45",
      "total_flops_so_far": 2.5102151245789308e+16,
      "budget_used_percent": 25.102151245789305
    },
    {
      "type": "training",
      "description": "Training step 1056",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:46",
      "total_flops_so_far": 2.5125915477997692e+16,
      "budget_used_percent": 25.12591547799769
    },
    {
      "type": "training",
      "description": "Training step 1057",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:47",
      "total_flops_so_far": 2.5149679710206076e+16,
      "budget_used_percent": 25.149679710206073
    },
    {
      "type": "training",
      "description": "Training step 1058",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:48",
      "total_flops_so_far": 2.517344394241446e+16,
      "budget_used_percent": 25.17344394241446
    },
    {
      "type": "training",
      "description": "Training step 1059",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:50",
      "total_flops_so_far": 2.5197208174622844e+16,
      "budget_used_percent": 25.197208174622844
    },
    {
      "type": "training",
      "description": "Training step 1060",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:51",
      "total_flops_so_far": 2.5220972406831228e+16,
      "budget_used_percent": 25.220972406831226
    },
    {
      "type": "training",
      "description": "Training step 1061",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:52",
      "total_flops_so_far": 2.5244736639039612e+16,
      "budget_used_percent": 25.244736639039612
    },
    {
      "type": "training",
      "description": "Training step 1062",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:54",
      "total_flops_so_far": 2.5268500871247996e+16,
      "budget_used_percent": 25.268500871247994
    },
    {
      "type": "training",
      "description": "Training step 1063",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:55",
      "total_flops_so_far": 2.529226510345638e+16,
      "budget_used_percent": 25.29226510345638
    },
    {
      "type": "training",
      "description": "Training step 1064",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:56",
      "total_flops_so_far": 2.5316029335664764e+16,
      "budget_used_percent": 25.316029335664762
    },
    {
      "type": "training",
      "description": "Training step 1065",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:57",
      "total_flops_so_far": 2.5339793567873148e+16,
      "budget_used_percent": 25.339793567873148
    },
    {
      "type": "training",
      "description": "Training step 1066",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:33:59",
      "total_flops_so_far": 2.5363557800081532e+16,
      "budget_used_percent": 25.363557800081534
    },
    {
      "type": "training",
      "description": "Training step 1067",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:00",
      "total_flops_so_far": 2.5387322032289916e+16,
      "budget_used_percent": 25.387322032289916
    },
    {
      "type": "training",
      "description": "Training step 1068",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:01",
      "total_flops_so_far": 2.54110862644983e+16,
      "budget_used_percent": 25.4110862644983
    },
    {
      "type": "training",
      "description": "Training step 1069",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:03",
      "total_flops_so_far": 2.5434850496706684e+16,
      "budget_used_percent": 25.434850496706684
    },
    {
      "type": "training",
      "description": "Training step 1070",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:04",
      "total_flops_so_far": 2.5458614728915068e+16,
      "budget_used_percent": 25.45861472891507
    },
    {
      "type": "training",
      "description": "Training step 1071",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:05",
      "total_flops_so_far": 2.5482378961123452e+16,
      "budget_used_percent": 25.48237896112345
    },
    {
      "type": "training",
      "description": "Training step 1072",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:07",
      "total_flops_so_far": 2.5506143193331836e+16,
      "budget_used_percent": 25.506143193331837
    },
    {
      "type": "training",
      "description": "Training step 1073",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:08",
      "total_flops_so_far": 2.552990742554022e+16,
      "budget_used_percent": 25.529907425540223
    },
    {
      "type": "training",
      "description": "Training step 1074",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:09",
      "total_flops_so_far": 2.5553671657748604e+16,
      "budget_used_percent": 25.553671657748605
    },
    {
      "type": "training",
      "description": "Training step 1075",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:10",
      "total_flops_so_far": 2.5577435889956988e+16,
      "budget_used_percent": 25.57743588995699
    },
    {
      "type": "training",
      "description": "Training step 1076",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:12",
      "total_flops_so_far": 2.5601200122165372e+16,
      "budget_used_percent": 25.601200122165373
    },
    {
      "type": "training",
      "description": "Training step 1077",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:13",
      "total_flops_so_far": 2.5624964354373756e+16,
      "budget_used_percent": 25.62496435437376
    },
    {
      "type": "training",
      "description": "Training step 1078",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:14",
      "total_flops_so_far": 2.564872858658214e+16,
      "budget_used_percent": 25.64872858658214
    },
    {
      "type": "training",
      "description": "Training step 1079",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:16",
      "total_flops_so_far": 2.5672492818790524e+16,
      "budget_used_percent": 25.672492818790527
    },
    {
      "type": "training",
      "description": "Training step 1080",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:17",
      "total_flops_so_far": 2.5696257050998908e+16,
      "budget_used_percent": 25.696257050998906
    },
    {
      "type": "training",
      "description": "Training step 1081",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:18",
      "total_flops_so_far": 2.5720021283207292e+16,
      "budget_used_percent": 25.720021283207288
    },
    {
      "type": "training",
      "description": "Training step 1082",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:20",
      "total_flops_so_far": 2.5743785515415676e+16,
      "budget_used_percent": 25.743785515415674
    },
    {
      "type": "training",
      "description": "Training step 1083",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:21",
      "total_flops_so_far": 2.576754974762406e+16,
      "budget_used_percent": 25.76754974762406
    },
    {
      "type": "training",
      "description": "Training step 1084",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:22",
      "total_flops_so_far": 2.5791313979832444e+16,
      "budget_used_percent": 25.79131397983244
    },
    {
      "type": "training",
      "description": "Training step 1085",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:24",
      "total_flops_so_far": 2.5815078212040828e+16,
      "budget_used_percent": 25.815078212040827
    },
    {
      "type": "training",
      "description": "Training step 1086",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:25",
      "total_flops_so_far": 2.5838842444249212e+16,
      "budget_used_percent": 25.83884244424921
    },
    {
      "type": "training",
      "description": "Training step 1087",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:26",
      "total_flops_so_far": 2.5862606676457596e+16,
      "budget_used_percent": 25.862606676457595
    },
    {
      "type": "training",
      "description": "Training step 1088",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:27",
      "total_flops_so_far": 2.588637090866598e+16,
      "budget_used_percent": 25.886370908665977
    },
    {
      "type": "training",
      "description": "Training step 1089",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:29",
      "total_flops_so_far": 2.5910135140874364e+16,
      "budget_used_percent": 25.910135140874363
    },
    {
      "type": "training",
      "description": "Training step 1090",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:30",
      "total_flops_so_far": 2.5933899373082748e+16,
      "budget_used_percent": 25.933899373082745
    },
    {
      "type": "training",
      "description": "Training step 1091",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:31",
      "total_flops_so_far": 2.5957663605291132e+16,
      "budget_used_percent": 25.95766360529113
    },
    {
      "type": "training",
      "description": "Training step 1092",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:33",
      "total_flops_so_far": 2.5981427837499516e+16,
      "budget_used_percent": 25.981427837499517
    },
    {
      "type": "training",
      "description": "Training step 1093",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:34",
      "total_flops_so_far": 2.60051920697079e+16,
      "budget_used_percent": 26.0051920697079
    },
    {
      "type": "training",
      "description": "Training step 1094",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:35",
      "total_flops_so_far": 2.6028956301916284e+16,
      "budget_used_percent": 26.028956301916285
    },
    {
      "type": "training",
      "description": "Training step 1095",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:37",
      "total_flops_so_far": 2.6052720534124668e+16,
      "budget_used_percent": 26.052720534124667
    },
    {
      "type": "training",
      "description": "Training step 1096",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:38",
      "total_flops_so_far": 2.6076484766333052e+16,
      "budget_used_percent": 26.076484766333053
    },
    {
      "type": "training",
      "description": "Training step 1097",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:39",
      "total_flops_so_far": 2.6100248998541436e+16,
      "budget_used_percent": 26.100248998541435
    },
    {
      "type": "training",
      "description": "Training step 1098",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:41",
      "total_flops_so_far": 2.612401323074982e+16,
      "budget_used_percent": 26.12401323074982
    },
    {
      "type": "training",
      "description": "Training step 1099",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:42",
      "total_flops_so_far": 2.6147777462958204e+16,
      "budget_used_percent": 26.147777462958206
    },
    {
      "type": "training",
      "description": "Training step 1100",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:43",
      "total_flops_so_far": 2.6171541695166588e+16,
      "budget_used_percent": 26.17154169516659
    },
    {
      "type": "training",
      "description": "Training step 1101",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:44",
      "total_flops_so_far": 2.6195305927374972e+16,
      "budget_used_percent": 26.195305927374974
    },
    {
      "type": "training",
      "description": "Training step 1102",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:46",
      "total_flops_so_far": 2.6219070159583356e+16,
      "budget_used_percent": 26.219070159583357
    },
    {
      "type": "training",
      "description": "Training step 1103",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:47",
      "total_flops_so_far": 2.624283439179174e+16,
      "budget_used_percent": 26.242834391791742
    },
    {
      "type": "training",
      "description": "Training step 1104",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:48",
      "total_flops_so_far": 2.6266598624000124e+16,
      "budget_used_percent": 26.266598624000125
    },
    {
      "type": "training",
      "description": "Training step 1105",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:50",
      "total_flops_so_far": 2.6290362856208508e+16,
      "budget_used_percent": 26.29036285620851
    },
    {
      "type": "training",
      "description": "Training step 1106",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:51",
      "total_flops_so_far": 2.6314127088416892e+16,
      "budget_used_percent": 26.314127088416896
    },
    {
      "type": "training",
      "description": "Training step 1107",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:52",
      "total_flops_so_far": 2.6337891320625276e+16,
      "budget_used_percent": 26.33789132062528
    },
    {
      "type": "training",
      "description": "Training step 1108",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:54",
      "total_flops_so_far": 2.636165555283366e+16,
      "budget_used_percent": 26.361655552833664
    },
    {
      "type": "training",
      "description": "Training step 1109",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:55",
      "total_flops_so_far": 2.6385419785042044e+16,
      "budget_used_percent": 26.385419785042043
    },
    {
      "type": "training",
      "description": "Training step 1110",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:56",
      "total_flops_so_far": 2.6409184017250428e+16,
      "budget_used_percent": 26.409184017250425
    },
    {
      "type": "training",
      "description": "Training step 1111",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:58",
      "total_flops_so_far": 2.6432948249458812e+16,
      "budget_used_percent": 26.43294824945881
    },
    {
      "type": "training",
      "description": "Training step 1112",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:34:59",
      "total_flops_so_far": 2.6456712481667196e+16,
      "budget_used_percent": 26.456712481667193
    },
    {
      "type": "training",
      "description": "Training step 1113",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:00",
      "total_flops_so_far": 2.648047671387558e+16,
      "budget_used_percent": 26.48047671387558
    },
    {
      "type": "training",
      "description": "Training step 1114",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:01",
      "total_flops_so_far": 2.6504240946083964e+16,
      "budget_used_percent": 26.50424094608396
    },
    {
      "type": "training",
      "description": "Training step 1115",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:03",
      "total_flops_so_far": 2.6528005178292348e+16,
      "budget_used_percent": 26.528005178292346
    },
    {
      "type": "training",
      "description": "Training step 1116",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:04",
      "total_flops_so_far": 2.6551769410500732e+16,
      "budget_used_percent": 26.551769410500732
    },
    {
      "type": "training",
      "description": "Training step 1117",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:05",
      "total_flops_so_far": 2.6575533642709116e+16,
      "budget_used_percent": 26.575533642709114
    },
    {
      "type": "training",
      "description": "Training step 1118",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:07",
      "total_flops_so_far": 2.65992978749175e+16,
      "budget_used_percent": 26.5992978749175
    },
    {
      "type": "training",
      "description": "Training step 1119",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:08",
      "total_flops_so_far": 2.6623062107125884e+16,
      "budget_used_percent": 26.623062107125882
    },
    {
      "type": "training",
      "description": "Training step 1120",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:09",
      "total_flops_so_far": 2.6646826339334268e+16,
      "budget_used_percent": 26.646826339334268
    },
    {
      "type": "training",
      "description": "Training step 1121",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:11",
      "total_flops_so_far": 2.6670590571542652e+16,
      "budget_used_percent": 26.67059057154265
    },
    {
      "type": "training",
      "description": "Training step 1122",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:13",
      "total_flops_so_far": 2.6694354803751036e+16,
      "budget_used_percent": 26.694354803751036
    },
    {
      "type": "training",
      "description": "Training step 1123",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:14",
      "total_flops_so_far": 2.671811903595942e+16,
      "budget_used_percent": 26.71811903595942
    },
    {
      "type": "training",
      "description": "Training step 1124",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:15",
      "total_flops_so_far": 2.6741883268167804e+16,
      "budget_used_percent": 26.741883268167804
    },
    {
      "type": "training",
      "description": "Training step 1125",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:17",
      "total_flops_so_far": 2.6765647500376188e+16,
      "budget_used_percent": 26.76564750037619
    },
    {
      "type": "training",
      "description": "Training step 1126",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:18",
      "total_flops_so_far": 2.6789411732584572e+16,
      "budget_used_percent": 26.789411732584572
    },
    {
      "type": "training",
      "description": "Training step 1127",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:19",
      "total_flops_so_far": 2.6813175964792956e+16,
      "budget_used_percent": 26.813175964792958
    },
    {
      "type": "training",
      "description": "Training step 1128",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:20",
      "total_flops_so_far": 2.683694019700134e+16,
      "budget_used_percent": 26.83694019700134
    },
    {
      "type": "training",
      "description": "Training step 1129",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:22",
      "total_flops_so_far": 2.6860704429209724e+16,
      "budget_used_percent": 26.860704429209726
    },
    {
      "type": "training",
      "description": "Training step 1130",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:23",
      "total_flops_so_far": 2.6884468661418108e+16,
      "budget_used_percent": 26.884468661418108
    },
    {
      "type": "training",
      "description": "Training step 1131",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:24",
      "total_flops_so_far": 2.6908232893626492e+16,
      "budget_used_percent": 26.908232893626494
    },
    {
      "type": "training",
      "description": "Training step 1132",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:26",
      "total_flops_so_far": 2.6931997125834876e+16,
      "budget_used_percent": 26.93199712583488
    },
    {
      "type": "training",
      "description": "Training step 1133",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:27",
      "total_flops_so_far": 2.695576135804326e+16,
      "budget_used_percent": 26.95576135804326
    },
    {
      "type": "training",
      "description": "Training step 1134",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:28",
      "total_flops_so_far": 2.6979525590251644e+16,
      "budget_used_percent": 26.979525590251647
    },
    {
      "type": "training",
      "description": "Training step 1135",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:30",
      "total_flops_so_far": 2.7003289822460028e+16,
      "budget_used_percent": 27.00328982246003
    },
    {
      "type": "training",
      "description": "Training step 1136",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:31",
      "total_flops_so_far": 2.7027054054668412e+16,
      "budget_used_percent": 27.027054054668415
    },
    {
      "type": "training",
      "description": "Training step 1137",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:32",
      "total_flops_so_far": 2.7050818286876796e+16,
      "budget_used_percent": 27.050818286876794
    },
    {
      "type": "training",
      "description": "Training step 1138",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:34",
      "total_flops_so_far": 2.707458251908518e+16,
      "budget_used_percent": 27.074582519085176
    },
    {
      "type": "training",
      "description": "Training step 1139",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:35",
      "total_flops_so_far": 2.7098346751293564e+16,
      "budget_used_percent": 27.098346751293562
    },
    {
      "type": "training",
      "description": "Training step 1140",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:36",
      "total_flops_so_far": 2.7122110983501948e+16,
      "budget_used_percent": 27.122110983501948
    },
    {
      "type": "training",
      "description": "Training step 1141",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:37",
      "total_flops_so_far": 2.7145875215710332e+16,
      "budget_used_percent": 27.14587521571033
    },
    {
      "type": "training",
      "description": "Training step 1142",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:39",
      "total_flops_so_far": 2.7169639447918716e+16,
      "budget_used_percent": 27.169639447918716
    },
    {
      "type": "training",
      "description": "Training step 1143",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:40",
      "total_flops_so_far": 2.71934036801271e+16,
      "budget_used_percent": 27.193403680127098
    },
    {
      "type": "training",
      "description": "Training step 1144",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:41",
      "total_flops_so_far": 2.7217167912335484e+16,
      "budget_used_percent": 27.217167912335483
    },
    {
      "type": "training",
      "description": "Training step 1145",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:43",
      "total_flops_so_far": 2.7240932144543868e+16,
      "budget_used_percent": 27.240932144543866
    },
    {
      "type": "training",
      "description": "Training step 1146",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:44",
      "total_flops_so_far": 2.7264696376752252e+16,
      "budget_used_percent": 27.26469637675225
    },
    {
      "type": "training",
      "description": "Training step 1147",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:45",
      "total_flops_so_far": 2.7288460608960636e+16,
      "budget_used_percent": 27.288460608960634
    },
    {
      "type": "training",
      "description": "Training step 1148",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:47",
      "total_flops_so_far": 2.731222484116902e+16,
      "budget_used_percent": 27.31222484116902
    },
    {
      "type": "training",
      "description": "Training step 1149",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:48",
      "total_flops_so_far": 2.7335989073377404e+16,
      "budget_used_percent": 27.335989073377405
    },
    {
      "type": "training",
      "description": "Training step 1150",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:49",
      "total_flops_so_far": 2.7359753305585788e+16,
      "budget_used_percent": 27.359753305585787
    },
    {
      "type": "training",
      "description": "Training step 1151",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:50",
      "total_flops_so_far": 2.7383517537794172e+16,
      "budget_used_percent": 27.383517537794173
    },
    {
      "type": "training",
      "description": "Training step 1152",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:52",
      "total_flops_so_far": 2.7407281770002556e+16,
      "budget_used_percent": 27.407281770002555
    },
    {
      "type": "training",
      "description": "Training step 1153",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:53",
      "total_flops_so_far": 2.743104600221094e+16,
      "budget_used_percent": 27.43104600221094
    },
    {
      "type": "training",
      "description": "Training step 1154",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:54",
      "total_flops_so_far": 2.7454810234419324e+16,
      "budget_used_percent": 27.454810234419323
    },
    {
      "type": "training",
      "description": "Training step 1155",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:56",
      "total_flops_so_far": 2.7478574466627708e+16,
      "budget_used_percent": 27.47857446662771
    },
    {
      "type": "training",
      "description": "Training step 1156",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:57",
      "total_flops_so_far": 2.7502338698836092e+16,
      "budget_used_percent": 27.50233869883609
    },
    {
      "type": "training",
      "description": "Training step 1157",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:35:58",
      "total_flops_so_far": 2.7526102931044476e+16,
      "budget_used_percent": 27.526102931044477
    },
    {
      "type": "training",
      "description": "Training step 1158",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:00",
      "total_flops_so_far": 2.754986716325286e+16,
      "budget_used_percent": 27.549867163252863
    },
    {
      "type": "training",
      "description": "Training step 1159",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:01",
      "total_flops_so_far": 2.7573631395461244e+16,
      "budget_used_percent": 27.573631395461245
    },
    {
      "type": "training",
      "description": "Training step 1160",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:02",
      "total_flops_so_far": 2.7597395627669628e+16,
      "budget_used_percent": 27.59739562766963
    },
    {
      "type": "training",
      "description": "Training step 1161",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:04",
      "total_flops_so_far": 2.7621159859878012e+16,
      "budget_used_percent": 27.621159859878013
    },
    {
      "type": "training",
      "description": "Training step 1162",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:05",
      "total_flops_so_far": 2.7644924092086396e+16,
      "budget_used_percent": 27.6449240920864
    },
    {
      "type": "training",
      "description": "Training step 1163",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:06",
      "total_flops_so_far": 2.766868832429478e+16,
      "budget_used_percent": 27.66868832429478
    },
    {
      "type": "training",
      "description": "Training step 1164",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:07",
      "total_flops_so_far": 2.7692452556503164e+16,
      "budget_used_percent": 27.692452556503167
    },
    {
      "type": "training",
      "description": "Training step 1165",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:09",
      "total_flops_so_far": 2.7716216788711548e+16,
      "budget_used_percent": 27.716216788711552
    },
    {
      "type": "training",
      "description": "Training step 1166",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:10",
      "total_flops_so_far": 2.7739981020919932e+16,
      "budget_used_percent": 27.73998102091993
    },
    {
      "type": "training",
      "description": "Training step 1167",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:11",
      "total_flops_so_far": 2.7763745253128316e+16,
      "budget_used_percent": 27.763745253128313
    },
    {
      "type": "training",
      "description": "Training step 1168",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:13",
      "total_flops_so_far": 2.77875094853367e+16,
      "budget_used_percent": 27.7875094853367
    },
    {
      "type": "training",
      "description": "Training step 1169",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:14",
      "total_flops_so_far": 2.7811273717545084e+16,
      "budget_used_percent": 27.81127371754508
    },
    {
      "type": "training",
      "description": "Training step 1170",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:15",
      "total_flops_so_far": 2.7835037949753468e+16,
      "budget_used_percent": 27.835037949753467
    },
    {
      "type": "training",
      "description": "Training step 1171",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:17",
      "total_flops_so_far": 2.7858802181961852e+16,
      "budget_used_percent": 27.85880218196185
    },
    {
      "type": "training",
      "description": "Training step 1172",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:18",
      "total_flops_so_far": 2.7882566414170236e+16,
      "budget_used_percent": 27.882566414170235
    },
    {
      "type": "training",
      "description": "Training step 1173",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:19",
      "total_flops_so_far": 2.790633064637862e+16,
      "budget_used_percent": 27.90633064637862
    },
    {
      "type": "training",
      "description": "Training step 1174",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:20",
      "total_flops_so_far": 2.7930094878587004e+16,
      "budget_used_percent": 27.930094878587003
    },
    {
      "type": "training",
      "description": "Training step 1175",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:22",
      "total_flops_so_far": 2.7953859110795388e+16,
      "budget_used_percent": 27.95385911079539
    },
    {
      "type": "training",
      "description": "Training step 1176",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:23",
      "total_flops_so_far": 2.7977623343003772e+16,
      "budget_used_percent": 27.97762334300377
    },
    {
      "type": "training",
      "description": "Training step 1177",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:24",
      "total_flops_so_far": 2.8001387575212156e+16,
      "budget_used_percent": 28.001387575212156
    },
    {
      "type": "training",
      "description": "Training step 1178",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:26",
      "total_flops_so_far": 2.802515180742054e+16,
      "budget_used_percent": 28.02515180742054
    },
    {
      "type": "training",
      "description": "Training step 1179",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:27",
      "total_flops_so_far": 2.8048916039628924e+16,
      "budget_used_percent": 28.048916039628924
    },
    {
      "type": "training",
      "description": "Training step 1180",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:28",
      "total_flops_so_far": 2.8072680271837308e+16,
      "budget_used_percent": 28.072680271837307
    },
    {
      "type": "training",
      "description": "Training step 1181",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:30",
      "total_flops_so_far": 2.8096444504045692e+16,
      "budget_used_percent": 28.096444504045692
    },
    {
      "type": "training",
      "description": "Training step 1182",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:31",
      "total_flops_so_far": 2.8120208736254076e+16,
      "budget_used_percent": 28.120208736254078
    },
    {
      "type": "training",
      "description": "Training step 1183",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:32",
      "total_flops_so_far": 2.814397296846246e+16,
      "budget_used_percent": 28.14397296846246
    },
    {
      "type": "training",
      "description": "Training step 1184",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:34",
      "total_flops_so_far": 2.8167737200670844e+16,
      "budget_used_percent": 28.167737200670846
    },
    {
      "type": "training",
      "description": "Training step 1185",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:35",
      "total_flops_so_far": 2.8191501432879228e+16,
      "budget_used_percent": 28.191501432879228
    },
    {
      "type": "training",
      "description": "Training step 1186",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:36",
      "total_flops_so_far": 2.8215265665087612e+16,
      "budget_used_percent": 28.215265665087614
    },
    {
      "type": "training",
      "description": "Training step 1187",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:37",
      "total_flops_so_far": 2.8239029897295996e+16,
      "budget_used_percent": 28.239029897295996
    },
    {
      "type": "training",
      "description": "Training step 1188",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:38",
      "total_flops_so_far": 2.826279412950438e+16,
      "budget_used_percent": 28.262794129504382
    },
    {
      "type": "training",
      "description": "Training step 1189",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:39",
      "total_flops_so_far": 2.8286558361712764e+16,
      "budget_used_percent": 28.286558361712764
    },
    {
      "type": "training",
      "description": "Training step 1190",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:40",
      "total_flops_so_far": 2.8310322593921148e+16,
      "budget_used_percent": 28.31032259392115
    },
    {
      "type": "training",
      "description": "Training step 1191",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:42",
      "total_flops_so_far": 2.8334086826129532e+16,
      "budget_used_percent": 28.334086826129536
    },
    {
      "type": "training",
      "description": "Training step 1192",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:43",
      "total_flops_so_far": 2.8357851058337916e+16,
      "budget_used_percent": 28.357851058337918
    },
    {
      "type": "training",
      "description": "Training step 1193",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:44",
      "total_flops_so_far": 2.83816152905463e+16,
      "budget_used_percent": 28.381615290546303
    },
    {
      "type": "training",
      "description": "Training step 1194",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:46",
      "total_flops_so_far": 2.8405379522754684e+16,
      "budget_used_percent": 28.405379522754682
    },
    {
      "type": "training",
      "description": "Training step 1195",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:47",
      "total_flops_so_far": 2.8429143754963068e+16,
      "budget_used_percent": 28.429143754963064
    },
    {
      "type": "training",
      "description": "Training step 1196",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:48",
      "total_flops_so_far": 2.8452907987171452e+16,
      "budget_used_percent": 28.45290798717145
    },
    {
      "type": "training",
      "description": "Training step 1197",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:50",
      "total_flops_so_far": 2.8476672219379836e+16,
      "budget_used_percent": 28.476672219379832
    },
    {
      "type": "training",
      "description": "Training step 1198",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:51",
      "total_flops_so_far": 2.850043645158822e+16,
      "budget_used_percent": 28.500436451588218
    },
    {
      "type": "training",
      "description": "Training step 1199",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:52",
      "total_flops_so_far": 2.8524200683796604e+16,
      "budget_used_percent": 28.524200683796604
    },
    {
      "type": "training",
      "description": "Training step 1200",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:54",
      "total_flops_so_far": 2.8547964916004988e+16,
      "budget_used_percent": 28.547964916004986
    },
    {
      "type": "training",
      "description": "Training step 1201",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:55",
      "total_flops_so_far": 2.8571729148213372e+16,
      "budget_used_percent": 28.57172914821337
    },
    {
      "type": "training",
      "description": "Training step 1202",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:56",
      "total_flops_so_far": 2.8595493380421756e+16,
      "budget_used_percent": 28.595493380421754
    },
    {
      "type": "training",
      "description": "Training step 1203",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:57",
      "total_flops_so_far": 2.861925761263014e+16,
      "budget_used_percent": 28.61925761263014
    },
    {
      "type": "training",
      "description": "Training step 1204",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:36:59",
      "total_flops_so_far": 2.8643021844838524e+16,
      "budget_used_percent": 28.643021844838522
    },
    {
      "type": "training",
      "description": "Training step 1205",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:00",
      "total_flops_so_far": 2.8666786077046908e+16,
      "budget_used_percent": 28.666786077046908
    },
    {
      "type": "training",
      "description": "Training step 1206",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:01",
      "total_flops_so_far": 2.8690550309255292e+16,
      "budget_used_percent": 28.690550309255293
    },
    {
      "type": "training",
      "description": "Training step 1207",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:03",
      "total_flops_so_far": 2.8714314541463676e+16,
      "budget_used_percent": 28.714314541463676
    },
    {
      "type": "training",
      "description": "Training step 1208",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:05",
      "total_flops_so_far": 2.873807877367206e+16,
      "budget_used_percent": 28.73807877367206
    },
    {
      "type": "training",
      "description": "Training step 1209",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:06",
      "total_flops_so_far": 2.8761843005880444e+16,
      "budget_used_percent": 28.761843005880444
    },
    {
      "type": "training",
      "description": "Training step 1210",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:07",
      "total_flops_so_far": 2.8785607238088828e+16,
      "budget_used_percent": 28.78560723808883
    },
    {
      "type": "training",
      "description": "Training step 1211",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:09",
      "total_flops_so_far": 2.8809371470297212e+16,
      "budget_used_percent": 28.80937147029721
    },
    {
      "type": "training",
      "description": "Training step 1212",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:10",
      "total_flops_so_far": 2.8833135702505596e+16,
      "budget_used_percent": 28.833135702505597
    },
    {
      "type": "training",
      "description": "Training step 1213",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:11",
      "total_flops_so_far": 2.885689993471398e+16,
      "budget_used_percent": 28.85689993471398
    },
    {
      "type": "training",
      "description": "Training step 1214",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:13",
      "total_flops_so_far": 2.8880664166922364e+16,
      "budget_used_percent": 28.880664166922365
    },
    {
      "type": "training",
      "description": "Training step 1215",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:14",
      "total_flops_so_far": 2.8904428399130748e+16,
      "budget_used_percent": 28.90442839913075
    },
    {
      "type": "training",
      "description": "Training step 1216",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:15",
      "total_flops_so_far": 2.8928192631339132e+16,
      "budget_used_percent": 28.928192631339133
    },
    {
      "type": "training",
      "description": "Training step 1217",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:16",
      "total_flops_so_far": 2.8951956863547516e+16,
      "budget_used_percent": 28.95195686354752
    },
    {
      "type": "training",
      "description": "Training step 1218",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:18",
      "total_flops_so_far": 2.89757210957559e+16,
      "budget_used_percent": 28.9757210957559
    },
    {
      "type": "training",
      "description": "Training step 1219",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:19",
      "total_flops_so_far": 2.8999485327964284e+16,
      "budget_used_percent": 28.999485327964287
    },
    {
      "type": "training",
      "description": "Training step 1220",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:20",
      "total_flops_so_far": 2.9023249560172668e+16,
      "budget_used_percent": 29.02324956017267
    },
    {
      "type": "training",
      "description": "Training step 1221",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:22",
      "total_flops_so_far": 2.9047013792381052e+16,
      "budget_used_percent": 29.047013792381055
    },
    {
      "type": "training",
      "description": "Training step 1222",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:23",
      "total_flops_so_far": 2.9070778024589436e+16,
      "budget_used_percent": 29.070778024589437
    },
    {
      "type": "training",
      "description": "Training step 1223",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:24",
      "total_flops_so_far": 2.909454225679782e+16,
      "budget_used_percent": 29.094542256797816
    },
    {
      "type": "training",
      "description": "Training step 1224",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:26",
      "total_flops_so_far": 2.9118306489006204e+16,
      "budget_used_percent": 29.1183064890062
    },
    {
      "type": "training",
      "description": "Training step 1225",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:27",
      "total_flops_so_far": 2.9142070721214588e+16,
      "budget_used_percent": 29.142070721214587
    },
    {
      "type": "training",
      "description": "Training step 1226",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:28",
      "total_flops_so_far": 2.9165834953422972e+16,
      "budget_used_percent": 29.16583495342297
    },
    {
      "type": "training",
      "description": "Training step 1227",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:29",
      "total_flops_so_far": 2.9189599185631356e+16,
      "budget_used_percent": 29.189599185631355
    },
    {
      "type": "training",
      "description": "Training step 1228",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:31",
      "total_flops_so_far": 2.921336341783974e+16,
      "budget_used_percent": 29.213363417839737
    },
    {
      "type": "training",
      "description": "Training step 1229",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:32",
      "total_flops_so_far": 2.9237127650048124e+16,
      "budget_used_percent": 29.237127650048123
    },
    {
      "type": "training",
      "description": "Training step 1230",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:33",
      "total_flops_so_far": 2.9260891882256508e+16,
      "budget_used_percent": 29.260891882256505
    },
    {
      "type": "training",
      "description": "Training step 1231",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:35",
      "total_flops_so_far": 2.9284656114464892e+16,
      "budget_used_percent": 29.28465611446489
    },
    {
      "type": "training",
      "description": "Training step 1232",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:36",
      "total_flops_so_far": 2.9308420346673276e+16,
      "budget_used_percent": 29.308420346673277
    },
    {
      "type": "training",
      "description": "Training step 1233",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:37",
      "total_flops_so_far": 2.933218457888166e+16,
      "budget_used_percent": 29.33218457888166
    },
    {
      "type": "training",
      "description": "Training step 1234",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:39",
      "total_flops_so_far": 2.9355948811090044e+16,
      "budget_used_percent": 29.355948811090045
    },
    {
      "type": "training",
      "description": "Training step 1235",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:40",
      "total_flops_so_far": 2.9379713043298428e+16,
      "budget_used_percent": 29.379713043298427
    },
    {
      "type": "training",
      "description": "Training step 1236",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:41",
      "total_flops_so_far": 2.9403477275506812e+16,
      "budget_used_percent": 29.403477275506813
    },
    {
      "type": "training",
      "description": "Training step 1237",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:43",
      "total_flops_so_far": 2.9427241507715196e+16,
      "budget_used_percent": 29.427241507715195
    },
    {
      "type": "training",
      "description": "Training step 1238",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:44",
      "total_flops_so_far": 2.945100573992358e+16,
      "budget_used_percent": 29.45100573992358
    },
    {
      "type": "training",
      "description": "Training step 1239",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:45",
      "total_flops_so_far": 2.9474769972131964e+16,
      "budget_used_percent": 29.474769972131966
    },
    {
      "type": "training",
      "description": "Training step 1240",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:47",
      "total_flops_so_far": 2.9498534204340348e+16,
      "budget_used_percent": 29.49853420434035
    },
    {
      "type": "training",
      "description": "Training step 1241",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:48",
      "total_flops_so_far": 2.9522298436548732e+16,
      "budget_used_percent": 29.522298436548734
    },
    {
      "type": "training",
      "description": "Training step 1242",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:49",
      "total_flops_so_far": 2.9546062668757116e+16,
      "budget_used_percent": 29.546062668757116
    },
    {
      "type": "training",
      "description": "Training step 1243",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:50",
      "total_flops_so_far": 2.95698269009655e+16,
      "budget_used_percent": 29.569826900965502
    },
    {
      "type": "training",
      "description": "Training step 1244",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:52",
      "total_flops_so_far": 2.9593591133173884e+16,
      "budget_used_percent": 29.593591133173884
    },
    {
      "type": "training",
      "description": "Training step 1245",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:53",
      "total_flops_so_far": 2.9617355365382268e+16,
      "budget_used_percent": 29.61735536538227
    },
    {
      "type": "training",
      "description": "Training step 1246",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:54",
      "total_flops_so_far": 2.9641119597590652e+16,
      "budget_used_percent": 29.641119597590652
    },
    {
      "type": "training",
      "description": "Training step 1247",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:56",
      "total_flops_so_far": 2.9664883829799036e+16,
      "budget_used_percent": 29.664883829799038
    },
    {
      "type": "training",
      "description": "Training step 1248",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:57",
      "total_flops_so_far": 2.968864806200742e+16,
      "budget_used_percent": 29.688648062007424
    },
    {
      "type": "training",
      "description": "Training step 1249",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:37:58",
      "total_flops_so_far": 2.9712412294215804e+16,
      "budget_used_percent": 29.712412294215806
    },
    {
      "type": "training",
      "description": "Training step 1250",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:00",
      "total_flops_so_far": 2.9736176526424188e+16,
      "budget_used_percent": 29.73617652642419
    },
    {
      "type": "training",
      "description": "Training step 1251",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:01",
      "total_flops_so_far": 2.9759940758632572e+16,
      "budget_used_percent": 29.75994075863257
    },
    {
      "type": "training",
      "description": "Training step 1252",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:02",
      "total_flops_so_far": 2.9783704990840956e+16,
      "budget_used_percent": 29.783704990840953
    },
    {
      "type": "training",
      "description": "Training step 1253",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:04",
      "total_flops_so_far": 2.980746922304934e+16,
      "budget_used_percent": 29.80746922304934
    },
    {
      "type": "training",
      "description": "Training step 1254",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:05",
      "total_flops_so_far": 2.9831233455257724e+16,
      "budget_used_percent": 29.83123345525772
    },
    {
      "type": "training",
      "description": "Training step 1255",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:06",
      "total_flops_so_far": 2.9854997687466108e+16,
      "budget_used_percent": 29.854997687466106
    },
    {
      "type": "training",
      "description": "Training step 1256",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:07",
      "total_flops_so_far": 2.9878761919674492e+16,
      "budget_used_percent": 29.87876191967449
    },
    {
      "type": "training",
      "description": "Training step 1257",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:09",
      "total_flops_so_far": 2.9902526151882876e+16,
      "budget_used_percent": 29.902526151882874
    },
    {
      "type": "training",
      "description": "Training step 1258",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:10",
      "total_flops_so_far": 2.992629038409126e+16,
      "budget_used_percent": 29.92629038409126
    },
    {
      "type": "training",
      "description": "Training step 1259",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:11",
      "total_flops_so_far": 2.9950054616299644e+16,
      "budget_used_percent": 29.950054616299642
    },
    {
      "type": "training",
      "description": "Training step 1260",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:13",
      "total_flops_so_far": 2.9973818848508028e+16,
      "budget_used_percent": 29.973818848508028
    },
    {
      "type": "training",
      "description": "Training step 1261",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:14",
      "total_flops_so_far": 2.9997583080716412e+16,
      "budget_used_percent": 29.99758308071641
    },
    {
      "type": "training",
      "description": "Training step 1262",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:15",
      "total_flops_so_far": 3.0021347312924796e+16,
      "budget_used_percent": 30.021347312924796
    },
    {
      "type": "training",
      "description": "Training step 1263",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:17",
      "total_flops_so_far": 3.004511154513318e+16,
      "budget_used_percent": 30.045111545133178
    },
    {
      "type": "training",
      "description": "Training step 1264",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:18",
      "total_flops_so_far": 3.0068875777341564e+16,
      "budget_used_percent": 30.068875777341564
    },
    {
      "type": "training",
      "description": "Training step 1265",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:19",
      "total_flops_so_far": 3.0092640009549948e+16,
      "budget_used_percent": 30.09264000954995
    },
    {
      "type": "training",
      "description": "Training step 1266",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:21",
      "total_flops_so_far": 3.0116404241758332e+16,
      "budget_used_percent": 30.11640424175833
    },
    {
      "type": "training",
      "description": "Training step 1267",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:22",
      "total_flops_so_far": 3.0140168473966716e+16,
      "budget_used_percent": 30.140168473966717
    },
    {
      "type": "training",
      "description": "Training step 1268",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:23",
      "total_flops_so_far": 3.01639327061751e+16,
      "budget_used_percent": 30.1639327061751
    },
    {
      "type": "training",
      "description": "Training step 1269",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:24",
      "total_flops_so_far": 3.0187696938383484e+16,
      "budget_used_percent": 30.187696938383485
    },
    {
      "type": "training",
      "description": "Training step 1270",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:26",
      "total_flops_so_far": 3.0211461170591868e+16,
      "budget_used_percent": 30.211461170591868
    },
    {
      "type": "training",
      "description": "Training step 1271",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:27",
      "total_flops_so_far": 3.0235225402800252e+16,
      "budget_used_percent": 30.235225402800253
    },
    {
      "type": "training",
      "description": "Training step 1272",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:28",
      "total_flops_so_far": 3.0258989635008636e+16,
      "budget_used_percent": 30.25898963500864
    },
    {
      "type": "training",
      "description": "Training step 1273",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:30",
      "total_flops_so_far": 3.028275386721702e+16,
      "budget_used_percent": 30.28275386721702
    },
    {
      "type": "training",
      "description": "Training step 1274",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:31",
      "total_flops_so_far": 3.0306518099425404e+16,
      "budget_used_percent": 30.306518099425407
    },
    {
      "type": "training",
      "description": "Training step 1275",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:32",
      "total_flops_so_far": 3.0330282331633788e+16,
      "budget_used_percent": 30.33028233163379
    },
    {
      "type": "training",
      "description": "Training step 1276",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:34",
      "total_flops_so_far": 3.0354046563842172e+16,
      "budget_used_percent": 30.354046563842175
    },
    {
      "type": "training",
      "description": "Training step 1277",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:35",
      "total_flops_so_far": 3.0377810796050556e+16,
      "budget_used_percent": 30.377810796050557
    },
    {
      "type": "training",
      "description": "Training step 1278",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:36",
      "total_flops_so_far": 3.040157502825894e+16,
      "budget_used_percent": 30.401575028258943
    },
    {
      "type": "training",
      "description": "Training step 1279",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:38",
      "total_flops_so_far": 3.0425339260467324e+16,
      "budget_used_percent": 30.425339260467325
    },
    {
      "type": "training",
      "description": "Training step 1280",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:39",
      "total_flops_so_far": 3.0449103492675708e+16,
      "budget_used_percent": 30.449103492675704
    },
    {
      "type": "training",
      "description": "Training step 1281",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:40",
      "total_flops_so_far": 3.0472867724884092e+16,
      "budget_used_percent": 30.47286772488409
    },
    {
      "type": "training",
      "description": "Training step 1282",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:41",
      "total_flops_so_far": 3.0496631957092476e+16,
      "budget_used_percent": 30.496631957092475
    },
    {
      "type": "training",
      "description": "Training step 1283",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:43",
      "total_flops_so_far": 3.052039618930086e+16,
      "budget_used_percent": 30.520396189300858
    },
    {
      "type": "training",
      "description": "Training step 1284",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:44",
      "total_flops_so_far": 3.0544160421509244e+16,
      "budget_used_percent": 30.544160421509243
    },
    {
      "type": "training",
      "description": "Training step 1285",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:45",
      "total_flops_so_far": 3.0567924653717628e+16,
      "budget_used_percent": 30.567924653717625
    },
    {
      "type": "training",
      "description": "Training step 1286",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:47",
      "total_flops_so_far": 3.0591688885926012e+16,
      "budget_used_percent": 30.59168888592601
    },
    {
      "type": "training",
      "description": "Training step 1287",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:48",
      "total_flops_so_far": 3.0615453118134396e+16,
      "budget_used_percent": 30.615453118134393
    },
    {
      "type": "training",
      "description": "Training step 1288",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:49",
      "total_flops_so_far": 3.063921735034278e+16,
      "budget_used_percent": 30.63921735034278
    },
    {
      "type": "training",
      "description": "Training step 1289",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:51",
      "total_flops_so_far": 3.0662981582551164e+16,
      "budget_used_percent": 30.66298158255116
    },
    {
      "type": "training",
      "description": "Training step 1290",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:52",
      "total_flops_so_far": 3.0686745814759548e+16,
      "budget_used_percent": 30.686745814759547
    },
    {
      "type": "training",
      "description": "Training step 1291",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:53",
      "total_flops_so_far": 3.0710510046967932e+16,
      "budget_used_percent": 30.710510046967933
    },
    {
      "type": "training",
      "description": "Training step 1292",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:55",
      "total_flops_so_far": 3.0734274279176316e+16,
      "budget_used_percent": 30.734274279176315
    },
    {
      "type": "training",
      "description": "Training step 1293",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:56",
      "total_flops_so_far": 3.07580385113847e+16,
      "budget_used_percent": 30.7580385113847
    },
    {
      "type": "training",
      "description": "Training step 1294",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:57",
      "total_flops_so_far": 3.0781802743593084e+16,
      "budget_used_percent": 30.781802743593083
    },
    {
      "type": "training",
      "description": "Training step 1295",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:38:58",
      "total_flops_so_far": 3.0805566975801468e+16,
      "budget_used_percent": 30.80556697580147
    },
    {
      "type": "training",
      "description": "Training step 1296",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:00",
      "total_flops_so_far": 3.0829331208009852e+16,
      "budget_used_percent": 30.82933120800985
    },
    {
      "type": "training",
      "description": "Training step 1297",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:01",
      "total_flops_so_far": 3.0853095440218236e+16,
      "budget_used_percent": 30.853095440218237
    },
    {
      "type": "training",
      "description": "Training step 1298",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:02",
      "total_flops_so_far": 3.087685967242662e+16,
      "budget_used_percent": 30.876859672426622
    },
    {
      "type": "training",
      "description": "Training step 1299",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:04",
      "total_flops_so_far": 3.0900623904635004e+16,
      "budget_used_percent": 30.900623904635005
    },
    {
      "type": "training",
      "description": "Training step 1300",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:05",
      "total_flops_so_far": 3.0924388136843388e+16,
      "budget_used_percent": 30.92438813684339
    },
    {
      "type": "training",
      "description": "Training step 1301",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:06",
      "total_flops_so_far": 3.0948152369051772e+16,
      "budget_used_percent": 30.948152369051773
    },
    {
      "type": "training",
      "description": "Training step 1302",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:08",
      "total_flops_so_far": 3.0971916601260156e+16,
      "budget_used_percent": 30.97191660126016
    },
    {
      "type": "training",
      "description": "Training step 1303",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:09",
      "total_flops_so_far": 3.099568083346854e+16,
      "budget_used_percent": 30.99568083346854
    },
    {
      "type": "training",
      "description": "Training step 1304",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:10",
      "total_flops_so_far": 3.1019445065676924e+16,
      "budget_used_percent": 31.019445065676926
    },
    {
      "type": "training",
      "description": "Training step 1305",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:12",
      "total_flops_so_far": 3.1043209297885308e+16,
      "budget_used_percent": 31.043209297885312
    },
    {
      "type": "training",
      "description": "Training step 1306",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:13",
      "total_flops_so_far": 3.1066973530093692e+16,
      "budget_used_percent": 31.066973530093694
    },
    {
      "type": "training",
      "description": "Training step 1307",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:14",
      "total_flops_so_far": 3.1090737762302076e+16,
      "budget_used_percent": 31.09073776230208
    },
    {
      "type": "training",
      "description": "Training step 1308",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:15",
      "total_flops_so_far": 3.111450199451046e+16,
      "budget_used_percent": 31.11450199451046
    },
    {
      "type": "training",
      "description": "Training step 1309",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:17",
      "total_flops_so_far": 3.1138266226718844e+16,
      "budget_used_percent": 31.13826622671884
    },
    {
      "type": "training",
      "description": "Training step 1310",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:18",
      "total_flops_so_far": 3.1162030458927228e+16,
      "budget_used_percent": 31.162030458927227
    },
    {
      "type": "training",
      "description": "Training step 1311",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:19",
      "total_flops_so_far": 3.1185794691135612e+16,
      "budget_used_percent": 31.18579469113561
    },
    {
      "type": "training",
      "description": "Training step 1312",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:21",
      "total_flops_so_far": 3.1209558923343996e+16,
      "budget_used_percent": 31.209558923343995
    },
    {
      "type": "training",
      "description": "Training step 1313",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:22",
      "total_flops_so_far": 3.123332315555238e+16,
      "budget_used_percent": 31.233323155552377
    },
    {
      "type": "training",
      "description": "Training step 1314",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:23",
      "total_flops_so_far": 3.1257087387760764e+16,
      "budget_used_percent": 31.257087387760762
    },
    {
      "type": "training",
      "description": "Training step 1315",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:25",
      "total_flops_so_far": 3.1280851619969148e+16,
      "budget_used_percent": 31.280851619969148
    },
    {
      "type": "training",
      "description": "Training step 1316",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:26",
      "total_flops_so_far": 3.1304615852177532e+16,
      "budget_used_percent": 31.30461585217753
    },
    {
      "type": "training",
      "description": "Training step 1317",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:27",
      "total_flops_so_far": 3.1328380084385916e+16,
      "budget_used_percent": 31.328380084385916
    },
    {
      "type": "training",
      "description": "Training step 1318",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:29",
      "total_flops_so_far": 3.13521443165943e+16,
      "budget_used_percent": 31.3521443165943
    },
    {
      "type": "training",
      "description": "Training step 1319",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:30",
      "total_flops_so_far": 3.1375908548802684e+16,
      "budget_used_percent": 31.375908548802684
    },
    {
      "type": "training",
      "description": "Training step 1320",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:30",
      "total_flops_so_far": 3.1399672781011068e+16,
      "budget_used_percent": 31.399672781011066
    },
    {
      "type": "training",
      "description": "Training step 1321",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:32",
      "total_flops_so_far": 3.1423437013219452e+16,
      "budget_used_percent": 31.423437013219452
    },
    {
      "type": "training",
      "description": "Training step 1322",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:33",
      "total_flops_so_far": 3.1447201245427836e+16,
      "budget_used_percent": 31.447201245427834
    },
    {
      "type": "training",
      "description": "Training step 1323",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:34",
      "total_flops_so_far": 3.147096547763622e+16,
      "budget_used_percent": 31.47096547763622
    },
    {
      "type": "training",
      "description": "Training step 1324",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:36",
      "total_flops_so_far": 3.1494729709844604e+16,
      "budget_used_percent": 31.494729709844606
    },
    {
      "type": "training",
      "description": "Training step 1325",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:37",
      "total_flops_so_far": 3.1518493942052988e+16,
      "budget_used_percent": 31.518493942052988
    },
    {
      "type": "training",
      "description": "Training step 1326",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:38",
      "total_flops_so_far": 3.1542258174261372e+16,
      "budget_used_percent": 31.542258174261374
    },
    {
      "type": "training",
      "description": "Training step 1327",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:40",
      "total_flops_so_far": 3.1566022406469756e+16,
      "budget_used_percent": 31.566022406469756
    },
    {
      "type": "training",
      "description": "Training step 1328",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:41",
      "total_flops_so_far": 3.158978663867814e+16,
      "budget_used_percent": 31.58978663867814
    },
    {
      "type": "training",
      "description": "Training step 1329",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:42",
      "total_flops_so_far": 3.1613550870886524e+16,
      "budget_used_percent": 31.613550870886524
    },
    {
      "type": "training",
      "description": "Training step 1330",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:43",
      "total_flops_so_far": 3.1637315103094908e+16,
      "budget_used_percent": 31.63731510309491
    },
    {
      "type": "training",
      "description": "Training step 1331",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:45",
      "total_flops_so_far": 3.1661079335303292e+16,
      "budget_used_percent": 31.661079335303295
    },
    {
      "type": "training",
      "description": "Training step 1332",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:46",
      "total_flops_so_far": 3.1684843567511676e+16,
      "budget_used_percent": 31.684843567511678
    },
    {
      "type": "training",
      "description": "Training step 1333",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:47",
      "total_flops_so_far": 3.170860779972006e+16,
      "budget_used_percent": 31.708607799720063
    },
    {
      "type": "training",
      "description": "Training step 1334",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:49",
      "total_flops_so_far": 3.1732372031928444e+16,
      "budget_used_percent": 31.732372031928445
    },
    {
      "type": "training",
      "description": "Training step 1335",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:50",
      "total_flops_so_far": 3.1756136264136828e+16,
      "budget_used_percent": 31.75613626413683
    },
    {
      "type": "training",
      "description": "Training step 1336",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:51",
      "total_flops_so_far": 3.1779900496345212e+16,
      "budget_used_percent": 31.779900496345213
    },
    {
      "type": "training",
      "description": "Training step 1337",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:53",
      "total_flops_so_far": 3.1803664728553596e+16,
      "budget_used_percent": 31.803664728553592
    },
    {
      "type": "training",
      "description": "Training step 1338",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:54",
      "total_flops_so_far": 3.182742896076198e+16,
      "budget_used_percent": 31.827428960761978
    },
    {
      "type": "training",
      "description": "Training step 1339",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:55",
      "total_flops_so_far": 3.1851193192970364e+16,
      "budget_used_percent": 31.851193192970364
    },
    {
      "type": "training",
      "description": "Training step 1340",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:57",
      "total_flops_so_far": 3.1874957425178748e+16,
      "budget_used_percent": 31.874957425178746
    },
    {
      "type": "training",
      "description": "Training step 1341",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:58",
      "total_flops_so_far": 3.1898721657387132e+16,
      "budget_used_percent": 31.89872165738713
    },
    {
      "type": "training",
      "description": "Training step 1342",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:39:59",
      "total_flops_so_far": 3.1922485889595516e+16,
      "budget_used_percent": 31.922485889595514
    },
    {
      "type": "training",
      "description": "Training step 1343",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:00",
      "total_flops_so_far": 3.19462501218039e+16,
      "budget_used_percent": 31.9462501218039
    },
    {
      "type": "training",
      "description": "Training step 1344",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:02",
      "total_flops_so_far": 3.1970014354012284e+16,
      "budget_used_percent": 31.97001435401228
    },
    {
      "type": "training",
      "description": "Training step 1345",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:03",
      "total_flops_so_far": 3.1993778586220668e+16,
      "budget_used_percent": 31.993778586220667
    },
    {
      "type": "training",
      "description": "Training step 1346",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:06",
      "total_flops_so_far": 3.2017542818429052e+16,
      "budget_used_percent": 32.01754281842905
    },
    {
      "type": "training",
      "description": "Training step 1347",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:08",
      "total_flops_so_far": 3.2041307050637436e+16,
      "budget_used_percent": 32.041307050637435
    },
    {
      "type": "training",
      "description": "Training step 1348",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:09",
      "total_flops_so_far": 3.206507128284582e+16,
      "budget_used_percent": 32.06507128284582
    },
    {
      "type": "training",
      "description": "Training step 1349",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:11",
      "total_flops_so_far": 3.2088835515054204e+16,
      "budget_used_percent": 32.08883551505421
    },
    {
      "type": "training",
      "description": "Training step 1350",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:12",
      "total_flops_so_far": 3.2112599747262588e+16,
      "budget_used_percent": 32.112599747262585
    },
    {
      "type": "training",
      "description": "Training step 1351",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:13",
      "total_flops_so_far": 3.2136363979470972e+16,
      "budget_used_percent": 32.13636397947097
    },
    {
      "type": "training",
      "description": "Training step 1352",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:15",
      "total_flops_so_far": 3.2160128211679356e+16,
      "budget_used_percent": 32.16012821167936
    },
    {
      "type": "training",
      "description": "Training step 1353",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:16",
      "total_flops_so_far": 3.218389244388774e+16,
      "budget_used_percent": 32.18389244388774
    },
    {
      "type": "training",
      "description": "Training step 1354",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:17",
      "total_flops_so_far": 3.2207656676096124e+16,
      "budget_used_percent": 32.20765667609612
    },
    {
      "type": "training",
      "description": "Training step 1355",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:18",
      "total_flops_so_far": 3.2231420908304508e+16,
      "budget_used_percent": 32.23142090830451
    },
    {
      "type": "training",
      "description": "Training step 1356",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:20",
      "total_flops_so_far": 3.2255185140512892e+16,
      "budget_used_percent": 32.25518514051289
    },
    {
      "type": "training",
      "description": "Training step 1357",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:21",
      "total_flops_so_far": 3.2278949372721276e+16,
      "budget_used_percent": 32.27894937272128
    },
    {
      "type": "training",
      "description": "Training step 1358",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:22",
      "total_flops_so_far": 3.230271360492966e+16,
      "budget_used_percent": 32.302713604929664
    },
    {
      "type": "training",
      "description": "Training step 1359",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:24",
      "total_flops_so_far": 3.2326477837138044e+16,
      "budget_used_percent": 32.32647783713804
    },
    {
      "type": "training",
      "description": "Training step 1360",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:25",
      "total_flops_so_far": 3.2350242069346428e+16,
      "budget_used_percent": 32.35024206934643
    },
    {
      "type": "training",
      "description": "Training step 1361",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:26",
      "total_flops_so_far": 3.2374006301554812e+16,
      "budget_used_percent": 32.374006301554815
    },
    {
      "type": "training",
      "description": "Training step 1362",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:28",
      "total_flops_so_far": 3.2397770533763196e+16,
      "budget_used_percent": 32.3977705337632
    },
    {
      "type": "training",
      "description": "Training step 1363",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:29",
      "total_flops_so_far": 3.242153476597158e+16,
      "budget_used_percent": 32.421534765971586
    },
    {
      "type": "training",
      "description": "Training step 1364",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:30",
      "total_flops_so_far": 3.2445298998179964e+16,
      "budget_used_percent": 32.445298998179965
    },
    {
      "type": "training",
      "description": "Training step 1365",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:32",
      "total_flops_so_far": 3.2469063230388348e+16,
      "budget_used_percent": 32.46906323038834
    },
    {
      "type": "training",
      "description": "Training step 1366",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:33",
      "total_flops_so_far": 3.2492827462596732e+16,
      "budget_used_percent": 32.49282746259673
    },
    {
      "type": "training",
      "description": "Training step 1367",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:34",
      "total_flops_so_far": 3.2516591694805116e+16,
      "budget_used_percent": 32.516591694805115
    },
    {
      "type": "training",
      "description": "Training step 1368",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:36",
      "total_flops_so_far": 3.25403559270135e+16,
      "budget_used_percent": 32.5403559270135
    },
    {
      "type": "training",
      "description": "Training step 1369",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:37",
      "total_flops_so_far": 3.2564120159221884e+16,
      "budget_used_percent": 32.56412015922188
    },
    {
      "type": "training",
      "description": "Training step 1370",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:38",
      "total_flops_so_far": 3.2587884391430268e+16,
      "budget_used_percent": 32.587884391430265
    },
    {
      "type": "training",
      "description": "Training step 1371",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:39",
      "total_flops_so_far": 3.2611648623638652e+16,
      "budget_used_percent": 32.61164862363865
    },
    {
      "type": "training",
      "description": "Training step 1372",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:41",
      "total_flops_so_far": 3.2635412855847036e+16,
      "budget_used_percent": 32.635412855847036
    },
    {
      "type": "training",
      "description": "Training step 1373",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:42",
      "total_flops_so_far": 3.265917708805542e+16,
      "budget_used_percent": 32.65917708805542
    },
    {
      "type": "training",
      "description": "Training step 1374",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:43",
      "total_flops_so_far": 3.2682941320263804e+16,
      "budget_used_percent": 32.6829413202638
    },
    {
      "type": "training",
      "description": "Training step 1375",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:45",
      "total_flops_so_far": 3.2706705552472188e+16,
      "budget_used_percent": 32.70670555247219
    },
    {
      "type": "training",
      "description": "Training step 1376",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:46",
      "total_flops_so_far": 3.2730469784680572e+16,
      "budget_used_percent": 32.73046978468057
    },
    {
      "type": "training",
      "description": "Training step 1377",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:47",
      "total_flops_so_far": 3.2754234016888956e+16,
      "budget_used_percent": 32.75423401688896
    },
    {
      "type": "training",
      "description": "Training step 1378",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:49",
      "total_flops_so_far": 3.277799824909734e+16,
      "budget_used_percent": 32.77799824909734
    },
    {
      "type": "training",
      "description": "Training step 1379",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:50",
      "total_flops_so_far": 3.2801762481305724e+16,
      "budget_used_percent": 32.80176248130572
    },
    {
      "type": "training",
      "description": "Training step 1380",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:51",
      "total_flops_so_far": 3.2825526713514108e+16,
      "budget_used_percent": 32.82552671351411
    },
    {
      "type": "training",
      "description": "Training step 1381",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:53",
      "total_flops_so_far": 3.2849290945722492e+16,
      "budget_used_percent": 32.849290945722494
    },
    {
      "type": "training",
      "description": "Training step 1382",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:54",
      "total_flops_so_far": 3.2873055177930876e+16,
      "budget_used_percent": 32.87305517793088
    },
    {
      "type": "training",
      "description": "Training step 1383",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:55",
      "total_flops_so_far": 3.289681941013926e+16,
      "budget_used_percent": 32.89681941013926
    },
    {
      "type": "training",
      "description": "Training step 1384",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:56",
      "total_flops_so_far": 3.2920583642347644e+16,
      "budget_used_percent": 32.920583642347644
    },
    {
      "type": "training",
      "description": "Training step 1385",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:58",
      "total_flops_so_far": 3.2944347874556028e+16,
      "budget_used_percent": 32.94434787455603
    },
    {
      "type": "training",
      "description": "Training step 1386",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:40:59",
      "total_flops_so_far": 3.2968112106764412e+16,
      "budget_used_percent": 32.968112106764416
    },
    {
      "type": "training",
      "description": "Training step 1387",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:00",
      "total_flops_so_far": 3.2991876338972796e+16,
      "budget_used_percent": 32.991876338972794
    },
    {
      "type": "training",
      "description": "Training step 1388",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:02",
      "total_flops_so_far": 3.301564057118118e+16,
      "budget_used_percent": 33.01564057118118
    },
    {
      "type": "training",
      "description": "Training step 1389",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:03",
      "total_flops_so_far": 3.3039404803389564e+16,
      "budget_used_percent": 33.039404803389566
    },
    {
      "type": "training",
      "description": "Training step 1390",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:04",
      "total_flops_so_far": 3.3063169035597948e+16,
      "budget_used_percent": 33.06316903559795
    },
    {
      "type": "training",
      "description": "Training step 1391",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:06",
      "total_flops_so_far": 3.3086933267806332e+16,
      "budget_used_percent": 33.08693326780634
    },
    {
      "type": "training",
      "description": "Training step 1392",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:07",
      "total_flops_so_far": 3.3110697500014716e+16,
      "budget_used_percent": 33.110697500014716
    },
    {
      "type": "training",
      "description": "Training step 1393",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:08",
      "total_flops_so_far": 3.31344617322231e+16,
      "budget_used_percent": 33.1344617322231
    },
    {
      "type": "training",
      "description": "Training step 1394",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:10",
      "total_flops_so_far": 3.3158225964431484e+16,
      "budget_used_percent": 33.15822596443148
    },
    {
      "type": "training",
      "description": "Training step 1395",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:11",
      "total_flops_so_far": 3.3181990196639868e+16,
      "budget_used_percent": 33.181990196639866
    },
    {
      "type": "training",
      "description": "Training step 1396",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:12",
      "total_flops_so_far": 3.3205754428848252e+16,
      "budget_used_percent": 33.20575442884825
    },
    {
      "type": "training",
      "description": "Training step 1397",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:14",
      "total_flops_so_far": 3.3229518661056636e+16,
      "budget_used_percent": 33.22951866105663
    },
    {
      "type": "training",
      "description": "Training step 1398",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:15",
      "total_flops_so_far": 3.325328289326502e+16,
      "budget_used_percent": 33.253282893265016
    },
    {
      "type": "training",
      "description": "Training step 1399",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:16",
      "total_flops_so_far": 3.3277047125473404e+16,
      "budget_used_percent": 33.2770471254734
    },
    {
      "type": "training",
      "description": "Training step 1400",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:17",
      "total_flops_so_far": 3.3300811357681788e+16,
      "budget_used_percent": 33.30081135768179
    },
    {
      "type": "training",
      "description": "Training step 1401",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:19",
      "total_flops_so_far": 3.3324575589890172e+16,
      "budget_used_percent": 33.32457558989017
    },
    {
      "type": "training",
      "description": "Training step 1402",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:20",
      "total_flops_so_far": 3.3348339822098556e+16,
      "budget_used_percent": 33.34833982209855
    },
    {
      "type": "training",
      "description": "Training step 1403",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:21",
      "total_flops_so_far": 3.337210405430694e+16,
      "budget_used_percent": 33.37210405430694
    },
    {
      "type": "training",
      "description": "Training step 1404",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:23",
      "total_flops_so_far": 3.3395868286515324e+16,
      "budget_used_percent": 33.395868286515324
    },
    {
      "type": "training",
      "description": "Training step 1405",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:24",
      "total_flops_so_far": 3.3419632518723708e+16,
      "budget_used_percent": 33.41963251872371
    },
    {
      "type": "training",
      "description": "Training step 1406",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:25",
      "total_flops_so_far": 3.3443396750932092e+16,
      "budget_used_percent": 33.443396750932095
    },
    {
      "type": "training",
      "description": "Training step 1407",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:27",
      "total_flops_so_far": 3.3467160983140476e+16,
      "budget_used_percent": 33.467160983140474
    },
    {
      "type": "training",
      "description": "Training step 1408",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:28",
      "total_flops_so_far": 3.349092521534886e+16,
      "budget_used_percent": 33.49092521534886
    },
    {
      "type": "training",
      "description": "Training step 1409",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:29",
      "total_flops_so_far": 3.3514689447557244e+16,
      "budget_used_percent": 33.514689447557245
    },
    {
      "type": "training",
      "description": "Training step 1410",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:31",
      "total_flops_so_far": 3.3538453679765628e+16,
      "budget_used_percent": 33.53845367976563
    },
    {
      "type": "training",
      "description": "Training step 1411",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:32",
      "total_flops_so_far": 3.3562217911974012e+16,
      "budget_used_percent": 33.56221791197401
    },
    {
      "type": "training",
      "description": "Training step 1412",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:33",
      "total_flops_so_far": 3.3585982144182396e+16,
      "budget_used_percent": 33.585982144182395
    },
    {
      "type": "training",
      "description": "Training step 1413",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:35",
      "total_flops_so_far": 3.360974637639078e+16,
      "budget_used_percent": 33.60974637639078
    },
    {
      "type": "training",
      "description": "Training step 1414",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:36",
      "total_flops_so_far": 3.3633510608599164e+16,
      "budget_used_percent": 33.63351060859917
    },
    {
      "type": "training",
      "description": "Training step 1415",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:37",
      "total_flops_so_far": 3.3657274840807548e+16,
      "budget_used_percent": 33.65727484080755
    },
    {
      "type": "training",
      "description": "Training step 1416",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:38",
      "total_flops_so_far": 3.3681039073015932e+16,
      "budget_used_percent": 33.68103907301593
    },
    {
      "type": "training",
      "description": "Training step 1417",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:40",
      "total_flops_so_far": 3.3704803305224316e+16,
      "budget_used_percent": 33.70480330522432
    },
    {
      "type": "training",
      "description": "Training step 1418",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:41",
      "total_flops_so_far": 3.37285675374327e+16,
      "budget_used_percent": 33.7285675374327
    },
    {
      "type": "training",
      "description": "Training step 1419",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:42",
      "total_flops_so_far": 3.3752331769641084e+16,
      "budget_used_percent": 33.75233176964109
    },
    {
      "type": "training",
      "description": "Training step 1420",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:44",
      "total_flops_so_far": 3.3776096001849468e+16,
      "budget_used_percent": 33.77609600184947
    },
    {
      "type": "training",
      "description": "Training step 1421",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:45",
      "total_flops_so_far": 3.3799860234057852e+16,
      "budget_used_percent": 33.79986023405785
    },
    {
      "type": "training",
      "description": "Training step 1422",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:46",
      "total_flops_so_far": 3.3823624466266236e+16,
      "budget_used_percent": 33.82362446626623
    },
    {
      "type": "training",
      "description": "Training step 1423",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:48",
      "total_flops_so_far": 3.384738869847462e+16,
      "budget_used_percent": 33.84738869847462
    },
    {
      "type": "training",
      "description": "Training step 1424",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:49",
      "total_flops_so_far": 3.3871152930683004e+16,
      "budget_used_percent": 33.871152930683
    },
    {
      "type": "training",
      "description": "Training step 1425",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:50",
      "total_flops_so_far": 3.3894917162891388e+16,
      "budget_used_percent": 33.89491716289139
    },
    {
      "type": "training",
      "description": "Training step 1426",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:52",
      "total_flops_so_far": 3.3918681395099772e+16,
      "budget_used_percent": 33.91868139509977
    },
    {
      "type": "training",
      "description": "Training step 1427",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:53",
      "total_flops_so_far": 3.3942445627308156e+16,
      "budget_used_percent": 33.94244562730815
    },
    {
      "type": "training",
      "description": "Training step 1428",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:54",
      "total_flops_so_far": 3.396620985951654e+16,
      "budget_used_percent": 33.96620985951654
    },
    {
      "type": "training",
      "description": "Training step 1429",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:56",
      "total_flops_so_far": 3.3989974091724924e+16,
      "budget_used_percent": 33.989974091724925
    },
    {
      "type": "training",
      "description": "Training step 1430",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:57",
      "total_flops_so_far": 3.4013738323933308e+16,
      "budget_used_percent": 34.01373832393331
    },
    {
      "type": "training",
      "description": "Training step 1431",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:41:58",
      "total_flops_so_far": 3.4037502556141692e+16,
      "budget_used_percent": 34.03750255614169
    },
    {
      "type": "training",
      "description": "Training step 1432",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:00",
      "total_flops_so_far": 3.4061266788350076e+16,
      "budget_used_percent": 34.061266788350075
    },
    {
      "type": "training",
      "description": "Training step 1433",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:01",
      "total_flops_so_far": 3.408503102055846e+16,
      "budget_used_percent": 34.08503102055846
    },
    {
      "type": "training",
      "description": "Training step 1434",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:02",
      "total_flops_so_far": 3.4108795252766844e+16,
      "budget_used_percent": 34.108795252766846
    },
    {
      "type": "training",
      "description": "Training step 1435",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:03",
      "total_flops_so_far": 3.4132559484975228e+16,
      "budget_used_percent": 34.132559484975225
    },
    {
      "type": "training",
      "description": "Training step 1436",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:05",
      "total_flops_so_far": 3.4156323717183612e+16,
      "budget_used_percent": 34.15632371718361
    },
    {
      "type": "training",
      "description": "Training step 1437",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:06",
      "total_flops_so_far": 3.4180087949391996e+16,
      "budget_used_percent": 34.180087949392
    },
    {
      "type": "training",
      "description": "Training step 1438",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:07",
      "total_flops_so_far": 3.420385218160038e+16,
      "budget_used_percent": 34.20385218160038
    },
    {
      "type": "training",
      "description": "Training step 1439",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:09",
      "total_flops_so_far": 3.4227616413808764e+16,
      "budget_used_percent": 34.22761641380877
    },
    {
      "type": "training",
      "description": "Training step 1440",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:10",
      "total_flops_so_far": 3.4251380646017148e+16,
      "budget_used_percent": 34.25138064601715
    },
    {
      "type": "training",
      "description": "Training step 1441",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:11",
      "total_flops_so_far": 3.4275144878225532e+16,
      "budget_used_percent": 34.27514487822553
    },
    {
      "type": "training",
      "description": "Training step 1442",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:13",
      "total_flops_so_far": 3.4298909110433916e+16,
      "budget_used_percent": 34.29890911043392
    },
    {
      "type": "training",
      "description": "Training step 1443",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:14",
      "total_flops_so_far": 3.43226733426423e+16,
      "budget_used_percent": 34.322673342642304
    },
    {
      "type": "training",
      "description": "Training step 1444",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:15",
      "total_flops_so_far": 3.4346437574850684e+16,
      "budget_used_percent": 34.34643757485068
    },
    {
      "type": "training",
      "description": "Training step 1445",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:17",
      "total_flops_so_far": 3.4370201807059068e+16,
      "budget_used_percent": 34.37020180705907
    },
    {
      "type": "training",
      "description": "Training step 1446",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:18",
      "total_flops_so_far": 3.4393966039267452e+16,
      "budget_used_percent": 34.393966039267454
    },
    {
      "type": "training",
      "description": "Training step 1447",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:19",
      "total_flops_so_far": 3.4417730271475836e+16,
      "budget_used_percent": 34.41773027147584
    },
    {
      "type": "training",
      "description": "Training step 1448",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:20",
      "total_flops_so_far": 3.444149450368422e+16,
      "budget_used_percent": 34.441494503684225
    },
    {
      "type": "training",
      "description": "Training step 1449",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:22",
      "total_flops_so_far": 3.4465258735892604e+16,
      "budget_used_percent": 34.465258735892604
    },
    {
      "type": "training",
      "description": "Training step 1450",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:23",
      "total_flops_so_far": 3.4489022968100988e+16,
      "budget_used_percent": 34.48902296810099
    },
    {
      "type": "training",
      "description": "Training step 1451",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:24",
      "total_flops_so_far": 3.4512787200309372e+16,
      "budget_used_percent": 34.51278720030937
    },
    {
      "type": "training",
      "description": "Training step 1452",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:25",
      "total_flops_so_far": 3.4536551432517756e+16,
      "budget_used_percent": 34.536551432517754
    },
    {
      "type": "training",
      "description": "Training step 1453",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:26",
      "total_flops_so_far": 3.456031566472614e+16,
      "budget_used_percent": 34.56031566472614
    },
    {
      "type": "training",
      "description": "Training step 1454",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:27",
      "total_flops_so_far": 3.4584079896934524e+16,
      "budget_used_percent": 34.58407989693452
    },
    {
      "type": "training",
      "description": "Training step 1455",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:29",
      "total_flops_so_far": 3.4607844129142908e+16,
      "budget_used_percent": 34.607844129142904
    },
    {
      "type": "training",
      "description": "Training step 1456",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:30",
      "total_flops_so_far": 3.4631608361351292e+16,
      "budget_used_percent": 34.63160836135129
    },
    {
      "type": "training",
      "description": "Training step 1457",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:31",
      "total_flops_so_far": 3.4655372593559676e+16,
      "budget_used_percent": 34.655372593559676
    },
    {
      "type": "training",
      "description": "Training step 1458",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:33",
      "total_flops_so_far": 3.467913682576806e+16,
      "budget_used_percent": 34.67913682576806
    },
    {
      "type": "training",
      "description": "Training step 1459",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:34",
      "total_flops_so_far": 3.4702901057976444e+16,
      "budget_used_percent": 34.70290105797644
    },
    {
      "type": "training",
      "description": "Training step 1460",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:35",
      "total_flops_so_far": 3.4726665290184828e+16,
      "budget_used_percent": 34.726665290184826
    },
    {
      "type": "training",
      "description": "Training step 1461",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:37",
      "total_flops_so_far": 3.4750429522393212e+16,
      "budget_used_percent": 34.75042952239321
    },
    {
      "type": "training",
      "description": "Training step 1462",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:38",
      "total_flops_so_far": 3.4774193754601596e+16,
      "budget_used_percent": 34.7741937546016
    },
    {
      "type": "training",
      "description": "Training step 1463",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:39",
      "total_flops_so_far": 3.479795798680998e+16,
      "budget_used_percent": 34.797957986809976
    },
    {
      "type": "training",
      "description": "Training step 1464",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:41",
      "total_flops_so_far": 3.4821722219018364e+16,
      "budget_used_percent": 34.82172221901836
    },
    {
      "type": "training",
      "description": "Training step 1465",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:42",
      "total_flops_so_far": 3.4845486451226748e+16,
      "budget_used_percent": 34.84548645122675
    },
    {
      "type": "training",
      "description": "Training step 1466",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:43",
      "total_flops_so_far": 3.4869250683435132e+16,
      "budget_used_percent": 34.86925068343513
    },
    {
      "type": "training",
      "description": "Training step 1467",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:44",
      "total_flops_so_far": 3.4893014915643516e+16,
      "budget_used_percent": 34.89301491564352
    },
    {
      "type": "training",
      "description": "Training step 1468",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:46",
      "total_flops_so_far": 3.49167791478519e+16,
      "budget_used_percent": 34.9167791478519
    },
    {
      "type": "training",
      "description": "Training step 1469",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:47",
      "total_flops_so_far": 3.4940543380060284e+16,
      "budget_used_percent": 34.940543380060284
    },
    {
      "type": "training",
      "description": "Training step 1470",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:48",
      "total_flops_so_far": 3.4964307612268668e+16,
      "budget_used_percent": 34.96430761226867
    },
    {
      "type": "training",
      "description": "Training step 1471",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:50",
      "total_flops_so_far": 3.4988071844477052e+16,
      "budget_used_percent": 34.988071844477055
    },
    {
      "type": "training",
      "description": "Training step 1472",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:51",
      "total_flops_so_far": 3.5011836076685436e+16,
      "budget_used_percent": 35.01183607668544
    },
    {
      "type": "training",
      "description": "Training step 1473",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:52",
      "total_flops_so_far": 3.503560030889382e+16,
      "budget_used_percent": 35.03560030889382
    },
    {
      "type": "training",
      "description": "Training step 1474",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:54",
      "total_flops_so_far": 3.5059364541102204e+16,
      "budget_used_percent": 35.059364541102205
    },
    {
      "type": "training",
      "description": "Training step 1475",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:55",
      "total_flops_so_far": 3.5083128773310588e+16,
      "budget_used_percent": 35.08312877331059
    },
    {
      "type": "training",
      "description": "Training step 1476",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:56",
      "total_flops_so_far": 3.5106893005518972e+16,
      "budget_used_percent": 35.10689300551898
    },
    {
      "type": "training",
      "description": "Training step 1477",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:58",
      "total_flops_so_far": 3.5130657237727356e+16,
      "budget_used_percent": 35.130657237727355
    },
    {
      "type": "training",
      "description": "Training step 1478",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:42:59",
      "total_flops_so_far": 3.515442146993574e+16,
      "budget_used_percent": 35.15442146993574
    },
    {
      "type": "training",
      "description": "Training step 1479",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:00",
      "total_flops_so_far": 3.5178185702144124e+16,
      "budget_used_percent": 35.17818570214412
    },
    {
      "type": "training",
      "description": "Training step 1480",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:02",
      "total_flops_so_far": 3.5201949934352508e+16,
      "budget_used_percent": 35.201949934352506
    },
    {
      "type": "training",
      "description": "Training step 1481",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:03",
      "total_flops_so_far": 3.5225714166560892e+16,
      "budget_used_percent": 35.22571416656089
    },
    {
      "type": "training",
      "description": "Training step 1482",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:04",
      "total_flops_so_far": 3.5249478398769276e+16,
      "budget_used_percent": 35.24947839876928
    },
    {
      "type": "training",
      "description": "Training step 1483",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:05",
      "total_flops_so_far": 3.527324263097766e+16,
      "budget_used_percent": 35.273242630977656
    },
    {
      "type": "training",
      "description": "Training step 1484",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:07",
      "total_flops_so_far": 3.5297006863186044e+16,
      "budget_used_percent": 35.29700686318604
    },
    {
      "type": "training",
      "description": "Training step 1485",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:08",
      "total_flops_so_far": 3.5320771095394428e+16,
      "budget_used_percent": 35.32077109539443
    },
    {
      "type": "training",
      "description": "Training step 1486",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:09",
      "total_flops_so_far": 3.5344535327602812e+16,
      "budget_used_percent": 35.34453532760281
    },
    {
      "type": "training",
      "description": "Training step 1487",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:11",
      "total_flops_so_far": 3.5368299559811196e+16,
      "budget_used_percent": 35.36829955981119
    },
    {
      "type": "training",
      "description": "Training step 1488",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:12",
      "total_flops_so_far": 3.539206379201958e+16,
      "budget_used_percent": 35.39206379201958
    },
    {
      "type": "training",
      "description": "Training step 1489",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:14",
      "total_flops_so_far": 3.5415828024227964e+16,
      "budget_used_percent": 35.41582802422796
    },
    {
      "type": "training",
      "description": "Training step 1490",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:15",
      "total_flops_so_far": 3.5439592256436348e+16,
      "budget_used_percent": 35.43959225643635
    },
    {
      "type": "training",
      "description": "Training step 1491",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:16",
      "total_flops_so_far": 3.5463356488644732e+16,
      "budget_used_percent": 35.463356488644735
    },
    {
      "type": "training",
      "description": "Training step 1492",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:17",
      "total_flops_so_far": 3.5487120720853116e+16,
      "budget_used_percent": 35.48712072085311
    },
    {
      "type": "training",
      "description": "Training step 1493",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:19",
      "total_flops_so_far": 3.55108849530615e+16,
      "budget_used_percent": 35.5108849530615
    },
    {
      "type": "training",
      "description": "Training step 1494",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:20",
      "total_flops_so_far": 3.5534649185269884e+16,
      "budget_used_percent": 35.534649185269885
    },
    {
      "type": "training",
      "description": "Training step 1495",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:21",
      "total_flops_so_far": 3.5558413417478268e+16,
      "budget_used_percent": 35.55841341747827
    },
    {
      "type": "training",
      "description": "Training step 1496",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:23",
      "total_flops_so_far": 3.5582177649686652e+16,
      "budget_used_percent": 35.582177649686656
    },
    {
      "type": "training",
      "description": "Training step 1497",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:24",
      "total_flops_so_far": 3.5605941881895036e+16,
      "budget_used_percent": 35.605941881895035
    },
    {
      "type": "training",
      "description": "Training step 1498",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:25",
      "total_flops_so_far": 3.562970611410342e+16,
      "budget_used_percent": 35.62970611410342
    },
    {
      "type": "training",
      "description": "Training step 1499",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:27",
      "total_flops_so_far": 3.5653470346311804e+16,
      "budget_used_percent": 35.653470346311806
    },
    {
      "type": "training",
      "description": "Training step 1500",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:28",
      "total_flops_so_far": 3.5677234578520188e+16,
      "budget_used_percent": 35.67723457852019
    },
    {
      "type": "training",
      "description": "Training step 1501",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:29",
      "total_flops_so_far": 3.5700998810728572e+16,
      "budget_used_percent": 35.70099881072857
    },
    {
      "type": "training",
      "description": "Training step 1502",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:31",
      "total_flops_so_far": 3.5724763042936956e+16,
      "budget_used_percent": 35.72476304293696
    },
    {
      "type": "training",
      "description": "Training step 1503",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:32",
      "total_flops_so_far": 3.574852727514534e+16,
      "budget_used_percent": 35.74852727514534
    },
    {
      "type": "training",
      "description": "Training step 1504",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:33",
      "total_flops_so_far": 3.5772291507353724e+16,
      "budget_used_percent": 35.77229150735373
    },
    {
      "type": "training",
      "description": "Training step 1505",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:35",
      "total_flops_so_far": 3.5796055739562108e+16,
      "budget_used_percent": 35.796055739562114
    },
    {
      "type": "training",
      "description": "Training step 1506",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:36",
      "total_flops_so_far": 3.5819819971770492e+16,
      "budget_used_percent": 35.81981997177049
    },
    {
      "type": "training",
      "description": "Training step 1507",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:37",
      "total_flops_so_far": 3.5843584203978876e+16,
      "budget_used_percent": 35.84358420397887
    },
    {
      "type": "training",
      "description": "Training step 1508",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:38",
      "total_flops_so_far": 3.586734843618726e+16,
      "budget_used_percent": 35.86734843618726
    },
    {
      "type": "training",
      "description": "Training step 1509",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:40",
      "total_flops_so_far": 3.5891112668395644e+16,
      "budget_used_percent": 35.89111266839564
    },
    {
      "type": "training",
      "description": "Training step 1510",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:41",
      "total_flops_so_far": 3.5914876900604028e+16,
      "budget_used_percent": 35.91487690060403
    },
    {
      "type": "training",
      "description": "Training step 1511",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:42",
      "total_flops_so_far": 3.5938641132812412e+16,
      "budget_used_percent": 35.93864113281241
    },
    {
      "type": "training",
      "description": "Training step 1512",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:44",
      "total_flops_so_far": 3.5962405365020796e+16,
      "budget_used_percent": 35.96240536502079
    },
    {
      "type": "training",
      "description": "Training step 1513",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:45",
      "total_flops_so_far": 3.598616959722918e+16,
      "budget_used_percent": 35.98616959722918
    },
    {
      "type": "training",
      "description": "Training step 1514",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:46",
      "total_flops_so_far": 3.6009933829437564e+16,
      "budget_used_percent": 36.009933829437564
    },
    {
      "type": "training",
      "description": "Training step 1515",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:48",
      "total_flops_so_far": 3.603369806164595e+16,
      "budget_used_percent": 36.03369806164595
    },
    {
      "type": "training",
      "description": "Training step 1516",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:49",
      "total_flops_so_far": 3.605746229385434e+16,
      "budget_used_percent": 36.057462293854336
    },
    {
      "type": "training",
      "description": "Training step 1517",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:50",
      "total_flops_so_far": 3.608122652606272e+16,
      "budget_used_percent": 36.08122652606272
    },
    {
      "type": "training",
      "description": "Training step 1518",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:52",
      "total_flops_so_far": 3.61049907582711e+16,
      "budget_used_percent": 36.10499075827111
    },
    {
      "type": "training",
      "description": "Training step 1519",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:53",
      "total_flops_so_far": 3.612875499047949e+16,
      "budget_used_percent": 36.128754990479486
    },
    {
      "type": "training",
      "description": "Training step 1520",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:54",
      "total_flops_so_far": 3.615251922268787e+16,
      "budget_used_percent": 36.15251922268787
    },
    {
      "type": "training",
      "description": "Training step 1521",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:56",
      "total_flops_so_far": 3.617628345489626e+16,
      "budget_used_percent": 36.17628345489626
    },
    {
      "type": "training",
      "description": "Training step 1522",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:57",
      "total_flops_so_far": 3.620004768710464e+16,
      "budget_used_percent": 36.20004768710464
    },
    {
      "type": "training",
      "description": "Training step 1523",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:58",
      "total_flops_so_far": 3.622381191931302e+16,
      "budget_used_percent": 36.22381191931303
    },
    {
      "type": "training",
      "description": "Training step 1524",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:43:59",
      "total_flops_so_far": 3.624757615152141e+16,
      "budget_used_percent": 36.24757615152141
    },
    {
      "type": "training",
      "description": "Training step 1525",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:01",
      "total_flops_so_far": 3.627134038372979e+16,
      "budget_used_percent": 36.27134038372979
    },
    {
      "type": "training",
      "description": "Training step 1526",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:02",
      "total_flops_so_far": 3.629510461593818e+16,
      "budget_used_percent": 36.29510461593818
    },
    {
      "type": "training",
      "description": "Training step 1527",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:03",
      "total_flops_so_far": 3.631886884814656e+16,
      "budget_used_percent": 36.318868848146565
    },
    {
      "type": "training",
      "description": "Training step 1528",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:05",
      "total_flops_so_far": 3.634263308035494e+16,
      "budget_used_percent": 36.34263308035494
    },
    {
      "type": "training",
      "description": "Training step 1529",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:06",
      "total_flops_so_far": 3.636639731256333e+16,
      "budget_used_percent": 36.36639731256332
    },
    {
      "type": "training",
      "description": "Training step 1530",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:07",
      "total_flops_so_far": 3.639016154477171e+16,
      "budget_used_percent": 36.39016154477171
    },
    {
      "type": "training",
      "description": "Training step 1531",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:09",
      "total_flops_so_far": 3.64139257769801e+16,
      "budget_used_percent": 36.41392577698009
    },
    {
      "type": "training",
      "description": "Training step 1532",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:10",
      "total_flops_so_far": 3.643769000918848e+16,
      "budget_used_percent": 36.43769000918848
    },
    {
      "type": "training",
      "description": "Training step 1533",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:11",
      "total_flops_so_far": 3.646145424139686e+16,
      "budget_used_percent": 36.461454241396865
    },
    {
      "type": "training",
      "description": "Training step 1534",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:13",
      "total_flops_so_far": 3.648521847360525e+16,
      "budget_used_percent": 36.485218473605244
    },
    {
      "type": "training",
      "description": "Training step 1535",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:14",
      "total_flops_so_far": 3.650898270581363e+16,
      "budget_used_percent": 36.50898270581363
    },
    {
      "type": "training",
      "description": "Training step 1536",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:15",
      "total_flops_so_far": 3.653274693802202e+16,
      "budget_used_percent": 36.532746938022015
    },
    {
      "type": "training",
      "description": "Training step 1537",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:17",
      "total_flops_so_far": 3.65565111702304e+16,
      "budget_used_percent": 36.5565111702304
    },
    {
      "type": "training",
      "description": "Training step 1538",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:18",
      "total_flops_so_far": 3.658027540243878e+16,
      "budget_used_percent": 36.58027540243879
    },
    {
      "type": "training",
      "description": "Training step 1539",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:19",
      "total_flops_so_far": 3.660403963464717e+16,
      "budget_used_percent": 36.604039634647165
    },
    {
      "type": "training",
      "description": "Training step 1540",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:21",
      "total_flops_so_far": 3.662780386685555e+16,
      "budget_used_percent": 36.62780386685555
    },
    {
      "type": "training",
      "description": "Training step 1541",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:22",
      "total_flops_so_far": 3.665156809906394e+16,
      "budget_used_percent": 36.65156809906394
    },
    {
      "type": "training",
      "description": "Training step 1542",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:23",
      "total_flops_so_far": 3.667533233127232e+16,
      "budget_used_percent": 36.67533233127232
    },
    {
      "type": "training",
      "description": "Training step 1543",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:24",
      "total_flops_so_far": 3.66990965634807e+16,
      "budget_used_percent": 36.6990965634807
    },
    {
      "type": "training",
      "description": "Training step 1544",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:26",
      "total_flops_so_far": 3.672286079568909e+16,
      "budget_used_percent": 36.72286079568909
    },
    {
      "type": "training",
      "description": "Training step 1545",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:27",
      "total_flops_so_far": 3.674662502789747e+16,
      "budget_used_percent": 36.74662502789747
    },
    {
      "type": "training",
      "description": "Training step 1546",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:28",
      "total_flops_so_far": 3.677038926010586e+16,
      "budget_used_percent": 36.77038926010586
    },
    {
      "type": "training",
      "description": "Training step 1547",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:30",
      "total_flops_so_far": 3.679415349231424e+16,
      "budget_used_percent": 36.794153492314244
    },
    {
      "type": "training",
      "description": "Training step 1548",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:31",
      "total_flops_so_far": 3.681791772452262e+16,
      "budget_used_percent": 36.81791772452262
    },
    {
      "type": "training",
      "description": "Training step 1549",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:32",
      "total_flops_so_far": 3.684168195673101e+16,
      "budget_used_percent": 36.84168195673101
    },
    {
      "type": "training",
      "description": "Training step 1550",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:34",
      "total_flops_so_far": 3.686544618893939e+16,
      "budget_used_percent": 36.865446188939394
    },
    {
      "type": "training",
      "description": "Training step 1551",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:35",
      "total_flops_so_far": 3.688921042114778e+16,
      "budget_used_percent": 36.88921042114778
    },
    {
      "type": "training",
      "description": "Training step 1552",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:36",
      "total_flops_so_far": 3.691297465335616e+16,
      "budget_used_percent": 36.91297465335616
    },
    {
      "type": "training",
      "description": "Training step 1553",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:38",
      "total_flops_so_far": 3.693673888556454e+16,
      "budget_used_percent": 36.936738885564544
    },
    {
      "type": "training",
      "description": "Training step 1554",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:39",
      "total_flops_so_far": 3.696050311777293e+16,
      "budget_used_percent": 36.96050311777293
    },
    {
      "type": "training",
      "description": "Training step 1555",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:40",
      "total_flops_so_far": 3.698426734998131e+16,
      "budget_used_percent": 36.984267349981316
    },
    {
      "type": "training",
      "description": "Training step 1556",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:42",
      "total_flops_so_far": 3.70080315821897e+16,
      "budget_used_percent": 37.008031582189695
    },
    {
      "type": "training",
      "description": "Training step 1557",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:43",
      "total_flops_so_far": 3.703179581439808e+16,
      "budget_used_percent": 37.03179581439808
    },
    {
      "type": "training",
      "description": "Training step 1558",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:44",
      "total_flops_so_far": 3.705556004660646e+16,
      "budget_used_percent": 37.05556004660646
    },
    {
      "type": "training",
      "description": "Training step 1559",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:45",
      "total_flops_so_far": 3.707932427881485e+16,
      "budget_used_percent": 37.079324278814845
    },
    {
      "type": "training",
      "description": "Training step 1560",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:47",
      "total_flops_so_far": 3.710308851102323e+16,
      "budget_used_percent": 37.10308851102323
    },
    {
      "type": "training",
      "description": "Training step 1561",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:48",
      "total_flops_so_far": 3.712685274323162e+16,
      "budget_used_percent": 37.126852743231616
    },
    {
      "type": "training",
      "description": "Training step 1562",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:49",
      "total_flops_so_far": 3.715061697544e+16,
      "budget_used_percent": 37.15061697544
    },
    {
      "type": "training",
      "description": "Training step 1563",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:51",
      "total_flops_so_far": 3.717438120764838e+16,
      "budget_used_percent": 37.17438120764838
    },
    {
      "type": "training",
      "description": "Training step 1564",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:52",
      "total_flops_so_far": 3.719814543985677e+16,
      "budget_used_percent": 37.198145439856766
    },
    {
      "type": "training",
      "description": "Training step 1565",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:53",
      "total_flops_so_far": 3.722190967206515e+16,
      "budget_used_percent": 37.22190967206515
    },
    {
      "type": "training",
      "description": "Training step 1566",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:55",
      "total_flops_so_far": 3.724567390427354e+16,
      "budget_used_percent": 37.24567390427354
    },
    {
      "type": "training",
      "description": "Training step 1567",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:56",
      "total_flops_so_far": 3.726943813648192e+16,
      "budget_used_percent": 37.26943813648192
    },
    {
      "type": "training",
      "description": "Training step 1568",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:57",
      "total_flops_so_far": 3.72932023686903e+16,
      "budget_used_percent": 37.2932023686903
    },
    {
      "type": "training",
      "description": "Training step 1569",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:44:59",
      "total_flops_so_far": 3.731696660089869e+16,
      "budget_used_percent": 37.31696660089869
    },
    {
      "type": "training",
      "description": "Training step 1570",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:00",
      "total_flops_so_far": 3.734073083310707e+16,
      "budget_used_percent": 37.340730833107074
    },
    {
      "type": "training",
      "description": "Training step 1571",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:01",
      "total_flops_so_far": 3.736449506531546e+16,
      "budget_used_percent": 37.36449506531546
    },
    {
      "type": "training",
      "description": "Training step 1572",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:03",
      "total_flops_so_far": 3.738825929752384e+16,
      "budget_used_percent": 37.38825929752384
    },
    {
      "type": "training",
      "description": "Training step 1573",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:04",
      "total_flops_so_far": 3.741202352973222e+16,
      "budget_used_percent": 37.412023529732224
    },
    {
      "type": "training",
      "description": "Training step 1574",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:05",
      "total_flops_so_far": 3.743578776194061e+16,
      "budget_used_percent": 37.43578776194061
    },
    {
      "type": "training",
      "description": "Training step 1575",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:07",
      "total_flops_so_far": 3.745955199414899e+16,
      "budget_used_percent": 37.459551994148995
    },
    {
      "type": "training",
      "description": "Training step 1576",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:08",
      "total_flops_so_far": 3.748331622635738e+16,
      "budget_used_percent": 37.483316226357374
    },
    {
      "type": "training",
      "description": "Training step 1577",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:09",
      "total_flops_so_far": 3.750708045856576e+16,
      "budget_used_percent": 37.50708045856576
    },
    {
      "type": "training",
      "description": "Training step 1578",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:10",
      "total_flops_so_far": 3.753084469077414e+16,
      "budget_used_percent": 37.530844690774146
    },
    {
      "type": "training",
      "description": "Training step 1579",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:12",
      "total_flops_so_far": 3.755460892298253e+16,
      "budget_used_percent": 37.55460892298253
    },
    {
      "type": "training",
      "description": "Training step 1580",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:13",
      "total_flops_so_far": 3.757837315519091e+16,
      "budget_used_percent": 37.57837315519092
    },
    {
      "type": "training",
      "description": "Training step 1581",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:14",
      "total_flops_so_far": 3.76021373873993e+16,
      "budget_used_percent": 37.602137387399296
    },
    {
      "type": "training",
      "description": "Training step 1582",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:16",
      "total_flops_so_far": 3.762590161960768e+16,
      "budget_used_percent": 37.62590161960768
    },
    {
      "type": "training",
      "description": "Training step 1583",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:17",
      "total_flops_so_far": 3.764966585181606e+16,
      "budget_used_percent": 37.64966585181607
    },
    {
      "type": "training",
      "description": "Training step 1584",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:17",
      "total_flops_so_far": 3.767343008402445e+16,
      "budget_used_percent": 37.67343008402445
    },
    {
      "type": "training",
      "description": "Training step 1585",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:19",
      "total_flops_so_far": 3.769719431623283e+16,
      "budget_used_percent": 37.69719431623283
    },
    {
      "type": "training",
      "description": "Training step 1586",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:20",
      "total_flops_so_far": 3.772095854844122e+16,
      "budget_used_percent": 37.72095854844121
    },
    {
      "type": "training",
      "description": "Training step 1587",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:21",
      "total_flops_so_far": 3.77447227806496e+16,
      "budget_used_percent": 37.744722780649596
    },
    {
      "type": "training",
      "description": "Training step 1588",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:23",
      "total_flops_so_far": 3.776848701285798e+16,
      "budget_used_percent": 37.76848701285798
    },
    {
      "type": "training",
      "description": "Training step 1589",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:24",
      "total_flops_so_far": 3.779225124506637e+16,
      "budget_used_percent": 37.79225124506637
    },
    {
      "type": "training",
      "description": "Training step 1590",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:25",
      "total_flops_so_far": 3.781601547727475e+16,
      "budget_used_percent": 37.81601547727475
    },
    {
      "type": "training",
      "description": "Training step 1591",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:27",
      "total_flops_so_far": 3.783977970948314e+16,
      "budget_used_percent": 37.83977970948313
    },
    {
      "type": "training",
      "description": "Training step 1592",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:28",
      "total_flops_so_far": 3.786354394169152e+16,
      "budget_used_percent": 37.86354394169152
    },
    {
      "type": "training",
      "description": "Training step 1593",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:29",
      "total_flops_so_far": 3.78873081738999e+16,
      "budget_used_percent": 37.8873081738999
    },
    {
      "type": "training",
      "description": "Training step 1594",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:31",
      "total_flops_so_far": 3.791107240610829e+16,
      "budget_used_percent": 37.91107240610829
    },
    {
      "type": "training",
      "description": "Training step 1595",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:32",
      "total_flops_so_far": 3.793483663831667e+16,
      "budget_used_percent": 37.93483663831667
    },
    {
      "type": "training",
      "description": "Training step 1596",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:33",
      "total_flops_so_far": 3.795860087052506e+16,
      "budget_used_percent": 37.95860087052505
    },
    {
      "type": "training",
      "description": "Training step 1597",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:35",
      "total_flops_so_far": 3.798236510273344e+16,
      "budget_used_percent": 37.98236510273344
    },
    {
      "type": "training",
      "description": "Training step 1598",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:36",
      "total_flops_so_far": 3.800612933494182e+16,
      "budget_used_percent": 38.006129334941825
    },
    {
      "type": "training",
      "description": "Training step 1599",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:37",
      "total_flops_so_far": 3.802989356715021e+16,
      "budget_used_percent": 38.02989356715021
    },
    {
      "type": "training",
      "description": "Training step 1600",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:38",
      "total_flops_so_far": 3.805365779935859e+16,
      "budget_used_percent": 38.05365779935859
    },
    {
      "type": "training",
      "description": "Training step 1601",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:40",
      "total_flops_so_far": 3.807742203156698e+16,
      "budget_used_percent": 38.077422031566975
    },
    {
      "type": "training",
      "description": "Training step 1602",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:41",
      "total_flops_so_far": 3.810118626377536e+16,
      "budget_used_percent": 38.10118626377536
    },
    {
      "type": "training",
      "description": "Training step 1603",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:42",
      "total_flops_so_far": 3.812495049598374e+16,
      "budget_used_percent": 38.12495049598375
    },
    {
      "type": "training",
      "description": "Training step 1604",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:44",
      "total_flops_so_far": 3.814871472819213e+16,
      "budget_used_percent": 38.14871472819213
    },
    {
      "type": "training",
      "description": "Training step 1605",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:45",
      "total_flops_so_far": 3.817247896040051e+16,
      "budget_used_percent": 38.17247896040051
    },
    {
      "type": "training",
      "description": "Training step 1606",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:46",
      "total_flops_so_far": 3.81962431926089e+16,
      "budget_used_percent": 38.1962431926089
    },
    {
      "type": "training",
      "description": "Training step 1607",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:48",
      "total_flops_so_far": 3.822000742481728e+16,
      "budget_used_percent": 38.22000742481728
    },
    {
      "type": "training",
      "description": "Training step 1608",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:49",
      "total_flops_so_far": 3.824377165702566e+16,
      "budget_used_percent": 38.24377165702567
    },
    {
      "type": "training",
      "description": "Training step 1609",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:50",
      "total_flops_so_far": 3.826753588923405e+16,
      "budget_used_percent": 38.26753588923405
    },
    {
      "type": "training",
      "description": "Training step 1610",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:52",
      "total_flops_so_far": 3.829130012144243e+16,
      "budget_used_percent": 38.29130012144243
    },
    {
      "type": "training",
      "description": "Training step 1611",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:53",
      "total_flops_so_far": 3.831506435365082e+16,
      "budget_used_percent": 38.31506435365082
    },
    {
      "type": "training",
      "description": "Training step 1612",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:54",
      "total_flops_so_far": 3.83388285858592e+16,
      "budget_used_percent": 38.338828585859204
    },
    {
      "type": "training",
      "description": "Training step 1613",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:56",
      "total_flops_so_far": 3.836259281806758e+16,
      "budget_used_percent": 38.36259281806758
    },
    {
      "type": "training",
      "description": "Training step 1614",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:57",
      "total_flops_so_far": 3.838635705027597e+16,
      "budget_used_percent": 38.38635705027597
    },
    {
      "type": "training",
      "description": "Training step 1615",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:45:58",
      "total_flops_so_far": 3.841012128248435e+16,
      "budget_used_percent": 38.41012128248435
    },
    {
      "type": "training",
      "description": "Training step 1616",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:00",
      "total_flops_so_far": 3.843388551469274e+16,
      "budget_used_percent": 38.43388551469273
    },
    {
      "type": "training",
      "description": "Training step 1617",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:01",
      "total_flops_so_far": 3.845764974690112e+16,
      "budget_used_percent": 38.45764974690112
    },
    {
      "type": "training",
      "description": "Training step 1618",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:02",
      "total_flops_so_far": 3.84814139791095e+16,
      "budget_used_percent": 38.481413979109504
    },
    {
      "type": "training",
      "description": "Training step 1619",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:03",
      "total_flops_so_far": 3.850517821131789e+16,
      "budget_used_percent": 38.50517821131788
    },
    {
      "type": "training",
      "description": "Training step 1620",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:05",
      "total_flops_so_far": 3.852894244352627e+16,
      "budget_used_percent": 38.52894244352627
    },
    {
      "type": "training",
      "description": "Training step 1621",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:06",
      "total_flops_so_far": 3.855270667573466e+16,
      "budget_used_percent": 38.552706675734655
    },
    {
      "type": "training",
      "description": "Training step 1622",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:07",
      "total_flops_so_far": 3.857647090794304e+16,
      "budget_used_percent": 38.57647090794304
    },
    {
      "type": "training",
      "description": "Training step 1623",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:09",
      "total_flops_so_far": 3.860023514015142e+16,
      "budget_used_percent": 38.600235140151426
    },
    {
      "type": "training",
      "description": "Training step 1624",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:10",
      "total_flops_so_far": 3.862399937235981e+16,
      "budget_used_percent": 38.623999372359805
    },
    {
      "type": "training",
      "description": "Training step 1625",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:11",
      "total_flops_so_far": 3.864776360456819e+16,
      "budget_used_percent": 38.64776360456819
    },
    {
      "type": "training",
      "description": "Training step 1626",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:13",
      "total_flops_so_far": 3.867152783677658e+16,
      "budget_used_percent": 38.671527836776576
    },
    {
      "type": "training",
      "description": "Training step 1627",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:14",
      "total_flops_so_far": 3.869529206898496e+16,
      "budget_used_percent": 38.69529206898496
    },
    {
      "type": "training",
      "description": "Training step 1628",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:15",
      "total_flops_so_far": 3.871905630119334e+16,
      "budget_used_percent": 38.71905630119335
    },
    {
      "type": "training",
      "description": "Training step 1629",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:17",
      "total_flops_so_far": 3.874282053340173e+16,
      "budget_used_percent": 38.742820533401726
    },
    {
      "type": "training",
      "description": "Training step 1630",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:18",
      "total_flops_so_far": 3.876658476561011e+16,
      "budget_used_percent": 38.76658476561011
    },
    {
      "type": "training",
      "description": "Training step 1631",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:19",
      "total_flops_so_far": 3.87903489978185e+16,
      "budget_used_percent": 38.7903489978185
    },
    {
      "type": "training",
      "description": "Training step 1632",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:21",
      "total_flops_so_far": 3.881411323002688e+16,
      "budget_used_percent": 38.814113230026884
    },
    {
      "type": "training",
      "description": "Training step 1633",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:22",
      "total_flops_so_far": 3.883787746223526e+16,
      "budget_used_percent": 38.83787746223526
    },
    {
      "type": "training",
      "description": "Training step 1634",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:23",
      "total_flops_so_far": 3.886164169444365e+16,
      "budget_used_percent": 38.86164169444365
    },
    {
      "type": "training",
      "description": "Training step 1635",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:25",
      "total_flops_so_far": 3.888540592665203e+16,
      "budget_used_percent": 38.885405926652034
    },
    {
      "type": "training",
      "description": "Training step 1636",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:26",
      "total_flops_so_far": 3.890917015886042e+16,
      "budget_used_percent": 38.90917015886042
    },
    {
      "type": "training",
      "description": "Training step 1637",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:27",
      "total_flops_so_far": 3.89329343910688e+16,
      "budget_used_percent": 38.932934391068805
    },
    {
      "type": "training",
      "description": "Training step 1638",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:28",
      "total_flops_so_far": 3.895669862327718e+16,
      "budget_used_percent": 38.956698623277184
    },
    {
      "type": "training",
      "description": "Training step 1639",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:30",
      "total_flops_so_far": 3.898046285548557e+16,
      "budget_used_percent": 38.98046285548557
    },
    {
      "type": "training",
      "description": "Training step 1640",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:31",
      "total_flops_so_far": 3.900422708769395e+16,
      "budget_used_percent": 39.004227087693955
    },
    {
      "type": "training",
      "description": "Training step 1641",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:32",
      "total_flops_so_far": 3.902799131990234e+16,
      "budget_used_percent": 39.02799131990234
    },
    {
      "type": "training",
      "description": "Training step 1642",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:34",
      "total_flops_so_far": 3.905175555211072e+16,
      "budget_used_percent": 39.05175555211072
    },
    {
      "type": "training",
      "description": "Training step 1643",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:35",
      "total_flops_so_far": 3.90755197843191e+16,
      "budget_used_percent": 39.0755197843191
    },
    {
      "type": "training",
      "description": "Training step 1644",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:36",
      "total_flops_so_far": 3.909928401652749e+16,
      "budget_used_percent": 39.099284016527484
    },
    {
      "type": "training",
      "description": "Training step 1645",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:38",
      "total_flops_so_far": 3.912304824873587e+16,
      "budget_used_percent": 39.12304824873587
    },
    {
      "type": "training",
      "description": "Training step 1646",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:39",
      "total_flops_so_far": 3.914681248094426e+16,
      "budget_used_percent": 39.146812480944256
    },
    {
      "type": "training",
      "description": "Training step 1647",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:40",
      "total_flops_so_far": 3.917057671315264e+16,
      "budget_used_percent": 39.17057671315264
    },
    {
      "type": "training",
      "description": "Training step 1648",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:42",
      "total_flops_so_far": 3.919434094536102e+16,
      "budget_used_percent": 39.19434094536102
    },
    {
      "type": "training",
      "description": "Training step 1649",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:43",
      "total_flops_so_far": 3.921810517756941e+16,
      "budget_used_percent": 39.218105177569406
    },
    {
      "type": "training",
      "description": "Training step 1650",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:44",
      "total_flops_so_far": 3.924186940977779e+16,
      "budget_used_percent": 39.24186940977779
    },
    {
      "type": "training",
      "description": "Training step 1651",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:46",
      "total_flops_so_far": 3.926563364198618e+16,
      "budget_used_percent": 39.26563364198618
    },
    {
      "type": "training",
      "description": "Training step 1652",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:47",
      "total_flops_so_far": 3.928939787419456e+16,
      "budget_used_percent": 39.289397874194556
    },
    {
      "type": "training",
      "description": "Training step 1653",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:48",
      "total_flops_so_far": 3.931316210640294e+16,
      "budget_used_percent": 39.31316210640294
    },
    {
      "type": "training",
      "description": "Training step 1654",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:50",
      "total_flops_so_far": 3.933692633861133e+16,
      "budget_used_percent": 39.33692633861133
    },
    {
      "type": "training",
      "description": "Training step 1655",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:51",
      "total_flops_so_far": 3.936069057081971e+16,
      "budget_used_percent": 39.36069057081971
    },
    {
      "type": "training",
      "description": "Training step 1656",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:52",
      "total_flops_so_far": 3.93844548030281e+16,
      "budget_used_percent": 39.3844548030281
    },
    {
      "type": "training",
      "description": "Training step 1657",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:53",
      "total_flops_so_far": 3.940821903523648e+16,
      "budget_used_percent": 39.40821903523648
    },
    {
      "type": "training",
      "description": "Training step 1658",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:55",
      "total_flops_so_far": 3.943198326744486e+16,
      "budget_used_percent": 39.43198326744486
    },
    {
      "type": "training",
      "description": "Training step 1659",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:56",
      "total_flops_so_far": 3.945574749965325e+16,
      "budget_used_percent": 39.45574749965325
    },
    {
      "type": "training",
      "description": "Training step 1660",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:57",
      "total_flops_so_far": 3.947951173186163e+16,
      "budget_used_percent": 39.479511731861635
    },
    {
      "type": "training",
      "description": "Training step 1661",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:46:59",
      "total_flops_so_far": 3.950327596407002e+16,
      "budget_used_percent": 39.50327596407001
    },
    {
      "type": "training",
      "description": "Training step 1662",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:00",
      "total_flops_so_far": 3.95270401962784e+16,
      "budget_used_percent": 39.5270401962784
    },
    {
      "type": "training",
      "description": "Training step 1663",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:01",
      "total_flops_so_far": 3.955080442848678e+16,
      "budget_used_percent": 39.550804428486785
    },
    {
      "type": "training",
      "description": "Training step 1664",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:03",
      "total_flops_so_far": 3.957456866069517e+16,
      "budget_used_percent": 39.57456866069517
    },
    {
      "type": "training",
      "description": "Training step 1665",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:04",
      "total_flops_so_far": 3.959833289290355e+16,
      "budget_used_percent": 39.59833289290356
    },
    {
      "type": "training",
      "description": "Training step 1666",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:06",
      "total_flops_so_far": 3.962209712511194e+16,
      "budget_used_percent": 39.622097125111935
    },
    {
      "type": "training",
      "description": "Training step 1667",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:07",
      "total_flops_so_far": 3.964586135732032e+16,
      "budget_used_percent": 39.64586135732032
    },
    {
      "type": "training",
      "description": "Training step 1668",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:08",
      "total_flops_so_far": 3.96696255895287e+16,
      "budget_used_percent": 39.66962558952871
    },
    {
      "type": "training",
      "description": "Training step 1669",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:09",
      "total_flops_so_far": 3.969338982173709e+16,
      "budget_used_percent": 39.69338982173709
    },
    {
      "type": "training",
      "description": "Training step 1670",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:11",
      "total_flops_so_far": 3.971715405394547e+16,
      "budget_used_percent": 39.71715405394547
    },
    {
      "type": "training",
      "description": "Training step 1671",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:12",
      "total_flops_so_far": 3.974091828615386e+16,
      "budget_used_percent": 39.74091828615386
    },
    {
      "type": "training",
      "description": "Training step 1672",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:13",
      "total_flops_so_far": 3.976468251836224e+16,
      "budget_used_percent": 39.764682518362235
    },
    {
      "type": "training",
      "description": "Training step 1673",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:15",
      "total_flops_so_far": 3.978844675057062e+16,
      "budget_used_percent": 39.78844675057062
    },
    {
      "type": "training",
      "description": "Training step 1674",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:16",
      "total_flops_so_far": 3.981221098277901e+16,
      "budget_used_percent": 39.81221098277901
    },
    {
      "type": "training",
      "description": "Training step 1675",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:17",
      "total_flops_so_far": 3.983597521498739e+16,
      "budget_used_percent": 39.83597521498739
    },
    {
      "type": "training",
      "description": "Training step 1676",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:19",
      "total_flops_so_far": 3.985973944719578e+16,
      "budget_used_percent": 39.85973944719577
    },
    {
      "type": "training",
      "description": "Training step 1677",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:20",
      "total_flops_so_far": 3.988350367940416e+16,
      "budget_used_percent": 39.88350367940416
    },
    {
      "type": "training",
      "description": "Training step 1678",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:21",
      "total_flops_so_far": 3.990726791161254e+16,
      "budget_used_percent": 39.90726791161254
    },
    {
      "type": "training",
      "description": "Training step 1679",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:23",
      "total_flops_so_far": 3.993103214382093e+16,
      "budget_used_percent": 39.93103214382093
    },
    {
      "type": "training",
      "description": "Training step 1680",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:24",
      "total_flops_so_far": 3.995479637602931e+16,
      "budget_used_percent": 39.954796376029314
    },
    {
      "type": "training",
      "description": "Training step 1681",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:25",
      "total_flops_so_far": 3.99785606082377e+16,
      "budget_used_percent": 39.97856060823769
    },
    {
      "type": "training",
      "description": "Training step 1682",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:27",
      "total_flops_so_far": 4.000232484044608e+16,
      "budget_used_percent": 40.00232484044608
    },
    {
      "type": "training",
      "description": "Training step 1683",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:28",
      "total_flops_so_far": 4.002608907265446e+16,
      "budget_used_percent": 40.026089072654464
    },
    {
      "type": "training",
      "description": "Training step 1684",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:29",
      "total_flops_so_far": 4.004985330486285e+16,
      "budget_used_percent": 40.04985330486285
    },
    {
      "type": "training",
      "description": "Training step 1685",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:31",
      "total_flops_so_far": 4.007361753707123e+16,
      "budget_used_percent": 40.07361753707123
    },
    {
      "type": "training",
      "description": "Training step 1686",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:32",
      "total_flops_so_far": 4.009738176927962e+16,
      "budget_used_percent": 40.097381769279615
    },
    {
      "type": "training",
      "description": "Training step 1687",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:33",
      "total_flops_so_far": 4.0121146001488e+16,
      "budget_used_percent": 40.121146001488
    },
    {
      "type": "training",
      "description": "Training step 1688",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:34",
      "total_flops_so_far": 4.014491023369638e+16,
      "budget_used_percent": 40.144910233696386
    },
    {
      "type": "training",
      "description": "Training step 1689",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:36",
      "total_flops_so_far": 4.016867446590477e+16,
      "budget_used_percent": 40.16867446590477
    },
    {
      "type": "training",
      "description": "Training step 1690",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:37",
      "total_flops_so_far": 4.019243869811315e+16,
      "budget_used_percent": 40.19243869811315
    },
    {
      "type": "training",
      "description": "Training step 1691",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:38",
      "total_flops_so_far": 4.021620293032154e+16,
      "budget_used_percent": 40.216202930321536
    },
    {
      "type": "training",
      "description": "Training step 1692",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:40",
      "total_flops_so_far": 4.023996716252992e+16,
      "budget_used_percent": 40.23996716252992
    },
    {
      "type": "training",
      "description": "Training step 1693",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:41",
      "total_flops_so_far": 4.02637313947383e+16,
      "budget_used_percent": 40.26373139473831
    },
    {
      "type": "training",
      "description": "Training step 1694",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:42",
      "total_flops_so_far": 4.028749562694669e+16,
      "budget_used_percent": 40.28749562694669
    },
    {
      "type": "training",
      "description": "Training step 1695",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:44",
      "total_flops_so_far": 4.031125985915507e+16,
      "budget_used_percent": 40.31125985915507
    },
    {
      "type": "training",
      "description": "Training step 1696",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:45",
      "total_flops_so_far": 4.033502409136346e+16,
      "budget_used_percent": 40.33502409136346
    },
    {
      "type": "training",
      "description": "Training step 1697",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:46",
      "total_flops_so_far": 4.035878832357184e+16,
      "budget_used_percent": 40.358788323571844
    },
    {
      "type": "training",
      "description": "Training step 1698",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:48",
      "total_flops_so_far": 4.038255255578022e+16,
      "budget_used_percent": 40.38255255578023
    },
    {
      "type": "training",
      "description": "Training step 1699",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:49",
      "total_flops_so_far": 4.040631678798861e+16,
      "budget_used_percent": 40.40631678798861
    },
    {
      "type": "training",
      "description": "Training step 1700",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:50",
      "total_flops_so_far": 4.043008102019699e+16,
      "budget_used_percent": 40.43008102019699
    },
    {
      "type": "training",
      "description": "Training step 1701",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:52",
      "total_flops_so_far": 4.045384525240538e+16,
      "budget_used_percent": 40.45384525240537
    },
    {
      "type": "training",
      "description": "Training step 1702",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:53",
      "total_flops_so_far": 4.047760948461376e+16,
      "budget_used_percent": 40.47760948461376
    },
    {
      "type": "training",
      "description": "Training step 1703",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:54",
      "total_flops_so_far": 4.050137371682214e+16,
      "budget_used_percent": 40.501373716822144
    },
    {
      "type": "training",
      "description": "Training step 1704",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:56",
      "total_flops_so_far": 4.052513794903053e+16,
      "budget_used_percent": 40.52513794903053
    },
    {
      "type": "training",
      "description": "Training step 1705",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:57",
      "total_flops_so_far": 4.054890218123891e+16,
      "budget_used_percent": 40.54890218123891
    },
    {
      "type": "training",
      "description": "Training step 1706",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:47:58",
      "total_flops_so_far": 4.05726664134473e+16,
      "budget_used_percent": 40.572666413447294
    },
    {
      "type": "training",
      "description": "Training step 1707",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:00",
      "total_flops_so_far": 4.059643064565568e+16,
      "budget_used_percent": 40.59643064565568
    },
    {
      "type": "training",
      "description": "Training step 1708",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:01",
      "total_flops_so_far": 4.062019487786406e+16,
      "budget_used_percent": 40.620194877864066
    },
    {
      "type": "training",
      "description": "Training step 1709",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:02",
      "total_flops_so_far": 4.064395911007245e+16,
      "budget_used_percent": 40.643959110072444
    },
    {
      "type": "training",
      "description": "Training step 1710",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:03",
      "total_flops_so_far": 4.066772334228083e+16,
      "budget_used_percent": 40.66772334228083
    },
    {
      "type": "training",
      "description": "Training step 1711",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:05",
      "total_flops_so_far": 4.069148757448922e+16,
      "budget_used_percent": 40.691487574489216
    },
    {
      "type": "training",
      "description": "Training step 1712",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:06",
      "total_flops_so_far": 4.07152518066976e+16,
      "budget_used_percent": 40.7152518066976
    },
    {
      "type": "training",
      "description": "Training step 1713",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:07",
      "total_flops_so_far": 4.073901603890598e+16,
      "budget_used_percent": 40.73901603890599
    },
    {
      "type": "training",
      "description": "Training step 1714",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:09",
      "total_flops_so_far": 4.076278027111437e+16,
      "budget_used_percent": 40.762780271114366
    },
    {
      "type": "training",
      "description": "Training step 1715",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:10",
      "total_flops_so_far": 4.078654450332275e+16,
      "budget_used_percent": 40.78654450332275
    },
    {
      "type": "training",
      "description": "Training step 1716",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:10",
      "total_flops_so_far": 4.081030873553114e+16,
      "budget_used_percent": 40.81030873553114
    },
    {
      "type": "training",
      "description": "Training step 1717",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:12",
      "total_flops_so_far": 4.083407296773952e+16,
      "budget_used_percent": 40.83407296773952
    },
    {
      "type": "training",
      "description": "Training step 1718",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:13",
      "total_flops_so_far": 4.08578371999479e+16,
      "budget_used_percent": 40.8578371999479
    },
    {
      "type": "training",
      "description": "Training step 1719",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:14",
      "total_flops_so_far": 4.088160143215629e+16,
      "budget_used_percent": 40.88160143215629
    },
    {
      "type": "training",
      "description": "Training step 1720",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:16",
      "total_flops_so_far": 4.090536566436467e+16,
      "budget_used_percent": 40.90536566436467
    },
    {
      "type": "training",
      "description": "Training step 1721",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:17",
      "total_flops_so_far": 4.092912989657306e+16,
      "budget_used_percent": 40.92912989657306
    },
    {
      "type": "training",
      "description": "Training step 1722",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:18",
      "total_flops_so_far": 4.095289412878144e+16,
      "budget_used_percent": 40.952894128781445
    },
    {
      "type": "training",
      "description": "Training step 1723",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:20",
      "total_flops_so_far": 4.097665836098982e+16,
      "budget_used_percent": 40.97665836098982
    },
    {
      "type": "training",
      "description": "Training step 1724",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:21",
      "total_flops_so_far": 4.100042259319821e+16,
      "budget_used_percent": 41.00042259319821
    },
    {
      "type": "training",
      "description": "Training step 1725",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:22",
      "total_flops_so_far": 4.102418682540659e+16,
      "budget_used_percent": 41.024186825406595
    },
    {
      "type": "training",
      "description": "Training step 1726",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:24",
      "total_flops_so_far": 4.104795105761498e+16,
      "budget_used_percent": 41.04795105761498
    },
    {
      "type": "training",
      "description": "Training step 1727",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:25",
      "total_flops_so_far": 4.107171528982336e+16,
      "budget_used_percent": 41.07171528982336
    },
    {
      "type": "training",
      "description": "Training step 1728",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:26",
      "total_flops_so_far": 4.109547952203174e+16,
      "budget_used_percent": 41.09547952203174
    },
    {
      "type": "training",
      "description": "Training step 1729",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:28",
      "total_flops_so_far": 4.111924375424013e+16,
      "budget_used_percent": 41.119243754240124
    },
    {
      "type": "training",
      "description": "Training step 1730",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:29",
      "total_flops_so_far": 4.114300798644851e+16,
      "budget_used_percent": 41.14300798644851
    },
    {
      "type": "training",
      "description": "Training step 1731",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:30",
      "total_flops_so_far": 4.11667722186569e+16,
      "budget_used_percent": 41.166772218656895
    },
    {
      "type": "training",
      "description": "Training step 1732",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:32",
      "total_flops_so_far": 4.119053645086528e+16,
      "budget_used_percent": 41.19053645086528
    },
    {
      "type": "training",
      "description": "Training step 1733",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:33",
      "total_flops_so_far": 4.121430068307366e+16,
      "budget_used_percent": 41.21430068307366
    },
    {
      "type": "training",
      "description": "Training step 1734",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:34",
      "total_flops_so_far": 4.123806491528205e+16,
      "budget_used_percent": 41.238064915282045
    },
    {
      "type": "training",
      "description": "Training step 1735",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:35",
      "total_flops_so_far": 4.126182914749043e+16,
      "budget_used_percent": 41.26182914749043
    },
    {
      "type": "training",
      "description": "Training step 1736",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:37",
      "total_flops_so_far": 4.128559337969882e+16,
      "budget_used_percent": 41.28559337969882
    },
    {
      "type": "training",
      "description": "Training step 1737",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:38",
      "total_flops_so_far": 4.13093576119072e+16,
      "budget_used_percent": 41.3093576119072
    },
    {
      "type": "training",
      "description": "Training step 1738",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:39",
      "total_flops_so_far": 4.133312184411558e+16,
      "budget_used_percent": 41.33312184411558
    },
    {
      "type": "training",
      "description": "Training step 1739",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:41",
      "total_flops_so_far": 4.135688607632397e+16,
      "budget_used_percent": 41.35688607632397
    },
    {
      "type": "training",
      "description": "Training step 1740",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:42",
      "total_flops_so_far": 4.138065030853235e+16,
      "budget_used_percent": 41.38065030853235
    },
    {
      "type": "training",
      "description": "Training step 1741",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:43",
      "total_flops_so_far": 4.140441454074074e+16,
      "budget_used_percent": 41.40441454074074
    },
    {
      "type": "training",
      "description": "Training step 1742",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:45",
      "total_flops_so_far": 4.142817877294912e+16,
      "budget_used_percent": 41.42817877294912
    },
    {
      "type": "training",
      "description": "Training step 1743",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:46",
      "total_flops_so_far": 4.14519430051575e+16,
      "budget_used_percent": 41.4519430051575
    },
    {
      "type": "training",
      "description": "Training step 1744",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:47",
      "total_flops_so_far": 4.147570723736589e+16,
      "budget_used_percent": 41.47570723736589
    },
    {
      "type": "training",
      "description": "Training step 1745",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:49",
      "total_flops_so_far": 4.149947146957427e+16,
      "budget_used_percent": 41.499471469574274
    },
    {
      "type": "training",
      "description": "Training step 1746",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:50",
      "total_flops_so_far": 4.152323570178266e+16,
      "budget_used_percent": 41.52323570178266
    },
    {
      "type": "training",
      "description": "Training step 1747",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:51",
      "total_flops_so_far": 4.154699993399104e+16,
      "budget_used_percent": 41.54699993399104
    },
    {
      "type": "training",
      "description": "Training step 1748",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:53",
      "total_flops_so_far": 4.157076416619942e+16,
      "budget_used_percent": 41.570764166199424
    },
    {
      "type": "training",
      "description": "Training step 1749",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:54",
      "total_flops_so_far": 4.159452839840781e+16,
      "budget_used_percent": 41.59452839840781
    },
    {
      "type": "training",
      "description": "Training step 1750",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:55",
      "total_flops_so_far": 4.161829263061619e+16,
      "budget_used_percent": 41.618292630616196
    },
    {
      "type": "training",
      "description": "Training step 1751",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:57",
      "total_flops_so_far": 4.164205686282458e+16,
      "budget_used_percent": 41.642056862824575
    },
    {
      "type": "training",
      "description": "Training step 1752",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:58",
      "total_flops_so_far": 4.166582109503296e+16,
      "budget_used_percent": 41.66582109503296
    },
    {
      "type": "training",
      "description": "Training step 1753",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:48:59",
      "total_flops_so_far": 4.168958532724134e+16,
      "budget_used_percent": 41.689585327241346
    },
    {
      "type": "training",
      "description": "Training step 1754",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:01",
      "total_flops_so_far": 4.171334955944973e+16,
      "budget_used_percent": 41.71334955944973
    },
    {
      "type": "training",
      "description": "Training step 1755",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:02",
      "total_flops_so_far": 4.173711379165811e+16,
      "budget_used_percent": 41.73711379165812
    },
    {
      "type": "training",
      "description": "Training step 1756",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:03",
      "total_flops_so_far": 4.17608780238665e+16,
      "budget_used_percent": 41.760878023866496
    },
    {
      "type": "training",
      "description": "Training step 1757",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:04",
      "total_flops_so_far": 4.178464225607488e+16,
      "budget_used_percent": 41.784642256074875
    },
    {
      "type": "training",
      "description": "Training step 1758",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:06",
      "total_flops_so_far": 4.180840648828326e+16,
      "budget_used_percent": 41.80840648828326
    },
    {
      "type": "training",
      "description": "Training step 1759",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:07",
      "total_flops_so_far": 4.183217072049165e+16,
      "budget_used_percent": 41.832170720491646
    },
    {
      "type": "training",
      "description": "Training step 1760",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:08",
      "total_flops_so_far": 4.185593495270003e+16,
      "budget_used_percent": 41.85593495270003
    },
    {
      "type": "training",
      "description": "Training step 1761",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:10",
      "total_flops_so_far": 4.187969918490842e+16,
      "budget_used_percent": 41.87969918490842
    },
    {
      "type": "training",
      "description": "Training step 1762",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:11",
      "total_flops_so_far": 4.19034634171168e+16,
      "budget_used_percent": 41.9034634171168
    },
    {
      "type": "training",
      "description": "Training step 1763",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:12",
      "total_flops_so_far": 4.192722764932518e+16,
      "budget_used_percent": 41.92722764932518
    },
    {
      "type": "training",
      "description": "Training step 1764",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:14",
      "total_flops_so_far": 4.195099188153357e+16,
      "budget_used_percent": 41.95099188153357
    },
    {
      "type": "training",
      "description": "Training step 1765",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:15",
      "total_flops_so_far": 4.197475611374195e+16,
      "budget_used_percent": 41.974756113741954
    },
    {
      "type": "training",
      "description": "Training step 1766",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:16",
      "total_flops_so_far": 4.199852034595034e+16,
      "budget_used_percent": 41.99852034595033
    },
    {
      "type": "training",
      "description": "Training step 1767",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:18",
      "total_flops_so_far": 4.202228457815872e+16,
      "budget_used_percent": 42.02228457815872
    },
    {
      "type": "training",
      "description": "Training step 1768",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:19",
      "total_flops_so_far": 4.20460488103671e+16,
      "budget_used_percent": 42.046048810367104
    },
    {
      "type": "training",
      "description": "Training step 1769",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:20",
      "total_flops_so_far": 4.206981304257549e+16,
      "budget_used_percent": 42.06981304257549
    },
    {
      "type": "training",
      "description": "Training step 1770",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:22",
      "total_flops_so_far": 4.209357727478387e+16,
      "budget_used_percent": 42.093577274783875
    },
    {
      "type": "training",
      "description": "Training step 1771",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:23",
      "total_flops_so_far": 4.211734150699226e+16,
      "budget_used_percent": 42.117341506992254
    },
    {
      "type": "training",
      "description": "Training step 1772",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:24",
      "total_flops_so_far": 4.214110573920064e+16,
      "budget_used_percent": 42.14110573920064
    },
    {
      "type": "training",
      "description": "Training step 1773",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:26",
      "total_flops_so_far": 4.216486997140902e+16,
      "budget_used_percent": 42.164869971409026
    },
    {
      "type": "training",
      "description": "Training step 1774",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:27",
      "total_flops_so_far": 4.218863420361741e+16,
      "budget_used_percent": 42.18863420361741
    },
    {
      "type": "training",
      "description": "Training step 1775",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:28",
      "total_flops_so_far": 4.221239843582579e+16,
      "budget_used_percent": 42.21239843582579
    },
    {
      "type": "training",
      "description": "Training step 1776",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:30",
      "total_flops_so_far": 4.223616266803418e+16,
      "budget_used_percent": 42.236162668034176
    },
    {
      "type": "training",
      "description": "Training step 1777",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:31",
      "total_flops_so_far": 4.225992690024256e+16,
      "budget_used_percent": 42.25992690024256
    },
    {
      "type": "training",
      "description": "Training step 1778",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:32",
      "total_flops_so_far": 4.228369113245094e+16,
      "budget_used_percent": 42.28369113245095
    },
    {
      "type": "training",
      "description": "Training step 1779",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:34",
      "total_flops_so_far": 4.230745536465933e+16,
      "budget_used_percent": 42.30745536465933
    },
    {
      "type": "training",
      "description": "Training step 1780",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:35",
      "total_flops_so_far": 4.233121959686771e+16,
      "budget_used_percent": 42.33121959686771
    },
    {
      "type": "training",
      "description": "Training step 1781",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:36",
      "total_flops_so_far": 4.23549838290761e+16,
      "budget_used_percent": 42.3549838290761
    },
    {
      "type": "training",
      "description": "Training step 1782",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:37",
      "total_flops_so_far": 4.237874806128448e+16,
      "budget_used_percent": 42.37874806128448
    },
    {
      "type": "training",
      "description": "Training step 1783",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:39",
      "total_flops_so_far": 4.240251229349286e+16,
      "budget_used_percent": 42.40251229349287
    },
    {
      "type": "training",
      "description": "Training step 1784",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:40",
      "total_flops_so_far": 4.242627652570125e+16,
      "budget_used_percent": 42.42627652570125
    },
    {
      "type": "training",
      "description": "Training step 1785",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:41",
      "total_flops_so_far": 4.245004075790963e+16,
      "budget_used_percent": 42.450040757909626
    },
    {
      "type": "training",
      "description": "Training step 1786",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:43",
      "total_flops_so_far": 4.247380499011802e+16,
      "budget_used_percent": 42.47380499011801
    },
    {
      "type": "training",
      "description": "Training step 1787",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:44",
      "total_flops_so_far": 4.24975692223264e+16,
      "budget_used_percent": 42.4975692223264
    },
    {
      "type": "training",
      "description": "Training step 1788",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:45",
      "total_flops_so_far": 4.252133345453478e+16,
      "budget_used_percent": 42.52133345453478
    },
    {
      "type": "training",
      "description": "Training step 1789",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:47",
      "total_flops_so_far": 4.254509768674317e+16,
      "budget_used_percent": 42.54509768674317
    },
    {
      "type": "training",
      "description": "Training step 1790",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:48",
      "total_flops_so_far": 4.256886191895155e+16,
      "budget_used_percent": 42.56886191895155
    },
    {
      "type": "training",
      "description": "Training step 1791",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:49",
      "total_flops_so_far": 4.259262615115994e+16,
      "budget_used_percent": 42.592626151159934
    },
    {
      "type": "training",
      "description": "Training step 1792",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:51",
      "total_flops_so_far": 4.261639038336832e+16,
      "budget_used_percent": 42.61639038336832
    },
    {
      "type": "training",
      "description": "Training step 1793",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:52",
      "total_flops_so_far": 4.26401546155767e+16,
      "budget_used_percent": 42.640154615576705
    },
    {
      "type": "training",
      "description": "Training step 1794",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:53",
      "total_flops_so_far": 4.266391884778509e+16,
      "budget_used_percent": 42.663918847785084
    },
    {
      "type": "training",
      "description": "Training step 1795",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:55",
      "total_flops_so_far": 4.268768307999347e+16,
      "budget_used_percent": 42.68768307999347
    },
    {
      "type": "training",
      "description": "Training step 1796",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:56",
      "total_flops_so_far": 4.271144731220186e+16,
      "budget_used_percent": 42.711447312201855
    },
    {
      "type": "training",
      "description": "Training step 1797",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:57",
      "total_flops_so_far": 4.273521154441024e+16,
      "budget_used_percent": 42.73521154441024
    },
    {
      "type": "training",
      "description": "Training step 1798",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:49:59",
      "total_flops_so_far": 4.275897577661862e+16,
      "budget_used_percent": 42.75897577661863
    },
    {
      "type": "training",
      "description": "Training step 1799",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:00",
      "total_flops_so_far": 4.278274000882701e+16,
      "budget_used_percent": 42.782740008827005
    },
    {
      "type": "training",
      "description": "Training step 1800",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:01",
      "total_flops_so_far": 4.280650424103539e+16,
      "budget_used_percent": 42.80650424103539
    },
    {
      "type": "training",
      "description": "Training step 1801",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:03",
      "total_flops_so_far": 4.283026847324378e+16,
      "budget_used_percent": 42.83026847324378
    },
    {
      "type": "training",
      "description": "Training step 1802",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:04",
      "total_flops_so_far": 4.285403270545216e+16,
      "budget_used_percent": 42.85403270545216
    },
    {
      "type": "training",
      "description": "Training step 1803",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:06",
      "total_flops_so_far": 4.287779693766054e+16,
      "budget_used_percent": 42.87779693766055
    },
    {
      "type": "training",
      "description": "Training step 1804",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:08",
      "total_flops_so_far": 4.290156116986893e+16,
      "budget_used_percent": 42.90156116986893
    },
    {
      "type": "training",
      "description": "Training step 1805",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:09",
      "total_flops_so_far": 4.292532540207731e+16,
      "budget_used_percent": 42.92532540207731
    },
    {
      "type": "training",
      "description": "Training step 1806",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:10",
      "total_flops_so_far": 4.29490896342857e+16,
      "budget_used_percent": 42.9490896342857
    },
    {
      "type": "training",
      "description": "Training step 1807",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:11",
      "total_flops_so_far": 4.297285386649408e+16,
      "budget_used_percent": 42.972853866494084
    },
    {
      "type": "training",
      "description": "Training step 1808",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:13",
      "total_flops_so_far": 4.299661809870246e+16,
      "budget_used_percent": 42.99661809870246
    },
    {
      "type": "training",
      "description": "Training step 1809",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:14",
      "total_flops_so_far": 4.302038233091085e+16,
      "budget_used_percent": 43.02038233091085
    },
    {
      "type": "training",
      "description": "Training step 1810",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:15",
      "total_flops_so_far": 4.304414656311923e+16,
      "budget_used_percent": 43.044146563119234
    },
    {
      "type": "training",
      "description": "Training step 1811",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:17",
      "total_flops_so_far": 4.306791079532762e+16,
      "budget_used_percent": 43.06791079532762
    },
    {
      "type": "training",
      "description": "Training step 1812",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:18",
      "total_flops_so_far": 4.3091675027536e+16,
      "budget_used_percent": 43.091675027536006
    },
    {
      "type": "training",
      "description": "Training step 1813",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:19",
      "total_flops_so_far": 4.311543925974438e+16,
      "budget_used_percent": 43.115439259744385
    },
    {
      "type": "training",
      "description": "Training step 1814",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:21",
      "total_flops_so_far": 4.313920349195277e+16,
      "budget_used_percent": 43.13920349195276
    },
    {
      "type": "training",
      "description": "Training step 1815",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:22",
      "total_flops_so_far": 4.316296772416115e+16,
      "budget_used_percent": 43.16296772416115
    },
    {
      "type": "training",
      "description": "Training step 1816",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:23",
      "total_flops_so_far": 4.318673195636954e+16,
      "budget_used_percent": 43.186731956369535
    },
    {
      "type": "training",
      "description": "Training step 1817",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:25",
      "total_flops_so_far": 4.321049618857792e+16,
      "budget_used_percent": 43.21049618857792
    },
    {
      "type": "training",
      "description": "Training step 1818",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:26",
      "total_flops_so_far": 4.32342604207863e+16,
      "budget_used_percent": 43.2342604207863
    },
    {
      "type": "training",
      "description": "Training step 1819",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:27",
      "total_flops_so_far": 4.325802465299469e+16,
      "budget_used_percent": 43.258024652994685
    },
    {
      "type": "training",
      "description": "Training step 1820",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:29",
      "total_flops_so_far": 4.328178888520307e+16,
      "budget_used_percent": 43.28178888520307
    },
    {
      "type": "training",
      "description": "Training step 1821",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:30",
      "total_flops_so_far": 4.330555311741146e+16,
      "budget_used_percent": 43.305553117411456
    },
    {
      "type": "training",
      "description": "Training step 1822",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:31",
      "total_flops_so_far": 4.332931734961984e+16,
      "budget_used_percent": 43.32931734961984
    },
    {
      "type": "training",
      "description": "Training step 1823",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:33",
      "total_flops_so_far": 4.335308158182822e+16,
      "budget_used_percent": 43.35308158182822
    },
    {
      "type": "training",
      "description": "Training step 1824",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:34",
      "total_flops_so_far": 4.337684581403661e+16,
      "budget_used_percent": 43.376845814036606
    },
    {
      "type": "training",
      "description": "Training step 1825",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:35",
      "total_flops_so_far": 4.340061004624499e+16,
      "budget_used_percent": 43.40061004624499
    },
    {
      "type": "training",
      "description": "Training step 1826",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:37",
      "total_flops_so_far": 4.342437427845338e+16,
      "budget_used_percent": 43.42437427845338
    },
    {
      "type": "training",
      "description": "Training step 1827",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:38",
      "total_flops_so_far": 4.344813851066176e+16,
      "budget_used_percent": 43.448138510661764
    },
    {
      "type": "training",
      "description": "Training step 1828",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:39",
      "total_flops_so_far": 4.347190274287014e+16,
      "budget_used_percent": 43.47190274287014
    },
    {
      "type": "training",
      "description": "Training step 1829",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:41",
      "total_flops_so_far": 4.349566697507853e+16,
      "budget_used_percent": 43.49566697507853
    },
    {
      "type": "training",
      "description": "Training step 1830",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:42",
      "total_flops_so_far": 4.351943120728691e+16,
      "budget_used_percent": 43.519431207286914
    },
    {
      "type": "training",
      "description": "Training step 1831",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:43",
      "total_flops_so_far": 4.35431954394953e+16,
      "budget_used_percent": 43.5431954394953
    },
    {
      "type": "training",
      "description": "Training step 1832",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:45",
      "total_flops_so_far": 4.356695967170368e+16,
      "budget_used_percent": 43.56695967170368
    },
    {
      "type": "training",
      "description": "Training step 1833",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:46",
      "total_flops_so_far": 4.359072390391206e+16,
      "budget_used_percent": 43.590723903912064
    },
    {
      "type": "training",
      "description": "Training step 1834",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:47",
      "total_flops_so_far": 4.361448813612045e+16,
      "budget_used_percent": 43.61448813612045
    },
    {
      "type": "training",
      "description": "Training step 1835",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:48",
      "total_flops_so_far": 4.363825236832883e+16,
      "budget_used_percent": 43.638252368328835
    },
    {
      "type": "training",
      "description": "Training step 1836",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:50",
      "total_flops_so_far": 4.366201660053722e+16,
      "budget_used_percent": 43.66201660053722
    },
    {
      "type": "training",
      "description": "Training step 1837",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:51",
      "total_flops_so_far": 4.36857808327456e+16,
      "budget_used_percent": 43.6857808327456
    },
    {
      "type": "training",
      "description": "Training step 1838",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:52",
      "total_flops_so_far": 4.370954506495398e+16,
      "budget_used_percent": 43.709545064953986
    },
    {
      "type": "training",
      "description": "Training step 1839",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:54",
      "total_flops_so_far": 4.373330929716237e+16,
      "budget_used_percent": 43.73330929716237
    },
    {
      "type": "training",
      "description": "Training step 1840",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:55",
      "total_flops_so_far": 4.375707352937075e+16,
      "budget_used_percent": 43.75707352937076
    },
    {
      "type": "training",
      "description": "Training step 1841",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:56",
      "total_flops_so_far": 4.378083776157914e+16,
      "budget_used_percent": 43.780837761579136
    },
    {
      "type": "training",
      "description": "Training step 1842",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:58",
      "total_flops_so_far": 4.380460199378752e+16,
      "budget_used_percent": 43.804601993787514
    },
    {
      "type": "training",
      "description": "Training step 1843",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:50:59",
      "total_flops_so_far": 4.38283662259959e+16,
      "budget_used_percent": 43.8283662259959
    },
    {
      "type": "training",
      "description": "Training step 1844",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:00",
      "total_flops_so_far": 4.385213045820429e+16,
      "budget_used_percent": 43.852130458204286
    },
    {
      "type": "training",
      "description": "Training step 1845",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:02",
      "total_flops_so_far": 4.387589469041267e+16,
      "budget_used_percent": 43.87589469041267
    },
    {
      "type": "training",
      "description": "Training step 1846",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:03",
      "total_flops_so_far": 4.389965892262106e+16,
      "budget_used_percent": 43.89965892262106
    },
    {
      "type": "training",
      "description": "Training step 1847",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:04",
      "total_flops_so_far": 4.392342315482944e+16,
      "budget_used_percent": 43.923423154829436
    },
    {
      "type": "training",
      "description": "Training step 1848",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:05",
      "total_flops_so_far": 4.394718738703782e+16,
      "budget_used_percent": 43.94718738703782
    },
    {
      "type": "training",
      "description": "Training step 1849",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:06",
      "total_flops_so_far": 4.397095161924621e+16,
      "budget_used_percent": 43.97095161924621
    },
    {
      "type": "training",
      "description": "Training step 1850",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:07",
      "total_flops_so_far": 4.399471585145459e+16,
      "budget_used_percent": 43.99471585145459
    },
    {
      "type": "training",
      "description": "Training step 1851",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:09",
      "total_flops_so_far": 4.401848008366298e+16,
      "budget_used_percent": 44.01848008366297
    },
    {
      "type": "training",
      "description": "Training step 1852",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:10",
      "total_flops_so_far": 4.404224431587136e+16,
      "budget_used_percent": 44.04224431587136
    },
    {
      "type": "training",
      "description": "Training step 1853",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:11",
      "total_flops_so_far": 4.406600854807974e+16,
      "budget_used_percent": 44.06600854807974
    },
    {
      "type": "training",
      "description": "Training step 1854",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:13",
      "total_flops_so_far": 4.408977278028813e+16,
      "budget_used_percent": 44.08977278028813
    },
    {
      "type": "training",
      "description": "Training step 1855",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:14",
      "total_flops_so_far": 4.411353701249651e+16,
      "budget_used_percent": 44.113537012496515
    },
    {
      "type": "training",
      "description": "Training step 1856",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:15",
      "total_flops_so_far": 4.41373012447049e+16,
      "budget_used_percent": 44.137301244704894
    },
    {
      "type": "training",
      "description": "Training step 1857",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:17",
      "total_flops_so_far": 4.416106547691328e+16,
      "budget_used_percent": 44.16106547691328
    },
    {
      "type": "training",
      "description": "Training step 1858",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:18",
      "total_flops_so_far": 4.418482970912166e+16,
      "budget_used_percent": 44.184829709121665
    },
    {
      "type": "training",
      "description": "Training step 1859",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:19",
      "total_flops_so_far": 4.420859394133005e+16,
      "budget_used_percent": 44.20859394133005
    },
    {
      "type": "training",
      "description": "Training step 1860",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:21",
      "total_flops_so_far": 4.423235817353843e+16,
      "budget_used_percent": 44.23235817353843
    },
    {
      "type": "training",
      "description": "Training step 1861",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:22",
      "total_flops_so_far": 4.425612240574682e+16,
      "budget_used_percent": 44.256122405746815
    },
    {
      "type": "training",
      "description": "Training step 1862",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:23",
      "total_flops_so_far": 4.42798866379552e+16,
      "budget_used_percent": 44.2798866379552
    },
    {
      "type": "training",
      "description": "Training step 1863",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:25",
      "total_flops_so_far": 4.430365087016358e+16,
      "budget_used_percent": 44.30365087016359
    },
    {
      "type": "training",
      "description": "Training step 1864",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:26",
      "total_flops_so_far": 4.432741510237197e+16,
      "budget_used_percent": 44.32741510237197
    },
    {
      "type": "training",
      "description": "Training step 1865",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:27",
      "total_flops_so_far": 4.435117933458035e+16,
      "budget_used_percent": 44.35117933458035
    },
    {
      "type": "training",
      "description": "Training step 1866",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:29",
      "total_flops_so_far": 4.437494356678874e+16,
      "budget_used_percent": 44.37494356678874
    },
    {
      "type": "training",
      "description": "Training step 1867",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:30",
      "total_flops_so_far": 4.439870779899712e+16,
      "budget_used_percent": 44.39870779899712
    },
    {
      "type": "training",
      "description": "Training step 1868",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:31",
      "total_flops_so_far": 4.44224720312055e+16,
      "budget_used_percent": 44.42247203120551
    },
    {
      "type": "training",
      "description": "Training step 1869",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:33",
      "total_flops_so_far": 4.444623626341389e+16,
      "budget_used_percent": 44.44623626341389
    },
    {
      "type": "training",
      "description": "Training step 1870",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:34",
      "total_flops_so_far": 4.447000049562227e+16,
      "budget_used_percent": 44.47000049562227
    },
    {
      "type": "training",
      "description": "Training step 1871",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:35",
      "total_flops_so_far": 4.449376472783066e+16,
      "budget_used_percent": 44.49376472783065
    },
    {
      "type": "training",
      "description": "Training step 1872",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:36",
      "total_flops_so_far": 4.451752896003904e+16,
      "budget_used_percent": 44.51752896003904
    },
    {
      "type": "training",
      "description": "Training step 1873",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:38",
      "total_flops_so_far": 4.454129319224742e+16,
      "budget_used_percent": 44.54129319224742
    },
    {
      "type": "training",
      "description": "Training step 1874",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:39",
      "total_flops_so_far": 4.456505742445581e+16,
      "budget_used_percent": 44.56505742445581
    },
    {
      "type": "training",
      "description": "Training step 1875",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:40",
      "total_flops_so_far": 4.458882165666419e+16,
      "budget_used_percent": 44.58882165666419
    },
    {
      "type": "training",
      "description": "Training step 1876",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:42",
      "total_flops_so_far": 4.461258588887258e+16,
      "budget_used_percent": 44.61258588887257
    },
    {
      "type": "training",
      "description": "Training step 1877",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:43",
      "total_flops_so_far": 4.463635012108096e+16,
      "budget_used_percent": 44.63635012108096
    },
    {
      "type": "training",
      "description": "Training step 1878",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:44",
      "total_flops_so_far": 4.466011435328934e+16,
      "budget_used_percent": 44.660114353289345
    },
    {
      "type": "training",
      "description": "Training step 1879",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:46",
      "total_flops_so_far": 4.468387858549773e+16,
      "budget_used_percent": 44.68387858549773
    },
    {
      "type": "training",
      "description": "Training step 1880",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:47",
      "total_flops_so_far": 4.470764281770611e+16,
      "budget_used_percent": 44.70764281770611
    },
    {
      "type": "training",
      "description": "Training step 1881",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:48",
      "total_flops_so_far": 4.47314070499145e+16,
      "budget_used_percent": 44.731407049914495
    },
    {
      "type": "training",
      "description": "Training step 1882",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:50",
      "total_flops_so_far": 4.475517128212288e+16,
      "budget_used_percent": 44.75517128212288
    },
    {
      "type": "training",
      "description": "Training step 1883",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:51",
      "total_flops_so_far": 4.477893551433126e+16,
      "budget_used_percent": 44.778935514331266
    },
    {
      "type": "training",
      "description": "Training step 1884",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:52",
      "total_flops_so_far": 4.480269974653965e+16,
      "budget_used_percent": 44.802699746539645
    },
    {
      "type": "training",
      "description": "Training step 1885",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:54",
      "total_flops_so_far": 4.482646397874803e+16,
      "budget_used_percent": 44.82646397874803
    },
    {
      "type": "training",
      "description": "Training step 1886",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:55",
      "total_flops_so_far": 4.485022821095642e+16,
      "budget_used_percent": 44.850228210956416
    },
    {
      "type": "training",
      "description": "Training step 1887",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:56",
      "total_flops_so_far": 4.48739924431648e+16,
      "budget_used_percent": 44.8739924431648
    },
    {
      "type": "training",
      "description": "Training step 1888",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:58",
      "total_flops_so_far": 4.489775667537318e+16,
      "budget_used_percent": 44.89775667537319
    },
    {
      "type": "training",
      "description": "Training step 1889",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:51:59",
      "total_flops_so_far": 4.492152090758157e+16,
      "budget_used_percent": 44.92152090758157
    },
    {
      "type": "training",
      "description": "Training step 1890",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:00",
      "total_flops_so_far": 4.494528513978995e+16,
      "budget_used_percent": 44.94528513978995
    },
    {
      "type": "training",
      "description": "Training step 1891",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:02",
      "total_flops_so_far": 4.496904937199834e+16,
      "budget_used_percent": 44.96904937199834
    },
    {
      "type": "training",
      "description": "Training step 1892",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:03",
      "total_flops_so_far": 4.499281360420672e+16,
      "budget_used_percent": 44.992813604206724
    },
    {
      "type": "training",
      "description": "Training step 1893",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:04",
      "total_flops_so_far": 4.50165778364151e+16,
      "budget_used_percent": 45.01657783641511
    },
    {
      "type": "training",
      "description": "Training step 1894",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:06",
      "total_flops_so_far": 4.504034206862349e+16,
      "budget_used_percent": 45.04034206862349
    },
    {
      "type": "training",
      "description": "Training step 1895",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:07",
      "total_flops_so_far": 4.506410630083187e+16,
      "budget_used_percent": 45.064106300831874
    },
    {
      "type": "training",
      "description": "Training step 1896",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:08",
      "total_flops_so_far": 4.508787053304026e+16,
      "budget_used_percent": 45.08787053304026
    },
    {
      "type": "training",
      "description": "Training step 1897",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:10",
      "total_flops_so_far": 4.511163476524864e+16,
      "budget_used_percent": 45.111634765248645
    },
    {
      "type": "training",
      "description": "Training step 1898",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:11",
      "total_flops_so_far": 4.513539899745702e+16,
      "budget_used_percent": 45.135398997457024
    },
    {
      "type": "training",
      "description": "Training step 1899",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:12",
      "total_flops_so_far": 4.515916322966541e+16,
      "budget_used_percent": 45.1591632296654
    },
    {
      "type": "training",
      "description": "Training step 1900",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:13",
      "total_flops_so_far": 4.518292746187379e+16,
      "budget_used_percent": 45.18292746187379
    },
    {
      "type": "training",
      "description": "Training step 1901",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:15",
      "total_flops_so_far": 4.520669169408218e+16,
      "budget_used_percent": 45.206691694082174
    },
    {
      "type": "training",
      "description": "Training step 1902",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:16",
      "total_flops_so_far": 4.523045592629056e+16,
      "budget_used_percent": 45.23045592629056
    },
    {
      "type": "training",
      "description": "Training step 1903",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:17",
      "total_flops_so_far": 4.525422015849894e+16,
      "budget_used_percent": 45.254220158498946
    },
    {
      "type": "training",
      "description": "Training step 1904",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:19",
      "total_flops_so_far": 4.527798439070733e+16,
      "budget_used_percent": 45.277984390707324
    },
    {
      "type": "training",
      "description": "Training step 1905",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:20",
      "total_flops_so_far": 4.530174862291571e+16,
      "budget_used_percent": 45.30174862291571
    },
    {
      "type": "training",
      "description": "Training step 1906",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:21",
      "total_flops_so_far": 4.53255128551241e+16,
      "budget_used_percent": 45.325512855124096
    },
    {
      "type": "training",
      "description": "Training step 1907",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:23",
      "total_flops_so_far": 4.534927708733248e+16,
      "budget_used_percent": 45.34927708733248
    },
    {
      "type": "training",
      "description": "Training step 1908",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:24",
      "total_flops_so_far": 4.537304131954086e+16,
      "budget_used_percent": 45.37304131954086
    },
    {
      "type": "training",
      "description": "Training step 1909",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:25",
      "total_flops_so_far": 4.539680555174925e+16,
      "budget_used_percent": 45.396805551749246
    },
    {
      "type": "training",
      "description": "Training step 1910",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:27",
      "total_flops_so_far": 4.542056978395763e+16,
      "budget_used_percent": 45.42056978395763
    },
    {
      "type": "training",
      "description": "Training step 1911",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:28",
      "total_flops_so_far": 4.544433401616602e+16,
      "budget_used_percent": 45.44433401616602
    },
    {
      "type": "training",
      "description": "Training step 1912",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:29",
      "total_flops_so_far": 4.54680982483744e+16,
      "budget_used_percent": 45.4680982483744
    },
    {
      "type": "training",
      "description": "Training step 1913",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:31",
      "total_flops_so_far": 4.549186248058278e+16,
      "budget_used_percent": 45.49186248058278
    },
    {
      "type": "training",
      "description": "Training step 1914",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:32",
      "total_flops_so_far": 4.551562671279117e+16,
      "budget_used_percent": 45.51562671279117
    },
    {
      "type": "training",
      "description": "Training step 1915",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:33",
      "total_flops_so_far": 4.553939094499955e+16,
      "budget_used_percent": 45.53939094499955
    },
    {
      "type": "training",
      "description": "Training step 1916",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:35",
      "total_flops_so_far": 4.556315517720794e+16,
      "budget_used_percent": 45.56315517720794
    },
    {
      "type": "training",
      "description": "Training step 1917",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:36",
      "total_flops_so_far": 4.558691940941632e+16,
      "budget_used_percent": 45.58691940941632
    },
    {
      "type": "training",
      "description": "Training step 1918",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:37",
      "total_flops_so_far": 4.56106836416247e+16,
      "budget_used_percent": 45.6106836416247
    },
    {
      "type": "training",
      "description": "Training step 1919",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:39",
      "total_flops_so_far": 4.563444787383309e+16,
      "budget_used_percent": 45.63444787383309
    },
    {
      "type": "training",
      "description": "Training step 1920",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:40",
      "total_flops_so_far": 4.565821210604147e+16,
      "budget_used_percent": 45.658212106041475
    },
    {
      "type": "training",
      "description": "Training step 1921",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:41",
      "total_flops_so_far": 4.568197633824986e+16,
      "budget_used_percent": 45.68197633824986
    },
    {
      "type": "training",
      "description": "Training step 1922",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:43",
      "total_flops_so_far": 4.570574057045824e+16,
      "budget_used_percent": 45.70574057045824
    },
    {
      "type": "training",
      "description": "Training step 1923",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:44",
      "total_flops_so_far": 4.572950480266662e+16,
      "budget_used_percent": 45.729504802666625
    },
    {
      "type": "training",
      "description": "Training step 1924",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:45",
      "total_flops_so_far": 4.575326903487501e+16,
      "budget_used_percent": 45.75326903487501
    },
    {
      "type": "training",
      "description": "Training step 1925",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:47",
      "total_flops_so_far": 4.577703326708339e+16,
      "budget_used_percent": 45.7770332670834
    },
    {
      "type": "training",
      "description": "Training step 1926",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:48",
      "total_flops_so_far": 4.580079749929178e+16,
      "budget_used_percent": 45.800797499291775
    },
    {
      "type": "training",
      "description": "Training step 1927",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:49",
      "total_flops_so_far": 4.582456173150016e+16,
      "budget_used_percent": 45.824561731500154
    },
    {
      "type": "training",
      "description": "Training step 1928",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:51",
      "total_flops_so_far": 4.584832596370854e+16,
      "budget_used_percent": 45.84832596370854
    },
    {
      "type": "training",
      "description": "Training step 1929",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:52",
      "total_flops_so_far": 4.587209019591693e+16,
      "budget_used_percent": 45.872090195916925
    },
    {
      "type": "training",
      "description": "Training step 1930",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:53",
      "total_flops_so_far": 4.589585442812531e+16,
      "budget_used_percent": 45.89585442812531
    },
    {
      "type": "training",
      "description": "Training step 1931",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:54",
      "total_flops_so_far": 4.59196186603337e+16,
      "budget_used_percent": 45.9196186603337
    },
    {
      "type": "training",
      "description": "Training step 1932",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:56",
      "total_flops_so_far": 4.594338289254208e+16,
      "budget_used_percent": 45.943382892542076
    },
    {
      "type": "training",
      "description": "Training step 1933",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:57",
      "total_flops_so_far": 4.596714712475046e+16,
      "budget_used_percent": 45.96714712475046
    },
    {
      "type": "training",
      "description": "Training step 1934",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:52:58",
      "total_flops_so_far": 4.599091135695885e+16,
      "budget_used_percent": 45.99091135695885
    },
    {
      "type": "training",
      "description": "Training step 1935",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:00",
      "total_flops_so_far": 4.601467558916723e+16,
      "budget_used_percent": 46.01467558916723
    },
    {
      "type": "training",
      "description": "Training step 1936",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:01",
      "total_flops_so_far": 4.603843982137562e+16,
      "budget_used_percent": 46.03843982137562
    },
    {
      "type": "training",
      "description": "Training step 1937",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:02",
      "total_flops_so_far": 4.6062204053584e+16,
      "budget_used_percent": 46.062204053584
    },
    {
      "type": "training",
      "description": "Training step 1938",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:04",
      "total_flops_so_far": 4.608596828579238e+16,
      "budget_used_percent": 46.08596828579238
    },
    {
      "type": "training",
      "description": "Training step 1939",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:05",
      "total_flops_so_far": 4.610973251800077e+16,
      "budget_used_percent": 46.10973251800077
    },
    {
      "type": "training",
      "description": "Training step 1940",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:06",
      "total_flops_so_far": 4.613349675020915e+16,
      "budget_used_percent": 46.133496750209154
    },
    {
      "type": "training",
      "description": "Training step 1941",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:08",
      "total_flops_so_far": 4.615726098241754e+16,
      "budget_used_percent": 46.15726098241753
    },
    {
      "type": "training",
      "description": "Training step 1942",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:09",
      "total_flops_so_far": 4.618102521462592e+16,
      "budget_used_percent": 46.18102521462592
    },
    {
      "type": "training",
      "description": "Training step 1943",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:11",
      "total_flops_so_far": 4.62047894468343e+16,
      "budget_used_percent": 46.204789446834305
    },
    {
      "type": "training",
      "description": "Training step 1944",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:12",
      "total_flops_so_far": 4.622855367904269e+16,
      "budget_used_percent": 46.22855367904269
    },
    {
      "type": "training",
      "description": "Training step 1945",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:13",
      "total_flops_so_far": 4.625231791125107e+16,
      "budget_used_percent": 46.252317911251076
    },
    {
      "type": "training",
      "description": "Training step 1946",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:14",
      "total_flops_so_far": 4.627608214345946e+16,
      "budget_used_percent": 46.276082143459455
    },
    {
      "type": "training",
      "description": "Training step 1947",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:16",
      "total_flops_so_far": 4.629984637566784e+16,
      "budget_used_percent": 46.29984637566784
    },
    {
      "type": "training",
      "description": "Training step 1948",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:17",
      "total_flops_so_far": 4.632361060787622e+16,
      "budget_used_percent": 46.323610607876226
    },
    {
      "type": "training",
      "description": "Training step 1949",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:18",
      "total_flops_so_far": 4.634737484008461e+16,
      "budget_used_percent": 46.34737484008461
    },
    {
      "type": "training",
      "description": "Training step 1950",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:20",
      "total_flops_so_far": 4.637113907229299e+16,
      "budget_used_percent": 46.37113907229299
    },
    {
      "type": "training",
      "description": "Training step 1951",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:21",
      "total_flops_so_far": 4.639490330450138e+16,
      "budget_used_percent": 46.394903304501376
    },
    {
      "type": "training",
      "description": "Training step 1952",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:22",
      "total_flops_so_far": 4.641866753670976e+16,
      "budget_used_percent": 46.41866753670976
    },
    {
      "type": "training",
      "description": "Training step 1953",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:24",
      "total_flops_so_far": 4.644243176891814e+16,
      "budget_used_percent": 46.44243176891815
    },
    {
      "type": "training",
      "description": "Training step 1954",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:25",
      "total_flops_so_far": 4.646619600112653e+16,
      "budget_used_percent": 46.466196001126534
    },
    {
      "type": "training",
      "description": "Training step 1955",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:26",
      "total_flops_so_far": 4.648996023333491e+16,
      "budget_used_percent": 46.48996023333491
    },
    {
      "type": "training",
      "description": "Training step 1956",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:28",
      "total_flops_so_far": 4.65137244655433e+16,
      "budget_used_percent": 46.51372446554329
    },
    {
      "type": "training",
      "description": "Training step 1957",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:29",
      "total_flops_so_far": 4.653748869775168e+16,
      "budget_used_percent": 46.53748869775168
    },
    {
      "type": "training",
      "description": "Training step 1958",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:30",
      "total_flops_so_far": 4.656125292996006e+16,
      "budget_used_percent": 46.56125292996006
    },
    {
      "type": "training",
      "description": "Training step 1959",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:32",
      "total_flops_so_far": 4.658501716216845e+16,
      "budget_used_percent": 46.58501716216845
    },
    {
      "type": "training",
      "description": "Training step 1960",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:33",
      "total_flops_so_far": 4.660878139437683e+16,
      "budget_used_percent": 46.608781394376834
    },
    {
      "type": "training",
      "description": "Training step 1961",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:34",
      "total_flops_so_far": 4.663254562658522e+16,
      "budget_used_percent": 46.63254562658521
    },
    {
      "type": "training",
      "description": "Training step 1962",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:36",
      "total_flops_so_far": 4.66563098587936e+16,
      "budget_used_percent": 46.6563098587936
    },
    {
      "type": "training",
      "description": "Training step 1963",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:37",
      "total_flops_so_far": 4.668007409100198e+16,
      "budget_used_percent": 46.680074091001984
    },
    {
      "type": "training",
      "description": "Training step 1964",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:38",
      "total_flops_so_far": 4.670383832321037e+16,
      "budget_used_percent": 46.70383832321037
    },
    {
      "type": "training",
      "description": "Training step 1965",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:40",
      "total_flops_so_far": 4.672760255541875e+16,
      "budget_used_percent": 46.72760255541875
    },
    {
      "type": "training",
      "description": "Training step 1966",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:41",
      "total_flops_so_far": 4.675136678762714e+16,
      "budget_used_percent": 46.751366787627134
    },
    {
      "type": "training",
      "description": "Training step 1967",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:42",
      "total_flops_so_far": 4.677513101983552e+16,
      "budget_used_percent": 46.77513101983552
    },
    {
      "type": "training",
      "description": "Training step 1968",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:44",
      "total_flops_so_far": 4.67988952520439e+16,
      "budget_used_percent": 46.798895252043906
    },
    {
      "type": "training",
      "description": "Training step 1969",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:45",
      "total_flops_so_far": 4.682265948425229e+16,
      "budget_used_percent": 46.82265948425229
    },
    {
      "type": "training",
      "description": "Training step 1970",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:46",
      "total_flops_so_far": 4.684642371646067e+16,
      "budget_used_percent": 46.84642371646067
    },
    {
      "type": "training",
      "description": "Training step 1971",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:48",
      "total_flops_so_far": 4.687018794866906e+16,
      "budget_used_percent": 46.870187948669056
    },
    {
      "type": "training",
      "description": "Training step 1972",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:49",
      "total_flops_so_far": 4.689395218087744e+16,
      "budget_used_percent": 46.89395218087744
    },
    {
      "type": "training",
      "description": "Training step 1973",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:50",
      "total_flops_so_far": 4.691771641308582e+16,
      "budget_used_percent": 46.91771641308583
    },
    {
      "type": "training",
      "description": "Training step 1974",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:52",
      "total_flops_so_far": 4.694148064529421e+16,
      "budget_used_percent": 46.941480645294206
    },
    {
      "type": "training",
      "description": "Training step 1975",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:53",
      "total_flops_so_far": 4.696524487750259e+16,
      "budget_used_percent": 46.96524487750259
    },
    {
      "type": "training",
      "description": "Training step 1976",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:54",
      "total_flops_so_far": 4.698900910971098e+16,
      "budget_used_percent": 46.98900910971098
    },
    {
      "type": "training",
      "description": "Training step 1977",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:56",
      "total_flops_so_far": 4.701277334191936e+16,
      "budget_used_percent": 47.01277334191936
    },
    {
      "type": "training",
      "description": "Training step 1978",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:57",
      "total_flops_so_far": 4.703653757412774e+16,
      "budget_used_percent": 47.03653757412775
    },
    {
      "type": "training",
      "description": "Training step 1979",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:58",
      "total_flops_so_far": 4.706030180633613e+16,
      "budget_used_percent": 47.06030180633613
    },
    {
      "type": "training",
      "description": "Training step 1980",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:53:59",
      "total_flops_so_far": 4.708406603854451e+16,
      "budget_used_percent": 47.08406603854451
    },
    {
      "type": "training",
      "description": "Training step 1981",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:54:00",
      "total_flops_so_far": 4.71078302707529e+16,
      "budget_used_percent": 47.1078302707529
    },
    {
      "type": "training",
      "description": "Training step 1982",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:54:01",
      "total_flops_so_far": 4.713159450296128e+16,
      "budget_used_percent": 47.131594502961285
    },
    {
      "type": "training",
      "description": "Training step 1983",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:54:03",
      "total_flops_so_far": 4.715535873516966e+16,
      "budget_used_percent": 47.15535873516966
    },
    {
      "type": "training",
      "description": "Training step 1984",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:54:04",
      "total_flops_so_far": 4.717912296737805e+16,
      "budget_used_percent": 47.17912296737804
    },
    {
      "type": "training",
      "description": "Training step 1985",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:54:05",
      "total_flops_so_far": 4.720288719958643e+16,
      "budget_used_percent": 47.20288719958643
    },
    {
      "type": "training",
      "description": "Training step 1986",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:54:07",
      "total_flops_so_far": 4.722665143179482e+16,
      "budget_used_percent": 47.226651431794814
    },
    {
      "type": "training",
      "description": "Training step 1987",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:54:08",
      "total_flops_so_far": 4.72504156640032e+16,
      "budget_used_percent": 47.2504156640032
    },
    {
      "type": "training",
      "description": "Training step 1988",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:54:09",
      "total_flops_so_far": 4.727417989621158e+16,
      "budget_used_percent": 47.274179896211585
    },
    {
      "type": "training",
      "description": "Training step 1989",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:54:11",
      "total_flops_so_far": 4.729794412841997e+16,
      "budget_used_percent": 47.297944128419964
    },
    {
      "type": "training",
      "description": "Training step 1990",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:54:12",
      "total_flops_so_far": 4.732170836062835e+16,
      "budget_used_percent": 47.32170836062835
    },
    {
      "type": "training",
      "description": "Training step 1991",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:54:13",
      "total_flops_so_far": 4.734547259283674e+16,
      "budget_used_percent": 47.345472592836735
    },
    {
      "type": "training",
      "description": "Training step 1992",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:54:15",
      "total_flops_so_far": 4.736923682504512e+16,
      "budget_used_percent": 47.36923682504512
    },
    {
      "type": "training",
      "description": "Training step 1993",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:54:16",
      "total_flops_so_far": 4.73930010572535e+16,
      "budget_used_percent": 47.3930010572535
    },
    {
      "type": "training",
      "description": "Training step 1994",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:54:17",
      "total_flops_so_far": 4.741676528946189e+16,
      "budget_used_percent": 47.416765289461885
    },
    {
      "type": "training",
      "description": "Training step 1995",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:54:19",
      "total_flops_so_far": 4.744052952167027e+16,
      "budget_used_percent": 47.44052952167027
    },
    {
      "type": "training",
      "description": "Training step 1996",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:54:20",
      "total_flops_so_far": 4.746429375387866e+16,
      "budget_used_percent": 47.46429375387866
    },
    {
      "type": "training",
      "description": "Training step 1997",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:54:21",
      "total_flops_so_far": 4.748805798608704e+16,
      "budget_used_percent": 47.48805798608704
    },
    {
      "type": "training",
      "description": "Training step 1998",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:54:22",
      "total_flops_so_far": 4.751182221829542e+16,
      "budget_used_percent": 47.51182221829542
    },
    {
      "type": "training",
      "description": "Training step 1999",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:54:24",
      "total_flops_so_far": 4.753558645050381e+16,
      "budget_used_percent": 47.53558645050381
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:54:30",
      "total_flops_so_far": 4.753629707964166e+16,
      "budget_used_percent": 47.53629707964166
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 714333709232.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:54:35",
      "total_flops_so_far": 4.75370114133509e+16,
      "budget_used_percent": 47.537011413350896
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:54:40",
      "total_flops_so_far": 4.753772389441425e+16,
      "budget_used_percent": 47.53772389441425
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:54:45",
      "total_flops_so_far": 4.7538434523552104e+16,
      "budget_used_percent": 47.538434523552105
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 713407296244.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:54:50",
      "total_flops_so_far": 4.753914793084835e+16,
      "budget_used_percent": 47.53914793084835
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 5",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:54:55",
      "total_flops_so_far": 4.753985855998621e+16,
      "budget_used_percent": 47.53985855998621
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 6",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:00",
      "total_flops_so_far": 4.754057104104956e+16,
      "budget_used_percent": 47.54057104104956
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 7",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:05",
      "total_flops_so_far": 4.754128352211291e+16,
      "budget_used_percent": 47.541283522112906
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 8",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:11",
      "total_flops_so_far": 4.7541996003176264e+16,
      "budget_used_percent": 47.541996003176266
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 9",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:16",
      "total_flops_so_far": 4.754270848423962e+16,
      "budget_used_percent": 47.54270848423962
    },
    {
      "type": "training",
      "description": "Training step 2000",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:16",
      "total_flops_so_far": 4.7566472716448e+16,
      "budget_used_percent": 47.566472716448
    },
    {
      "type": "training",
      "description": "Training step 2001",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:17",
      "total_flops_so_far": 4.759023694865638e+16,
      "budget_used_percent": 47.59023694865638
    },
    {
      "type": "training",
      "description": "Training step 2002",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:18",
      "total_flops_so_far": 4.761400118086477e+16,
      "budget_used_percent": 47.61400118086477
    },
    {
      "type": "training",
      "description": "Training step 2003",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:20",
      "total_flops_so_far": 4.763776541307315e+16,
      "budget_used_percent": 47.637765413073154
    },
    {
      "type": "training",
      "description": "Training step 2004",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:21",
      "total_flops_so_far": 4.766152964528154e+16,
      "budget_used_percent": 47.66152964528153
    },
    {
      "type": "training",
      "description": "Training step 2005",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:22",
      "total_flops_so_far": 4.768529387748992e+16,
      "budget_used_percent": 47.68529387748992
    },
    {
      "type": "training",
      "description": "Training step 2006",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:24",
      "total_flops_so_far": 4.77090581096983e+16,
      "budget_used_percent": 47.709058109698304
    },
    {
      "type": "training",
      "description": "Training step 2007",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:25",
      "total_flops_so_far": 4.773282234190669e+16,
      "budget_used_percent": 47.73282234190669
    },
    {
      "type": "training",
      "description": "Training step 2008",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:26",
      "total_flops_so_far": 4.775658657411507e+16,
      "budget_used_percent": 47.756586574115076
    },
    {
      "type": "training",
      "description": "Training step 2009",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:28",
      "total_flops_so_far": 4.778035080632346e+16,
      "budget_used_percent": 47.780350806323455
    },
    {
      "type": "training",
      "description": "Training step 2010",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:29",
      "total_flops_so_far": 4.780411503853184e+16,
      "budget_used_percent": 47.80411503853184
    },
    {
      "type": "training",
      "description": "Training step 2011",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:30",
      "total_flops_so_far": 4.782787927074022e+16,
      "budget_used_percent": 47.827879270740226
    },
    {
      "type": "training",
      "description": "Training step 2012",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:32",
      "total_flops_so_far": 4.785164350294861e+16,
      "budget_used_percent": 47.85164350294861
    },
    {
      "type": "training",
      "description": "Training step 2013",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:33",
      "total_flops_so_far": 4.787540773515699e+16,
      "budget_used_percent": 47.875407735157
    },
    {
      "type": "training",
      "description": "Training step 2014",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:34",
      "total_flops_so_far": 4.789917196736538e+16,
      "budget_used_percent": 47.899171967365376
    },
    {
      "type": "training",
      "description": "Training step 2015",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:36",
      "total_flops_so_far": 4.792293619957376e+16,
      "budget_used_percent": 47.92293619957376
    },
    {
      "type": "training",
      "description": "Training step 2016",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:37",
      "total_flops_so_far": 4.794670043178214e+16,
      "budget_used_percent": 47.94670043178215
    },
    {
      "type": "training",
      "description": "Training step 2017",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:38",
      "total_flops_so_far": 4.797046466399053e+16,
      "budget_used_percent": 47.97046466399053
    },
    {
      "type": "training",
      "description": "Training step 2018",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:40",
      "total_flops_so_far": 4.799422889619891e+16,
      "budget_used_percent": 47.99422889619891
    },
    {
      "type": "training",
      "description": "Training step 2019",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:41",
      "total_flops_so_far": 4.80179931284073e+16,
      "budget_used_percent": 48.01799312840729
    },
    {
      "type": "training",
      "description": "Training step 2020",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:42",
      "total_flops_so_far": 4.804175736061568e+16,
      "budget_used_percent": 48.04175736061568
    },
    {
      "type": "training",
      "description": "Training step 2021",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:43",
      "total_flops_so_far": 4.806552159282406e+16,
      "budget_used_percent": 48.06552159282406
    },
    {
      "type": "training",
      "description": "Training step 2022",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:45",
      "total_flops_so_far": 4.808928582503245e+16,
      "budget_used_percent": 48.08928582503245
    },
    {
      "type": "training",
      "description": "Training step 2023",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:46",
      "total_flops_so_far": 4.811305005724083e+16,
      "budget_used_percent": 48.113050057240834
    },
    {
      "type": "training",
      "description": "Training step 2024",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:47",
      "total_flops_so_far": 4.813681428944922e+16,
      "budget_used_percent": 48.13681428944921
    },
    {
      "type": "training",
      "description": "Training step 2025",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:49",
      "total_flops_so_far": 4.81605785216576e+16,
      "budget_used_percent": 48.1605785216576
    },
    {
      "type": "training",
      "description": "Training step 2026",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:50",
      "total_flops_so_far": 4.818434275386598e+16,
      "budget_used_percent": 48.184342753865984
    },
    {
      "type": "training",
      "description": "Training step 2027",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:51",
      "total_flops_so_far": 4.820810698607437e+16,
      "budget_used_percent": 48.20810698607437
    },
    {
      "type": "training",
      "description": "Training step 2028",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:53",
      "total_flops_so_far": 4.823187121828275e+16,
      "budget_used_percent": 48.23187121828275
    },
    {
      "type": "training",
      "description": "Training step 2029",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:54",
      "total_flops_so_far": 4.825563545049114e+16,
      "budget_used_percent": 48.255635450491134
    },
    {
      "type": "training",
      "description": "Training step 2030",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:55",
      "total_flops_so_far": 4.827939968269952e+16,
      "budget_used_percent": 48.27939968269952
    },
    {
      "type": "training",
      "description": "Training step 2031",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:57",
      "total_flops_so_far": 4.83031639149079e+16,
      "budget_used_percent": 48.303163914907906
    },
    {
      "type": "training",
      "description": "Training step 2032",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:58",
      "total_flops_so_far": 4.832692814711629e+16,
      "budget_used_percent": 48.32692814711629
    },
    {
      "type": "training",
      "description": "Training step 2033",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:55:59",
      "total_flops_so_far": 4.835069237932467e+16,
      "budget_used_percent": 48.35069237932467
    },
    {
      "type": "training",
      "description": "Training step 2034",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:01",
      "total_flops_so_far": 4.837445661153306e+16,
      "budget_used_percent": 48.374456611533056
    },
    {
      "type": "training",
      "description": "Training step 2035",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:02",
      "total_flops_so_far": 4.839822084374144e+16,
      "budget_used_percent": 48.39822084374144
    },
    {
      "type": "training",
      "description": "Training step 2036",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:03",
      "total_flops_so_far": 4.842198507594982e+16,
      "budget_used_percent": 48.42198507594983
    },
    {
      "type": "training",
      "description": "Training step 2037",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:05",
      "total_flops_so_far": 4.844574930815821e+16,
      "budget_used_percent": 48.445749308158206
    },
    {
      "type": "training",
      "description": "Training step 2038",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:06",
      "total_flops_so_far": 4.846951354036659e+16,
      "budget_used_percent": 48.46951354036659
    },
    {
      "type": "training",
      "description": "Training step 2039",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:07",
      "total_flops_so_far": 4.849327777257498e+16,
      "budget_used_percent": 48.49327777257498
    },
    {
      "type": "training",
      "description": "Training step 2040",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:09",
      "total_flops_so_far": 4.851704200478336e+16,
      "budget_used_percent": 48.51704200478336
    },
    {
      "type": "training",
      "description": "Training step 2041",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:10",
      "total_flops_so_far": 4.854080623699174e+16,
      "budget_used_percent": 48.54080623699175
    },
    {
      "type": "training",
      "description": "Training step 2042",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:11",
      "total_flops_so_far": 4.856457046920013e+16,
      "budget_used_percent": 48.56457046920013
    },
    {
      "type": "training",
      "description": "Training step 2043",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:13",
      "total_flops_so_far": 4.858833470140851e+16,
      "budget_used_percent": 48.58833470140851
    },
    {
      "type": "training",
      "description": "Training step 2044",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:14",
      "total_flops_so_far": 4.86120989336169e+16,
      "budget_used_percent": 48.6120989336169
    },
    {
      "type": "training",
      "description": "Training step 2045",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:15",
      "total_flops_so_far": 4.863586316582528e+16,
      "budget_used_percent": 48.635863165825285
    },
    {
      "type": "training",
      "description": "Training step 2046",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:17",
      "total_flops_so_far": 4.865962739803366e+16,
      "budget_used_percent": 48.65962739803366
    },
    {
      "type": "training",
      "description": "Training step 2047",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:18",
      "total_flops_so_far": 4.868339163024205e+16,
      "budget_used_percent": 48.68339163024204
    },
    {
      "type": "training",
      "description": "Training step 2048",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:19",
      "total_flops_so_far": 4.870715586245043e+16,
      "budget_used_percent": 48.70715586245043
    },
    {
      "type": "training",
      "description": "Training step 2049",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:21",
      "total_flops_so_far": 4.873092009465882e+16,
      "budget_used_percent": 48.73092009465881
    },
    {
      "type": "training",
      "description": "Training step 2050",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:22",
      "total_flops_so_far": 4.87546843268672e+16,
      "budget_used_percent": 48.7546843268672
    },
    {
      "type": "training",
      "description": "Training step 2051",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:23",
      "total_flops_so_far": 4.877844855907558e+16,
      "budget_used_percent": 48.778448559075585
    },
    {
      "type": "training",
      "description": "Training step 2052",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:25",
      "total_flops_so_far": 4.880221279128397e+16,
      "budget_used_percent": 48.802212791283964
    },
    {
      "type": "training",
      "description": "Training step 2053",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:26",
      "total_flops_so_far": 4.882597702349235e+16,
      "budget_used_percent": 48.82597702349235
    },
    {
      "type": "training",
      "description": "Training step 2054",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:27",
      "total_flops_so_far": 4.884974125570074e+16,
      "budget_used_percent": 48.849741255700735
    },
    {
      "type": "training",
      "description": "Training step 2055",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:29",
      "total_flops_so_far": 4.887350548790912e+16,
      "budget_used_percent": 48.87350548790912
    },
    {
      "type": "training",
      "description": "Training step 2056",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:30",
      "total_flops_so_far": 4.88972697201175e+16,
      "budget_used_percent": 48.89726972011751
    },
    {
      "type": "training",
      "description": "Training step 2057",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:31",
      "total_flops_so_far": 4.892103395232589e+16,
      "budget_used_percent": 48.921033952325885
    },
    {
      "type": "training",
      "description": "Training step 2058",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:33",
      "total_flops_so_far": 4.894479818453427e+16,
      "budget_used_percent": 48.94479818453427
    },
    {
      "type": "training",
      "description": "Training step 2059",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:34",
      "total_flops_so_far": 4.896856241674266e+16,
      "budget_used_percent": 48.96856241674266
    },
    {
      "type": "training",
      "description": "Training step 2060",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:35",
      "total_flops_so_far": 4.899232664895104e+16,
      "budget_used_percent": 48.99232664895104
    },
    {
      "type": "training",
      "description": "Training step 2061",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:37",
      "total_flops_so_far": 4.901609088115942e+16,
      "budget_used_percent": 49.01609088115942
    },
    {
      "type": "training",
      "description": "Training step 2062",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:38",
      "total_flops_so_far": 4.903985511336781e+16,
      "budget_used_percent": 49.03985511336781
    },
    {
      "type": "training",
      "description": "Training step 2063",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:39",
      "total_flops_so_far": 4.906361934557619e+16,
      "budget_used_percent": 49.06361934557619
    },
    {
      "type": "training",
      "description": "Training step 2064",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:40",
      "total_flops_so_far": 4.908738357778458e+16,
      "budget_used_percent": 49.08738357778458
    },
    {
      "type": "training",
      "description": "Training step 2065",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:42",
      "total_flops_so_far": 4.911114780999296e+16,
      "budget_used_percent": 49.111147809992964
    },
    {
      "type": "training",
      "description": "Training step 2066",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:43",
      "total_flops_so_far": 4.913491204220134e+16,
      "budget_used_percent": 49.13491204220134
    },
    {
      "type": "training",
      "description": "Training step 2067",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:45",
      "total_flops_so_far": 4.915867627440973e+16,
      "budget_used_percent": 49.15867627440973
    },
    {
      "type": "training",
      "description": "Training step 2068",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:46",
      "total_flops_so_far": 4.918244050661811e+16,
      "budget_used_percent": 49.182440506618114
    },
    {
      "type": "training",
      "description": "Training step 2069",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:47",
      "total_flops_so_far": 4.92062047388265e+16,
      "budget_used_percent": 49.2062047388265
    },
    {
      "type": "training",
      "description": "Training step 2070",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:49",
      "total_flops_so_far": 4.922996897103488e+16,
      "budget_used_percent": 49.22996897103488
    },
    {
      "type": "training",
      "description": "Training step 2071",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:50",
      "total_flops_so_far": 4.925373320324326e+16,
      "budget_used_percent": 49.253733203243264
    },
    {
      "type": "training",
      "description": "Training step 2072",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:51",
      "total_flops_so_far": 4.927749743545165e+16,
      "budget_used_percent": 49.27749743545165
    },
    {
      "type": "training",
      "description": "Training step 2073",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:53",
      "total_flops_so_far": 4.930126166766003e+16,
      "budget_used_percent": 49.301261667660036
    },
    {
      "type": "training",
      "description": "Training step 2074",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:54",
      "total_flops_so_far": 4.932502589986842e+16,
      "budget_used_percent": 49.325025899868415
    },
    {
      "type": "training",
      "description": "Training step 2075",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:55",
      "total_flops_so_far": 4.93487901320768e+16,
      "budget_used_percent": 49.3487901320768
    },
    {
      "type": "training",
      "description": "Training step 2076",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:57",
      "total_flops_so_far": 4.937255436428518e+16,
      "budget_used_percent": 49.37255436428518
    },
    {
      "type": "training",
      "description": "Training step 2077",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:58",
      "total_flops_so_far": 4.939631859649357e+16,
      "budget_used_percent": 49.396318596493565
    },
    {
      "type": "training",
      "description": "Training step 2078",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:56:59",
      "total_flops_so_far": 4.942008282870195e+16,
      "budget_used_percent": 49.42008282870195
    },
    {
      "type": "training",
      "description": "Training step 2079",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:01",
      "total_flops_so_far": 4.944384706091034e+16,
      "budget_used_percent": 49.443847060910336
    },
    {
      "type": "training",
      "description": "Training step 2080",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:02",
      "total_flops_so_far": 4.946761129311872e+16,
      "budget_used_percent": 49.46761129311872
    },
    {
      "type": "training",
      "description": "Training step 2081",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:03",
      "total_flops_so_far": 4.94913755253271e+16,
      "budget_used_percent": 49.4913755253271
    },
    {
      "type": "training",
      "description": "Training step 2082",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:05",
      "total_flops_so_far": 4.951513975753549e+16,
      "budget_used_percent": 49.515139757535486
    },
    {
      "type": "training",
      "description": "Training step 2083",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:06",
      "total_flops_so_far": 4.953890398974387e+16,
      "budget_used_percent": 49.53890398974387
    },
    {
      "type": "training",
      "description": "Training step 2084",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:07",
      "total_flops_so_far": 4.956266822195226e+16,
      "budget_used_percent": 49.56266822195226
    },
    {
      "type": "training",
      "description": "Training step 2085",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:08",
      "total_flops_so_far": 4.958643245416064e+16,
      "budget_used_percent": 49.58643245416064
    },
    {
      "type": "training",
      "description": "Training step 2086",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:10",
      "total_flops_so_far": 4.961019668636902e+16,
      "budget_used_percent": 49.61019668636902
    },
    {
      "type": "training",
      "description": "Training step 2087",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:11",
      "total_flops_so_far": 4.963396091857741e+16,
      "budget_used_percent": 49.63396091857741
    },
    {
      "type": "training",
      "description": "Training step 2088",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:12",
      "total_flops_so_far": 4.965772515078579e+16,
      "budget_used_percent": 49.657725150785794
    },
    {
      "type": "training",
      "description": "Training step 2089",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:14",
      "total_flops_so_far": 4.968148938299418e+16,
      "budget_used_percent": 49.68148938299418
    },
    {
      "type": "training",
      "description": "Training step 2090",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:15",
      "total_flops_so_far": 4.970525361520256e+16,
      "budget_used_percent": 49.70525361520256
    },
    {
      "type": "training",
      "description": "Training step 2091",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:16",
      "total_flops_so_far": 4.972901784741094e+16,
      "budget_used_percent": 49.729017847410944
    },
    {
      "type": "training",
      "description": "Training step 2092",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:18",
      "total_flops_so_far": 4.975278207961933e+16,
      "budget_used_percent": 49.75278207961933
    },
    {
      "type": "training",
      "description": "Training step 2093",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:19",
      "total_flops_so_far": 4.977654631182771e+16,
      "budget_used_percent": 49.776546311827715
    },
    {
      "type": "training",
      "description": "Training step 2094",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:20",
      "total_flops_so_far": 4.98003105440361e+16,
      "budget_used_percent": 49.800310544036094
    },
    {
      "type": "training",
      "description": "Training step 2095",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:22",
      "total_flops_so_far": 4.982407477624448e+16,
      "budget_used_percent": 49.82407477624448
    },
    {
      "type": "training",
      "description": "Training step 2096",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:23",
      "total_flops_so_far": 4.984783900845286e+16,
      "budget_used_percent": 49.847839008452866
    },
    {
      "type": "training",
      "description": "Training step 2097",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:24",
      "total_flops_so_far": 4.987160324066125e+16,
      "budget_used_percent": 49.87160324066125
    },
    {
      "type": "training",
      "description": "Training step 2098",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:26",
      "total_flops_so_far": 4.989536747286963e+16,
      "budget_used_percent": 49.89536747286964
    },
    {
      "type": "training",
      "description": "Training step 2099",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:27",
      "total_flops_so_far": 4.991913170507802e+16,
      "budget_used_percent": 49.919131705078016
    },
    {
      "type": "training",
      "description": "Training step 2100",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:28",
      "total_flops_so_far": 4.99428959372864e+16,
      "budget_used_percent": 49.9428959372864
    },
    {
      "type": "training",
      "description": "Training step 2101",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:30",
      "total_flops_so_far": 4.996666016949478e+16,
      "budget_used_percent": 49.96666016949479
    },
    {
      "type": "training",
      "description": "Training step 2102",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:31",
      "total_flops_so_far": 4.999042440170317e+16,
      "budget_used_percent": 49.99042440170317
    },
    {
      "type": "training",
      "description": "Training step 2103",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:32",
      "total_flops_so_far": 5.001418863391155e+16,
      "budget_used_percent": 50.01418863391155
    },
    {
      "type": "training",
      "description": "Training step 2104",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:34",
      "total_flops_so_far": 5.003795286611994e+16,
      "budget_used_percent": 50.03795286611994
    },
    {
      "type": "training",
      "description": "Training step 2105",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:35",
      "total_flops_so_far": 5.006171709832832e+16,
      "budget_used_percent": 50.061717098328316
    },
    {
      "type": "training",
      "description": "Training step 2106",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:36",
      "total_flops_so_far": 5.00854813305367e+16,
      "budget_used_percent": 50.08548133053671
    },
    {
      "type": "training",
      "description": "Training step 2107",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:38",
      "total_flops_so_far": 5.010924556274509e+16,
      "budget_used_percent": 50.10924556274509
    },
    {
      "type": "training",
      "description": "Training step 2108",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:39",
      "total_flops_so_far": 5.013300979495347e+16,
      "budget_used_percent": 50.13300979495347
    },
    {
      "type": "training",
      "description": "Training step 2109",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:40",
      "total_flops_so_far": 5.015677402716186e+16,
      "budget_used_percent": 50.15677402716185
    },
    {
      "type": "training",
      "description": "Training step 2110",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:42",
      "total_flops_so_far": 5.018053825937024e+16,
      "budget_used_percent": 50.180538259370245
    },
    {
      "type": "training",
      "description": "Training step 2111",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:43",
      "total_flops_so_far": 5.020430249157862e+16,
      "budget_used_percent": 50.20430249157862
    },
    {
      "type": "training",
      "description": "Training step 2112",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:43",
      "total_flops_so_far": 5.022806672378701e+16,
      "budget_used_percent": 50.22806672378701
    },
    {
      "type": "training",
      "description": "Training step 2113",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:45",
      "total_flops_so_far": 5.025183095599539e+16,
      "budget_used_percent": 50.25183095599539
    },
    {
      "type": "training",
      "description": "Training step 2114",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:46",
      "total_flops_so_far": 5.027559518820378e+16,
      "budget_used_percent": 50.27559518820378
    },
    {
      "type": "training",
      "description": "Training step 2115",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:47",
      "total_flops_so_far": 5.029935942041216e+16,
      "budget_used_percent": 50.29935942041216
    },
    {
      "type": "training",
      "description": "Training step 2116",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:49",
      "total_flops_so_far": 5.032312365262054e+16,
      "budget_used_percent": 50.32312365262055
    },
    {
      "type": "training",
      "description": "Training step 2117",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:50",
      "total_flops_so_far": 5.034688788482893e+16,
      "budget_used_percent": 50.34688788482893
    },
    {
      "type": "training",
      "description": "Training step 2118",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:51",
      "total_flops_so_far": 5.037065211703731e+16,
      "budget_used_percent": 50.37065211703731
    },
    {
      "type": "training",
      "description": "Training step 2119",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:53",
      "total_flops_so_far": 5.03944163492457e+16,
      "budget_used_percent": 50.394416349245695
    },
    {
      "type": "training",
      "description": "Training step 2120",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:54",
      "total_flops_so_far": 5.041818058145408e+16,
      "budget_used_percent": 50.418180581454074
    },
    {
      "type": "training",
      "description": "Training step 2121",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:55",
      "total_flops_so_far": 5.044194481366246e+16,
      "budget_used_percent": 50.44194481366247
    },
    {
      "type": "training",
      "description": "Training step 2122",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:57",
      "total_flops_so_far": 5.046570904587085e+16,
      "budget_used_percent": 50.465709045870845
    },
    {
      "type": "training",
      "description": "Training step 2123",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:58",
      "total_flops_so_far": 5.048947327807923e+16,
      "budget_used_percent": 50.48947327807923
    },
    {
      "type": "training",
      "description": "Training step 2124",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:57:59",
      "total_flops_so_far": 5.051323751028762e+16,
      "budget_used_percent": 50.51323751028761
    },
    {
      "type": "training",
      "description": "Training step 2125",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:01",
      "total_flops_so_far": 5.0537001742496e+16,
      "budget_used_percent": 50.537001742496
    },
    {
      "type": "training",
      "description": "Training step 2126",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:02",
      "total_flops_so_far": 5.056076597470438e+16,
      "budget_used_percent": 50.56076597470438
    },
    {
      "type": "training",
      "description": "Training step 2127",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:03",
      "total_flops_so_far": 5.058453020691277e+16,
      "budget_used_percent": 50.58453020691277
    },
    {
      "type": "training",
      "description": "Training step 2128",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:05",
      "total_flops_so_far": 5.060829443912115e+16,
      "budget_used_percent": 50.608294439121146
    },
    {
      "type": "training",
      "description": "Training step 2129",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:06",
      "total_flops_so_far": 5.063205867132954e+16,
      "budget_used_percent": 50.63205867132954
    },
    {
      "type": "training",
      "description": "Training step 2130",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:07",
      "total_flops_so_far": 5.065582290353792e+16,
      "budget_used_percent": 50.65582290353792
    },
    {
      "type": "training",
      "description": "Training step 2131",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:09",
      "total_flops_so_far": 5.06795871357463e+16,
      "budget_used_percent": 50.67958713574631
    },
    {
      "type": "training",
      "description": "Training step 2132",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:10",
      "total_flops_so_far": 5.070335136795469e+16,
      "budget_used_percent": 50.70335136795469
    },
    {
      "type": "training",
      "description": "Training step 2133",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:11",
      "total_flops_so_far": 5.072711560016307e+16,
      "budget_used_percent": 50.727115600163074
    },
    {
      "type": "training",
      "description": "Training step 2134",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:13",
      "total_flops_so_far": 5.075087983237146e+16,
      "budget_used_percent": 50.75087983237145
    },
    {
      "type": "training",
      "description": "Training step 2135",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:14",
      "total_flops_so_far": 5.077464406457984e+16,
      "budget_used_percent": 50.774644064579846
    },
    {
      "type": "training",
      "description": "Training step 2136",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:15",
      "total_flops_so_far": 5.079840829678822e+16,
      "budget_used_percent": 50.798408296788224
    },
    {
      "type": "training",
      "description": "Training step 2137",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:17",
      "total_flops_so_far": 5.082217252899661e+16,
      "budget_used_percent": 50.82217252899661
    },
    {
      "type": "training",
      "description": "Training step 2138",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:18",
      "total_flops_so_far": 5.084593676120499e+16,
      "budget_used_percent": 50.84593676120499
    },
    {
      "type": "training",
      "description": "Training step 2139",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:19",
      "total_flops_so_far": 5.086970099341338e+16,
      "budget_used_percent": 50.86970099341338
    },
    {
      "type": "training",
      "description": "Training step 2140",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:21",
      "total_flops_so_far": 5.089346522562176e+16,
      "budget_used_percent": 50.89346522562176
    },
    {
      "type": "training",
      "description": "Training step 2141",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:22",
      "total_flops_so_far": 5.091722945783014e+16,
      "budget_used_percent": 50.917229457830146
    },
    {
      "type": "training",
      "description": "Training step 2142",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:23",
      "total_flops_so_far": 5.094099369003853e+16,
      "budget_used_percent": 50.940993690038525
    },
    {
      "type": "training",
      "description": "Training step 2143",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:25",
      "total_flops_so_far": 5.096475792224691e+16,
      "budget_used_percent": 50.96475792224692
    },
    {
      "type": "training",
      "description": "Training step 2144",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:26",
      "total_flops_so_far": 5.09885221544553e+16,
      "budget_used_percent": 50.988522154455296
    },
    {
      "type": "training",
      "description": "Training step 2145",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:27",
      "total_flops_so_far": 5.101228638666368e+16,
      "budget_used_percent": 51.01228638666369
    },
    {
      "type": "training",
      "description": "Training step 2146",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:29",
      "total_flops_so_far": 5.103605061887206e+16,
      "budget_used_percent": 51.03605061887207
    },
    {
      "type": "training",
      "description": "Training step 2147",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:30",
      "total_flops_so_far": 5.105981485108045e+16,
      "budget_used_percent": 51.059814851080446
    },
    {
      "type": "training",
      "description": "Training step 2148",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:31",
      "total_flops_so_far": 5.108357908328883e+16,
      "budget_used_percent": 51.08357908328883
    },
    {
      "type": "training",
      "description": "Training step 2149",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:33",
      "total_flops_so_far": 5.110734331549722e+16,
      "budget_used_percent": 51.10734331549721
    },
    {
      "type": "training",
      "description": "Training step 2150",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:34",
      "total_flops_so_far": 5.11311075477056e+16,
      "budget_used_percent": 51.131107547705604
    },
    {
      "type": "training",
      "description": "Training step 2151",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:35",
      "total_flops_so_far": 5.115487177991398e+16,
      "budget_used_percent": 51.15487177991398
    },
    {
      "type": "training",
      "description": "Training step 2152",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:37",
      "total_flops_so_far": 5.117863601212237e+16,
      "budget_used_percent": 51.17863601212237
    },
    {
      "type": "training",
      "description": "Training step 2153",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:38",
      "total_flops_so_far": 5.120240024433075e+16,
      "budget_used_percent": 51.20240024433075
    },
    {
      "type": "training",
      "description": "Training step 2154",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:39",
      "total_flops_so_far": 5.122616447653914e+16,
      "budget_used_percent": 51.22616447653914
    },
    {
      "type": "training",
      "description": "Training step 2155",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:40",
      "total_flops_so_far": 5.124992870874752e+16,
      "budget_used_percent": 51.24992870874752
    },
    {
      "type": "training",
      "description": "Training step 2156",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:42",
      "total_flops_so_far": 5.12736929409559e+16,
      "budget_used_percent": 51.273692940955904
    },
    {
      "type": "training",
      "description": "Training step 2157",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:43",
      "total_flops_so_far": 5.129745717316429e+16,
      "budget_used_percent": 51.29745717316428
    },
    {
      "type": "training",
      "description": "Training step 2158",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:44",
      "total_flops_so_far": 5.132122140537267e+16,
      "budget_used_percent": 51.321221405372675
    },
    {
      "type": "training",
      "description": "Training step 2159",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:46",
      "total_flops_so_far": 5.134498563758106e+16,
      "budget_used_percent": 51.344985637581054
    },
    {
      "type": "training",
      "description": "Training step 2160",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:47",
      "total_flops_so_far": 5.136874986978944e+16,
      "budget_used_percent": 51.36874986978944
    },
    {
      "type": "training",
      "description": "Training step 2161",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:48",
      "total_flops_so_far": 5.139251410199782e+16,
      "budget_used_percent": 51.39251410199782
    },
    {
      "type": "training",
      "description": "Training step 2162",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:50",
      "total_flops_so_far": 5.141627833420621e+16,
      "budget_used_percent": 51.41627833420621
    },
    {
      "type": "training",
      "description": "Training step 2163",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:51",
      "total_flops_so_far": 5.144004256641459e+16,
      "budget_used_percent": 51.44004256641459
    },
    {
      "type": "training",
      "description": "Training step 2164",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:52",
      "total_flops_so_far": 5.146380679862298e+16,
      "budget_used_percent": 51.46380679862298
    },
    {
      "type": "training",
      "description": "Training step 2165",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:54",
      "total_flops_so_far": 5.148757103083136e+16,
      "budget_used_percent": 51.48757103083136
    },
    {
      "type": "training",
      "description": "Training step 2166",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:55",
      "total_flops_so_far": 5.151133526303974e+16,
      "budget_used_percent": 51.51133526303975
    },
    {
      "type": "training",
      "description": "Training step 2167",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:56",
      "total_flops_so_far": 5.153509949524813e+16,
      "budget_used_percent": 51.535099495248126
    },
    {
      "type": "training",
      "description": "Training step 2168",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:58",
      "total_flops_so_far": 5.155886372745651e+16,
      "budget_used_percent": 51.55886372745652
    },
    {
      "type": "training",
      "description": "Training step 2169",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:58:59",
      "total_flops_so_far": 5.15826279596649e+16,
      "budget_used_percent": 51.5826279596649
    },
    {
      "type": "training",
      "description": "Training step 2170",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:00",
      "total_flops_so_far": 5.160639219187328e+16,
      "budget_used_percent": 51.60639219187328
    },
    {
      "type": "training",
      "description": "Training step 2171",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:02",
      "total_flops_so_far": 5.163015642408166e+16,
      "budget_used_percent": 51.63015642408166
    },
    {
      "type": "training",
      "description": "Training step 2172",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:03",
      "total_flops_so_far": 5.165392065629005e+16,
      "budget_used_percent": 51.653920656290055
    },
    {
      "type": "training",
      "description": "Training step 2173",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:04",
      "total_flops_so_far": 5.167768488849843e+16,
      "budget_used_percent": 51.67768488849843
    },
    {
      "type": "training",
      "description": "Training step 2174",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:06",
      "total_flops_so_far": 5.170144912070682e+16,
      "budget_used_percent": 51.70144912070681
    },
    {
      "type": "training",
      "description": "Training step 2175",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:07",
      "total_flops_so_far": 5.17252133529152e+16,
      "budget_used_percent": 51.7252133529152
    },
    {
      "type": "training",
      "description": "Training step 2176",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:08",
      "total_flops_so_far": 5.174897758512358e+16,
      "budget_used_percent": 51.748977585123576
    },
    {
      "type": "training",
      "description": "Training step 2177",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:10",
      "total_flops_so_far": 5.177274181733197e+16,
      "budget_used_percent": 51.77274181733197
    },
    {
      "type": "training",
      "description": "Training step 2178",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:11",
      "total_flops_so_far": 5.179650604954035e+16,
      "budget_used_percent": 51.79650604954035
    },
    {
      "type": "training",
      "description": "Training step 2179",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:12",
      "total_flops_so_far": 5.182027028174874e+16,
      "budget_used_percent": 51.82027028174873
    },
    {
      "type": "training",
      "description": "Training step 2180",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:14",
      "total_flops_so_far": 5.184403451395712e+16,
      "budget_used_percent": 51.84403451395711
    },
    {
      "type": "training",
      "description": "Training step 2181",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:15",
      "total_flops_so_far": 5.18677987461655e+16,
      "budget_used_percent": 51.867798746165505
    },
    {
      "type": "training",
      "description": "Training step 2182",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:16",
      "total_flops_so_far": 5.189156297837389e+16,
      "budget_used_percent": 51.891562978373884
    },
    {
      "type": "training",
      "description": "Training step 2183",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:18",
      "total_flops_so_far": 5.191532721058227e+16,
      "budget_used_percent": 51.91532721058228
    },
    {
      "type": "training",
      "description": "Training step 2184",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:19",
      "total_flops_so_far": 5.193909144279066e+16,
      "budget_used_percent": 51.939091442790655
    },
    {
      "type": "training",
      "description": "Training step 2185",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:20",
      "total_flops_so_far": 5.196285567499904e+16,
      "budget_used_percent": 51.96285567499904
    },
    {
      "type": "training",
      "description": "Training step 2186",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:22",
      "total_flops_so_far": 5.198661990720742e+16,
      "budget_used_percent": 51.98661990720742
    },
    {
      "type": "training",
      "description": "Training step 2187",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:23",
      "total_flops_so_far": 5.201038413941581e+16,
      "budget_used_percent": 52.01038413941581
    },
    {
      "type": "training",
      "description": "Training step 2188",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:24",
      "total_flops_so_far": 5.203414837162419e+16,
      "budget_used_percent": 52.03414837162419
    },
    {
      "type": "training",
      "description": "Training step 2189",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:26",
      "total_flops_so_far": 5.205791260383258e+16,
      "budget_used_percent": 52.05791260383258
    },
    {
      "type": "training",
      "description": "Training step 2190",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:27",
      "total_flops_so_far": 5.208167683604096e+16,
      "budget_used_percent": 52.081676836040955
    },
    {
      "type": "training",
      "description": "Training step 2191",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:28",
      "total_flops_so_far": 5.210544106824934e+16,
      "budget_used_percent": 52.10544106824935
    },
    {
      "type": "training",
      "description": "Training step 2192",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:30",
      "total_flops_so_far": 5.212920530045773e+16,
      "budget_used_percent": 52.12920530045773
    },
    {
      "type": "training",
      "description": "Training step 2193",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:31",
      "total_flops_so_far": 5.215296953266611e+16,
      "budget_used_percent": 52.15296953266611
    },
    {
      "type": "training",
      "description": "Training step 2194",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:32",
      "total_flops_so_far": 5.21767337648745e+16,
      "budget_used_percent": 52.17673376487449
    },
    {
      "type": "training",
      "description": "Training step 2195",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:34",
      "total_flops_so_far": 5.220049799708288e+16,
      "budget_used_percent": 52.200497997082884
    },
    {
      "type": "training",
      "description": "Training step 2196",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:35",
      "total_flops_so_far": 5.222426222929126e+16,
      "budget_used_percent": 52.22426222929126
    },
    {
      "type": "training",
      "description": "Training step 2197",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:36",
      "total_flops_so_far": 5.224802646149965e+16,
      "budget_used_percent": 52.248026461499656
    },
    {
      "type": "training",
      "description": "Training step 2198",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:38",
      "total_flops_so_far": 5.227179069370803e+16,
      "budget_used_percent": 52.271790693708034
    },
    {
      "type": "training",
      "description": "Training step 2199",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:39",
      "total_flops_so_far": 5.229555492591642e+16,
      "budget_used_percent": 52.29555492591642
    },
    {
      "type": "training",
      "description": "Training step 2200",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:40",
      "total_flops_so_far": 5.23193191581248e+16,
      "budget_used_percent": 52.3193191581248
    },
    {
      "type": "training",
      "description": "Training step 2201",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:42",
      "total_flops_so_far": 5.234308339033318e+16,
      "budget_used_percent": 52.34308339033319
    },
    {
      "type": "training",
      "description": "Training step 2202",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:43",
      "total_flops_so_far": 5.236684762254157e+16,
      "budget_used_percent": 52.36684762254157
    },
    {
      "type": "training",
      "description": "Training step 2203",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:44",
      "total_flops_so_far": 5.239061185474995e+16,
      "budget_used_percent": 52.39061185474995
    },
    {
      "type": "training",
      "description": "Training step 2204",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:46",
      "total_flops_so_far": 5.241437608695834e+16,
      "budget_used_percent": 52.414376086958335
    },
    {
      "type": "training",
      "description": "Training step 2205",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:47",
      "total_flops_so_far": 5.243814031916672e+16,
      "budget_used_percent": 52.43814031916671
    },
    {
      "type": "training",
      "description": "Training step 2206",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:48",
      "total_flops_so_far": 5.24619045513751e+16,
      "budget_used_percent": 52.461904551375106
    },
    {
      "type": "training",
      "description": "Training step 2207",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:50",
      "total_flops_so_far": 5.248566878358349e+16,
      "budget_used_percent": 52.485668783583485
    },
    {
      "type": "training",
      "description": "Training step 2208",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:51",
      "total_flops_so_far": 5.250943301579187e+16,
      "budget_used_percent": 52.50943301579187
    },
    {
      "type": "training",
      "description": "Training step 2209",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:52",
      "total_flops_so_far": 5.253319724800026e+16,
      "budget_used_percent": 52.53319724800025
    },
    {
      "type": "training",
      "description": "Training step 2210",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:54",
      "total_flops_so_far": 5.255696148020864e+16,
      "budget_used_percent": 52.55696148020864
    },
    {
      "type": "training",
      "description": "Training step 2211",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:55",
      "total_flops_so_far": 5.258072571241702e+16,
      "budget_used_percent": 52.58072571241702
    },
    {
      "type": "training",
      "description": "Training step 2212",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:56",
      "total_flops_so_far": 5.260448994462541e+16,
      "budget_used_percent": 52.60448994462541
    },
    {
      "type": "training",
      "description": "Training step 2213",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:58",
      "total_flops_so_far": 5.262825417683379e+16,
      "budget_used_percent": 52.62825417683379
    },
    {
      "type": "training",
      "description": "Training step 2214",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 07:59:59",
      "total_flops_so_far": 5.265201840904218e+16,
      "budget_used_percent": 52.65201840904218
    },
    {
      "type": "training",
      "description": "Training step 2215",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:00",
      "total_flops_so_far": 5.267578264125056e+16,
      "budget_used_percent": 52.67578264125056
    },
    {
      "type": "training",
      "description": "Training step 2216",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:02",
      "total_flops_so_far": 5.269954687345894e+16,
      "budget_used_percent": 52.69954687345895
    },
    {
      "type": "training",
      "description": "Training step 2217",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:03",
      "total_flops_so_far": 5.272331110566733e+16,
      "budget_used_percent": 52.72331110566733
    },
    {
      "type": "training",
      "description": "Training step 2218",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:04",
      "total_flops_so_far": 5.274707533787571e+16,
      "budget_used_percent": 52.747075337875714
    },
    {
      "type": "training",
      "description": "Training step 2219",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:08",
      "total_flops_so_far": 5.27708395700841e+16,
      "budget_used_percent": 52.77083957008409
    },
    {
      "type": "training",
      "description": "Training step 2220",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:10",
      "total_flops_so_far": 5.279460380229248e+16,
      "budget_used_percent": 52.794603802292485
    },
    {
      "type": "training",
      "description": "Training step 2221",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:11",
      "total_flops_so_far": 5.281836803450086e+16,
      "budget_used_percent": 52.818368034500864
    },
    {
      "type": "training",
      "description": "Training step 2222",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:13",
      "total_flops_so_far": 5.284213226670925e+16,
      "budget_used_percent": 52.84213226670925
    },
    {
      "type": "training",
      "description": "Training step 2223",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:14",
      "total_flops_so_far": 5.286589649891763e+16,
      "budget_used_percent": 52.86589649891763
    },
    {
      "type": "training",
      "description": "Training step 2224",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:15",
      "total_flops_so_far": 5.288966073112602e+16,
      "budget_used_percent": 52.88966073112602
    },
    {
      "type": "training",
      "description": "Training step 2225",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:17",
      "total_flops_so_far": 5.29134249633344e+16,
      "budget_used_percent": 52.9134249633344
    },
    {
      "type": "training",
      "description": "Training step 2226",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:18",
      "total_flops_so_far": 5.293718919554278e+16,
      "budget_used_percent": 52.937189195542786
    },
    {
      "type": "training",
      "description": "Training step 2227",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:19",
      "total_flops_so_far": 5.296095342775117e+16,
      "budget_used_percent": 52.960953427751164
    },
    {
      "type": "training",
      "description": "Training step 2228",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:21",
      "total_flops_so_far": 5.298471765995955e+16,
      "budget_used_percent": 52.98471765995956
    },
    {
      "type": "training",
      "description": "Training step 2229",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:22",
      "total_flops_so_far": 5.300848189216794e+16,
      "budget_used_percent": 53.008481892167936
    },
    {
      "type": "training",
      "description": "Training step 2230",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:23",
      "total_flops_so_far": 5.303224612437632e+16,
      "budget_used_percent": 53.03224612437633
    },
    {
      "type": "training",
      "description": "Training step 2231",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:25",
      "total_flops_so_far": 5.30560103565847e+16,
      "budget_used_percent": 53.05601035658471
    },
    {
      "type": "training",
      "description": "Training step 2232",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:26",
      "total_flops_so_far": 5.307977458879309e+16,
      "budget_used_percent": 53.079774588793086
    },
    {
      "type": "training",
      "description": "Training step 2233",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:27",
      "total_flops_so_far": 5.310353882100147e+16,
      "budget_used_percent": 53.10353882100147
    },
    {
      "type": "training",
      "description": "Training step 2234",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:29",
      "total_flops_so_far": 5.312730305320986e+16,
      "budget_used_percent": 53.12730305320985
    },
    {
      "type": "training",
      "description": "Training step 2235",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:30",
      "total_flops_so_far": 5.315106728541824e+16,
      "budget_used_percent": 53.15106728541824
    },
    {
      "type": "training",
      "description": "Training step 2236",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:31",
      "total_flops_so_far": 5.317483151762662e+16,
      "budget_used_percent": 53.17483151762662
    },
    {
      "type": "training",
      "description": "Training step 2237",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:33",
      "total_flops_so_far": 5.319859574983501e+16,
      "budget_used_percent": 53.19859574983501
    },
    {
      "type": "training",
      "description": "Training step 2238",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:34",
      "total_flops_so_far": 5.322235998204339e+16,
      "budget_used_percent": 53.222359982043386
    },
    {
      "type": "training",
      "description": "Training step 2239",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:35",
      "total_flops_so_far": 5.324612421425178e+16,
      "budget_used_percent": 53.24612421425178
    },
    {
      "type": "training",
      "description": "Training step 2240",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:36",
      "total_flops_so_far": 5.326988844646016e+16,
      "budget_used_percent": 53.26988844646016
    },
    {
      "type": "training",
      "description": "Training step 2241",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:38",
      "total_flops_so_far": 5.329365267866854e+16,
      "budget_used_percent": 53.29365267866854
    },
    {
      "type": "training",
      "description": "Training step 2242",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:39",
      "total_flops_so_far": 5.331741691087693e+16,
      "budget_used_percent": 53.31741691087692
    },
    {
      "type": "training",
      "description": "Training step 2243",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:40",
      "total_flops_so_far": 5.334118114308531e+16,
      "budget_used_percent": 53.341181143085315
    },
    {
      "type": "training",
      "description": "Training step 2244",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:41",
      "total_flops_so_far": 5.33649453752937e+16,
      "budget_used_percent": 53.36494537529369
    },
    {
      "type": "training",
      "description": "Training step 2245",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:42",
      "total_flops_so_far": 5.338870960750208e+16,
      "budget_used_percent": 53.38870960750208
    },
    {
      "type": "training",
      "description": "Training step 2246",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:44",
      "total_flops_so_far": 5.341247383971046e+16,
      "budget_used_percent": 53.41247383971046
    },
    {
      "type": "training",
      "description": "Training step 2247",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:45",
      "total_flops_so_far": 5.343623807191885e+16,
      "budget_used_percent": 53.43623807191885
    },
    {
      "type": "training",
      "description": "Training step 2248",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:46",
      "total_flops_so_far": 5.346000230412723e+16,
      "budget_used_percent": 53.46000230412723
    },
    {
      "type": "training",
      "description": "Training step 2249",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:48",
      "total_flops_so_far": 5.348376653633562e+16,
      "budget_used_percent": 53.48376653633562
    },
    {
      "type": "training",
      "description": "Training step 2250",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:49",
      "total_flops_so_far": 5.3507530768544e+16,
      "budget_used_percent": 53.507530768544
    },
    {
      "type": "training",
      "description": "Training step 2251",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:50",
      "total_flops_so_far": 5.353129500075238e+16,
      "budget_used_percent": 53.53129500075239
    },
    {
      "type": "training",
      "description": "Training step 2252",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:52",
      "total_flops_so_far": 5.355505923296077e+16,
      "budget_used_percent": 53.555059232960765
    },
    {
      "type": "training",
      "description": "Training step 2253",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:53",
      "total_flops_so_far": 5.357882346516915e+16,
      "budget_used_percent": 53.57882346516916
    },
    {
      "type": "training",
      "description": "Training step 2254",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:54",
      "total_flops_so_far": 5.360258769737754e+16,
      "budget_used_percent": 53.60258769737754
    },
    {
      "type": "training",
      "description": "Training step 2255",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:56",
      "total_flops_so_far": 5.362635192958592e+16,
      "budget_used_percent": 53.62635192958592
    },
    {
      "type": "training",
      "description": "Training step 2256",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:57",
      "total_flops_so_far": 5.36501161617943e+16,
      "budget_used_percent": 53.6501161617943
    },
    {
      "type": "training",
      "description": "Training step 2257",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:00:58",
      "total_flops_so_far": 5.367388039400269e+16,
      "budget_used_percent": 53.673880394002694
    },
    {
      "type": "training",
      "description": "Training step 2258",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:00",
      "total_flops_so_far": 5.369764462621107e+16,
      "budget_used_percent": 53.69764462621107
    },
    {
      "type": "training",
      "description": "Training step 2259",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:01",
      "total_flops_so_far": 5.372140885841946e+16,
      "budget_used_percent": 53.72140885841945
    },
    {
      "type": "training",
      "description": "Training step 2260",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:02",
      "total_flops_so_far": 5.374517309062784e+16,
      "budget_used_percent": 53.74517309062784
    },
    {
      "type": "training",
      "description": "Training step 2261",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:04",
      "total_flops_so_far": 5.376893732283622e+16,
      "budget_used_percent": 53.768937322836216
    },
    {
      "type": "training",
      "description": "Training step 2262",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:05",
      "total_flops_so_far": 5.379270155504461e+16,
      "budget_used_percent": 53.79270155504461
    },
    {
      "type": "training",
      "description": "Training step 2263",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:06",
      "total_flops_so_far": 5.381646578725299e+16,
      "budget_used_percent": 53.81646578725299
    },
    {
      "type": "training",
      "description": "Training step 2264",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:07",
      "total_flops_so_far": 5.384023001946138e+16,
      "budget_used_percent": 53.84023001946138
    },
    {
      "type": "training",
      "description": "Training step 2265",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:09",
      "total_flops_so_far": 5.386399425166976e+16,
      "budget_used_percent": 53.86399425166976
    },
    {
      "type": "training",
      "description": "Training step 2266",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:10",
      "total_flops_so_far": 5.388775848387814e+16,
      "budget_used_percent": 53.887758483878144
    },
    {
      "type": "training",
      "description": "Training step 2267",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:11",
      "total_flops_so_far": 5.391152271608653e+16,
      "budget_used_percent": 53.91152271608652
    },
    {
      "type": "training",
      "description": "Training step 2268",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:13",
      "total_flops_so_far": 5.393528694829491e+16,
      "budget_used_percent": 53.935286948294916
    },
    {
      "type": "training",
      "description": "Training step 2269",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:14",
      "total_flops_so_far": 5.39590511805033e+16,
      "budget_used_percent": 53.959051180503295
    },
    {
      "type": "training",
      "description": "Training step 2270",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:15",
      "total_flops_so_far": 5.398281541271168e+16,
      "budget_used_percent": 53.98281541271168
    },
    {
      "type": "training",
      "description": "Training step 2271",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:17",
      "total_flops_so_far": 5.400657964492006e+16,
      "budget_used_percent": 54.00657964492006
    },
    {
      "type": "training",
      "description": "Training step 2272",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:18",
      "total_flops_so_far": 5.403034387712845e+16,
      "budget_used_percent": 54.03034387712845
    },
    {
      "type": "training",
      "description": "Training step 2273",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:19",
      "total_flops_so_far": 5.405410810933683e+16,
      "budget_used_percent": 54.05410810933683
    },
    {
      "type": "training",
      "description": "Training step 2274",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:21",
      "total_flops_so_far": 5.407787234154522e+16,
      "budget_used_percent": 54.077872341545216
    },
    {
      "type": "training",
      "description": "Training step 2275",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:22",
      "total_flops_so_far": 5.41016365737536e+16,
      "budget_used_percent": 54.101636573753595
    },
    {
      "type": "training",
      "description": "Training step 2276",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:23",
      "total_flops_so_far": 5.412540080596198e+16,
      "budget_used_percent": 54.12540080596199
    },
    {
      "type": "training",
      "description": "Training step 2277",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:25",
      "total_flops_so_far": 5.414916503817037e+16,
      "budget_used_percent": 54.149165038170366
    },
    {
      "type": "training",
      "description": "Training step 2278",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:26",
      "total_flops_so_far": 5.417292927037875e+16,
      "budget_used_percent": 54.17292927037876
    },
    {
      "type": "training",
      "description": "Training step 2279",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:28",
      "total_flops_so_far": 5.419669350258714e+16,
      "budget_used_percent": 54.19669350258714
    },
    {
      "type": "training",
      "description": "Training step 2280",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:29",
      "total_flops_so_far": 5.422045773479552e+16,
      "budget_used_percent": 54.220457734795524
    },
    {
      "type": "training",
      "description": "Training step 2281",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:30",
      "total_flops_so_far": 5.42442219670039e+16,
      "budget_used_percent": 54.2442219670039
    },
    {
      "type": "training",
      "description": "Training step 2282",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:32",
      "total_flops_so_far": 5.426798619921229e+16,
      "budget_used_percent": 54.267986199212295
    },
    {
      "type": "training",
      "description": "Training step 2283",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:33",
      "total_flops_so_far": 5.429175043142067e+16,
      "budget_used_percent": 54.291750431420674
    },
    {
      "type": "training",
      "description": "Training step 2284",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:34",
      "total_flops_so_far": 5.431551466362906e+16,
      "budget_used_percent": 54.31551466362906
    },
    {
      "type": "training",
      "description": "Training step 2285",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:36",
      "total_flops_so_far": 5.433927889583744e+16,
      "budget_used_percent": 54.33927889583744
    },
    {
      "type": "training",
      "description": "Training step 2286",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:37",
      "total_flops_so_far": 5.436304312804582e+16,
      "budget_used_percent": 54.36304312804583
    },
    {
      "type": "training",
      "description": "Training step 2287",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:38",
      "total_flops_so_far": 5.438680736025421e+16,
      "budget_used_percent": 54.38680736025421
    },
    {
      "type": "training",
      "description": "Training step 2288",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:40",
      "total_flops_so_far": 5.441057159246259e+16,
      "budget_used_percent": 54.41057159246259
    },
    {
      "type": "training",
      "description": "Training step 2289",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:41",
      "total_flops_so_far": 5.443433582467098e+16,
      "budget_used_percent": 54.434335824670974
    },
    {
      "type": "training",
      "description": "Training step 2290",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:42",
      "total_flops_so_far": 5.445810005687936e+16,
      "budget_used_percent": 54.45810005687935
    },
    {
      "type": "training",
      "description": "Training step 2291",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:44",
      "total_flops_so_far": 5.448186428908774e+16,
      "budget_used_percent": 54.481864289087746
    },
    {
      "type": "training",
      "description": "Training step 2292",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:45",
      "total_flops_so_far": 5.450562852129613e+16,
      "budget_used_percent": 54.505628521296124
    },
    {
      "type": "training",
      "description": "Training step 2293",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:46",
      "total_flops_so_far": 5.452939275350451e+16,
      "budget_used_percent": 54.52939275350451
    },
    {
      "type": "training",
      "description": "Training step 2294",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:48",
      "total_flops_so_far": 5.45531569857129e+16,
      "budget_used_percent": 54.55315698571289
    },
    {
      "type": "training",
      "description": "Training step 2295",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:49",
      "total_flops_so_far": 5.457692121792128e+16,
      "budget_used_percent": 54.57692121792128
    },
    {
      "type": "training",
      "description": "Training step 2296",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:50",
      "total_flops_so_far": 5.460068545012966e+16,
      "budget_used_percent": 54.60068545012966
    },
    {
      "type": "training",
      "description": "Training step 2297",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:52",
      "total_flops_so_far": 5.462444968233805e+16,
      "budget_used_percent": 54.62444968233805
    },
    {
      "type": "training",
      "description": "Training step 2298",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:53",
      "total_flops_so_far": 5.464821391454643e+16,
      "budget_used_percent": 54.64821391454643
    },
    {
      "type": "training",
      "description": "Training step 2299",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:54",
      "total_flops_so_far": 5.467197814675482e+16,
      "budget_used_percent": 54.67197814675482
    },
    {
      "type": "training",
      "description": "Training step 2300",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:56",
      "total_flops_so_far": 5.46957423789632e+16,
      "budget_used_percent": 54.695742378963196
    },
    {
      "type": "training",
      "description": "Training step 2301",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:57",
      "total_flops_so_far": 5.471950661117158e+16,
      "budget_used_percent": 54.71950661117159
    },
    {
      "type": "training",
      "description": "Training step 2302",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:01:58",
      "total_flops_so_far": 5.474327084337997e+16,
      "budget_used_percent": 54.74327084337997
    },
    {
      "type": "training",
      "description": "Training step 2303",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:00",
      "total_flops_so_far": 5.476703507558835e+16,
      "budget_used_percent": 54.76703507558835
    },
    {
      "type": "training",
      "description": "Training step 2304",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:01",
      "total_flops_so_far": 5.479079930779674e+16,
      "budget_used_percent": 54.79079930779673
    },
    {
      "type": "training",
      "description": "Training step 2305",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:02",
      "total_flops_so_far": 5.481456354000512e+16,
      "budget_used_percent": 54.814563540005125
    },
    {
      "type": "training",
      "description": "Training step 2306",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:04",
      "total_flops_so_far": 5.48383277722135e+16,
      "budget_used_percent": 54.8383277722135
    },
    {
      "type": "training",
      "description": "Training step 2307",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:05",
      "total_flops_so_far": 5.486209200442189e+16,
      "budget_used_percent": 54.86209200442189
    },
    {
      "type": "training",
      "description": "Training step 2308",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:06",
      "total_flops_so_far": 5.488585623663027e+16,
      "budget_used_percent": 54.88585623663027
    },
    {
      "type": "training",
      "description": "Training step 2309",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:08",
      "total_flops_so_far": 5.490962046883866e+16,
      "budget_used_percent": 54.90962046883866
    },
    {
      "type": "training",
      "description": "Training step 2310",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:09",
      "total_flops_so_far": 5.493338470104704e+16,
      "budget_used_percent": 54.93338470104704
    },
    {
      "type": "training",
      "description": "Training step 2311",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:10",
      "total_flops_so_far": 5.495714893325542e+16,
      "budget_used_percent": 54.957148933255425
    },
    {
      "type": "training",
      "description": "Training step 2312",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:12",
      "total_flops_so_far": 5.498091316546381e+16,
      "budget_used_percent": 54.980913165463804
    },
    {
      "type": "training",
      "description": "Training step 2313",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:13",
      "total_flops_so_far": 5.500467739767219e+16,
      "budget_used_percent": 55.0046773976722
    },
    {
      "type": "training",
      "description": "Training step 2314",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:14",
      "total_flops_so_far": 5.502844162988058e+16,
      "budget_used_percent": 55.028441629880575
    },
    {
      "type": "training",
      "description": "Training step 2315",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:16",
      "total_flops_so_far": 5.505220586208896e+16,
      "budget_used_percent": 55.05220586208897
    },
    {
      "type": "training",
      "description": "Training step 2316",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:17",
      "total_flops_so_far": 5.507597009429734e+16,
      "budget_used_percent": 55.07597009429735
    },
    {
      "type": "training",
      "description": "Training step 2317",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:18",
      "total_flops_so_far": 5.509973432650573e+16,
      "budget_used_percent": 55.099734326505725
    },
    {
      "type": "training",
      "description": "Training step 2318",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:20",
      "total_flops_so_far": 5.512349855871411e+16,
      "budget_used_percent": 55.12349855871411
    },
    {
      "type": "training",
      "description": "Training step 2319",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:21",
      "total_flops_so_far": 5.51472627909225e+16,
      "budget_used_percent": 55.14726279092249
    },
    {
      "type": "training",
      "description": "Training step 2320",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:22",
      "total_flops_so_far": 5.517102702313088e+16,
      "budget_used_percent": 55.17102702313088
    },
    {
      "type": "training",
      "description": "Training step 2321",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:24",
      "total_flops_so_far": 5.519479125533926e+16,
      "budget_used_percent": 55.19479125533926
    },
    {
      "type": "training",
      "description": "Training step 2322",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:25",
      "total_flops_so_far": 5.521855548754765e+16,
      "budget_used_percent": 55.21855548754765
    },
    {
      "type": "training",
      "description": "Training step 2323",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:26",
      "total_flops_so_far": 5.524231971975603e+16,
      "budget_used_percent": 55.242319719756026
    },
    {
      "type": "training",
      "description": "Training step 2324",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:28",
      "total_flops_so_far": 5.526608395196442e+16,
      "budget_used_percent": 55.26608395196442
    },
    {
      "type": "training",
      "description": "Training step 2325",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:29",
      "total_flops_so_far": 5.52898481841728e+16,
      "budget_used_percent": 55.2898481841728
    },
    {
      "type": "training",
      "description": "Training step 2326",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:30",
      "total_flops_so_far": 5.531361241638118e+16,
      "budget_used_percent": 55.31361241638118
    },
    {
      "type": "training",
      "description": "Training step 2327",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:31",
      "total_flops_so_far": 5.533737664858957e+16,
      "budget_used_percent": 55.33737664858956
    },
    {
      "type": "training",
      "description": "Training step 2328",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:33",
      "total_flops_so_far": 5.536114088079795e+16,
      "budget_used_percent": 55.361140880797954
    },
    {
      "type": "training",
      "description": "Training step 2329",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:34",
      "total_flops_so_far": 5.538490511300634e+16,
      "budget_used_percent": 55.38490511300633
    },
    {
      "type": "training",
      "description": "Training step 2330",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:35",
      "total_flops_so_far": 5.540866934521472e+16,
      "budget_used_percent": 55.408669345214726
    },
    {
      "type": "training",
      "description": "Training step 2331",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:37",
      "total_flops_so_far": 5.54324335774231e+16,
      "budget_used_percent": 55.432433577423105
    },
    {
      "type": "training",
      "description": "Training step 2332",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:38",
      "total_flops_so_far": 5.545619780963149e+16,
      "budget_used_percent": 55.45619780963149
    },
    {
      "type": "training",
      "description": "Training step 2333",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:39",
      "total_flops_so_far": 5.547996204183987e+16,
      "budget_used_percent": 55.47996204183987
    },
    {
      "type": "training",
      "description": "Training step 2334",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:41",
      "total_flops_so_far": 5.550372627404826e+16,
      "budget_used_percent": 55.50372627404826
    },
    {
      "type": "training",
      "description": "Training step 2335",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:42",
      "total_flops_so_far": 5.552749050625664e+16,
      "budget_used_percent": 55.52749050625664
    },
    {
      "type": "training",
      "description": "Training step 2336",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:43",
      "total_flops_so_far": 5.555125473846502e+16,
      "budget_used_percent": 55.551254738465026
    },
    {
      "type": "training",
      "description": "Training step 2337",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:45",
      "total_flops_so_far": 5.557501897067341e+16,
      "budget_used_percent": 55.575018970673405
    },
    {
      "type": "training",
      "description": "Training step 2338",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:46",
      "total_flops_so_far": 5.559878320288179e+16,
      "budget_used_percent": 55.5987832028818
    },
    {
      "type": "training",
      "description": "Training step 2339",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:47",
      "total_flops_so_far": 5.562254743509018e+16,
      "budget_used_percent": 55.622547435090176
    },
    {
      "type": "training",
      "description": "Training step 2340",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:49",
      "total_flops_so_far": 5.564631166729856e+16,
      "budget_used_percent": 55.64631166729856
    },
    {
      "type": "training",
      "description": "Training step 2341",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:50",
      "total_flops_so_far": 5.567007589950694e+16,
      "budget_used_percent": 55.67007589950694
    },
    {
      "type": "training",
      "description": "Training step 2342",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:51",
      "total_flops_so_far": 5.569384013171533e+16,
      "budget_used_percent": 55.69384013171533
    },
    {
      "type": "training",
      "description": "Training step 2343",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:53",
      "total_flops_so_far": 5.571760436392371e+16,
      "budget_used_percent": 55.71760436392371
    },
    {
      "type": "training",
      "description": "Training step 2344",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:54",
      "total_flops_so_far": 5.57413685961321e+16,
      "budget_used_percent": 55.741368596132105
    },
    {
      "type": "training",
      "description": "Training step 2345",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:55",
      "total_flops_so_far": 5.576513282834048e+16,
      "budget_used_percent": 55.765132828340484
    },
    {
      "type": "training",
      "description": "Training step 2346",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:57",
      "total_flops_so_far": 5.578889706054886e+16,
      "budget_used_percent": 55.78889706054886
    },
    {
      "type": "training",
      "description": "Training step 2347",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:58",
      "total_flops_so_far": 5.581266129275725e+16,
      "budget_used_percent": 55.81266129275725
    },
    {
      "type": "training",
      "description": "Training step 2348",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:02:59",
      "total_flops_so_far": 5.583642552496563e+16,
      "budget_used_percent": 55.83642552496563
    },
    {
      "type": "training",
      "description": "Training step 2349",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:01",
      "total_flops_so_far": 5.586018975717402e+16,
      "budget_used_percent": 55.86018975717402
    },
    {
      "type": "training",
      "description": "Training step 2350",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:02",
      "total_flops_so_far": 5.58839539893824e+16,
      "budget_used_percent": 55.8839539893824
    },
    {
      "type": "training",
      "description": "Training step 2351",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:03",
      "total_flops_so_far": 5.590771822159078e+16,
      "budget_used_percent": 55.907718221590784
    },
    {
      "type": "training",
      "description": "Training step 2352",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:05",
      "total_flops_so_far": 5.593148245379917e+16,
      "budget_used_percent": 55.93148245379916
    },
    {
      "type": "training",
      "description": "Training step 2353",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:06",
      "total_flops_so_far": 5.595524668600755e+16,
      "budget_used_percent": 55.955246686007555
    },
    {
      "type": "training",
      "description": "Training step 2354",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:08",
      "total_flops_so_far": 5.597901091821594e+16,
      "budget_used_percent": 55.979010918215934
    },
    {
      "type": "training",
      "description": "Training step 2355",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:09",
      "total_flops_so_far": 5.600277515042432e+16,
      "budget_used_percent": 56.00277515042432
    },
    {
      "type": "training",
      "description": "Training step 2356",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:10",
      "total_flops_so_far": 5.60265393826327e+16,
      "budget_used_percent": 56.0265393826327
    },
    {
      "type": "training",
      "description": "Training step 2357",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:12",
      "total_flops_so_far": 5.605030361484109e+16,
      "budget_used_percent": 56.05030361484109
    },
    {
      "type": "training",
      "description": "Training step 2358",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:13",
      "total_flops_so_far": 5.607406784704947e+16,
      "budget_used_percent": 56.07406784704947
    },
    {
      "type": "training",
      "description": "Training step 2359",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:14",
      "total_flops_so_far": 5.609783207925786e+16,
      "budget_used_percent": 56.097832079257856
    },
    {
      "type": "training",
      "description": "Training step 2360",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:16",
      "total_flops_so_far": 5.612159631146624e+16,
      "budget_used_percent": 56.121596311466234
    },
    {
      "type": "training",
      "description": "Training step 2361",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:17",
      "total_flops_so_far": 5.614536054367462e+16,
      "budget_used_percent": 56.14536054367463
    },
    {
      "type": "training",
      "description": "Training step 2362",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:18",
      "total_flops_so_far": 5.616912477588301e+16,
      "budget_used_percent": 56.169124775883006
    },
    {
      "type": "training",
      "description": "Training step 2363",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:20",
      "total_flops_so_far": 5.619288900809139e+16,
      "budget_used_percent": 56.1928890080914
    },
    {
      "type": "training",
      "description": "Training step 2364",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:21",
      "total_flops_so_far": 5.621665324029978e+16,
      "budget_used_percent": 56.21665324029978
    },
    {
      "type": "training",
      "description": "Training step 2365",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:22",
      "total_flops_so_far": 5.624041747250816e+16,
      "budget_used_percent": 56.24041747250816
    },
    {
      "type": "training",
      "description": "Training step 2366",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:24",
      "total_flops_so_far": 5.626418170471654e+16,
      "budget_used_percent": 56.26418170471654
    },
    {
      "type": "training",
      "description": "Training step 2367",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:25",
      "total_flops_so_far": 5.628794593692493e+16,
      "budget_used_percent": 56.287945936924935
    },
    {
      "type": "training",
      "description": "Training step 2368",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:26",
      "total_flops_so_far": 5.631171016913331e+16,
      "budget_used_percent": 56.31171016913331
    },
    {
      "type": "training",
      "description": "Training step 2369",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:28",
      "total_flops_so_far": 5.63354744013417e+16,
      "budget_used_percent": 56.3354744013417
    },
    {
      "type": "training",
      "description": "Training step 2370",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:29",
      "total_flops_so_far": 5.635923863355008e+16,
      "budget_used_percent": 56.35923863355008
    },
    {
      "type": "training",
      "description": "Training step 2371",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:30",
      "total_flops_so_far": 5.638300286575846e+16,
      "budget_used_percent": 56.38300286575847
    },
    {
      "type": "training",
      "description": "Training step 2372",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:32",
      "total_flops_so_far": 5.640676709796685e+16,
      "budget_used_percent": 56.40676709796685
    },
    {
      "type": "training",
      "description": "Training step 2373",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:33",
      "total_flops_so_far": 5.643053133017523e+16,
      "budget_used_percent": 56.43053133017523
    },
    {
      "type": "training",
      "description": "Training step 2374",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:34",
      "total_flops_so_far": 5.645429556238362e+16,
      "budget_used_percent": 56.454295562383614
    },
    {
      "type": "training",
      "description": "Training step 2375",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:36",
      "total_flops_so_far": 5.6478059794592e+16,
      "budget_used_percent": 56.47805979459199
    },
    {
      "type": "training",
      "description": "Training step 2376",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:36",
      "total_flops_so_far": 5.650182402680038e+16,
      "budget_used_percent": 56.501824026800385
    },
    {
      "type": "training",
      "description": "Training step 2377",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:37",
      "total_flops_so_far": 5.652558825900877e+16,
      "budget_used_percent": 56.525588259008764
    },
    {
      "type": "training",
      "description": "Training step 2378",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:39",
      "total_flops_so_far": 5.654935249121715e+16,
      "budget_used_percent": 56.54935249121715
    },
    {
      "type": "training",
      "description": "Training step 2379",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:40",
      "total_flops_so_far": 5.657311672342554e+16,
      "budget_used_percent": 56.57311672342553
    },
    {
      "type": "training",
      "description": "Training step 2380",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:41",
      "total_flops_so_far": 5.659688095563392e+16,
      "budget_used_percent": 56.59688095563392
    },
    {
      "type": "training",
      "description": "Training step 2381",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:43",
      "total_flops_so_far": 5.66206451878423e+16,
      "budget_used_percent": 56.6206451878423
    },
    {
      "type": "training",
      "description": "Training step 2382",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:44",
      "total_flops_so_far": 5.664440942005069e+16,
      "budget_used_percent": 56.64440942005069
    },
    {
      "type": "training",
      "description": "Training step 2383",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:45",
      "total_flops_so_far": 5.666817365225907e+16,
      "budget_used_percent": 56.66817365225907
    },
    {
      "type": "training",
      "description": "Training step 2384",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:47",
      "total_flops_so_far": 5.669193788446746e+16,
      "budget_used_percent": 56.69193788446746
    },
    {
      "type": "training",
      "description": "Training step 2385",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:48",
      "total_flops_so_far": 5.671570211667584e+16,
      "budget_used_percent": 56.715702116675835
    },
    {
      "type": "training",
      "description": "Training step 2386",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:49",
      "total_flops_so_far": 5.673946634888422e+16,
      "budget_used_percent": 56.73946634888423
    },
    {
      "type": "training",
      "description": "Training step 2387",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:51",
      "total_flops_so_far": 5.676323058109261e+16,
      "budget_used_percent": 56.76323058109261
    },
    {
      "type": "training",
      "description": "Training step 2388",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:52",
      "total_flops_so_far": 5.678699481330099e+16,
      "budget_used_percent": 56.78699481330099
    },
    {
      "type": "training",
      "description": "Training step 2389",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:53",
      "total_flops_so_far": 5.681075904550938e+16,
      "budget_used_percent": 56.81075904550937
    },
    {
      "type": "training",
      "description": "Training step 2390",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:55",
      "total_flops_so_far": 5.683452327771776e+16,
      "budget_used_percent": 56.834523277717764
    },
    {
      "type": "training",
      "description": "Training step 2391",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:56",
      "total_flops_so_far": 5.685828750992614e+16,
      "budget_used_percent": 56.85828750992614
    },
    {
      "type": "training",
      "description": "Training step 2392",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:57",
      "total_flops_so_far": 5.688205174213453e+16,
      "budget_used_percent": 56.88205174213453
    },
    {
      "type": "training",
      "description": "Training step 2393",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:03:59",
      "total_flops_so_far": 5.690581597434291e+16,
      "budget_used_percent": 56.90581597434291
    },
    {
      "type": "training",
      "description": "Training step 2394",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:00",
      "total_flops_so_far": 5.69295802065513e+16,
      "budget_used_percent": 56.9295802065513
    },
    {
      "type": "training",
      "description": "Training step 2395",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:01",
      "total_flops_so_far": 5.695334443875968e+16,
      "budget_used_percent": 56.95334443875968
    },
    {
      "type": "training",
      "description": "Training step 2396",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:03",
      "total_flops_so_far": 5.697710867096806e+16,
      "budget_used_percent": 56.97710867096807
    },
    {
      "type": "training",
      "description": "Training step 2397",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:04",
      "total_flops_so_far": 5.700087290317645e+16,
      "budget_used_percent": 57.00087290317645
    },
    {
      "type": "training",
      "description": "Training step 2398",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:05",
      "total_flops_so_far": 5.702463713538483e+16,
      "budget_used_percent": 57.024637135384836
    },
    {
      "type": "training",
      "description": "Training step 2399",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:07",
      "total_flops_so_far": 5.704840136759322e+16,
      "budget_used_percent": 57.048401367593215
    },
    {
      "type": "training",
      "description": "Training step 2400",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:08",
      "total_flops_so_far": 5.70721655998016e+16,
      "budget_used_percent": 57.07216559980161
    },
    {
      "type": "training",
      "description": "Training step 2401",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:09",
      "total_flops_so_far": 5.709592983200998e+16,
      "budget_used_percent": 57.095929832009986
    },
    {
      "type": "training",
      "description": "Training step 2402",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:11",
      "total_flops_so_far": 5.711969406421837e+16,
      "budget_used_percent": 57.119694064218365
    },
    {
      "type": "training",
      "description": "Training step 2403",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:12",
      "total_flops_so_far": 5.714345829642675e+16,
      "budget_used_percent": 57.14345829642675
    },
    {
      "type": "training",
      "description": "Training step 2404",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:13",
      "total_flops_so_far": 5.716722252863514e+16,
      "budget_used_percent": 57.16722252863513
    },
    {
      "type": "training",
      "description": "Training step 2405",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:15",
      "total_flops_so_far": 5.719098676084352e+16,
      "budget_used_percent": 57.19098676084352
    },
    {
      "type": "training",
      "description": "Training step 2406",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:16",
      "total_flops_so_far": 5.72147509930519e+16,
      "budget_used_percent": 57.2147509930519
    },
    {
      "type": "training",
      "description": "Training step 2407",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:17",
      "total_flops_so_far": 5.723851522526029e+16,
      "budget_used_percent": 57.23851522526029
    },
    {
      "type": "training",
      "description": "Training step 2408",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:19",
      "total_flops_so_far": 5.726227945746867e+16,
      "budget_used_percent": 57.262279457468665
    },
    {
      "type": "training",
      "description": "Training step 2409",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:20",
      "total_flops_so_far": 5.728604368967706e+16,
      "budget_used_percent": 57.28604368967706
    },
    {
      "type": "training",
      "description": "Training step 2410",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:21",
      "total_flops_so_far": 5.730980792188544e+16,
      "budget_used_percent": 57.30980792188544
    },
    {
      "type": "training",
      "description": "Training step 2411",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:23",
      "total_flops_so_far": 5.733357215409382e+16,
      "budget_used_percent": 57.33357215409383
    },
    {
      "type": "training",
      "description": "Training step 2412",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:24",
      "total_flops_so_far": 5.735733638630221e+16,
      "budget_used_percent": 57.35733638630221
    },
    {
      "type": "training",
      "description": "Training step 2413",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:25",
      "total_flops_so_far": 5.738110061851059e+16,
      "budget_used_percent": 57.381100618510594
    },
    {
      "type": "training",
      "description": "Training step 2414",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:27",
      "total_flops_so_far": 5.740486485071898e+16,
      "budget_used_percent": 57.40486485071897
    },
    {
      "type": "training",
      "description": "Training step 2415",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:28",
      "total_flops_so_far": 5.742862908292736e+16,
      "budget_used_percent": 57.428629082927365
    },
    {
      "type": "training",
      "description": "Training step 2416",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:30",
      "total_flops_so_far": 5.745239331513574e+16,
      "budget_used_percent": 57.452393315135744
    },
    {
      "type": "training",
      "description": "Training step 2417",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:31",
      "total_flops_so_far": 5.747615754734413e+16,
      "budget_used_percent": 57.47615754734413
    },
    {
      "type": "training",
      "description": "Training step 2418",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:32",
      "total_flops_so_far": 5.749992177955251e+16,
      "budget_used_percent": 57.49992177955251
    },
    {
      "type": "training",
      "description": "Training step 2419",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:34",
      "total_flops_so_far": 5.75236860117609e+16,
      "budget_used_percent": 57.5236860117609
    },
    {
      "type": "training",
      "description": "Training step 2420",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:35",
      "total_flops_so_far": 5.754745024396928e+16,
      "budget_used_percent": 57.54745024396928
    },
    {
      "type": "training",
      "description": "Training step 2421",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:36",
      "total_flops_so_far": 5.757121447617766e+16,
      "budget_used_percent": 57.571214476177666
    },
    {
      "type": "training",
      "description": "Training step 2422",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:38",
      "total_flops_so_far": 5.759497870838605e+16,
      "budget_used_percent": 57.594978708386044
    },
    {
      "type": "training",
      "description": "Training step 2423",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:39",
      "total_flops_so_far": 5.761874294059443e+16,
      "budget_used_percent": 57.61874294059444
    },
    {
      "type": "training",
      "description": "Training step 2424",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:40",
      "total_flops_so_far": 5.764250717280282e+16,
      "budget_used_percent": 57.642507172802816
    },
    {
      "type": "training",
      "description": "Training step 2425",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:42",
      "total_flops_so_far": 5.76662714050112e+16,
      "budget_used_percent": 57.6662714050112
    },
    {
      "type": "training",
      "description": "Training step 2426",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:43",
      "total_flops_so_far": 5.769003563721958e+16,
      "budget_used_percent": 57.69003563721958
    },
    {
      "type": "training",
      "description": "Training step 2427",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:44",
      "total_flops_so_far": 5.771379986942797e+16,
      "budget_used_percent": 57.71379986942797
    },
    {
      "type": "training",
      "description": "Training step 2428",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:46",
      "total_flops_so_far": 5.773756410163635e+16,
      "budget_used_percent": 57.73756410163635
    },
    {
      "type": "training",
      "description": "Training step 2429",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:47",
      "total_flops_so_far": 5.776132833384474e+16,
      "budget_used_percent": 57.761328333844745
    },
    {
      "type": "training",
      "description": "Training step 2430",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:48",
      "total_flops_so_far": 5.778509256605312e+16,
      "budget_used_percent": 57.78509256605312
    },
    {
      "type": "training",
      "description": "Training step 2431",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:50",
      "total_flops_so_far": 5.78088567982615e+16,
      "budget_used_percent": 57.8088567982615
    },
    {
      "type": "training",
      "description": "Training step 2432",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:51",
      "total_flops_so_far": 5.783262103046989e+16,
      "budget_used_percent": 57.83262103046989
    },
    {
      "type": "training",
      "description": "Training step 2433",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:52",
      "total_flops_so_far": 5.785638526267827e+16,
      "budget_used_percent": 57.856385262678266
    },
    {
      "type": "training",
      "description": "Training step 2434",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:54",
      "total_flops_so_far": 5.788014949488666e+16,
      "budget_used_percent": 57.88014949488666
    },
    {
      "type": "training",
      "description": "Training step 2435",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:55",
      "total_flops_so_far": 5.790391372709504e+16,
      "budget_used_percent": 57.90391372709504
    },
    {
      "type": "training",
      "description": "Training step 2436",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:56",
      "total_flops_so_far": 5.792767795930342e+16,
      "budget_used_percent": 57.92767795930342
    },
    {
      "type": "training",
      "description": "Training step 2437",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:58",
      "total_flops_so_far": 5.795144219151181e+16,
      "budget_used_percent": 57.9514421915118
    },
    {
      "type": "training",
      "description": "Training step 2438",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:04:59",
      "total_flops_so_far": 5.797520642372019e+16,
      "budget_used_percent": 57.975206423720195
    },
    {
      "type": "training",
      "description": "Training step 2439",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:00",
      "total_flops_so_far": 5.799897065592858e+16,
      "budget_used_percent": 57.998970655928574
    },
    {
      "type": "training",
      "description": "Training step 2440",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:02",
      "total_flops_so_far": 5.802273488813696e+16,
      "budget_used_percent": 58.02273488813696
    },
    {
      "type": "training",
      "description": "Training step 2441",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:03",
      "total_flops_so_far": 5.804649912034534e+16,
      "budget_used_percent": 58.04649912034534
    },
    {
      "type": "training",
      "description": "Training step 2442",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:04",
      "total_flops_so_far": 5.807026335255373e+16,
      "budget_used_percent": 58.07026335255373
    },
    {
      "type": "training",
      "description": "Training step 2443",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:06",
      "total_flops_so_far": 5.809402758476211e+16,
      "budget_used_percent": 58.09402758476211
    },
    {
      "type": "training",
      "description": "Training step 2444",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:07",
      "total_flops_so_far": 5.81177918169705e+16,
      "budget_used_percent": 58.117791816970495
    },
    {
      "type": "training",
      "description": "Training step 2445",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:08",
      "total_flops_so_far": 5.814155604917888e+16,
      "budget_used_percent": 58.141556049178874
    },
    {
      "type": "training",
      "description": "Training step 2446",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:12",
      "total_flops_so_far": 5.816532028138726e+16,
      "budget_used_percent": 58.16532028138727
    },
    {
      "type": "training",
      "description": "Training step 2447",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:13",
      "total_flops_so_far": 5.818908451359565e+16,
      "budget_used_percent": 58.189084513595645
    },
    {
      "type": "training",
      "description": "Training step 2448",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:14",
      "total_flops_so_far": 5.821284874580403e+16,
      "budget_used_percent": 58.21284874580404
    },
    {
      "type": "training",
      "description": "Training step 2449",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:16",
      "total_flops_so_far": 5.823661297801242e+16,
      "budget_used_percent": 58.23661297801242
    },
    {
      "type": "training",
      "description": "Training step 2450",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:17",
      "total_flops_so_far": 5.82603772102208e+16,
      "budget_used_percent": 58.2603772102208
    },
    {
      "type": "training",
      "description": "Training step 2451",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:18",
      "total_flops_so_far": 5.828414144242918e+16,
      "budget_used_percent": 58.28414144242918
    },
    {
      "type": "training",
      "description": "Training step 2452",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:20",
      "total_flops_so_far": 5.830790567463757e+16,
      "budget_used_percent": 58.307905674637574
    },
    {
      "type": "training",
      "description": "Training step 2453",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:21",
      "total_flops_so_far": 5.833166990684595e+16,
      "budget_used_percent": 58.33166990684595
    },
    {
      "type": "training",
      "description": "Training step 2454",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:22",
      "total_flops_so_far": 5.835543413905434e+16,
      "budget_used_percent": 58.35543413905434
    },
    {
      "type": "training",
      "description": "Training step 2455",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:24",
      "total_flops_so_far": 5.837919837126272e+16,
      "budget_used_percent": 58.37919837126272
    },
    {
      "type": "training",
      "description": "Training step 2456",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:25",
      "total_flops_so_far": 5.84029626034711e+16,
      "budget_used_percent": 58.40296260347111
    },
    {
      "type": "training",
      "description": "Training step 2457",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:26",
      "total_flops_so_far": 5.842672683567949e+16,
      "budget_used_percent": 58.42672683567949
    },
    {
      "type": "training",
      "description": "Training step 2458",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:28",
      "total_flops_so_far": 5.845049106788787e+16,
      "budget_used_percent": 58.450491067887874
    },
    {
      "type": "training",
      "description": "Training step 2459",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:29",
      "total_flops_so_far": 5.847425530009626e+16,
      "budget_used_percent": 58.47425530009625
    },
    {
      "type": "training",
      "description": "Training step 2460",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:30",
      "total_flops_so_far": 5.849801953230464e+16,
      "budget_used_percent": 58.49801953230463
    },
    {
      "type": "training",
      "description": "Training step 2461",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:32",
      "total_flops_so_far": 5.852178376451302e+16,
      "budget_used_percent": 58.521783764513025
    },
    {
      "type": "training",
      "description": "Training step 2462",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:33",
      "total_flops_so_far": 5.854554799672141e+16,
      "budget_used_percent": 58.5455479967214
    },
    {
      "type": "training",
      "description": "Training step 2463",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:34",
      "total_flops_so_far": 5.856931222892979e+16,
      "budget_used_percent": 58.569312228929796
    },
    {
      "type": "training",
      "description": "Training step 2464",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:36",
      "total_flops_so_far": 5.859307646113818e+16,
      "budget_used_percent": 58.593076461138175
    },
    {
      "type": "training",
      "description": "Training step 2465",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:37",
      "total_flops_so_far": 5.861684069334656e+16,
      "budget_used_percent": 58.61684069334656
    },
    {
      "type": "training",
      "description": "Training step 2466",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:38",
      "total_flops_so_far": 5.864060492555494e+16,
      "budget_used_percent": 58.64060492555494
    },
    {
      "type": "training",
      "description": "Training step 2467",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:40",
      "total_flops_so_far": 5.866436915776333e+16,
      "budget_used_percent": 58.66436915776333
    },
    {
      "type": "training",
      "description": "Training step 2468",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:41",
      "total_flops_so_far": 5.868813338997171e+16,
      "budget_used_percent": 58.68813338997171
    },
    {
      "type": "training",
      "description": "Training step 2469",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:42",
      "total_flops_so_far": 5.87118976221801e+16,
      "budget_used_percent": 58.711897622180096
    },
    {
      "type": "training",
      "description": "Training step 2470",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:44",
      "total_flops_so_far": 5.873566185438848e+16,
      "budget_used_percent": 58.735661854388475
    },
    {
      "type": "training",
      "description": "Training step 2471",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:45",
      "total_flops_so_far": 5.875942608659686e+16,
      "budget_used_percent": 58.75942608659687
    },
    {
      "type": "training",
      "description": "Training step 2472",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:46",
      "total_flops_so_far": 5.878319031880525e+16,
      "budget_used_percent": 58.78319031880525
    },
    {
      "type": "training",
      "description": "Training step 2473",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:48",
      "total_flops_so_far": 5.880695455101363e+16,
      "budget_used_percent": 58.80695455101363
    },
    {
      "type": "training",
      "description": "Training step 2474",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:49",
      "total_flops_so_far": 5.883071878322202e+16,
      "budget_used_percent": 58.83071878322201
    },
    {
      "type": "training",
      "description": "Training step 2475",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:50",
      "total_flops_so_far": 5.88544830154304e+16,
      "budget_used_percent": 58.854483015430404
    },
    {
      "type": "training",
      "description": "Training step 2476",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:52",
      "total_flops_so_far": 5.887824724763878e+16,
      "budget_used_percent": 58.87824724763878
    },
    {
      "type": "training",
      "description": "Training step 2477",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:53",
      "total_flops_so_far": 5.890201147984717e+16,
      "budget_used_percent": 58.902011479847175
    },
    {
      "type": "training",
      "description": "Training step 2478",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:54",
      "total_flops_so_far": 5.892577571205555e+16,
      "budget_used_percent": 58.925775712055554
    },
    {
      "type": "training",
      "description": "Training step 2479",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:56",
      "total_flops_so_far": 5.894953994426394e+16,
      "budget_used_percent": 58.94953994426394
    },
    {
      "type": "training",
      "description": "Training step 2480",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:57",
      "total_flops_so_far": 5.897330417647232e+16,
      "budget_used_percent": 58.97330417647232
    },
    {
      "type": "training",
      "description": "Training step 2481",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:05:58",
      "total_flops_so_far": 5.89970684086807e+16,
      "budget_used_percent": 58.99706840868071
    },
    {
      "type": "training",
      "description": "Training step 2482",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:00",
      "total_flops_so_far": 5.902083264088909e+16,
      "budget_used_percent": 59.02083264088909
    },
    {
      "type": "training",
      "description": "Training step 2483",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:01",
      "total_flops_so_far": 5.904459687309747e+16,
      "budget_used_percent": 59.044596873097476
    },
    {
      "type": "training",
      "description": "Training step 2484",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:02",
      "total_flops_so_far": 5.906836110530586e+16,
      "budget_used_percent": 59.068361105305854
    },
    {
      "type": "training",
      "description": "Training step 2485",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:04",
      "total_flops_so_far": 5.909212533751424e+16,
      "budget_used_percent": 59.09212533751425
    },
    {
      "type": "training",
      "description": "Training step 2486",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:05",
      "total_flops_so_far": 5.911588956972262e+16,
      "budget_used_percent": 59.115889569722626
    },
    {
      "type": "training",
      "description": "Training step 2487",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:06",
      "total_flops_so_far": 5.913965380193101e+16,
      "budget_used_percent": 59.139653801931004
    },
    {
      "type": "training",
      "description": "Training step 2488",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:08",
      "total_flops_so_far": 5.916341803413939e+16,
      "budget_used_percent": 59.16341803413939
    },
    {
      "type": "training",
      "description": "Training step 2489",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:09",
      "total_flops_so_far": 5.918718226634778e+16,
      "budget_used_percent": 59.18718226634777
    },
    {
      "type": "training",
      "description": "Training step 2490",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:10",
      "total_flops_so_far": 5.921094649855616e+16,
      "budget_used_percent": 59.21094649855616
    },
    {
      "type": "training",
      "description": "Training step 2491",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:12",
      "total_flops_so_far": 5.923471073076454e+16,
      "budget_used_percent": 59.23471073076454
    },
    {
      "type": "training",
      "description": "Training step 2492",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:13",
      "total_flops_so_far": 5.925847496297293e+16,
      "budget_used_percent": 59.258474962972926
    },
    {
      "type": "training",
      "description": "Training step 2493",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:14",
      "total_flops_so_far": 5.928223919518131e+16,
      "budget_used_percent": 59.282239195181305
    },
    {
      "type": "training",
      "description": "Training step 2494",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:16",
      "total_flops_so_far": 5.93060034273897e+16,
      "budget_used_percent": 59.3060034273897
    },
    {
      "type": "training",
      "description": "Training step 2495",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:17",
      "total_flops_so_far": 5.932976765959808e+16,
      "budget_used_percent": 59.329767659598076
    },
    {
      "type": "training",
      "description": "Training step 2496",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:18",
      "total_flops_so_far": 5.935353189180646e+16,
      "budget_used_percent": 59.35353189180647
    },
    {
      "type": "training",
      "description": "Training step 2497",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:20",
      "total_flops_so_far": 5.937729612401485e+16,
      "budget_used_percent": 59.37729612401485
    },
    {
      "type": "training",
      "description": "Training step 2498",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:21",
      "total_flops_so_far": 5.940106035622323e+16,
      "budget_used_percent": 59.40106035622323
    },
    {
      "type": "training",
      "description": "Training step 2499",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:22",
      "total_flops_so_far": 5.942482458843162e+16,
      "budget_used_percent": 59.42482458843161
    },
    {
      "type": "training",
      "description": "Training step 2500",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:24",
      "total_flops_so_far": 5.944858882064e+16,
      "budget_used_percent": 59.448588820640005
    },
    {
      "type": "training",
      "description": "Training step 2501",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:25",
      "total_flops_so_far": 5.947235305284838e+16,
      "budget_used_percent": 59.47235305284838
    },
    {
      "type": "training",
      "description": "Training step 2502",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:26",
      "total_flops_so_far": 5.949611728505677e+16,
      "budget_used_percent": 59.49611728505677
    },
    {
      "type": "training",
      "description": "Training step 2503",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:28",
      "total_flops_so_far": 5.951988151726515e+16,
      "budget_used_percent": 59.51988151726515
    },
    {
      "type": "training",
      "description": "Training step 2504",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:29",
      "total_flops_so_far": 5.954364574947354e+16,
      "budget_used_percent": 59.54364574947354
    },
    {
      "type": "training",
      "description": "Training step 2505",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:31",
      "total_flops_so_far": 5.956740998168192e+16,
      "budget_used_percent": 59.56740998168192
    },
    {
      "type": "training",
      "description": "Training step 2506",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:32",
      "total_flops_so_far": 5.95911742138903e+16,
      "budget_used_percent": 59.591174213890305
    },
    {
      "type": "training",
      "description": "Training step 2507",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:33",
      "total_flops_so_far": 5.961493844609869e+16,
      "budget_used_percent": 59.614938446098684
    },
    {
      "type": "training",
      "description": "Training step 2508",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:34",
      "total_flops_so_far": 5.963870267830707e+16,
      "budget_used_percent": 59.63870267830708
    },
    {
      "type": "training",
      "description": "Training step 2509",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:35",
      "total_flops_so_far": 5.966246691051546e+16,
      "budget_used_percent": 59.662466910515455
    },
    {
      "type": "training",
      "description": "Training step 2510",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:36",
      "total_flops_so_far": 5.968623114272384e+16,
      "budget_used_percent": 59.68623114272384
    },
    {
      "type": "training",
      "description": "Training step 2511",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:38",
      "total_flops_so_far": 5.970999537493222e+16,
      "budget_used_percent": 59.70999537493222
    },
    {
      "type": "training",
      "description": "Training step 2512",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:39",
      "total_flops_so_far": 5.973375960714061e+16,
      "budget_used_percent": 59.73375960714061
    },
    {
      "type": "training",
      "description": "Training step 2513",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:40",
      "total_flops_so_far": 5.975752383934899e+16,
      "budget_used_percent": 59.75752383934899
    },
    {
      "type": "training",
      "description": "Training step 2514",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:42",
      "total_flops_so_far": 5.978128807155738e+16,
      "budget_used_percent": 59.781288071557384
    },
    {
      "type": "training",
      "description": "Training step 2515",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:43",
      "total_flops_so_far": 5.980505230376576e+16,
      "budget_used_percent": 59.80505230376576
    },
    {
      "type": "training",
      "description": "Training step 2516",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:44",
      "total_flops_so_far": 5.982881653597414e+16,
      "budget_used_percent": 59.82881653597414
    },
    {
      "type": "training",
      "description": "Training step 2517",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:46",
      "total_flops_so_far": 5.985258076818253e+16,
      "budget_used_percent": 59.85258076818253
    },
    {
      "type": "training",
      "description": "Training step 2518",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:47",
      "total_flops_so_far": 5.987634500039091e+16,
      "budget_used_percent": 59.876345000390906
    },
    {
      "type": "training",
      "description": "Training step 2519",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:48",
      "total_flops_so_far": 5.99001092325993e+16,
      "budget_used_percent": 59.9001092325993
    },
    {
      "type": "training",
      "description": "Training step 2520",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:50",
      "total_flops_so_far": 5.992387346480768e+16,
      "budget_used_percent": 59.92387346480768
    },
    {
      "type": "training",
      "description": "Training step 2521",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:51",
      "total_flops_so_far": 5.994763769701606e+16,
      "budget_used_percent": 59.94763769701606
    },
    {
      "type": "training",
      "description": "Training step 2522",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:52",
      "total_flops_so_far": 5.997140192922445e+16,
      "budget_used_percent": 59.97140192922444
    },
    {
      "type": "training",
      "description": "Training step 2523",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:54",
      "total_flops_so_far": 5.999516616143283e+16,
      "budget_used_percent": 59.995166161432834
    },
    {
      "type": "training",
      "description": "Training step 2524",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:55",
      "total_flops_so_far": 6.001893039364122e+16,
      "budget_used_percent": 60.01893039364121
    },
    {
      "type": "training",
      "description": "Training step 2525",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:56",
      "total_flops_so_far": 6.00426946258496e+16,
      "budget_used_percent": 60.0426946258496
    },
    {
      "type": "training",
      "description": "Training step 2526",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:58",
      "total_flops_so_far": 6.006645885805798e+16,
      "budget_used_percent": 60.06645885805798
    },
    {
      "type": "training",
      "description": "Training step 2527",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:06:59",
      "total_flops_so_far": 6.009022309026637e+16,
      "budget_used_percent": 60.09022309026637
    },
    {
      "type": "training",
      "description": "Training step 2528",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:00",
      "total_flops_so_far": 6.011398732247475e+16,
      "budget_used_percent": 60.11398732247475
    },
    {
      "type": "training",
      "description": "Training step 2529",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:02",
      "total_flops_so_far": 6.013775155468314e+16,
      "budget_used_percent": 60.13775155468314
    },
    {
      "type": "training",
      "description": "Training step 2530",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:03",
      "total_flops_so_far": 6.016151578689152e+16,
      "budget_used_percent": 60.16151578689152
    },
    {
      "type": "training",
      "description": "Training step 2531",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:04",
      "total_flops_so_far": 6.01852800190999e+16,
      "budget_used_percent": 60.185280019099906
    },
    {
      "type": "training",
      "description": "Training step 2532",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:06",
      "total_flops_so_far": 6.020904425130829e+16,
      "budget_used_percent": 60.209044251308285
    },
    {
      "type": "training",
      "description": "Training step 2533",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:08",
      "total_flops_so_far": 6.023280848351667e+16,
      "budget_used_percent": 60.23280848351668
    },
    {
      "type": "training",
      "description": "Training step 2534",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:09",
      "total_flops_so_far": 6.025657271572506e+16,
      "budget_used_percent": 60.256572715725056
    },
    {
      "type": "training",
      "description": "Training step 2535",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:10",
      "total_flops_so_far": 6.028033694793344e+16,
      "budget_used_percent": 60.28033694793344
    },
    {
      "type": "training",
      "description": "Training step 2536",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:12",
      "total_flops_so_far": 6.030410118014182e+16,
      "budget_used_percent": 60.30410118014182
    },
    {
      "type": "training",
      "description": "Training step 2537",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:14",
      "total_flops_so_far": 6.032786541235021e+16,
      "budget_used_percent": 60.327865412350214
    },
    {
      "type": "training",
      "description": "Training step 2538",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:15",
      "total_flops_so_far": 6.035162964455859e+16,
      "budget_used_percent": 60.35162964455859
    },
    {
      "type": "training",
      "description": "Training step 2539",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:16",
      "total_flops_so_far": 6.037539387676698e+16,
      "budget_used_percent": 60.37539387676698
    },
    {
      "type": "training",
      "description": "Training step 2540",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:18",
      "total_flops_so_far": 6.039915810897536e+16,
      "budget_used_percent": 60.39915810897536
    },
    {
      "type": "training",
      "description": "Training step 2541",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:19",
      "total_flops_so_far": 6.042292234118374e+16,
      "budget_used_percent": 60.42292234118375
    },
    {
      "type": "training",
      "description": "Training step 2542",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:20",
      "total_flops_so_far": 6.044668657339213e+16,
      "budget_used_percent": 60.44668657339213
    },
    {
      "type": "training",
      "description": "Training step 2543",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:22",
      "total_flops_so_far": 6.047045080560051e+16,
      "budget_used_percent": 60.47045080560052
    },
    {
      "type": "training",
      "description": "Training step 2544",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:23",
      "total_flops_so_far": 6.04942150378089e+16,
      "budget_used_percent": 60.4942150378089
    },
    {
      "type": "training",
      "description": "Training step 2545",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:24",
      "total_flops_so_far": 6.051797927001728e+16,
      "budget_used_percent": 60.51797927001728
    },
    {
      "type": "training",
      "description": "Training step 2546",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:26",
      "total_flops_so_far": 6.054174350222566e+16,
      "budget_used_percent": 60.541743502225664
    },
    {
      "type": "training",
      "description": "Training step 2547",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:27",
      "total_flops_so_far": 6.056550773443405e+16,
      "budget_used_percent": 60.56550773443404
    },
    {
      "type": "training",
      "description": "Training step 2548",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:28",
      "total_flops_so_far": 6.058927196664243e+16,
      "budget_used_percent": 60.589271966642436
    },
    {
      "type": "training",
      "description": "Training step 2549",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:30",
      "total_flops_so_far": 6.061303619885082e+16,
      "budget_used_percent": 60.613036198850814
    },
    {
      "type": "training",
      "description": "Training step 2550",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:31",
      "total_flops_so_far": 6.06368004310592e+16,
      "budget_used_percent": 60.6368004310592
    },
    {
      "type": "training",
      "description": "Training step 2551",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:32",
      "total_flops_so_far": 6.066056466326758e+16,
      "budget_used_percent": 60.66056466326758
    },
    {
      "type": "training",
      "description": "Training step 2552",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:34",
      "total_flops_so_far": 6.068432889547597e+16,
      "budget_used_percent": 60.68432889547597
    },
    {
      "type": "training",
      "description": "Training step 2553",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:35",
      "total_flops_so_far": 6.070809312768435e+16,
      "budget_used_percent": 60.70809312768435
    },
    {
      "type": "training",
      "description": "Training step 2554",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:36",
      "total_flops_so_far": 6.073185735989274e+16,
      "budget_used_percent": 60.731857359892736
    },
    {
      "type": "training",
      "description": "Training step 2555",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:38",
      "total_flops_so_far": 6.075562159210112e+16,
      "budget_used_percent": 60.755621592101114
    },
    {
      "type": "training",
      "description": "Training step 2556",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:39",
      "total_flops_so_far": 6.07793858243095e+16,
      "budget_used_percent": 60.77938582430951
    },
    {
      "type": "training",
      "description": "Training step 2557",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:40",
      "total_flops_so_far": 6.080315005651789e+16,
      "budget_used_percent": 60.803150056517886
    },
    {
      "type": "training",
      "description": "Training step 2558",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:42",
      "total_flops_so_far": 6.082691428872627e+16,
      "budget_used_percent": 60.82691428872627
    },
    {
      "type": "training",
      "description": "Training step 2559",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:43",
      "total_flops_so_far": 6.085067852093466e+16,
      "budget_used_percent": 60.85067852093465
    },
    {
      "type": "training",
      "description": "Training step 2560",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:44",
      "total_flops_so_far": 6.087444275314304e+16,
      "budget_used_percent": 60.87444275314304
    },
    {
      "type": "training",
      "description": "Training step 2561",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:46",
      "total_flops_so_far": 6.089820698535142e+16,
      "budget_used_percent": 60.89820698535142
    },
    {
      "type": "training",
      "description": "Training step 2562",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:47",
      "total_flops_so_far": 6.092197121755981e+16,
      "budget_used_percent": 60.921971217559815
    },
    {
      "type": "training",
      "description": "Training step 2563",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:48",
      "total_flops_so_far": 6.094573544976819e+16,
      "budget_used_percent": 60.94573544976819
    },
    {
      "type": "training",
      "description": "Training step 2564",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:50",
      "total_flops_so_far": 6.096949968197658e+16,
      "budget_used_percent": 60.96949968197658
    },
    {
      "type": "training",
      "description": "Training step 2565",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:51",
      "total_flops_so_far": 6.099326391418496e+16,
      "budget_used_percent": 60.99326391418496
    },
    {
      "type": "training",
      "description": "Training step 2566",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:53",
      "total_flops_so_far": 6.101702814639334e+16,
      "budget_used_percent": 61.01702814639335
    },
    {
      "type": "training",
      "description": "Training step 2567",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:54",
      "total_flops_so_far": 6.104079237860173e+16,
      "budget_used_percent": 61.04079237860173
    },
    {
      "type": "training",
      "description": "Training step 2568",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:55",
      "total_flops_so_far": 6.106455661081011e+16,
      "budget_used_percent": 61.064556610810115
    },
    {
      "type": "training",
      "description": "Training step 2569",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:57",
      "total_flops_so_far": 6.10883208430185e+16,
      "budget_used_percent": 61.088320843018494
    },
    {
      "type": "training",
      "description": "Training step 2570",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:58",
      "total_flops_so_far": 6.111208507522688e+16,
      "budget_used_percent": 61.11208507522689
    },
    {
      "type": "training",
      "description": "Training step 2571",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:07:59",
      "total_flops_so_far": 6.113584930743526e+16,
      "budget_used_percent": 61.135849307435265
    },
    {
      "type": "training",
      "description": "Training step 2572",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:01",
      "total_flops_so_far": 6.115961353964365e+16,
      "budget_used_percent": 61.15961353964365
    },
    {
      "type": "training",
      "description": "Training step 2573",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:02",
      "total_flops_so_far": 6.118337777185203e+16,
      "budget_used_percent": 61.18337777185203
    },
    {
      "type": "training",
      "description": "Training step 2574",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:03",
      "total_flops_so_far": 6.120714200406042e+16,
      "budget_used_percent": 61.20714200406041
    },
    {
      "type": "training",
      "description": "Training step 2575",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:05",
      "total_flops_so_far": 6.12309062362688e+16,
      "budget_used_percent": 61.2309062362688
    },
    {
      "type": "training",
      "description": "Training step 2576",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:06",
      "total_flops_so_far": 6.125467046847718e+16,
      "budget_used_percent": 61.25467046847718
    },
    {
      "type": "training",
      "description": "Training step 2577",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:07",
      "total_flops_so_far": 6.127843470068557e+16,
      "budget_used_percent": 61.278434700685565
    },
    {
      "type": "training",
      "description": "Training step 2578",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:09",
      "total_flops_so_far": 6.130219893289395e+16,
      "budget_used_percent": 61.302198932893944
    },
    {
      "type": "training",
      "description": "Training step 2579",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:10",
      "total_flops_so_far": 6.132596316510234e+16,
      "budget_used_percent": 61.32596316510234
    },
    {
      "type": "training",
      "description": "Training step 2580",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:11",
      "total_flops_so_far": 6.134972739731072e+16,
      "budget_used_percent": 61.349727397310716
    },
    {
      "type": "training",
      "description": "Training step 2581",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:13",
      "total_flops_so_far": 6.13734916295191e+16,
      "budget_used_percent": 61.37349162951911
    },
    {
      "type": "training",
      "description": "Training step 2582",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:14",
      "total_flops_so_far": 6.139725586172749e+16,
      "budget_used_percent": 61.39725586172749
    },
    {
      "type": "training",
      "description": "Training step 2583",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:15",
      "total_flops_so_far": 6.142102009393587e+16,
      "budget_used_percent": 61.42102009393587
    },
    {
      "type": "training",
      "description": "Training step 2584",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:17",
      "total_flops_so_far": 6.144478432614426e+16,
      "budget_used_percent": 61.44478432614425
    },
    {
      "type": "training",
      "description": "Training step 2585",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:18",
      "total_flops_so_far": 6.146854855835264e+16,
      "budget_used_percent": 61.468548558352644
    },
    {
      "type": "training",
      "description": "Training step 2586",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:19",
      "total_flops_so_far": 6.149231279056102e+16,
      "budget_used_percent": 61.49231279056102
    },
    {
      "type": "training",
      "description": "Training step 2587",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:21",
      "total_flops_so_far": 6.151607702276941e+16,
      "budget_used_percent": 61.51607702276941
    },
    {
      "type": "training",
      "description": "Training step 2588",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:22",
      "total_flops_so_far": 6.153984125497779e+16,
      "budget_used_percent": 61.53984125497779
    },
    {
      "type": "training",
      "description": "Training step 2589",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:23",
      "total_flops_so_far": 6.156360548718618e+16,
      "budget_used_percent": 61.56360548718618
    },
    {
      "type": "training",
      "description": "Training step 2590",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:25",
      "total_flops_so_far": 6.158736971939456e+16,
      "budget_used_percent": 61.58736971939456
    },
    {
      "type": "training",
      "description": "Training step 2591",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:26",
      "total_flops_so_far": 6.161113395160294e+16,
      "budget_used_percent": 61.611133951602945
    },
    {
      "type": "training",
      "description": "Training step 2592",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:27",
      "total_flops_so_far": 6.163489818381133e+16,
      "budget_used_percent": 61.63489818381132
    },
    {
      "type": "training",
      "description": "Training step 2593",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:29",
      "total_flops_so_far": 6.165866241601971e+16,
      "budget_used_percent": 61.658662416019716
    },
    {
      "type": "training",
      "description": "Training step 2594",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:30",
      "total_flops_so_far": 6.16824266482281e+16,
      "budget_used_percent": 61.682426648228095
    },
    {
      "type": "training",
      "description": "Training step 2595",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:31",
      "total_flops_so_far": 6.170619088043648e+16,
      "budget_used_percent": 61.70619088043649
    },
    {
      "type": "training",
      "description": "Training step 2596",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:33",
      "total_flops_so_far": 6.172995511264486e+16,
      "budget_used_percent": 61.729955112644866
    },
    {
      "type": "training",
      "description": "Training step 2597",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:34",
      "total_flops_so_far": 6.175371934485325e+16,
      "budget_used_percent": 61.75371934485325
    },
    {
      "type": "training",
      "description": "Training step 2598",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:35",
      "total_flops_so_far": 6.177748357706163e+16,
      "budget_used_percent": 61.77748357706163
    },
    {
      "type": "training",
      "description": "Training step 2599",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:37",
      "total_flops_so_far": 6.180124780927002e+16,
      "budget_used_percent": 61.80124780927002
    },
    {
      "type": "training",
      "description": "Training step 2600",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:38",
      "total_flops_so_far": 6.18250120414784e+16,
      "budget_used_percent": 61.8250120414784
    },
    {
      "type": "training",
      "description": "Training step 2601",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:39",
      "total_flops_so_far": 6.184877627368678e+16,
      "budget_used_percent": 61.84877627368678
    },
    {
      "type": "training",
      "description": "Training step 2602",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:41",
      "total_flops_so_far": 6.187254050589517e+16,
      "budget_used_percent": 61.87254050589517
    },
    {
      "type": "training",
      "description": "Training step 2603",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:42",
      "total_flops_so_far": 6.189630473810355e+16,
      "budget_used_percent": 61.896304738103545
    },
    {
      "type": "training",
      "description": "Training step 2604",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:43",
      "total_flops_so_far": 6.192006897031194e+16,
      "budget_used_percent": 61.92006897031194
    },
    {
      "type": "training",
      "description": "Training step 2605",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:45",
      "total_flops_so_far": 6.194383320252032e+16,
      "budget_used_percent": 61.94383320252032
    },
    {
      "type": "training",
      "description": "Training step 2606",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:46",
      "total_flops_so_far": 6.19675974347287e+16,
      "budget_used_percent": 61.9675974347287
    },
    {
      "type": "training",
      "description": "Training step 2607",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:48",
      "total_flops_so_far": 6.199136166693709e+16,
      "budget_used_percent": 61.99136166693708
    },
    {
      "type": "training",
      "description": "Training step 2608",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:49",
      "total_flops_so_far": 6.201512589914547e+16,
      "budget_used_percent": 62.015125899145474
    },
    {
      "type": "training",
      "description": "Training step 2609",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:50",
      "total_flops_so_far": 6.203889013135386e+16,
      "budget_used_percent": 62.03889013135385
    },
    {
      "type": "training",
      "description": "Training step 2610",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:52",
      "total_flops_so_far": 6.206265436356224e+16,
      "budget_used_percent": 62.062654363562245
    },
    {
      "type": "training",
      "description": "Training step 2611",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:53",
      "total_flops_so_far": 6.208641859577062e+16,
      "budget_used_percent": 62.086418595770624
    },
    {
      "type": "training",
      "description": "Training step 2612",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:54",
      "total_flops_so_far": 6.211018282797901e+16,
      "budget_used_percent": 62.11018282797901
    },
    {
      "type": "training",
      "description": "Training step 2613",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:56",
      "total_flops_so_far": 6.213394706018739e+16,
      "budget_used_percent": 62.13394706018739
    },
    {
      "type": "training",
      "description": "Training step 2614",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:57",
      "total_flops_so_far": 6.215771129239578e+16,
      "budget_used_percent": 62.15771129239578
    },
    {
      "type": "training",
      "description": "Training step 2615",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:08:58",
      "total_flops_so_far": 6.218147552460416e+16,
      "budget_used_percent": 62.18147552460416
    },
    {
      "type": "training",
      "description": "Training step 2616",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:00",
      "total_flops_so_far": 6.220523975681254e+16,
      "budget_used_percent": 62.205239756812546
    },
    {
      "type": "training",
      "description": "Training step 2617",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:01",
      "total_flops_so_far": 6.222900398902093e+16,
      "budget_used_percent": 62.229003989020924
    },
    {
      "type": "training",
      "description": "Training step 2618",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:02",
      "total_flops_so_far": 6.225276822122931e+16,
      "budget_used_percent": 62.25276822122932
    },
    {
      "type": "training",
      "description": "Training step 2619",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:04",
      "total_flops_so_far": 6.22765324534377e+16,
      "budget_used_percent": 62.276532453437696
    },
    {
      "type": "training",
      "description": "Training step 2620",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:05",
      "total_flops_so_far": 6.230029668564608e+16,
      "budget_used_percent": 62.30029668564608
    },
    {
      "type": "training",
      "description": "Training step 2621",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:06",
      "total_flops_so_far": 6.232406091785446e+16,
      "budget_used_percent": 62.32406091785446
    },
    {
      "type": "training",
      "description": "Training step 2622",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:08",
      "total_flops_so_far": 6.234782515006285e+16,
      "budget_used_percent": 62.34782515006285
    },
    {
      "type": "training",
      "description": "Training step 2623",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:09",
      "total_flops_so_far": 6.237158938227123e+16,
      "budget_used_percent": 62.37158938227123
    },
    {
      "type": "training",
      "description": "Training step 2624",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:10",
      "total_flops_so_far": 6.239535361447962e+16,
      "budget_used_percent": 62.39535361447962
    },
    {
      "type": "training",
      "description": "Training step 2625",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:12",
      "total_flops_so_far": 6.2419117846688e+16,
      "budget_used_percent": 62.419117846687996
    },
    {
      "type": "training",
      "description": "Training step 2626",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:13",
      "total_flops_so_far": 6.244288207889638e+16,
      "budget_used_percent": 62.44288207889639
    },
    {
      "type": "training",
      "description": "Training step 2627",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:14",
      "total_flops_so_far": 6.246664631110477e+16,
      "budget_used_percent": 62.46664631110477
    },
    {
      "type": "training",
      "description": "Training step 2628",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:16",
      "total_flops_so_far": 6.249041054331315e+16,
      "budget_used_percent": 62.49041054331316
    },
    {
      "type": "training",
      "description": "Training step 2629",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:17",
      "total_flops_so_far": 6.251417477552154e+16,
      "budget_used_percent": 62.51417477552154
    },
    {
      "type": "training",
      "description": "Training step 2630",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:18",
      "total_flops_so_far": 6.253793900772992e+16,
      "budget_used_percent": 62.53793900772992
    },
    {
      "type": "training",
      "description": "Training step 2631",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:20",
      "total_flops_so_far": 6.25617032399383e+16,
      "budget_used_percent": 62.5617032399383
    },
    {
      "type": "training",
      "description": "Training step 2632",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:21",
      "total_flops_so_far": 6.258546747214669e+16,
      "budget_used_percent": 62.58546747214668
    },
    {
      "type": "training",
      "description": "Training step 2633",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:22",
      "total_flops_so_far": 6.260923170435507e+16,
      "budget_used_percent": 62.609231704355075
    },
    {
      "type": "training",
      "description": "Training step 2634",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:24",
      "total_flops_so_far": 6.263299593656346e+16,
      "budget_used_percent": 62.632995936563454
    },
    {
      "type": "training",
      "description": "Training step 2635",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:25",
      "total_flops_so_far": 6.265676016877184e+16,
      "budget_used_percent": 62.65676016877184
    },
    {
      "type": "training",
      "description": "Training step 2636",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:26",
      "total_flops_so_far": 6.268052440098022e+16,
      "budget_used_percent": 62.68052440098022
    },
    {
      "type": "training",
      "description": "Training step 2637",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:28",
      "total_flops_so_far": 6.270428863318861e+16,
      "budget_used_percent": 62.70428863318861
    },
    {
      "type": "training",
      "description": "Training step 2638",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:29",
      "total_flops_so_far": 6.272805286539699e+16,
      "budget_used_percent": 62.72805286539699
    },
    {
      "type": "training",
      "description": "Training step 2639",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:30",
      "total_flops_so_far": 6.275181709760538e+16,
      "budget_used_percent": 62.751817097605375
    },
    {
      "type": "training",
      "description": "Training step 2640",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:31",
      "total_flops_so_far": 6.277558132981376e+16,
      "budget_used_percent": 62.775581329813754
    },
    {
      "type": "training",
      "description": "Training step 2641",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:32",
      "total_flops_so_far": 6.279934556202214e+16,
      "budget_used_percent": 62.79934556202215
    },
    {
      "type": "training",
      "description": "Training step 2642",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:33",
      "total_flops_so_far": 6.282310979423053e+16,
      "budget_used_percent": 62.823109794230525
    },
    {
      "type": "training",
      "description": "Training step 2643",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:35",
      "total_flops_so_far": 6.284687402643891e+16,
      "budget_used_percent": 62.84687402643891
    },
    {
      "type": "training",
      "description": "Training step 2644",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:36",
      "total_flops_so_far": 6.28706382586473e+16,
      "budget_used_percent": 62.87063825864729
    },
    {
      "type": "training",
      "description": "Training step 2645",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:37",
      "total_flops_so_far": 6.289440249085568e+16,
      "budget_used_percent": 62.89440249085568
    },
    {
      "type": "training",
      "description": "Training step 2646",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:39",
      "total_flops_so_far": 6.291816672306406e+16,
      "budget_used_percent": 62.91816672306406
    },
    {
      "type": "training",
      "description": "Training step 2647",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:40",
      "total_flops_so_far": 6.294193095527245e+16,
      "budget_used_percent": 62.941930955272454
    },
    {
      "type": "training",
      "description": "Training step 2648",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:41",
      "total_flops_so_far": 6.296569518748083e+16,
      "budget_used_percent": 62.96569518748083
    },
    {
      "type": "training",
      "description": "Training step 2649",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:43",
      "total_flops_so_far": 6.298945941968922e+16,
      "budget_used_percent": 62.98945941968922
    },
    {
      "type": "training",
      "description": "Training step 2650",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:44",
      "total_flops_so_far": 6.30132236518976e+16,
      "budget_used_percent": 63.0132236518976
    },
    {
      "type": "training",
      "description": "Training step 2651",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:46",
      "total_flops_so_far": 6.303698788410598e+16,
      "budget_used_percent": 63.03698788410599
    },
    {
      "type": "training",
      "description": "Training step 2652",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:47",
      "total_flops_so_far": 6.306075211631437e+16,
      "budget_used_percent": 63.06075211631437
    },
    {
      "type": "training",
      "description": "Training step 2653",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:48",
      "total_flops_so_far": 6.308451634852275e+16,
      "budget_used_percent": 63.084516348522754
    },
    {
      "type": "training",
      "description": "Training step 2654",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:50",
      "total_flops_so_far": 6.310828058073114e+16,
      "budget_used_percent": 63.10828058073113
    },
    {
      "type": "training",
      "description": "Training step 2655",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:51",
      "total_flops_so_far": 6.313204481293952e+16,
      "budget_used_percent": 63.132044812939526
    },
    {
      "type": "training",
      "description": "Training step 2656",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:52",
      "total_flops_so_far": 6.31558090451479e+16,
      "budget_used_percent": 63.155809045147905
    },
    {
      "type": "training",
      "description": "Training step 2657",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:54",
      "total_flops_so_far": 6.317957327735629e+16,
      "budget_used_percent": 63.17957327735629
    },
    {
      "type": "training",
      "description": "Training step 2658",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:55",
      "total_flops_so_far": 6.320333750956467e+16,
      "budget_used_percent": 63.20333750956467
    },
    {
      "type": "training",
      "description": "Training step 2659",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:56",
      "total_flops_so_far": 6.322710174177306e+16,
      "budget_used_percent": 63.22710174177305
    },
    {
      "type": "training",
      "description": "Training step 2660",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:58",
      "total_flops_so_far": 6.325086597398144e+16,
      "budget_used_percent": 63.25086597398144
    },
    {
      "type": "training",
      "description": "Training step 2661",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:09:59",
      "total_flops_so_far": 6.327463020618982e+16,
      "budget_used_percent": 63.27463020618982
    },
    {
      "type": "training",
      "description": "Training step 2662",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:00",
      "total_flops_so_far": 6.329839443839821e+16,
      "budget_used_percent": 63.29839443839821
    },
    {
      "type": "training",
      "description": "Training step 2663",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:02",
      "total_flops_so_far": 6.332215867060659e+16,
      "budget_used_percent": 63.32215867060659
    },
    {
      "type": "training",
      "description": "Training step 2664",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:03",
      "total_flops_so_far": 6.334592290281498e+16,
      "budget_used_percent": 63.345922902814976
    },
    {
      "type": "training",
      "description": "Training step 2665",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:04",
      "total_flops_so_far": 6.336968713502336e+16,
      "budget_used_percent": 63.369687135023355
    },
    {
      "type": "training",
      "description": "Training step 2666",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:06",
      "total_flops_so_far": 6.339345136723174e+16,
      "budget_used_percent": 63.39345136723175
    },
    {
      "type": "training",
      "description": "Training step 2667",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:08",
      "total_flops_so_far": 6.341721559944013e+16,
      "budget_used_percent": 63.41721559944013
    },
    {
      "type": "training",
      "description": "Training step 2668",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:09",
      "total_flops_so_far": 6.344097983164851e+16,
      "budget_used_percent": 63.44097983164851
    },
    {
      "type": "training",
      "description": "Training step 2669",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:10",
      "total_flops_so_far": 6.34647440638569e+16,
      "budget_used_percent": 63.46474406385689
    },
    {
      "type": "training",
      "description": "Training step 2670",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:12",
      "total_flops_so_far": 6.348850829606528e+16,
      "budget_used_percent": 63.488508296065284
    },
    {
      "type": "training",
      "description": "Training step 2671",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:13",
      "total_flops_so_far": 6.351227252827366e+16,
      "budget_used_percent": 63.51227252827366
    },
    {
      "type": "training",
      "description": "Training step 2672",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:14",
      "total_flops_so_far": 6.353603676048205e+16,
      "budget_used_percent": 63.53603676048205
    },
    {
      "type": "training",
      "description": "Training step 2673",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:16",
      "total_flops_so_far": 6.355980099269043e+16,
      "budget_used_percent": 63.55980099269043
    },
    {
      "type": "training",
      "description": "Training step 2674",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:17",
      "total_flops_so_far": 6.358356522489882e+16,
      "budget_used_percent": 63.58356522489882
    },
    {
      "type": "training",
      "description": "Training step 2675",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:18",
      "total_flops_so_far": 6.36073294571072e+16,
      "budget_used_percent": 63.6073294571072
    },
    {
      "type": "training",
      "description": "Training step 2676",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:20",
      "total_flops_so_far": 6.363109368931558e+16,
      "budget_used_percent": 63.63109368931559
    },
    {
      "type": "training",
      "description": "Training step 2677",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:21",
      "total_flops_so_far": 6.365485792152397e+16,
      "budget_used_percent": 63.65485792152397
    },
    {
      "type": "training",
      "description": "Training step 2678",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:23",
      "total_flops_so_far": 6.367862215373235e+16,
      "budget_used_percent": 63.678622153732356
    },
    {
      "type": "training",
      "description": "Training step 2679",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:24",
      "total_flops_so_far": 6.370238638594074e+16,
      "budget_used_percent": 63.702386385940734
    },
    {
      "type": "training",
      "description": "Training step 2680",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:25",
      "total_flops_so_far": 6.372615061814912e+16,
      "budget_used_percent": 63.72615061814913
    },
    {
      "type": "training",
      "description": "Training step 2681",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:27",
      "total_flops_so_far": 6.37499148503575e+16,
      "budget_used_percent": 63.749914850357506
    },
    {
      "type": "training",
      "description": "Training step 2682",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:28",
      "total_flops_so_far": 6.377367908256589e+16,
      "budget_used_percent": 63.77367908256589
    },
    {
      "type": "training",
      "description": "Training step 2683",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:29",
      "total_flops_so_far": 6.379744331477427e+16,
      "budget_used_percent": 63.79744331477427
    },
    {
      "type": "training",
      "description": "Training step 2684",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:31",
      "total_flops_so_far": 6.382120754698266e+16,
      "budget_used_percent": 63.82120754698266
    },
    {
      "type": "training",
      "description": "Training step 2685",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:32",
      "total_flops_so_far": 6.384497177919104e+16,
      "budget_used_percent": 63.84497177919104
    },
    {
      "type": "training",
      "description": "Training step 2686",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:33",
      "total_flops_so_far": 6.386873601139942e+16,
      "budget_used_percent": 63.86873601139942
    },
    {
      "type": "training",
      "description": "Training step 2687",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:35",
      "total_flops_so_far": 6.389250024360781e+16,
      "budget_used_percent": 63.892500243607806
    },
    {
      "type": "training",
      "description": "Training step 2688",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:36",
      "total_flops_so_far": 6.391626447581619e+16,
      "budget_used_percent": 63.916264475816185
    },
    {
      "type": "training",
      "description": "Training step 2689",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:37",
      "total_flops_so_far": 6.394002870802458e+16,
      "budget_used_percent": 63.94002870802458
    },
    {
      "type": "training",
      "description": "Training step 2690",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:39",
      "total_flops_so_far": 6.396379294023296e+16,
      "budget_used_percent": 63.963792940232956
    },
    {
      "type": "training",
      "description": "Training step 2691",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:40",
      "total_flops_so_far": 6.398755717244134e+16,
      "budget_used_percent": 63.98755717244134
    },
    {
      "type": "training",
      "description": "Training step 2692",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:41",
      "total_flops_so_far": 6.401132140464973e+16,
      "budget_used_percent": 64.01132140464972
    },
    {
      "type": "training",
      "description": "Training step 2693",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:43",
      "total_flops_so_far": 6.403508563685811e+16,
      "budget_used_percent": 64.03508563685811
    },
    {
      "type": "training",
      "description": "Training step 2694",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:44",
      "total_flops_so_far": 6.40588498690665e+16,
      "budget_used_percent": 64.05884986906649
    },
    {
      "type": "training",
      "description": "Training step 2695",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:45",
      "total_flops_so_far": 6.408261410127488e+16,
      "budget_used_percent": 64.08261410127488
    },
    {
      "type": "training",
      "description": "Training step 2696",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:47",
      "total_flops_so_far": 6.410637833348326e+16,
      "budget_used_percent": 64.10637833348326
    },
    {
      "type": "training",
      "description": "Training step 2697",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:48",
      "total_flops_so_far": 6.413014256569165e+16,
      "budget_used_percent": 64.13014256569166
    },
    {
      "type": "training",
      "description": "Training step 2698",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:49",
      "total_flops_so_far": 6.415390679790003e+16,
      "budget_used_percent": 64.15390679790004
    },
    {
      "type": "training",
      "description": "Training step 2699",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:51",
      "total_flops_so_far": 6.417767103010842e+16,
      "budget_used_percent": 64.17767103010841
    },
    {
      "type": "training",
      "description": "Training step 2700",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:52",
      "total_flops_so_far": 6.42014352623168e+16,
      "budget_used_percent": 64.20143526231679
    },
    {
      "type": "training",
      "description": "Training step 2701",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:53",
      "total_flops_so_far": 6.422519949452518e+16,
      "budget_used_percent": 64.22519949452519
    },
    {
      "type": "training",
      "description": "Training step 2702",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:55",
      "total_flops_so_far": 6.424896372673357e+16,
      "budget_used_percent": 64.24896372673356
    },
    {
      "type": "training",
      "description": "Training step 2703",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:56",
      "total_flops_so_far": 6.427272795894195e+16,
      "budget_used_percent": 64.27272795894196
    },
    {
      "type": "training",
      "description": "Training step 2704",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:57",
      "total_flops_so_far": 6.429649219115034e+16,
      "budget_used_percent": 64.29649219115034
    },
    {
      "type": "training",
      "description": "Training step 2705",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:10:59",
      "total_flops_so_far": 6.432025642335872e+16,
      "budget_used_percent": 64.32025642335873
    },
    {
      "type": "training",
      "description": "Training step 2706",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:00",
      "total_flops_so_far": 6.43440206555671e+16,
      "budget_used_percent": 64.3440206555671
    },
    {
      "type": "training",
      "description": "Training step 2707",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:01",
      "total_flops_so_far": 6.436778488777549e+16,
      "budget_used_percent": 64.36778488777549
    },
    {
      "type": "training",
      "description": "Training step 2708",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:03",
      "total_flops_so_far": 6.439154911998387e+16,
      "budget_used_percent": 64.39154911998386
    },
    {
      "type": "training",
      "description": "Training step 2709",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:04",
      "total_flops_so_far": 6.441531335219226e+16,
      "budget_used_percent": 64.41531335219226
    },
    {
      "type": "training",
      "description": "Training step 2710",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:05",
      "total_flops_so_far": 6.443907758440064e+16,
      "budget_used_percent": 64.43907758440064
    },
    {
      "type": "training",
      "description": "Training step 2711",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:07",
      "total_flops_so_far": 6.446284181660902e+16,
      "budget_used_percent": 64.46284181660903
    },
    {
      "type": "training",
      "description": "Training step 2712",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:08",
      "total_flops_so_far": 6.448660604881741e+16,
      "budget_used_percent": 64.4866060488174
    },
    {
      "type": "training",
      "description": "Training step 2713",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:09",
      "total_flops_so_far": 6.451037028102579e+16,
      "budget_used_percent": 64.5103702810258
    },
    {
      "type": "training",
      "description": "Training step 2714",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:11",
      "total_flops_so_far": 6.453413451323418e+16,
      "budget_used_percent": 64.53413451323418
    },
    {
      "type": "training",
      "description": "Training step 2715",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:12",
      "total_flops_so_far": 6.455789874544256e+16,
      "budget_used_percent": 64.55789874544256
    },
    {
      "type": "training",
      "description": "Training step 2716",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:13",
      "total_flops_so_far": 6.458166297765094e+16,
      "budget_used_percent": 64.58166297765095
    },
    {
      "type": "training",
      "description": "Training step 2717",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:15",
      "total_flops_so_far": 6.460542720985933e+16,
      "budget_used_percent": 64.60542720985933
    },
    {
      "type": "training",
      "description": "Training step 2718",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:16",
      "total_flops_so_far": 6.462919144206771e+16,
      "budget_used_percent": 64.62919144206771
    },
    {
      "type": "training",
      "description": "Training step 2719",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:17",
      "total_flops_so_far": 6.46529556742761e+16,
      "budget_used_percent": 64.65295567427609
    },
    {
      "type": "training",
      "description": "Training step 2720",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:19",
      "total_flops_so_far": 6.467671990648448e+16,
      "budget_used_percent": 64.67671990648448
    },
    {
      "type": "training",
      "description": "Training step 2721",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:20",
      "total_flops_so_far": 6.470048413869286e+16,
      "budget_used_percent": 64.70048413869286
    },
    {
      "type": "training",
      "description": "Training step 2722",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:21",
      "total_flops_so_far": 6.472424837090125e+16,
      "budget_used_percent": 64.72424837090125
    },
    {
      "type": "training",
      "description": "Training step 2723",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:23",
      "total_flops_so_far": 6.474801260310963e+16,
      "budget_used_percent": 64.74801260310963
    },
    {
      "type": "training",
      "description": "Training step 2724",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:24",
      "total_flops_so_far": 6.477177683531802e+16,
      "budget_used_percent": 64.77177683531802
    },
    {
      "type": "training",
      "description": "Training step 2725",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:26",
      "total_flops_so_far": 6.47955410675264e+16,
      "budget_used_percent": 64.7955410675264
    },
    {
      "type": "training",
      "description": "Training step 2726",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:27",
      "total_flops_so_far": 6.481930529973478e+16,
      "budget_used_percent": 64.8193052997348
    },
    {
      "type": "training",
      "description": "Training step 2727",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:28",
      "total_flops_so_far": 6.484306953194317e+16,
      "budget_used_percent": 64.84306953194317
    },
    {
      "type": "training",
      "description": "Training step 2728",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:30",
      "total_flops_so_far": 6.486683376415155e+16,
      "budget_used_percent": 64.86683376415155
    },
    {
      "type": "training",
      "description": "Training step 2729",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:31",
      "total_flops_so_far": 6.489059799635994e+16,
      "budget_used_percent": 64.89059799635993
    },
    {
      "type": "training",
      "description": "Training step 2730",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:32",
      "total_flops_so_far": 6.491436222856832e+16,
      "budget_used_percent": 64.91436222856832
    },
    {
      "type": "training",
      "description": "Training step 2731",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:34",
      "total_flops_so_far": 6.49381264607767e+16,
      "budget_used_percent": 64.9381264607767
    },
    {
      "type": "training",
      "description": "Training step 2732",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:35",
      "total_flops_so_far": 6.496189069298509e+16,
      "budget_used_percent": 64.9618906929851
    },
    {
      "type": "training",
      "description": "Training step 2733",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:36",
      "total_flops_so_far": 6.498565492519347e+16,
      "budget_used_percent": 64.98565492519347
    },
    {
      "type": "training",
      "description": "Training step 2734",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:38",
      "total_flops_so_far": 6.500941915740186e+16,
      "budget_used_percent": 65.00941915740187
    },
    {
      "type": "training",
      "description": "Training step 2735",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:39",
      "total_flops_so_far": 6.503318338961024e+16,
      "budget_used_percent": 65.03318338961024
    },
    {
      "type": "training",
      "description": "Training step 2736",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:40",
      "total_flops_so_far": 6.505694762181862e+16,
      "budget_used_percent": 65.05694762181862
    },
    {
      "type": "training",
      "description": "Training step 2737",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:42",
      "total_flops_so_far": 6.508071185402701e+16,
      "budget_used_percent": 65.080711854027
    },
    {
      "type": "training",
      "description": "Training step 2738",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:43",
      "total_flops_so_far": 6.510447608623539e+16,
      "budget_used_percent": 65.1044760862354
    },
    {
      "type": "training",
      "description": "Training step 2739",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:44",
      "total_flops_so_far": 6.512824031844378e+16,
      "budget_used_percent": 65.12824031844377
    },
    {
      "type": "training",
      "description": "Training step 2740",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:46",
      "total_flops_so_far": 6.515200455065216e+16,
      "budget_used_percent": 65.15200455065217
    },
    {
      "type": "training",
      "description": "Training step 2741",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:47",
      "total_flops_so_far": 6.517576878286054e+16,
      "budget_used_percent": 65.17576878286054
    },
    {
      "type": "training",
      "description": "Training step 2742",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:48",
      "total_flops_so_far": 6.519953301506893e+16,
      "budget_used_percent": 65.19953301506894
    },
    {
      "type": "training",
      "description": "Training step 2743",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:50",
      "total_flops_so_far": 6.522329724727731e+16,
      "budget_used_percent": 65.22329724727732
    },
    {
      "type": "training",
      "description": "Training step 2744",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:51",
      "total_flops_so_far": 6.52470614794857e+16,
      "budget_used_percent": 65.2470614794857
    },
    {
      "type": "training",
      "description": "Training step 2745",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:52",
      "total_flops_so_far": 6.527082571169408e+16,
      "budget_used_percent": 65.27082571169409
    },
    {
      "type": "training",
      "description": "Training step 2746",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:54",
      "total_flops_so_far": 6.529458994390246e+16,
      "budget_used_percent": 65.29458994390247
    },
    {
      "type": "training",
      "description": "Training step 2747",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:55",
      "total_flops_so_far": 6.531835417611085e+16,
      "budget_used_percent": 65.31835417611084
    },
    {
      "type": "training",
      "description": "Training step 2748",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:57",
      "total_flops_so_far": 6.534211840831923e+16,
      "budget_used_percent": 65.34211840831922
    },
    {
      "type": "training",
      "description": "Training step 2749",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:58",
      "total_flops_so_far": 6.536588264052762e+16,
      "budget_used_percent": 65.36588264052762
    },
    {
      "type": "training",
      "description": "Training step 2750",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:11:59",
      "total_flops_so_far": 6.5389646872736e+16,
      "budget_used_percent": 65.389646872736
    },
    {
      "type": "training",
      "description": "Training step 2751",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:01",
      "total_flops_so_far": 6.541341110494438e+16,
      "budget_used_percent": 65.41341110494439
    },
    {
      "type": "training",
      "description": "Training step 2752",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:02",
      "total_flops_so_far": 6.543717533715277e+16,
      "budget_used_percent": 65.43717533715277
    },
    {
      "type": "training",
      "description": "Training step 2753",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:03",
      "total_flops_so_far": 6.546093956936115e+16,
      "budget_used_percent": 65.46093956936116
    },
    {
      "type": "training",
      "description": "Training step 2754",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:05",
      "total_flops_so_far": 6.548470380156954e+16,
      "budget_used_percent": 65.48470380156954
    },
    {
      "type": "training",
      "description": "Training step 2755",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:06",
      "total_flops_so_far": 6.550846803377792e+16,
      "budget_used_percent": 65.50846803377792
    },
    {
      "type": "training",
      "description": "Training step 2756",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:07",
      "total_flops_so_far": 6.55322322659863e+16,
      "budget_used_percent": 65.5322322659863
    },
    {
      "type": "training",
      "description": "Training step 2757",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:09",
      "total_flops_so_far": 6.555599649819469e+16,
      "budget_used_percent": 65.55599649819469
    },
    {
      "type": "training",
      "description": "Training step 2758",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:10",
      "total_flops_so_far": 6.557976073040307e+16,
      "budget_used_percent": 65.57976073040307
    },
    {
      "type": "training",
      "description": "Training step 2759",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:11",
      "total_flops_so_far": 6.560352496261146e+16,
      "budget_used_percent": 65.60352496261146
    },
    {
      "type": "training",
      "description": "Training step 2760",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:13",
      "total_flops_so_far": 6.562728919481984e+16,
      "budget_used_percent": 65.62728919481984
    },
    {
      "type": "training",
      "description": "Training step 2761",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:14",
      "total_flops_so_far": 6.565105342702822e+16,
      "budget_used_percent": 65.65105342702823
    },
    {
      "type": "training",
      "description": "Training step 2762",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:15",
      "total_flops_so_far": 6.567481765923661e+16,
      "budget_used_percent": 65.67481765923661
    },
    {
      "type": "training",
      "description": "Training step 2763",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:17",
      "total_flops_so_far": 6.569858189144499e+16,
      "budget_used_percent": 65.698581891445
    },
    {
      "type": "training",
      "description": "Training step 2764",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:18",
      "total_flops_so_far": 6.572234612365338e+16,
      "budget_used_percent": 65.72234612365338
    },
    {
      "type": "training",
      "description": "Training step 2765",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:19",
      "total_flops_so_far": 6.574611035586176e+16,
      "budget_used_percent": 65.74611035586176
    },
    {
      "type": "training",
      "description": "Training step 2766",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:21",
      "total_flops_so_far": 6.576987458807014e+16,
      "budget_used_percent": 65.76987458807014
    },
    {
      "type": "training",
      "description": "Training step 2767",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:22",
      "total_flops_so_far": 6.579363882027853e+16,
      "budget_used_percent": 65.79363882027853
    },
    {
      "type": "training",
      "description": "Training step 2768",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:23",
      "total_flops_so_far": 6.581740305248691e+16,
      "budget_used_percent": 65.81740305248691
    },
    {
      "type": "training",
      "description": "Training step 2769",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:25",
      "total_flops_so_far": 6.58411672846953e+16,
      "budget_used_percent": 65.8411672846953
    },
    {
      "type": "training",
      "description": "Training step 2770",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:26",
      "total_flops_so_far": 6.586493151690368e+16,
      "budget_used_percent": 65.86493151690368
    },
    {
      "type": "training",
      "description": "Training step 2771",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:27",
      "total_flops_so_far": 6.588869574911206e+16,
      "budget_used_percent": 65.88869574911207
    },
    {
      "type": "training",
      "description": "Training step 2772",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:28",
      "total_flops_so_far": 6.591245998132045e+16,
      "budget_used_percent": 65.91245998132045
    },
    {
      "type": "training",
      "description": "Training step 2773",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:29",
      "total_flops_so_far": 6.593622421352883e+16,
      "budget_used_percent": 65.93622421352883
    },
    {
      "type": "training",
      "description": "Training step 2774",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:31",
      "total_flops_so_far": 6.595998844573722e+16,
      "budget_used_percent": 65.95998844573721
    },
    {
      "type": "training",
      "description": "Training step 2775",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:32",
      "total_flops_so_far": 6.59837526779456e+16,
      "budget_used_percent": 65.98375267794559
    },
    {
      "type": "training",
      "description": "Training step 2776",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:33",
      "total_flops_so_far": 6.600751691015398e+16,
      "budget_used_percent": 66.00751691015398
    },
    {
      "type": "training",
      "description": "Training step 2777",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:35",
      "total_flops_so_far": 6.603128114236237e+16,
      "budget_used_percent": 66.03128114236236
    },
    {
      "type": "training",
      "description": "Training step 2778",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:36",
      "total_flops_so_far": 6.605504537457075e+16,
      "budget_used_percent": 66.05504537457075
    },
    {
      "type": "training",
      "description": "Training step 2779",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:37",
      "total_flops_so_far": 6.607880960677914e+16,
      "budget_used_percent": 66.07880960677913
    },
    {
      "type": "training",
      "description": "Training step 2780",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:39",
      "total_flops_so_far": 6.610257383898752e+16,
      "budget_used_percent": 66.10257383898752
    },
    {
      "type": "training",
      "description": "Training step 2781",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:40",
      "total_flops_so_far": 6.61263380711959e+16,
      "budget_used_percent": 66.1263380711959
    },
    {
      "type": "training",
      "description": "Training step 2782",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:41",
      "total_flops_so_far": 6.615010230340429e+16,
      "budget_used_percent": 66.1501023034043
    },
    {
      "type": "training",
      "description": "Training step 2783",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:43",
      "total_flops_so_far": 6.617386653561267e+16,
      "budget_used_percent": 66.17386653561267
    },
    {
      "type": "training",
      "description": "Training step 2784",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:44",
      "total_flops_so_far": 6.619763076782106e+16,
      "budget_used_percent": 66.19763076782105
    },
    {
      "type": "training",
      "description": "Training step 2785",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:45",
      "total_flops_so_far": 6.622139500002944e+16,
      "budget_used_percent": 66.22139500002943
    },
    {
      "type": "training",
      "description": "Training step 2786",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:47",
      "total_flops_so_far": 6.624515923223782e+16,
      "budget_used_percent": 66.24515923223782
    },
    {
      "type": "training",
      "description": "Training step 2787",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:48",
      "total_flops_so_far": 6.626892346444621e+16,
      "budget_used_percent": 66.2689234644462
    },
    {
      "type": "training",
      "description": "Training step 2788",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:49",
      "total_flops_so_far": 6.629268769665459e+16,
      "budget_used_percent": 66.2926876966546
    },
    {
      "type": "training",
      "description": "Training step 2789",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:51",
      "total_flops_so_far": 6.631645192886298e+16,
      "budget_used_percent": 66.31645192886297
    },
    {
      "type": "training",
      "description": "Training step 2790",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:52",
      "total_flops_so_far": 6.634021616107136e+16,
      "budget_used_percent": 66.34021616107137
    },
    {
      "type": "training",
      "description": "Training step 2791",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:53",
      "total_flops_so_far": 6.636398039327974e+16,
      "budget_used_percent": 66.36398039327975
    },
    {
      "type": "training",
      "description": "Training step 2792",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:55",
      "total_flops_so_far": 6.638774462548813e+16,
      "budget_used_percent": 66.38774462548812
    },
    {
      "type": "training",
      "description": "Training step 2793",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:56",
      "total_flops_so_far": 6.641150885769651e+16,
      "budget_used_percent": 66.4115088576965
    },
    {
      "type": "training",
      "description": "Training step 2794",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:57",
      "total_flops_so_far": 6.64352730899049e+16,
      "budget_used_percent": 66.4352730899049
    },
    {
      "type": "training",
      "description": "Training step 2795",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:12:59",
      "total_flops_so_far": 6.645903732211328e+16,
      "budget_used_percent": 66.45903732211328
    },
    {
      "type": "training",
      "description": "Training step 2796",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:00",
      "total_flops_so_far": 6.648280155432166e+16,
      "budget_used_percent": 66.48280155432167
    },
    {
      "type": "training",
      "description": "Training step 2797",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:01",
      "total_flops_so_far": 6.650656578653005e+16,
      "budget_used_percent": 66.50656578653005
    },
    {
      "type": "training",
      "description": "Training step 2798",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:03",
      "total_flops_so_far": 6.653033001873843e+16,
      "budget_used_percent": 66.53033001873844
    },
    {
      "type": "training",
      "description": "Training step 2799",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:04",
      "total_flops_so_far": 6.655409425094682e+16,
      "budget_used_percent": 66.55409425094682
    },
    {
      "type": "training",
      "description": "Training step 2800",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:05",
      "total_flops_so_far": 6.65778584831552e+16,
      "budget_used_percent": 66.5778584831552
    },
    {
      "type": "training",
      "description": "Training step 2801",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:07",
      "total_flops_so_far": 6.660162271536358e+16,
      "budget_used_percent": 66.60162271536359
    },
    {
      "type": "training",
      "description": "Training step 2802",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:08",
      "total_flops_so_far": 6.662538694757197e+16,
      "budget_used_percent": 66.62538694757197
    },
    {
      "type": "training",
      "description": "Training step 2803",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:09",
      "total_flops_so_far": 6.664915117978035e+16,
      "budget_used_percent": 66.64915117978035
    },
    {
      "type": "training",
      "description": "Training step 2804",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:11",
      "total_flops_so_far": 6.667291541198874e+16,
      "budget_used_percent": 66.67291541198873
    },
    {
      "type": "training",
      "description": "Training step 2805",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:12",
      "total_flops_so_far": 6.669667964419712e+16,
      "budget_used_percent": 66.69667964419712
    },
    {
      "type": "training",
      "description": "Training step 2806",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:14",
      "total_flops_so_far": 6.67204438764055e+16,
      "budget_used_percent": 66.7204438764055
    },
    {
      "type": "training",
      "description": "Training step 2807",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:15",
      "total_flops_so_far": 6.674420810861389e+16,
      "budget_used_percent": 66.74420810861389
    },
    {
      "type": "training",
      "description": "Training step 2808",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:16",
      "total_flops_so_far": 6.676797234082227e+16,
      "budget_used_percent": 66.76797234082227
    },
    {
      "type": "training",
      "description": "Training step 2809",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:18",
      "total_flops_so_far": 6.679173657303066e+16,
      "budget_used_percent": 66.79173657303066
    },
    {
      "type": "training",
      "description": "Training step 2810",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:19",
      "total_flops_so_far": 6.681550080523904e+16,
      "budget_used_percent": 66.81550080523904
    },
    {
      "type": "training",
      "description": "Training step 2811",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:20",
      "total_flops_so_far": 6.683926503744742e+16,
      "budget_used_percent": 66.83926503744743
    },
    {
      "type": "training",
      "description": "Training step 2812",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:22",
      "total_flops_so_far": 6.686302926965581e+16,
      "budget_used_percent": 66.86302926965581
    },
    {
      "type": "training",
      "description": "Training step 2813",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:23",
      "total_flops_so_far": 6.688679350186419e+16,
      "budget_used_percent": 66.88679350186419
    },
    {
      "type": "training",
      "description": "Training step 2814",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:24",
      "total_flops_so_far": 6.691055773407258e+16,
      "budget_used_percent": 66.91055773407257
    },
    {
      "type": "training",
      "description": "Training step 2815",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:26",
      "total_flops_so_far": 6.693432196628096e+16,
      "budget_used_percent": 66.93432196628096
    },
    {
      "type": "training",
      "description": "Training step 2816",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:27",
      "total_flops_so_far": 6.695808619848934e+16,
      "budget_used_percent": 66.95808619848934
    },
    {
      "type": "training",
      "description": "Training step 2817",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:28",
      "total_flops_so_far": 6.698185043069773e+16,
      "budget_used_percent": 66.98185043069773
    },
    {
      "type": "training",
      "description": "Training step 2818",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:30",
      "total_flops_so_far": 6.700561466290611e+16,
      "budget_used_percent": 67.00561466290611
    },
    {
      "type": "training",
      "description": "Training step 2819",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:31",
      "total_flops_so_far": 6.70293788951145e+16,
      "budget_used_percent": 67.0293788951145
    },
    {
      "type": "training",
      "description": "Training step 2820",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:32",
      "total_flops_so_far": 6.705314312732288e+16,
      "budget_used_percent": 67.05314312732288
    },
    {
      "type": "training",
      "description": "Training step 2821",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:34",
      "total_flops_so_far": 6.707690735953126e+16,
      "budget_used_percent": 67.07690735953126
    },
    {
      "type": "training",
      "description": "Training step 2822",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:35",
      "total_flops_so_far": 6.710067159173965e+16,
      "budget_used_percent": 67.10067159173964
    },
    {
      "type": "training",
      "description": "Training step 2823",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:36",
      "total_flops_so_far": 6.712443582394803e+16,
      "budget_used_percent": 67.12443582394803
    },
    {
      "type": "training",
      "description": "Training step 2824",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:38",
      "total_flops_so_far": 6.714820005615642e+16,
      "budget_used_percent": 67.14820005615641
    },
    {
      "type": "training",
      "description": "Training step 2825",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:39",
      "total_flops_so_far": 6.71719642883648e+16,
      "budget_used_percent": 67.1719642883648
    },
    {
      "type": "training",
      "description": "Training step 2826",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:41",
      "total_flops_so_far": 6.719572852057318e+16,
      "budget_used_percent": 67.19572852057318
    },
    {
      "type": "training",
      "description": "Training step 2827",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:42",
      "total_flops_so_far": 6.721949275278157e+16,
      "budget_used_percent": 67.21949275278158
    },
    {
      "type": "training",
      "description": "Training step 2828",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:43",
      "total_flops_so_far": 6.724325698498995e+16,
      "budget_used_percent": 67.24325698498996
    },
    {
      "type": "training",
      "description": "Training step 2829",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:45",
      "total_flops_so_far": 6.726702121719834e+16,
      "budget_used_percent": 67.26702121719833
    },
    {
      "type": "training",
      "description": "Training step 2830",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:46",
      "total_flops_so_far": 6.729078544940672e+16,
      "budget_used_percent": 67.29078544940673
    },
    {
      "type": "training",
      "description": "Training step 2831",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:47",
      "total_flops_so_far": 6.73145496816151e+16,
      "budget_used_percent": 67.3145496816151
    },
    {
      "type": "training",
      "description": "Training step 2832",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:49",
      "total_flops_so_far": 6.733831391382349e+16,
      "budget_used_percent": 67.33831391382348
    },
    {
      "type": "training",
      "description": "Training step 2833",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:50",
      "total_flops_so_far": 6.736207814603187e+16,
      "budget_used_percent": 67.36207814603186
    },
    {
      "type": "training",
      "description": "Training step 2834",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:51",
      "total_flops_so_far": 6.738584237824026e+16,
      "budget_used_percent": 67.38584237824026
    },
    {
      "type": "training",
      "description": "Training step 2835",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:53",
      "total_flops_so_far": 6.740960661044864e+16,
      "budget_used_percent": 67.40960661044863
    },
    {
      "type": "training",
      "description": "Training step 2836",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:54",
      "total_flops_so_far": 6.743337084265702e+16,
      "budget_used_percent": 67.43337084265703
    },
    {
      "type": "training",
      "description": "Training step 2837",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:55",
      "total_flops_so_far": 6.745713507486541e+16,
      "budget_used_percent": 67.4571350748654
    },
    {
      "type": "training",
      "description": "Training step 2838",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:57",
      "total_flops_so_far": 6.748089930707379e+16,
      "budget_used_percent": 67.4808993070738
    },
    {
      "type": "training",
      "description": "Training step 2839",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:58",
      "total_flops_so_far": 6.750466353928218e+16,
      "budget_used_percent": 67.50466353928218
    },
    {
      "type": "training",
      "description": "Training step 2840",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:13:59",
      "total_flops_so_far": 6.752842777149056e+16,
      "budget_used_percent": 67.52842777149056
    },
    {
      "type": "training",
      "description": "Training step 2841",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:01",
      "total_flops_so_far": 6.755219200369894e+16,
      "budget_used_percent": 67.55219200369893
    },
    {
      "type": "training",
      "description": "Training step 2842",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:02",
      "total_flops_so_far": 6.757595623590733e+16,
      "budget_used_percent": 67.57595623590733
    },
    {
      "type": "training",
      "description": "Training step 2843",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:03",
      "total_flops_so_far": 6.759972046811571e+16,
      "budget_used_percent": 67.5997204681157
    },
    {
      "type": "training",
      "description": "Training step 2844",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:05",
      "total_flops_so_far": 6.76234847003241e+16,
      "budget_used_percent": 67.6234847003241
    },
    {
      "type": "training",
      "description": "Training step 2845",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:06",
      "total_flops_so_far": 6.764724893253248e+16,
      "budget_used_percent": 67.64724893253248
    },
    {
      "type": "training",
      "description": "Training step 2846",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:07",
      "total_flops_so_far": 6.767101316474086e+16,
      "budget_used_percent": 67.67101316474087
    },
    {
      "type": "training",
      "description": "Training step 2847",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:09",
      "total_flops_so_far": 6.769477739694925e+16,
      "budget_used_percent": 67.69477739694925
    },
    {
      "type": "training",
      "description": "Training step 2848",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:10",
      "total_flops_so_far": 6.771854162915763e+16,
      "budget_used_percent": 67.71854162915764
    },
    {
      "type": "training",
      "description": "Training step 2849",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:11",
      "total_flops_so_far": 6.774230586136602e+16,
      "budget_used_percent": 67.74230586136602
    },
    {
      "type": "training",
      "description": "Training step 2850",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:13",
      "total_flops_so_far": 6.77660700935744e+16,
      "budget_used_percent": 67.7660700935744
    },
    {
      "type": "training",
      "description": "Training step 2851",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:14",
      "total_flops_so_far": 6.778983432578278e+16,
      "budget_used_percent": 67.78983432578278
    },
    {
      "type": "training",
      "description": "Training step 2852",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:15",
      "total_flops_so_far": 6.781359855799117e+16,
      "budget_used_percent": 67.81359855799117
    },
    {
      "type": "training",
      "description": "Training step 2853",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:17",
      "total_flops_so_far": 6.783736279019955e+16,
      "budget_used_percent": 67.83736279019955
    },
    {
      "type": "training",
      "description": "Training step 2854",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:18",
      "total_flops_so_far": 6.786112702240794e+16,
      "budget_used_percent": 67.86112702240794
    },
    {
      "type": "training",
      "description": "Training step 2855",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:19",
      "total_flops_so_far": 6.788489125461632e+16,
      "budget_used_percent": 67.88489125461632
    },
    {
      "type": "training",
      "description": "Training step 2856",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:21",
      "total_flops_so_far": 6.79086554868247e+16,
      "budget_used_percent": 67.90865548682471
    },
    {
      "type": "training",
      "description": "Training step 2857",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:22",
      "total_flops_so_far": 6.793241971903309e+16,
      "budget_used_percent": 67.93241971903309
    },
    {
      "type": "training",
      "description": "Training step 2858",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:23",
      "total_flops_so_far": 6.795618395124147e+16,
      "budget_used_percent": 67.95618395124147
    },
    {
      "type": "training",
      "description": "Training step 2859",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:25",
      "total_flops_so_far": 6.797994818344986e+16,
      "budget_used_percent": 67.97994818344986
    },
    {
      "type": "training",
      "description": "Training step 2860",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:26",
      "total_flops_so_far": 6.800371241565824e+16,
      "budget_used_percent": 68.00371241565824
    },
    {
      "type": "training",
      "description": "Training step 2861",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:28",
      "total_flops_so_far": 6.802747664786662e+16,
      "budget_used_percent": 68.02747664786662
    },
    {
      "type": "training",
      "description": "Training step 2862",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:29",
      "total_flops_so_far": 6.805124088007501e+16,
      "budget_used_percent": 68.051240880075
    },
    {
      "type": "training",
      "description": "Training step 2863",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:30",
      "total_flops_so_far": 6.807500511228339e+16,
      "budget_used_percent": 68.07500511228339
    },
    {
      "type": "training",
      "description": "Training step 2864",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:32",
      "total_flops_so_far": 6.809876934449178e+16,
      "budget_used_percent": 68.09876934449177
    },
    {
      "type": "training",
      "description": "Training step 2865",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:33",
      "total_flops_so_far": 6.812253357670016e+16,
      "budget_used_percent": 68.12253357670016
    },
    {
      "type": "training",
      "description": "Training step 2866",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:34",
      "total_flops_so_far": 6.814629780890854e+16,
      "budget_used_percent": 68.14629780890854
    },
    {
      "type": "training",
      "description": "Training step 2867",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:36",
      "total_flops_so_far": 6.817006204111693e+16,
      "budget_used_percent": 68.17006204111694
    },
    {
      "type": "training",
      "description": "Training step 2868",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:37",
      "total_flops_so_far": 6.819382627332531e+16,
      "budget_used_percent": 68.19382627332531
    },
    {
      "type": "training",
      "description": "Training step 2869",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:38",
      "total_flops_so_far": 6.82175905055337e+16,
      "budget_used_percent": 68.21759050553369
    },
    {
      "type": "training",
      "description": "Training step 2870",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:40",
      "total_flops_so_far": 6.824135473774208e+16,
      "budget_used_percent": 68.24135473774207
    },
    {
      "type": "training",
      "description": "Training step 2871",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:41",
      "total_flops_so_far": 6.826511896995046e+16,
      "budget_used_percent": 68.26511896995046
    },
    {
      "type": "training",
      "description": "Training step 2872",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:42",
      "total_flops_so_far": 6.828888320215885e+16,
      "budget_used_percent": 68.28888320215884
    },
    {
      "type": "training",
      "description": "Training step 2873",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:44",
      "total_flops_so_far": 6.831264743436723e+16,
      "budget_used_percent": 68.31264743436724
    },
    {
      "type": "training",
      "description": "Training step 2874",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:45",
      "total_flops_so_far": 6.833641166657562e+16,
      "budget_used_percent": 68.33641166657561
    },
    {
      "type": "training",
      "description": "Training step 2875",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:46",
      "total_flops_so_far": 6.8360175898784e+16,
      "budget_used_percent": 68.36017589878401
    },
    {
      "type": "training",
      "description": "Training step 2876",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:48",
      "total_flops_so_far": 6.838394013099238e+16,
      "budget_used_percent": 68.38394013099239
    },
    {
      "type": "training",
      "description": "Training step 2877",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:49",
      "total_flops_so_far": 6.840770436320077e+16,
      "budget_used_percent": 68.40770436320078
    },
    {
      "type": "training",
      "description": "Training step 2878",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:50",
      "total_flops_so_far": 6.843146859540915e+16,
      "budget_used_percent": 68.43146859540916
    },
    {
      "type": "training",
      "description": "Training step 2879",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:52",
      "total_flops_so_far": 6.845523282761754e+16,
      "budget_used_percent": 68.45523282761754
    },
    {
      "type": "training",
      "description": "Training step 2880",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:53",
      "total_flops_so_far": 6.847899705982592e+16,
      "budget_used_percent": 68.47899705982591
    },
    {
      "type": "training",
      "description": "Training step 2881",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:54",
      "total_flops_so_far": 6.85027612920343e+16,
      "budget_used_percent": 68.50276129203431
    },
    {
      "type": "training",
      "description": "Training step 2882",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:56",
      "total_flops_so_far": 6.852652552424269e+16,
      "budget_used_percent": 68.52652552424269
    },
    {
      "type": "training",
      "description": "Training step 2883",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:57",
      "total_flops_so_far": 6.855028975645107e+16,
      "budget_used_percent": 68.55028975645108
    },
    {
      "type": "training",
      "description": "Training step 2884",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:14:58",
      "total_flops_so_far": 6.857405398865946e+16,
      "budget_used_percent": 68.57405398865946
    },
    {
      "type": "training",
      "description": "Training step 2885",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:00",
      "total_flops_so_far": 6.859781822086784e+16,
      "budget_used_percent": 68.59781822086785
    },
    {
      "type": "training",
      "description": "Training step 2886",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:01",
      "total_flops_so_far": 6.862158245307622e+16,
      "budget_used_percent": 68.62158245307623
    },
    {
      "type": "training",
      "description": "Training step 2887",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:02",
      "total_flops_so_far": 6.864534668528461e+16,
      "budget_used_percent": 68.64534668528461
    },
    {
      "type": "training",
      "description": "Training step 2888",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:04",
      "total_flops_so_far": 6.866911091749299e+16,
      "budget_used_percent": 68.66911091749299
    },
    {
      "type": "training",
      "description": "Training step 2889",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:05",
      "total_flops_so_far": 6.869287514970138e+16,
      "budget_used_percent": 68.69287514970136
    },
    {
      "type": "training",
      "description": "Training step 2890",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:07",
      "total_flops_so_far": 6.871663938190976e+16,
      "budget_used_percent": 68.71663938190976
    },
    {
      "type": "training",
      "description": "Training step 2891",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:08",
      "total_flops_so_far": 6.874040361411814e+16,
      "budget_used_percent": 68.74040361411814
    },
    {
      "type": "training",
      "description": "Training step 2892",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:09",
      "total_flops_so_far": 6.876416784632653e+16,
      "budget_used_percent": 68.76416784632653
    },
    {
      "type": "training",
      "description": "Training step 2893",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:11",
      "total_flops_so_far": 6.878793207853491e+16,
      "budget_used_percent": 68.78793207853491
    },
    {
      "type": "training",
      "description": "Training step 2894",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:12",
      "total_flops_so_far": 6.88116963107433e+16,
      "budget_used_percent": 68.8116963107433
    },
    {
      "type": "training",
      "description": "Training step 2895",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:13",
      "total_flops_so_far": 6.883546054295168e+16,
      "budget_used_percent": 68.83546054295168
    },
    {
      "type": "training",
      "description": "Training step 2896",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:15",
      "total_flops_so_far": 6.885922477516006e+16,
      "budget_used_percent": 68.85922477516007
    },
    {
      "type": "training",
      "description": "Training step 2897",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:16",
      "total_flops_so_far": 6.888298900736845e+16,
      "budget_used_percent": 68.88298900736845
    },
    {
      "type": "training",
      "description": "Training step 2898",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:17",
      "total_flops_so_far": 6.890675323957683e+16,
      "budget_used_percent": 68.90675323957683
    },
    {
      "type": "training",
      "description": "Training step 2899",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:19",
      "total_flops_so_far": 6.893051747178522e+16,
      "budget_used_percent": 68.93051747178521
    },
    {
      "type": "training",
      "description": "Training step 2900",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:20",
      "total_flops_so_far": 6.89542817039936e+16,
      "budget_used_percent": 68.9542817039936
    },
    {
      "type": "training",
      "description": "Training step 2901",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:21",
      "total_flops_so_far": 6.897804593620198e+16,
      "budget_used_percent": 68.97804593620198
    },
    {
      "type": "training",
      "description": "Training step 2902",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:23",
      "total_flops_so_far": 6.900181016841037e+16,
      "budget_used_percent": 69.00181016841037
    },
    {
      "type": "training",
      "description": "Training step 2903",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:24",
      "total_flops_so_far": 6.902557440061875e+16,
      "budget_used_percent": 69.02557440061875
    },
    {
      "type": "training",
      "description": "Training step 2904",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:25",
      "total_flops_so_far": 6.904933863282714e+16,
      "budget_used_percent": 69.04933863282714
    },
    {
      "type": "training",
      "description": "Training step 2905",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:26",
      "total_flops_so_far": 6.907310286503552e+16,
      "budget_used_percent": 69.07310286503552
    },
    {
      "type": "training",
      "description": "Training step 2906",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:27",
      "total_flops_so_far": 6.90968670972439e+16,
      "budget_used_percent": 69.0968670972439
    },
    {
      "type": "training",
      "description": "Training step 2907",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:29",
      "total_flops_so_far": 6.912063132945229e+16,
      "budget_used_percent": 69.12063132945228
    },
    {
      "type": "training",
      "description": "Training step 2908",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:30",
      "total_flops_so_far": 6.914439556166067e+16,
      "budget_used_percent": 69.14439556166067
    },
    {
      "type": "training",
      "description": "Training step 2909",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:31",
      "total_flops_so_far": 6.916815979386906e+16,
      "budget_used_percent": 69.16815979386905
    },
    {
      "type": "training",
      "description": "Training step 2910",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:33",
      "total_flops_so_far": 6.919192402607744e+16,
      "budget_used_percent": 69.19192402607744
    },
    {
      "type": "training",
      "description": "Training step 2911",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:34",
      "total_flops_so_far": 6.921568825828582e+16,
      "budget_used_percent": 69.21568825828582
    },
    {
      "type": "training",
      "description": "Training step 2912",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:35",
      "total_flops_so_far": 6.923945249049421e+16,
      "budget_used_percent": 69.23945249049422
    },
    {
      "type": "training",
      "description": "Training step 2913",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:37",
      "total_flops_so_far": 6.926321672270259e+16,
      "budget_used_percent": 69.2632167227026
    },
    {
      "type": "training",
      "description": "Training step 2914",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:38",
      "total_flops_so_far": 6.928698095491098e+16,
      "budget_used_percent": 69.28698095491097
    },
    {
      "type": "training",
      "description": "Training step 2915",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:39",
      "total_flops_so_far": 6.931074518711936e+16,
      "budget_used_percent": 69.31074518711937
    },
    {
      "type": "training",
      "description": "Training step 2916",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:41",
      "total_flops_so_far": 6.933450941932774e+16,
      "budget_used_percent": 69.33450941932774
    },
    {
      "type": "training",
      "description": "Training step 2917",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:42",
      "total_flops_so_far": 6.935827365153613e+16,
      "budget_used_percent": 69.35827365153612
    },
    {
      "type": "training",
      "description": "Training step 2918",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:43",
      "total_flops_so_far": 6.938203788374451e+16,
      "budget_used_percent": 69.3820378837445
    },
    {
      "type": "training",
      "description": "Training step 2919",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:45",
      "total_flops_so_far": 6.94058021159529e+16,
      "budget_used_percent": 69.4058021159529
    },
    {
      "type": "training",
      "description": "Training step 2920",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:46",
      "total_flops_so_far": 6.942956634816128e+16,
      "budget_used_percent": 69.42956634816127
    },
    {
      "type": "training",
      "description": "Training step 2921",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:47",
      "total_flops_so_far": 6.945333058036966e+16,
      "budget_used_percent": 69.45333058036967
    },
    {
      "type": "training",
      "description": "Training step 2922",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:49",
      "total_flops_so_far": 6.947709481257805e+16,
      "budget_used_percent": 69.47709481257804
    },
    {
      "type": "training",
      "description": "Training step 2923",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:50",
      "total_flops_so_far": 6.950085904478643e+16,
      "budget_used_percent": 69.50085904478644
    },
    {
      "type": "training",
      "description": "Training step 2924",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:51",
      "total_flops_so_far": 6.952462327699482e+16,
      "budget_used_percent": 69.52462327699482
    },
    {
      "type": "training",
      "description": "Training step 2925",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:53",
      "total_flops_so_far": 6.95483875092032e+16,
      "budget_used_percent": 69.5483875092032
    },
    {
      "type": "training",
      "description": "Training step 2926",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:54",
      "total_flops_so_far": 6.957215174141158e+16,
      "budget_used_percent": 69.57215174141157
    },
    {
      "type": "training",
      "description": "Training step 2927",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:56",
      "total_flops_so_far": 6.959591597361997e+16,
      "budget_used_percent": 69.59591597361997
    },
    {
      "type": "training",
      "description": "Training step 2928",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:57",
      "total_flops_so_far": 6.961968020582835e+16,
      "budget_used_percent": 69.61968020582835
    },
    {
      "type": "training",
      "description": "Training step 2929",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:15:58",
      "total_flops_so_far": 6.964344443803674e+16,
      "budget_used_percent": 69.64344443803674
    },
    {
      "type": "training",
      "description": "Training step 2930",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:00",
      "total_flops_so_far": 6.966720867024512e+16,
      "budget_used_percent": 69.66720867024512
    },
    {
      "type": "training",
      "description": "Training step 2931",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:01",
      "total_flops_so_far": 6.96909729024535e+16,
      "budget_used_percent": 69.69097290245351
    },
    {
      "type": "training",
      "description": "Training step 2932",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:02",
      "total_flops_so_far": 6.971473713466189e+16,
      "budget_used_percent": 69.71473713466189
    },
    {
      "type": "training",
      "description": "Training step 2933",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:04",
      "total_flops_so_far": 6.973850136687027e+16,
      "budget_used_percent": 69.73850136687028
    },
    {
      "type": "training",
      "description": "Training step 2934",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:05",
      "total_flops_so_far": 6.976226559907866e+16,
      "budget_used_percent": 69.76226559907866
    },
    {
      "type": "training",
      "description": "Training step 2935",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:06",
      "total_flops_so_far": 6.978602983128704e+16,
      "budget_used_percent": 69.78602983128704
    },
    {
      "type": "training",
      "description": "Training step 2936",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:08",
      "total_flops_so_far": 6.980979406349542e+16,
      "budget_used_percent": 69.80979406349542
    },
    {
      "type": "training",
      "description": "Training step 2937",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:09",
      "total_flops_so_far": 6.983355829570381e+16,
      "budget_used_percent": 69.83355829570381
    },
    {
      "type": "training",
      "description": "Training step 2938",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:10",
      "total_flops_so_far": 6.985732252791219e+16,
      "budget_used_percent": 69.85732252791219
    },
    {
      "type": "training",
      "description": "Training step 2939",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:12",
      "total_flops_so_far": 6.988108676012058e+16,
      "budget_used_percent": 69.88108676012058
    },
    {
      "type": "training",
      "description": "Training step 2940",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:13",
      "total_flops_so_far": 6.990485099232896e+16,
      "budget_used_percent": 69.90485099232896
    },
    {
      "type": "training",
      "description": "Training step 2941",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:14",
      "total_flops_so_far": 6.992861522453734e+16,
      "budget_used_percent": 69.92861522453735
    },
    {
      "type": "training",
      "description": "Training step 2942",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:16",
      "total_flops_so_far": 6.995237945674573e+16,
      "budget_used_percent": 69.95237945674573
    },
    {
      "type": "training",
      "description": "Training step 2943",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:17",
      "total_flops_so_far": 6.997614368895411e+16,
      "budget_used_percent": 69.97614368895411
    },
    {
      "type": "training",
      "description": "Training step 2944",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:18",
      "total_flops_so_far": 6.99999079211625e+16,
      "budget_used_percent": 69.9999079211625
    },
    {
      "type": "training",
      "description": "Training step 2945",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:20",
      "total_flops_so_far": 7.002367215337088e+16,
      "budget_used_percent": 70.02367215337088
    },
    {
      "type": "training",
      "description": "Training step 2946",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:21",
      "total_flops_so_far": 7.004743638557926e+16,
      "budget_used_percent": 70.04743638557926
    },
    {
      "type": "training",
      "description": "Training step 2947",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:22",
      "total_flops_so_far": 7.007120061778765e+16,
      "budget_used_percent": 70.07120061778764
    },
    {
      "type": "training",
      "description": "Training step 2948",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:24",
      "total_flops_so_far": 7.009496484999603e+16,
      "budget_used_percent": 70.09496484999603
    },
    {
      "type": "training",
      "description": "Training step 2949",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:25",
      "total_flops_so_far": 7.011872908220442e+16,
      "budget_used_percent": 70.11872908220441
    },
    {
      "type": "training",
      "description": "Training step 2950",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:26",
      "total_flops_so_far": 7.01424933144128e+16,
      "budget_used_percent": 70.1424933144128
    },
    {
      "type": "training",
      "description": "Training step 2951",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:28",
      "total_flops_so_far": 7.016625754662118e+16,
      "budget_used_percent": 70.16625754662118
    },
    {
      "type": "training",
      "description": "Training step 2952",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:29",
      "total_flops_so_far": 7.019002177882957e+16,
      "budget_used_percent": 70.19002177882957
    },
    {
      "type": "training",
      "description": "Training step 2953",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:30",
      "total_flops_so_far": 7.021378601103795e+16,
      "budget_used_percent": 70.21378601103795
    },
    {
      "type": "training",
      "description": "Training step 2954",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:32",
      "total_flops_so_far": 7.023755024324634e+16,
      "budget_used_percent": 70.23755024324633
    },
    {
      "type": "training",
      "description": "Training step 2955",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:33",
      "total_flops_so_far": 7.026131447545472e+16,
      "budget_used_percent": 70.26131447545471
    },
    {
      "type": "training",
      "description": "Training step 2956",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:35",
      "total_flops_so_far": 7.02850787076631e+16,
      "budget_used_percent": 70.2850787076631
    },
    {
      "type": "training",
      "description": "Training step 2957",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:36",
      "total_flops_so_far": 7.030884293987149e+16,
      "budget_used_percent": 70.30884293987148
    },
    {
      "type": "training",
      "description": "Training step 2958",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:37",
      "total_flops_so_far": 7.033260717207987e+16,
      "budget_used_percent": 70.33260717207988
    },
    {
      "type": "training",
      "description": "Training step 2959",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:39",
      "total_flops_so_far": 7.035637140428826e+16,
      "budget_used_percent": 70.35637140428825
    },
    {
      "type": "training",
      "description": "Training step 2960",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:40",
      "total_flops_so_far": 7.038013563649664e+16,
      "budget_used_percent": 70.38013563649665
    },
    {
      "type": "training",
      "description": "Training step 2961",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:41",
      "total_flops_so_far": 7.040389986870502e+16,
      "budget_used_percent": 70.40389986870503
    },
    {
      "type": "training",
      "description": "Training step 2962",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:43",
      "total_flops_so_far": 7.042766410091341e+16,
      "budget_used_percent": 70.42766410091342
    },
    {
      "type": "training",
      "description": "Training step 2963",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:44",
      "total_flops_so_far": 7.045142833312179e+16,
      "budget_used_percent": 70.4514283331218
    },
    {
      "type": "training",
      "description": "Training step 2964",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:45",
      "total_flops_so_far": 7.047519256533018e+16,
      "budget_used_percent": 70.47519256533018
    },
    {
      "type": "training",
      "description": "Training step 2965",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:47",
      "total_flops_so_far": 7.049895679753856e+16,
      "budget_used_percent": 70.49895679753855
    },
    {
      "type": "training",
      "description": "Training step 2966",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:48",
      "total_flops_so_far": 7.052272102974694e+16,
      "budget_used_percent": 70.52272102974695
    },
    {
      "type": "training",
      "description": "Training step 2967",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:49",
      "total_flops_so_far": 7.054648526195533e+16,
      "budget_used_percent": 70.54648526195533
    },
    {
      "type": "training",
      "description": "Training step 2968",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:51",
      "total_flops_so_far": 7.057024949416371e+16,
      "budget_used_percent": 70.57024949416372
    },
    {
      "type": "training",
      "description": "Training step 2969",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:52",
      "total_flops_so_far": 7.05940137263721e+16,
      "budget_used_percent": 70.5940137263721
    },
    {
      "type": "training",
      "description": "Training step 2970",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:54",
      "total_flops_so_far": 7.061777795858048e+16,
      "budget_used_percent": 70.61777795858049
    },
    {
      "type": "training",
      "description": "Training step 2971",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:55",
      "total_flops_so_far": 7.064154219078886e+16,
      "budget_used_percent": 70.64154219078887
    },
    {
      "type": "training",
      "description": "Training step 2972",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:56",
      "total_flops_so_far": 7.066530642299725e+16,
      "budget_used_percent": 70.66530642299725
    },
    {
      "type": "training",
      "description": "Training step 2973",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:58",
      "total_flops_so_far": 7.068907065520563e+16,
      "budget_used_percent": 70.68907065520563
    },
    {
      "type": "training",
      "description": "Training step 2974",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:16:59",
      "total_flops_so_far": 7.071283488741402e+16,
      "budget_used_percent": 70.712834887414
    },
    {
      "type": "training",
      "description": "Training step 2975",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:00",
      "total_flops_so_far": 7.07365991196224e+16,
      "budget_used_percent": 70.7365991196224
    },
    {
      "type": "training",
      "description": "Training step 2976",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:02",
      "total_flops_so_far": 7.076036335183078e+16,
      "budget_used_percent": 70.76036335183078
    },
    {
      "type": "training",
      "description": "Training step 2977",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:03",
      "total_flops_so_far": 7.078412758403917e+16,
      "budget_used_percent": 70.78412758403917
    },
    {
      "type": "training",
      "description": "Training step 2978",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:04",
      "total_flops_so_far": 7.080789181624755e+16,
      "budget_used_percent": 70.80789181624755
    },
    {
      "type": "training",
      "description": "Training step 2979",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:06",
      "total_flops_so_far": 7.083165604845594e+16,
      "budget_used_percent": 70.83165604845594
    },
    {
      "type": "training",
      "description": "Training step 2980",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:07",
      "total_flops_so_far": 7.085542028066432e+16,
      "budget_used_percent": 70.85542028066432
    },
    {
      "type": "training",
      "description": "Training step 2981",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:08",
      "total_flops_so_far": 7.08791845128727e+16,
      "budget_used_percent": 70.87918451287271
    },
    {
      "type": "training",
      "description": "Training step 2982",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:10",
      "total_flops_so_far": 7.090294874508109e+16,
      "budget_used_percent": 70.90294874508109
    },
    {
      "type": "training",
      "description": "Training step 2983",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:11",
      "total_flops_so_far": 7.092671297728947e+16,
      "budget_used_percent": 70.92671297728947
    },
    {
      "type": "training",
      "description": "Training step 2984",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:12",
      "total_flops_so_far": 7.095047720949786e+16,
      "budget_used_percent": 70.95047720949785
    },
    {
      "type": "training",
      "description": "Training step 2985",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:14",
      "total_flops_so_far": 7.097424144170624e+16,
      "budget_used_percent": 70.97424144170624
    },
    {
      "type": "training",
      "description": "Training step 2986",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:15",
      "total_flops_so_far": 7.099800567391462e+16,
      "budget_used_percent": 70.99800567391462
    },
    {
      "type": "training",
      "description": "Training step 2987",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:16",
      "total_flops_so_far": 7.102176990612301e+16,
      "budget_used_percent": 71.02176990612301
    },
    {
      "type": "training",
      "description": "Training step 2988",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:18",
      "total_flops_so_far": 7.104553413833139e+16,
      "budget_used_percent": 71.04553413833139
    },
    {
      "type": "training",
      "description": "Training step 2989",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:19",
      "total_flops_so_far": 7.106929837053978e+16,
      "budget_used_percent": 71.06929837053978
    },
    {
      "type": "training",
      "description": "Training step 2990",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:20",
      "total_flops_so_far": 7.109306260274816e+16,
      "budget_used_percent": 71.09306260274816
    },
    {
      "type": "training",
      "description": "Training step 2991",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:22",
      "total_flops_so_far": 7.111682683495654e+16,
      "budget_used_percent": 71.11682683495656
    },
    {
      "type": "training",
      "description": "Training step 2992",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:23",
      "total_flops_so_far": 7.114059106716493e+16,
      "budget_used_percent": 71.14059106716493
    },
    {
      "type": "training",
      "description": "Training step 2993",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:24",
      "total_flops_so_far": 7.116435529937331e+16,
      "budget_used_percent": 71.16435529937331
    },
    {
      "type": "training",
      "description": "Training step 2994",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:26",
      "total_flops_so_far": 7.11881195315817e+16,
      "budget_used_percent": 71.18811953158169
    },
    {
      "type": "training",
      "description": "Training step 2995",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:27",
      "total_flops_so_far": 7.121188376379008e+16,
      "budget_used_percent": 71.21188376379008
    },
    {
      "type": "training",
      "description": "Training step 2996",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:28",
      "total_flops_so_far": 7.123564799599846e+16,
      "budget_used_percent": 71.23564799599846
    },
    {
      "type": "training",
      "description": "Training step 2997",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:30",
      "total_flops_so_far": 7.125941222820685e+16,
      "budget_used_percent": 71.25941222820686
    },
    {
      "type": "training",
      "description": "Training step 2998",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:31",
      "total_flops_so_far": 7.128317646041523e+16,
      "budget_used_percent": 71.28317646041523
    },
    {
      "type": "training",
      "description": "Training step 2999",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:33",
      "total_flops_so_far": 7.130694069262362e+16,
      "budget_used_percent": 71.30694069262361
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:39",
      "total_flops_so_far": 7.130765132176147e+16,
      "budget_used_percent": 71.30765132176147
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 714333709232.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:44",
      "total_flops_so_far": 7.13083656554707e+16,
      "budget_used_percent": 71.30836565547071
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:49",
      "total_flops_so_far": 7.1309078136534056e+16,
      "budget_used_percent": 71.30907813653405
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:54",
      "total_flops_so_far": 7.130978876567191e+16,
      "budget_used_percent": 71.30978876567191
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 713407296244.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:17:59",
      "total_flops_so_far": 7.131050217296816e+16,
      "budget_used_percent": 71.31050217296816
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 5",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:04",
      "total_flops_so_far": 7.131121280210602e+16,
      "budget_used_percent": 71.31121280210601
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 6",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:09",
      "total_flops_so_far": 7.131192528316937e+16,
      "budget_used_percent": 71.31192528316936
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 7",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:14",
      "total_flops_so_far": 7.131263776423272e+16,
      "budget_used_percent": 71.31263776423272
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 8",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:19",
      "total_flops_so_far": 7.131335024529607e+16,
      "budget_used_percent": 71.31335024529606
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 9",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:24",
      "total_flops_so_far": 7.131406272635942e+16,
      "budget_used_percent": 71.31406272635942
    },
    {
      "type": "training",
      "description": "Training step 3000",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:25",
      "total_flops_so_far": 7.133782695856781e+16,
      "budget_used_percent": 71.3378269585678
    },
    {
      "type": "training",
      "description": "Training step 3001",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:26",
      "total_flops_so_far": 7.136159119077619e+16,
      "budget_used_percent": 71.3615911907762
    },
    {
      "type": "training",
      "description": "Training step 3002",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:28",
      "total_flops_so_far": 7.138535542298458e+16,
      "budget_used_percent": 71.38535542298457
    },
    {
      "type": "training",
      "description": "Training step 3003",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:29",
      "total_flops_so_far": 7.140911965519296e+16,
      "budget_used_percent": 71.40911965519297
    },
    {
      "type": "training",
      "description": "Training step 3004",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:30",
      "total_flops_so_far": 7.143288388740134e+16,
      "budget_used_percent": 71.43288388740135
    },
    {
      "type": "training",
      "description": "Training step 3005",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:32",
      "total_flops_so_far": 7.145664811960973e+16,
      "budget_used_percent": 71.45664811960972
    },
    {
      "type": "training",
      "description": "Training step 3006",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:33",
      "total_flops_so_far": 7.148041235181811e+16,
      "budget_used_percent": 71.48041235181812
    },
    {
      "type": "training",
      "description": "Training step 3007",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:34",
      "total_flops_so_far": 7.15041765840265e+16,
      "budget_used_percent": 71.5041765840265
    },
    {
      "type": "training",
      "description": "Training step 3008",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:36",
      "total_flops_so_far": 7.152794081623488e+16,
      "budget_used_percent": 71.52794081623487
    },
    {
      "type": "training",
      "description": "Training step 3009",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:37",
      "total_flops_so_far": 7.155170504844326e+16,
      "budget_used_percent": 71.55170504844325
    },
    {
      "type": "training",
      "description": "Training step 3010",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:39",
      "total_flops_so_far": 7.157546928065165e+16,
      "budget_used_percent": 71.57546928065165
    },
    {
      "type": "training",
      "description": "Training step 3011",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:40",
      "total_flops_so_far": 7.159923351286003e+16,
      "budget_used_percent": 71.59923351286002
    },
    {
      "type": "training",
      "description": "Training step 3012",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:41",
      "total_flops_so_far": 7.162299774506842e+16,
      "budget_used_percent": 71.62299774506842
    },
    {
      "type": "training",
      "description": "Training step 3013",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:43",
      "total_flops_so_far": 7.16467619772768e+16,
      "budget_used_percent": 71.6467619772768
    },
    {
      "type": "training",
      "description": "Training step 3014",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:44",
      "total_flops_so_far": 7.167052620948518e+16,
      "budget_used_percent": 71.67052620948519
    },
    {
      "type": "training",
      "description": "Training step 3015",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:45",
      "total_flops_so_far": 7.169429044169357e+16,
      "budget_used_percent": 71.69429044169357
    },
    {
      "type": "training",
      "description": "Training step 3016",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:47",
      "total_flops_so_far": 7.171805467390195e+16,
      "budget_used_percent": 71.71805467390196
    },
    {
      "type": "training",
      "description": "Training step 3017",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:48",
      "total_flops_so_far": 7.174181890611034e+16,
      "budget_used_percent": 71.74181890611034
    },
    {
      "type": "training",
      "description": "Training step 3018",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:49",
      "total_flops_so_far": 7.176558313831872e+16,
      "budget_used_percent": 71.76558313831872
    },
    {
      "type": "training",
      "description": "Training step 3019",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:51",
      "total_flops_so_far": 7.17893473705271e+16,
      "budget_used_percent": 71.7893473705271
    },
    {
      "type": "training",
      "description": "Training step 3020",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:52",
      "total_flops_so_far": 7.181311160273549e+16,
      "budget_used_percent": 71.81311160273549
    },
    {
      "type": "training",
      "description": "Training step 3021",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:53",
      "total_flops_so_far": 7.183687583494387e+16,
      "budget_used_percent": 71.83687583494387
    },
    {
      "type": "training",
      "description": "Training step 3022",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:55",
      "total_flops_so_far": 7.186064006715226e+16,
      "budget_used_percent": 71.86064006715226
    },
    {
      "type": "training",
      "description": "Training step 3023",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:56",
      "total_flops_so_far": 7.188440429936064e+16,
      "budget_used_percent": 71.88440429936064
    },
    {
      "type": "training",
      "description": "Training step 3024",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:57",
      "total_flops_so_far": 7.190816853156902e+16,
      "budget_used_percent": 71.90816853156903
    },
    {
      "type": "training",
      "description": "Training step 3025",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:18:59",
      "total_flops_so_far": 7.193193276377741e+16,
      "budget_used_percent": 71.93193276377741
    },
    {
      "type": "training",
      "description": "Training step 3026",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:00",
      "total_flops_so_far": 7.195569699598579e+16,
      "budget_used_percent": 71.95569699598579
    },
    {
      "type": "training",
      "description": "Training step 3027",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:01",
      "total_flops_so_far": 7.197946122819418e+16,
      "budget_used_percent": 71.97946122819417
    },
    {
      "type": "training",
      "description": "Training step 3028",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:03",
      "total_flops_so_far": 7.200322546040256e+16,
      "budget_used_percent": 72.00322546040256
    },
    {
      "type": "training",
      "description": "Training step 3029",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:04",
      "total_flops_so_far": 7.202698969261094e+16,
      "budget_used_percent": 72.02698969261094
    },
    {
      "type": "training",
      "description": "Training step 3030",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:05",
      "total_flops_so_far": 7.205075392481933e+16,
      "budget_used_percent": 72.05075392481933
    },
    {
      "type": "training",
      "description": "Training step 3031",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:07",
      "total_flops_so_far": 7.207451815702771e+16,
      "budget_used_percent": 72.07451815702771
    },
    {
      "type": "training",
      "description": "Training step 3032",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:08",
      "total_flops_so_far": 7.20982823892361e+16,
      "budget_used_percent": 72.0982823892361
    },
    {
      "type": "training",
      "description": "Training step 3033",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:10",
      "total_flops_so_far": 7.212204662144448e+16,
      "budget_used_percent": 72.12204662144448
    },
    {
      "type": "training",
      "description": "Training step 3034",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:11",
      "total_flops_so_far": 7.214581085365286e+16,
      "budget_used_percent": 72.14581085365286
    },
    {
      "type": "training",
      "description": "Training step 3035",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:12",
      "total_flops_so_far": 7.216957508586125e+16,
      "budget_used_percent": 72.16957508586125
    },
    {
      "type": "training",
      "description": "Training step 3036",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:13",
      "total_flops_so_far": 7.219333931806963e+16,
      "budget_used_percent": 72.19333931806963
    },
    {
      "type": "training",
      "description": "Training step 3037",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:14",
      "total_flops_so_far": 7.221710355027802e+16,
      "budget_used_percent": 72.21710355027801
    },
    {
      "type": "training",
      "description": "Training step 3038",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:15",
      "total_flops_so_far": 7.22408677824864e+16,
      "budget_used_percent": 72.24086778248639
    },
    {
      "type": "training",
      "description": "Training step 3039",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:17",
      "total_flops_so_far": 7.226463201469478e+16,
      "budget_used_percent": 72.26463201469478
    },
    {
      "type": "training",
      "description": "Training step 3040",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:18",
      "total_flops_so_far": 7.228839624690317e+16,
      "budget_used_percent": 72.28839624690316
    },
    {
      "type": "training",
      "description": "Training step 3041",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:19",
      "total_flops_so_far": 7.231216047911155e+16,
      "budget_used_percent": 72.31216047911155
    },
    {
      "type": "training",
      "description": "Training step 3042",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:21",
      "total_flops_so_far": 7.233592471131994e+16,
      "budget_used_percent": 72.33592471131993
    },
    {
      "type": "training",
      "description": "Training step 3043",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:22",
      "total_flops_so_far": 7.235968894352832e+16,
      "budget_used_percent": 72.35968894352833
    },
    {
      "type": "training",
      "description": "Training step 3044",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:23",
      "total_flops_so_far": 7.23834531757367e+16,
      "budget_used_percent": 72.3834531757367
    },
    {
      "type": "training",
      "description": "Training step 3045",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:25",
      "total_flops_so_far": 7.240721740794509e+16,
      "budget_used_percent": 72.4072174079451
    },
    {
      "type": "training",
      "description": "Training step 3046",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:26",
      "total_flops_so_far": 7.243098164015347e+16,
      "budget_used_percent": 72.43098164015348
    },
    {
      "type": "training",
      "description": "Training step 3047",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:27",
      "total_flops_so_far": 7.245474587236186e+16,
      "budget_used_percent": 72.45474587236185
    },
    {
      "type": "training",
      "description": "Training step 3048",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:29",
      "total_flops_so_far": 7.247851010457024e+16,
      "budget_used_percent": 72.47851010457023
    },
    {
      "type": "training",
      "description": "Training step 3049",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:30",
      "total_flops_so_far": 7.250227433677862e+16,
      "budget_used_percent": 72.50227433677863
    },
    {
      "type": "training",
      "description": "Training step 3050",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:32",
      "total_flops_so_far": 7.2526038568987e+16,
      "budget_used_percent": 72.526038568987
    },
    {
      "type": "training",
      "description": "Training step 3051",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:33",
      "total_flops_so_far": 7.25498028011954e+16,
      "budget_used_percent": 72.5498028011954
    },
    {
      "type": "training",
      "description": "Training step 3052",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:34",
      "total_flops_so_far": 7.257356703340378e+16,
      "budget_used_percent": 72.57356703340378
    },
    {
      "type": "training",
      "description": "Training step 3053",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:36",
      "total_flops_so_far": 7.259733126561216e+16,
      "budget_used_percent": 72.59733126561217
    },
    {
      "type": "training",
      "description": "Training step 3054",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:37",
      "total_flops_so_far": 7.262109549782054e+16,
      "budget_used_percent": 72.62109549782055
    },
    {
      "type": "training",
      "description": "Training step 3055",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:38",
      "total_flops_so_far": 7.264485973002893e+16,
      "budget_used_percent": 72.64485973002893
    },
    {
      "type": "training",
      "description": "Training step 3056",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:40",
      "total_flops_so_far": 7.266862396223731e+16,
      "budget_used_percent": 72.6686239622373
    },
    {
      "type": "training",
      "description": "Training step 3057",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:41",
      "total_flops_so_far": 7.26923881944457e+16,
      "budget_used_percent": 72.6923881944457
    },
    {
      "type": "training",
      "description": "Training step 3058",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:42",
      "total_flops_so_far": 7.271615242665408e+16,
      "budget_used_percent": 72.71615242665408
    },
    {
      "type": "training",
      "description": "Training step 3059",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:44",
      "total_flops_so_far": 7.273991665886246e+16,
      "budget_used_percent": 72.73991665886247
    },
    {
      "type": "training",
      "description": "Training step 3060",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:45",
      "total_flops_so_far": 7.276368089107085e+16,
      "budget_used_percent": 72.76368089107085
    },
    {
      "type": "training",
      "description": "Training step 3061",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:47",
      "total_flops_so_far": 7.278744512327923e+16,
      "budget_used_percent": 72.78744512327924
    },
    {
      "type": "training",
      "description": "Training step 3062",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:48",
      "total_flops_so_far": 7.281120935548762e+16,
      "budget_used_percent": 72.81120935548762
    },
    {
      "type": "training",
      "description": "Training step 3063",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:49",
      "total_flops_so_far": 7.2834973587696e+16,
      "budget_used_percent": 72.834973587696
    },
    {
      "type": "training",
      "description": "Training step 3064",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:51",
      "total_flops_so_far": 7.285873781990438e+16,
      "budget_used_percent": 72.85873781990439
    },
    {
      "type": "training",
      "description": "Training step 3065",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:52",
      "total_flops_so_far": 7.288250205211277e+16,
      "budget_used_percent": 72.88250205211277
    },
    {
      "type": "training",
      "description": "Training step 3066",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:53",
      "total_flops_so_far": 7.290626628432115e+16,
      "budget_used_percent": 72.90626628432115
    },
    {
      "type": "training",
      "description": "Training step 3067",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:55",
      "total_flops_so_far": 7.293003051652954e+16,
      "budget_used_percent": 72.93003051652953
    },
    {
      "type": "training",
      "description": "Training step 3068",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:56",
      "total_flops_so_far": 7.295379474873792e+16,
      "budget_used_percent": 72.95379474873792
    },
    {
      "type": "training",
      "description": "Training step 3069",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:57",
      "total_flops_so_far": 7.29775589809463e+16,
      "budget_used_percent": 72.9775589809463
    },
    {
      "type": "training",
      "description": "Training step 3070",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:19:59",
      "total_flops_so_far": 7.300132321315469e+16,
      "budget_used_percent": 73.00132321315469
    },
    {
      "type": "training",
      "description": "Training step 3071",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:00",
      "total_flops_so_far": 7.302508744536307e+16,
      "budget_used_percent": 73.02508744536307
    },
    {
      "type": "training",
      "description": "Training step 3072",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:01",
      "total_flops_so_far": 7.304885167757146e+16,
      "budget_used_percent": 73.04885167757146
    },
    {
      "type": "training",
      "description": "Training step 3073",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:03",
      "total_flops_so_far": 7.307261590977984e+16,
      "budget_used_percent": 73.07261590977984
    },
    {
      "type": "training",
      "description": "Training step 3074",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:04",
      "total_flops_so_far": 7.309638014198822e+16,
      "budget_used_percent": 73.09638014198822
    },
    {
      "type": "training",
      "description": "Training step 3075",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:07",
      "total_flops_so_far": 7.31201443741966e+16,
      "budget_used_percent": 73.1201443741966
    },
    {
      "type": "training",
      "description": "Training step 3076",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:08",
      "total_flops_so_far": 7.3143908606405e+16,
      "budget_used_percent": 73.14390860640499
    },
    {
      "type": "training",
      "description": "Training step 3077",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:10",
      "total_flops_so_far": 7.316767283861338e+16,
      "budget_used_percent": 73.16767283861337
    },
    {
      "type": "training",
      "description": "Training step 3078",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:12",
      "total_flops_so_far": 7.319143707082176e+16,
      "budget_used_percent": 73.19143707082176
    },
    {
      "type": "training",
      "description": "Training step 3079",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:13",
      "total_flops_so_far": 7.321520130303014e+16,
      "budget_used_percent": 73.21520130303014
    },
    {
      "type": "training",
      "description": "Training step 3080",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:14",
      "total_flops_so_far": 7.323896553523853e+16,
      "budget_used_percent": 73.23896553523853
    },
    {
      "type": "training",
      "description": "Training step 3081",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:16",
      "total_flops_so_far": 7.326272976744691e+16,
      "budget_used_percent": 73.26272976744691
    },
    {
      "type": "training",
      "description": "Training step 3082",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:17",
      "total_flops_so_far": 7.32864939996553e+16,
      "budget_used_percent": 73.2864939996553
    },
    {
      "type": "training",
      "description": "Training step 3083",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:18",
      "total_flops_so_far": 7.331025823186368e+16,
      "budget_used_percent": 73.31025823186368
    },
    {
      "type": "training",
      "description": "Training step 3084",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:20",
      "total_flops_so_far": 7.333402246407206e+16,
      "budget_used_percent": 73.33402246407206
    },
    {
      "type": "training",
      "description": "Training step 3085",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:21",
      "total_flops_so_far": 7.335778669628045e+16,
      "budget_used_percent": 73.35778669628044
    },
    {
      "type": "training",
      "description": "Training step 3086",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:22",
      "total_flops_so_far": 7.338155092848883e+16,
      "budget_used_percent": 73.38155092848883
    },
    {
      "type": "training",
      "description": "Training step 3087",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:24",
      "total_flops_so_far": 7.340531516069722e+16,
      "budget_used_percent": 73.40531516069721
    },
    {
      "type": "training",
      "description": "Training step 3088",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:25",
      "total_flops_so_far": 7.34290793929056e+16,
      "budget_used_percent": 73.4290793929056
    },
    {
      "type": "training",
      "description": "Training step 3089",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:27",
      "total_flops_so_far": 7.345284362511398e+16,
      "budget_used_percent": 73.45284362511399
    },
    {
      "type": "training",
      "description": "Training step 3090",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:28",
      "total_flops_so_far": 7.347660785732237e+16,
      "budget_used_percent": 73.47660785732238
    },
    {
      "type": "training",
      "description": "Training step 3091",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:29",
      "total_flops_so_far": 7.350037208953075e+16,
      "budget_used_percent": 73.50037208953076
    },
    {
      "type": "training",
      "description": "Training step 3092",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:31",
      "total_flops_so_far": 7.352413632173914e+16,
      "budget_used_percent": 73.52413632173914
    },
    {
      "type": "training",
      "description": "Training step 3093",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:32",
      "total_flops_so_far": 7.354790055394752e+16,
      "budget_used_percent": 73.54790055394751
    },
    {
      "type": "training",
      "description": "Training step 3094",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:33",
      "total_flops_so_far": 7.35716647861559e+16,
      "budget_used_percent": 73.57166478615589
    },
    {
      "type": "training",
      "description": "Training step 3095",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:35",
      "total_flops_so_far": 7.359542901836429e+16,
      "budget_used_percent": 73.59542901836429
    },
    {
      "type": "training",
      "description": "Training step 3096",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:36",
      "total_flops_so_far": 7.361919325057267e+16,
      "budget_used_percent": 73.61919325057266
    },
    {
      "type": "training",
      "description": "Training step 3097",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:37",
      "total_flops_so_far": 7.364295748278106e+16,
      "budget_used_percent": 73.64295748278106
    },
    {
      "type": "training",
      "description": "Training step 3098",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:39",
      "total_flops_so_far": 7.366672171498944e+16,
      "budget_used_percent": 73.66672171498944
    },
    {
      "type": "training",
      "description": "Training step 3099",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:40",
      "total_flops_so_far": 7.369048594719782e+16,
      "budget_used_percent": 73.69048594719783
    },
    {
      "type": "training",
      "description": "Training step 3100",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:41",
      "total_flops_so_far": 7.37142501794062e+16,
      "budget_used_percent": 73.7142501794062
    },
    {
      "type": "training",
      "description": "Training step 3101",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:43",
      "total_flops_so_far": 7.37380144116146e+16,
      "budget_used_percent": 73.7380144116146
    },
    {
      "type": "training",
      "description": "Training step 3102",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:44",
      "total_flops_so_far": 7.376177864382298e+16,
      "budget_used_percent": 73.76177864382298
    },
    {
      "type": "training",
      "description": "Training step 3103",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:45",
      "total_flops_so_far": 7.378554287603136e+16,
      "budget_used_percent": 73.78554287603136
    },
    {
      "type": "training",
      "description": "Training step 3104",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:47",
      "total_flops_so_far": 7.380930710823974e+16,
      "budget_used_percent": 73.80930710823974
    },
    {
      "type": "training",
      "description": "Training step 3105",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:48",
      "total_flops_so_far": 7.383307134044813e+16,
      "budget_used_percent": 73.83307134044813
    },
    {
      "type": "training",
      "description": "Training step 3106",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:49",
      "total_flops_so_far": 7.385683557265651e+16,
      "budget_used_percent": 73.85683557265651
    },
    {
      "type": "training",
      "description": "Training step 3107",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:51",
      "total_flops_so_far": 7.38805998048649e+16,
      "budget_used_percent": 73.8805998048649
    },
    {
      "type": "training",
      "description": "Training step 3108",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:52",
      "total_flops_so_far": 7.390436403707328e+16,
      "budget_used_percent": 73.90436403707328
    },
    {
      "type": "training",
      "description": "Training step 3109",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:53",
      "total_flops_so_far": 7.392812826928166e+16,
      "budget_used_percent": 73.92812826928167
    },
    {
      "type": "training",
      "description": "Training step 3110",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:55",
      "total_flops_so_far": 7.395189250149005e+16,
      "budget_used_percent": 73.95189250149005
    },
    {
      "type": "training",
      "description": "Training step 3111",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:56",
      "total_flops_so_far": 7.397565673369843e+16,
      "budget_used_percent": 73.97565673369843
    },
    {
      "type": "training",
      "description": "Training step 3112",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:58",
      "total_flops_so_far": 7.399942096590682e+16,
      "budget_used_percent": 73.99942096590681
    },
    {
      "type": "training",
      "description": "Training step 3113",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:20:59",
      "total_flops_so_far": 7.40231851981152e+16,
      "budget_used_percent": 74.0231851981152
    },
    {
      "type": "training",
      "description": "Training step 3114",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:00",
      "total_flops_so_far": 7.404694943032358e+16,
      "budget_used_percent": 74.04694943032358
    },
    {
      "type": "training",
      "description": "Training step 3115",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:02",
      "total_flops_so_far": 7.407071366253197e+16,
      "budget_used_percent": 74.07071366253197
    },
    {
      "type": "training",
      "description": "Training step 3116",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:03",
      "total_flops_so_far": 7.409447789474035e+16,
      "budget_used_percent": 74.09447789474035
    },
    {
      "type": "training",
      "description": "Training step 3117",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:04",
      "total_flops_so_far": 7.411824212694874e+16,
      "budget_used_percent": 74.11824212694874
    },
    {
      "type": "training",
      "description": "Training step 3118",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:06",
      "total_flops_so_far": 7.414200635915712e+16,
      "budget_used_percent": 74.14200635915712
    },
    {
      "type": "training",
      "description": "Training step 3119",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:07",
      "total_flops_so_far": 7.41657705913655e+16,
      "budget_used_percent": 74.1657705913655
    },
    {
      "type": "training",
      "description": "Training step 3120",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:08",
      "total_flops_so_far": 7.418953482357389e+16,
      "budget_used_percent": 74.1895348235739
    },
    {
      "type": "training",
      "description": "Training step 3121",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:10",
      "total_flops_so_far": 7.421329905578227e+16,
      "budget_used_percent": 74.21329905578227
    },
    {
      "type": "training",
      "description": "Training step 3122",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:11",
      "total_flops_so_far": 7.423706328799066e+16,
      "budget_used_percent": 74.23706328799065
    },
    {
      "type": "training",
      "description": "Training step 3123",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:12",
      "total_flops_so_far": 7.426082752019904e+16,
      "budget_used_percent": 74.26082752019903
    },
    {
      "type": "training",
      "description": "Training step 3124",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:14",
      "total_flops_so_far": 7.428459175240742e+16,
      "budget_used_percent": 74.28459175240742
    },
    {
      "type": "training",
      "description": "Training step 3125",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:15",
      "total_flops_so_far": 7.43083559846158e+16,
      "budget_used_percent": 74.3083559846158
    },
    {
      "type": "training",
      "description": "Training step 3126",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:16",
      "total_flops_so_far": 7.43321202168242e+16,
      "budget_used_percent": 74.3321202168242
    },
    {
      "type": "training",
      "description": "Training step 3127",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:18",
      "total_flops_so_far": 7.435588444903258e+16,
      "budget_used_percent": 74.35588444903257
    },
    {
      "type": "training",
      "description": "Training step 3128",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:19",
      "total_flops_so_far": 7.437964868124096e+16,
      "budget_used_percent": 74.37964868124097
    },
    {
      "type": "training",
      "description": "Training step 3129",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:20",
      "total_flops_so_far": 7.440341291344934e+16,
      "budget_used_percent": 74.40341291344934
    },
    {
      "type": "training",
      "description": "Training step 3130",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:22",
      "total_flops_so_far": 7.442717714565773e+16,
      "budget_used_percent": 74.42717714565774
    },
    {
      "type": "training",
      "description": "Training step 3131",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:23",
      "total_flops_so_far": 7.445094137786611e+16,
      "budget_used_percent": 74.45094137786612
    },
    {
      "type": "training",
      "description": "Training step 3132",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:25",
      "total_flops_so_far": 7.44747056100745e+16,
      "budget_used_percent": 74.4747056100745
    },
    {
      "type": "training",
      "description": "Training step 3133",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:26",
      "total_flops_so_far": 7.449846984228288e+16,
      "budget_used_percent": 74.49846984228287
    },
    {
      "type": "training",
      "description": "Training step 3134",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:27",
      "total_flops_so_far": 7.452223407449126e+16,
      "budget_used_percent": 74.52223407449127
    },
    {
      "type": "training",
      "description": "Training step 3135",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:29",
      "total_flops_so_far": 7.454599830669965e+16,
      "budget_used_percent": 74.54599830669964
    },
    {
      "type": "training",
      "description": "Training step 3136",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:30",
      "total_flops_so_far": 7.456976253890803e+16,
      "budget_used_percent": 74.56976253890804
    },
    {
      "type": "training",
      "description": "Training step 3137",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:31",
      "total_flops_so_far": 7.459352677111642e+16,
      "budget_used_percent": 74.59352677111642
    },
    {
      "type": "training",
      "description": "Training step 3138",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:33",
      "total_flops_so_far": 7.46172910033248e+16,
      "budget_used_percent": 74.61729100332481
    },
    {
      "type": "training",
      "description": "Training step 3139",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:34",
      "total_flops_so_far": 7.464105523553318e+16,
      "budget_used_percent": 74.64105523553319
    },
    {
      "type": "training",
      "description": "Training step 3140",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:35",
      "total_flops_so_far": 7.466481946774157e+16,
      "budget_used_percent": 74.66481946774157
    },
    {
      "type": "training",
      "description": "Training step 3141",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:37",
      "total_flops_so_far": 7.468858369994995e+16,
      "budget_used_percent": 74.68858369994994
    },
    {
      "type": "training",
      "description": "Training step 3142",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:38",
      "total_flops_so_far": 7.471234793215834e+16,
      "budget_used_percent": 74.71234793215834
    },
    {
      "type": "training",
      "description": "Training step 3143",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:40",
      "total_flops_so_far": 7.473611216436672e+16,
      "budget_used_percent": 74.73611216436672
    },
    {
      "type": "training",
      "description": "Training step 3144",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:41",
      "total_flops_so_far": 7.47598763965751e+16,
      "budget_used_percent": 74.75987639657511
    },
    {
      "type": "training",
      "description": "Training step 3145",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:42",
      "total_flops_so_far": 7.478364062878349e+16,
      "budget_used_percent": 74.78364062878349
    },
    {
      "type": "training",
      "description": "Training step 3146",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:44",
      "total_flops_so_far": 7.480740486099187e+16,
      "budget_used_percent": 74.80740486099188
    },
    {
      "type": "training",
      "description": "Training step 3147",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:45",
      "total_flops_so_far": 7.483116909320026e+16,
      "budget_used_percent": 74.83116909320026
    },
    {
      "type": "training",
      "description": "Training step 3148",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:46",
      "total_flops_so_far": 7.485493332540864e+16,
      "budget_used_percent": 74.85493332540864
    },
    {
      "type": "training",
      "description": "Training step 3149",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:48",
      "total_flops_so_far": 7.487869755761702e+16,
      "budget_used_percent": 74.87869755761703
    },
    {
      "type": "training",
      "description": "Training step 3150",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:49",
      "total_flops_so_far": 7.49024617898254e+16,
      "budget_used_percent": 74.90246178982541
    },
    {
      "type": "training",
      "description": "Training step 3151",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:50",
      "total_flops_so_far": 7.49262260220338e+16,
      "budget_used_percent": 74.92622602203379
    },
    {
      "type": "training",
      "description": "Training step 3152",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:52",
      "total_flops_so_far": 7.494999025424218e+16,
      "budget_used_percent": 74.94999025424217
    },
    {
      "type": "training",
      "description": "Training step 3153",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:53",
      "total_flops_so_far": 7.497375448645056e+16,
      "budget_used_percent": 74.97375448645056
    },
    {
      "type": "training",
      "description": "Training step 3154",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:54",
      "total_flops_so_far": 7.499751871865894e+16,
      "budget_used_percent": 74.99751871865894
    },
    {
      "type": "training",
      "description": "Training step 3155",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:56",
      "total_flops_so_far": 7.502128295086733e+16,
      "budget_used_percent": 75.02128295086733
    },
    {
      "type": "training",
      "description": "Training step 3156",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:57",
      "total_flops_so_far": 7.504504718307571e+16,
      "budget_used_percent": 75.04504718307571
    },
    {
      "type": "training",
      "description": "Training step 3157",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:21:58",
      "total_flops_so_far": 7.50688114152841e+16,
      "budget_used_percent": 75.0688114152841
    },
    {
      "type": "training",
      "description": "Training step 3158",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:00",
      "total_flops_so_far": 7.509257564749248e+16,
      "budget_used_percent": 75.09257564749248
    },
    {
      "type": "training",
      "description": "Training step 3159",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:01",
      "total_flops_so_far": 7.511633987970086e+16,
      "budget_used_percent": 75.11633987970086
    },
    {
      "type": "training",
      "description": "Training step 3160",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:03",
      "total_flops_so_far": 7.514010411190925e+16,
      "budget_used_percent": 75.14010411190924
    },
    {
      "type": "training",
      "description": "Training step 3161",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:04",
      "total_flops_so_far": 7.516386834411763e+16,
      "budget_used_percent": 75.16386834411763
    },
    {
      "type": "training",
      "description": "Training step 3162",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:05",
      "total_flops_so_far": 7.518763257632602e+16,
      "budget_used_percent": 75.18763257632601
    },
    {
      "type": "training",
      "description": "Training step 3163",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:07",
      "total_flops_so_far": 7.52113968085344e+16,
      "budget_used_percent": 75.2113968085344
    },
    {
      "type": "training",
      "description": "Training step 3164",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:08",
      "total_flops_so_far": 7.523516104074278e+16,
      "budget_used_percent": 75.23516104074278
    },
    {
      "type": "training",
      "description": "Training step 3165",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:09",
      "total_flops_so_far": 7.525892527295117e+16,
      "budget_used_percent": 75.25892527295117
    },
    {
      "type": "training",
      "description": "Training step 3166",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:11",
      "total_flops_so_far": 7.528268950515955e+16,
      "budget_used_percent": 75.28268950515955
    },
    {
      "type": "training",
      "description": "Training step 3167",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:12",
      "total_flops_so_far": 7.530645373736794e+16,
      "budget_used_percent": 75.30645373736795
    },
    {
      "type": "training",
      "description": "Training step 3168",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:12",
      "total_flops_so_far": 7.533021796957632e+16,
      "budget_used_percent": 75.33021796957632
    },
    {
      "type": "training",
      "description": "Training step 3169",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:14",
      "total_flops_so_far": 7.53539822017847e+16,
      "budget_used_percent": 75.3539822017847
    },
    {
      "type": "training",
      "description": "Training step 3170",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:15",
      "total_flops_so_far": 7.537774643399309e+16,
      "budget_used_percent": 75.37774643399308
    },
    {
      "type": "training",
      "description": "Training step 3171",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:16",
      "total_flops_so_far": 7.540151066620147e+16,
      "budget_used_percent": 75.40151066620147
    },
    {
      "type": "training",
      "description": "Training step 3172",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:18",
      "total_flops_so_far": 7.542527489840986e+16,
      "budget_used_percent": 75.42527489840985
    },
    {
      "type": "training",
      "description": "Training step 3173",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:19",
      "total_flops_so_far": 7.544903913061824e+16,
      "budget_used_percent": 75.44903913061825
    },
    {
      "type": "training",
      "description": "Training step 3174",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:21",
      "total_flops_so_far": 7.547280336282662e+16,
      "budget_used_percent": 75.47280336282662
    },
    {
      "type": "training",
      "description": "Training step 3175",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:22",
      "total_flops_so_far": 7.5496567595035e+16,
      "budget_used_percent": 75.49656759503502
    },
    {
      "type": "training",
      "description": "Training step 3176",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:23",
      "total_flops_so_far": 7.55203318272434e+16,
      "budget_used_percent": 75.5203318272434
    },
    {
      "type": "training",
      "description": "Training step 3177",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:25",
      "total_flops_so_far": 7.554409605945178e+16,
      "budget_used_percent": 75.54409605945177
    },
    {
      "type": "training",
      "description": "Training step 3178",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:26",
      "total_flops_so_far": 7.556786029166016e+16,
      "budget_used_percent": 75.56786029166017
    },
    {
      "type": "training",
      "description": "Training step 3179",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:27",
      "total_flops_so_far": 7.559162452386854e+16,
      "budget_used_percent": 75.59162452386855
    },
    {
      "type": "training",
      "description": "Training step 3180",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:29",
      "total_flops_so_far": 7.561538875607693e+16,
      "budget_used_percent": 75.61538875607692
    },
    {
      "type": "training",
      "description": "Training step 3181",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:30",
      "total_flops_so_far": 7.563915298828531e+16,
      "budget_used_percent": 75.6391529882853
    },
    {
      "type": "training",
      "description": "Training step 3182",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:31",
      "total_flops_so_far": 7.56629172204937e+16,
      "budget_used_percent": 75.6629172204937
    },
    {
      "type": "training",
      "description": "Training step 3183",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:33",
      "total_flops_so_far": 7.568668145270208e+16,
      "budget_used_percent": 75.68668145270208
    },
    {
      "type": "training",
      "description": "Training step 3184",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:34",
      "total_flops_so_far": 7.571044568491046e+16,
      "budget_used_percent": 75.71044568491047
    },
    {
      "type": "training",
      "description": "Training step 3185",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:35",
      "total_flops_so_far": 7.573420991711885e+16,
      "budget_used_percent": 75.73420991711885
    },
    {
      "type": "training",
      "description": "Training step 3186",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:37",
      "total_flops_so_far": 7.575797414932723e+16,
      "budget_used_percent": 75.75797414932724
    },
    {
      "type": "training",
      "description": "Training step 3187",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:38",
      "total_flops_so_far": 7.578173838153562e+16,
      "budget_used_percent": 75.78173838153562
    },
    {
      "type": "training",
      "description": "Training step 3188",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:39",
      "total_flops_so_far": 7.5805502613744e+16,
      "budget_used_percent": 75.805502613744
    },
    {
      "type": "training",
      "description": "Training step 3189",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:41",
      "total_flops_so_far": 7.582926684595238e+16,
      "budget_used_percent": 75.82926684595238
    },
    {
      "type": "training",
      "description": "Training step 3190",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:42",
      "total_flops_so_far": 7.585303107816077e+16,
      "budget_used_percent": 75.85303107816077
    },
    {
      "type": "training",
      "description": "Training step 3191",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:43",
      "total_flops_so_far": 7.587679531036915e+16,
      "budget_used_percent": 75.87679531036915
    },
    {
      "type": "training",
      "description": "Training step 3192",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:45",
      "total_flops_so_far": 7.590055954257754e+16,
      "budget_used_percent": 75.90055954257754
    },
    {
      "type": "training",
      "description": "Training step 3193",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:46",
      "total_flops_so_far": 7.592432377478592e+16,
      "budget_used_percent": 75.92432377478592
    },
    {
      "type": "training",
      "description": "Training step 3194",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:48",
      "total_flops_so_far": 7.59480880069943e+16,
      "budget_used_percent": 75.94808800699431
    },
    {
      "type": "training",
      "description": "Training step 3195",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:49",
      "total_flops_so_far": 7.597185223920269e+16,
      "budget_used_percent": 75.97185223920269
    },
    {
      "type": "training",
      "description": "Training step 3196",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:50",
      "total_flops_so_far": 7.599561647141107e+16,
      "budget_used_percent": 75.99561647141108
    },
    {
      "type": "training",
      "description": "Training step 3197",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:52",
      "total_flops_so_far": 7.601938070361946e+16,
      "budget_used_percent": 76.01938070361946
    },
    {
      "type": "training",
      "description": "Training step 3198",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:53",
      "total_flops_so_far": 7.604314493582784e+16,
      "budget_used_percent": 76.04314493582784
    },
    {
      "type": "training",
      "description": "Training step 3199",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:54",
      "total_flops_so_far": 7.606690916803622e+16,
      "budget_used_percent": 76.06690916803622
    },
    {
      "type": "training",
      "description": "Training step 3200",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:56",
      "total_flops_so_far": 7.60906734002446e+16,
      "budget_used_percent": 76.09067340024461
    },
    {
      "type": "training",
      "description": "Training step 3201",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:57",
      "total_flops_so_far": 7.6114437632453e+16,
      "budget_used_percent": 76.11443763245299
    },
    {
      "type": "training",
      "description": "Training step 3202",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:22:59",
      "total_flops_so_far": 7.613820186466138e+16,
      "budget_used_percent": 76.13820186466138
    },
    {
      "type": "training",
      "description": "Training step 3203",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:00",
      "total_flops_so_far": 7.616196609686976e+16,
      "budget_used_percent": 76.16196609686976
    },
    {
      "type": "training",
      "description": "Training step 3204",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:01",
      "total_flops_so_far": 7.618573032907814e+16,
      "budget_used_percent": 76.18573032907814
    },
    {
      "type": "training",
      "description": "Training step 3205",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:03",
      "total_flops_so_far": 7.620949456128653e+16,
      "budget_used_percent": 76.20949456128653
    },
    {
      "type": "training",
      "description": "Training step 3206",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:04",
      "total_flops_so_far": 7.623325879349491e+16,
      "budget_used_percent": 76.23325879349491
    },
    {
      "type": "training",
      "description": "Training step 3207",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:05",
      "total_flops_so_far": 7.62570230257033e+16,
      "budget_used_percent": 76.25702302570329
    },
    {
      "type": "training",
      "description": "Training step 3208",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:07",
      "total_flops_so_far": 7.628078725791168e+16,
      "budget_used_percent": 76.28078725791167
    },
    {
      "type": "training",
      "description": "Training step 3209",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:08",
      "total_flops_so_far": 7.630455149012006e+16,
      "budget_used_percent": 76.30455149012006
    },
    {
      "type": "training",
      "description": "Training step 3210",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:09",
      "total_flops_so_far": 7.632831572232845e+16,
      "budget_used_percent": 76.32831572232844
    },
    {
      "type": "training",
      "description": "Training step 3211",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:11",
      "total_flops_so_far": 7.635207995453683e+16,
      "budget_used_percent": 76.35207995453683
    },
    {
      "type": "training",
      "description": "Training step 3212",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:12",
      "total_flops_so_far": 7.637584418674522e+16,
      "budget_used_percent": 76.37584418674521
    },
    {
      "type": "training",
      "description": "Training step 3213",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:13",
      "total_flops_so_far": 7.63996084189536e+16,
      "budget_used_percent": 76.3996084189536
    },
    {
      "type": "training",
      "description": "Training step 3214",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:15",
      "total_flops_so_far": 7.642337265116198e+16,
      "budget_used_percent": 76.42337265116198
    },
    {
      "type": "training",
      "description": "Training step 3215",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:16",
      "total_flops_so_far": 7.644713688337037e+16,
      "budget_used_percent": 76.44713688337038
    },
    {
      "type": "training",
      "description": "Training step 3216",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:17",
      "total_flops_so_far": 7.647090111557875e+16,
      "budget_used_percent": 76.47090111557876
    },
    {
      "type": "training",
      "description": "Training step 3217",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:19",
      "total_flops_so_far": 7.649466534778714e+16,
      "budget_used_percent": 76.49466534778713
    },
    {
      "type": "training",
      "description": "Training step 3218",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:20",
      "total_flops_so_far": 7.651842957999552e+16,
      "budget_used_percent": 76.51842957999551
    },
    {
      "type": "training",
      "description": "Training step 3219",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:22",
      "total_flops_so_far": 7.65421938122039e+16,
      "budget_used_percent": 76.5421938122039
    },
    {
      "type": "training",
      "description": "Training step 3220",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:23",
      "total_flops_so_far": 7.656595804441229e+16,
      "budget_used_percent": 76.56595804441228
    },
    {
      "type": "training",
      "description": "Training step 3221",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:24",
      "total_flops_so_far": 7.658972227662067e+16,
      "budget_used_percent": 76.58972227662068
    },
    {
      "type": "training",
      "description": "Training step 3222",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:26",
      "total_flops_so_far": 7.661348650882906e+16,
      "budget_used_percent": 76.61348650882906
    },
    {
      "type": "training",
      "description": "Training step 3223",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:27",
      "total_flops_so_far": 7.663725074103744e+16,
      "budget_used_percent": 76.63725074103745
    },
    {
      "type": "training",
      "description": "Training step 3224",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:28",
      "total_flops_so_far": 7.666101497324582e+16,
      "budget_used_percent": 76.66101497324583
    },
    {
      "type": "training",
      "description": "Training step 3225",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:30",
      "total_flops_so_far": 7.66847792054542e+16,
      "budget_used_percent": 76.6847792054542
    },
    {
      "type": "training",
      "description": "Training step 3226",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:31",
      "total_flops_so_far": 7.67085434376626e+16,
      "budget_used_percent": 76.70854343766258
    },
    {
      "type": "training",
      "description": "Training step 3227",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:32",
      "total_flops_so_far": 7.673230766987098e+16,
      "budget_used_percent": 76.73230766987098
    },
    {
      "type": "training",
      "description": "Training step 3228",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:34",
      "total_flops_so_far": 7.675607190207936e+16,
      "budget_used_percent": 76.75607190207936
    },
    {
      "type": "training",
      "description": "Training step 3229",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:35",
      "total_flops_so_far": 7.677983613428774e+16,
      "budget_used_percent": 76.77983613428775
    },
    {
      "type": "training",
      "description": "Training step 3230",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:36",
      "total_flops_so_far": 7.680360036649613e+16,
      "budget_used_percent": 76.80360036649613
    },
    {
      "type": "training",
      "description": "Training step 3231",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:38",
      "total_flops_so_far": 7.682736459870451e+16,
      "budget_used_percent": 76.82736459870452
    },
    {
      "type": "training",
      "description": "Training step 3232",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:39",
      "total_flops_so_far": 7.68511288309129e+16,
      "budget_used_percent": 76.8511288309129
    },
    {
      "type": "training",
      "description": "Training step 3233",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:40",
      "total_flops_so_far": 7.687489306312128e+16,
      "budget_used_percent": 76.87489306312128
    },
    {
      "type": "training",
      "description": "Training step 3234",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:42",
      "total_flops_so_far": 7.689865729532966e+16,
      "budget_used_percent": 76.89865729532967
    },
    {
      "type": "training",
      "description": "Training step 3235",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:43",
      "total_flops_so_far": 7.692242152753805e+16,
      "budget_used_percent": 76.92242152753805
    },
    {
      "type": "training",
      "description": "Training step 3236",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:45",
      "total_flops_so_far": 7.694618575974643e+16,
      "budget_used_percent": 76.94618575974643
    },
    {
      "type": "training",
      "description": "Training step 3237",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:46",
      "total_flops_so_far": 7.696994999195482e+16,
      "budget_used_percent": 76.9699499919548
    },
    {
      "type": "training",
      "description": "Training step 3238",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:47",
      "total_flops_so_far": 7.69937142241632e+16,
      "budget_used_percent": 76.9937142241632
    },
    {
      "type": "training",
      "description": "Training step 3239",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:49",
      "total_flops_so_far": 7.701747845637158e+16,
      "budget_used_percent": 77.01747845637158
    },
    {
      "type": "training",
      "description": "Training step 3240",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:50",
      "total_flops_so_far": 7.704124268857997e+16,
      "budget_used_percent": 77.04124268857997
    },
    {
      "type": "training",
      "description": "Training step 3241",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:51",
      "total_flops_so_far": 7.706500692078835e+16,
      "budget_used_percent": 77.06500692078835
    },
    {
      "type": "training",
      "description": "Training step 3242",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:53",
      "total_flops_so_far": 7.708877115299674e+16,
      "budget_used_percent": 77.08877115299674
    },
    {
      "type": "training",
      "description": "Training step 3243",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:54",
      "total_flops_so_far": 7.711253538520512e+16,
      "budget_used_percent": 77.11253538520512
    },
    {
      "type": "training",
      "description": "Training step 3244",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:56",
      "total_flops_so_far": 7.71362996174135e+16,
      "budget_used_percent": 77.1362996174135
    },
    {
      "type": "training",
      "description": "Training step 3245",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:57",
      "total_flops_so_far": 7.716006384962189e+16,
      "budget_used_percent": 77.16006384962188
    },
    {
      "type": "training",
      "description": "Training step 3246",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:23:58",
      "total_flops_so_far": 7.718382808183027e+16,
      "budget_used_percent": 77.18382808183027
    },
    {
      "type": "training",
      "description": "Training step 3247",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:00",
      "total_flops_so_far": 7.720759231403866e+16,
      "budget_used_percent": 77.20759231403865
    },
    {
      "type": "training",
      "description": "Training step 3248",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:01",
      "total_flops_so_far": 7.723135654624704e+16,
      "budget_used_percent": 77.23135654624704
    },
    {
      "type": "training",
      "description": "Training step 3249",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:02",
      "total_flops_so_far": 7.725512077845542e+16,
      "budget_used_percent": 77.25512077845542
    },
    {
      "type": "training",
      "description": "Training step 3250",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:04",
      "total_flops_so_far": 7.72788850106638e+16,
      "budget_used_percent": 77.27888501066381
    },
    {
      "type": "training",
      "description": "Training step 3251",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:05",
      "total_flops_so_far": 7.73026492428722e+16,
      "budget_used_percent": 77.30264924287219
    },
    {
      "type": "training",
      "description": "Training step 3252",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:06",
      "total_flops_so_far": 7.732641347508058e+16,
      "budget_used_percent": 77.32641347508059
    },
    {
      "type": "training",
      "description": "Training step 3253",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:08",
      "total_flops_so_far": 7.735017770728896e+16,
      "budget_used_percent": 77.35017770728896
    },
    {
      "type": "training",
      "description": "Training step 3254",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:09",
      "total_flops_so_far": 7.737394193949734e+16,
      "budget_used_percent": 77.37394193949734
    },
    {
      "type": "training",
      "description": "Training step 3255",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:10",
      "total_flops_so_far": 7.739770617170573e+16,
      "budget_used_percent": 77.39770617170572
    },
    {
      "type": "training",
      "description": "Training step 3256",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:12",
      "total_flops_so_far": 7.742147040391411e+16,
      "budget_used_percent": 77.42147040391411
    },
    {
      "type": "training",
      "description": "Training step 3257",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:13",
      "total_flops_so_far": 7.74452346361225e+16,
      "budget_used_percent": 77.44523463612249
    },
    {
      "type": "training",
      "description": "Training step 3258",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:14",
      "total_flops_so_far": 7.746899886833088e+16,
      "budget_used_percent": 77.46899886833089
    },
    {
      "type": "training",
      "description": "Training step 3259",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:16",
      "total_flops_so_far": 7.749276310053926e+16,
      "budget_used_percent": 77.49276310053926
    },
    {
      "type": "training",
      "description": "Training step 3260",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:17",
      "total_flops_so_far": 7.751652733274765e+16,
      "budget_used_percent": 77.51652733274766
    },
    {
      "type": "training",
      "description": "Training step 3261",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:18",
      "total_flops_so_far": 7.754029156495603e+16,
      "budget_used_percent": 77.54029156495604
    },
    {
      "type": "training",
      "description": "Training step 3262",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:20",
      "total_flops_so_far": 7.756405579716442e+16,
      "budget_used_percent": 77.56405579716441
    },
    {
      "type": "training",
      "description": "Training step 3263",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:21",
      "total_flops_so_far": 7.75878200293728e+16,
      "budget_used_percent": 77.5878200293728
    },
    {
      "type": "training",
      "description": "Training step 3264",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:23",
      "total_flops_so_far": 7.761158426158118e+16,
      "budget_used_percent": 77.61158426158119
    },
    {
      "type": "training",
      "description": "Training step 3265",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:24",
      "total_flops_so_far": 7.763534849378957e+16,
      "budget_used_percent": 77.63534849378956
    },
    {
      "type": "training",
      "description": "Training step 3266",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:25",
      "total_flops_so_far": 7.765911272599795e+16,
      "budget_used_percent": 77.65911272599794
    },
    {
      "type": "training",
      "description": "Training step 3267",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:27",
      "total_flops_so_far": 7.768287695820634e+16,
      "budget_used_percent": 77.68287695820634
    },
    {
      "type": "training",
      "description": "Training step 3268",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:28",
      "total_flops_so_far": 7.770664119041472e+16,
      "budget_used_percent": 77.70664119041471
    },
    {
      "type": "training",
      "description": "Training step 3269",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:29",
      "total_flops_so_far": 7.77304054226231e+16,
      "budget_used_percent": 77.73040542262311
    },
    {
      "type": "training",
      "description": "Training step 3270",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:31",
      "total_flops_so_far": 7.775416965483149e+16,
      "budget_used_percent": 77.75416965483149
    },
    {
      "type": "training",
      "description": "Training step 3271",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:32",
      "total_flops_so_far": 7.777793388703987e+16,
      "budget_used_percent": 77.77793388703988
    },
    {
      "type": "training",
      "description": "Training step 3272",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:33",
      "total_flops_so_far": 7.780169811924826e+16,
      "budget_used_percent": 77.80169811924826
    },
    {
      "type": "training",
      "description": "Training step 3273",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:35",
      "total_flops_so_far": 7.782546235145664e+16,
      "budget_used_percent": 77.82546235145664
    },
    {
      "type": "training",
      "description": "Training step 3274",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:36",
      "total_flops_so_far": 7.784922658366502e+16,
      "budget_used_percent": 77.84922658366501
    },
    {
      "type": "training",
      "description": "Training step 3275",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:37",
      "total_flops_so_far": 7.78729908158734e+16,
      "budget_used_percent": 77.87299081587341
    },
    {
      "type": "training",
      "description": "Training step 3276",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:39",
      "total_flops_so_far": 7.78967550480818e+16,
      "budget_used_percent": 77.89675504808179
    },
    {
      "type": "training",
      "description": "Training step 3277",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:40",
      "total_flops_so_far": 7.792051928029018e+16,
      "budget_used_percent": 77.92051928029018
    },
    {
      "type": "training",
      "description": "Training step 3278",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:42",
      "total_flops_so_far": 7.794428351249856e+16,
      "budget_used_percent": 77.94428351249856
    },
    {
      "type": "training",
      "description": "Training step 3279",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:43",
      "total_flops_so_far": 7.796804774470694e+16,
      "budget_used_percent": 77.96804774470695
    },
    {
      "type": "training",
      "description": "Training step 3280",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:44",
      "total_flops_so_far": 7.799181197691533e+16,
      "budget_used_percent": 77.99181197691533
    },
    {
      "type": "training",
      "description": "Training step 3281",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:46",
      "total_flops_so_far": 7.801557620912371e+16,
      "budget_used_percent": 78.01557620912372
    },
    {
      "type": "training",
      "description": "Training step 3282",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:47",
      "total_flops_so_far": 7.80393404413321e+16,
      "budget_used_percent": 78.0393404413321
    },
    {
      "type": "training",
      "description": "Training step 3283",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:48",
      "total_flops_so_far": 7.806310467354048e+16,
      "budget_used_percent": 78.06310467354048
    },
    {
      "type": "training",
      "description": "Training step 3284",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:50",
      "total_flops_so_far": 7.808686890574886e+16,
      "budget_used_percent": 78.08686890574886
    },
    {
      "type": "training",
      "description": "Training step 3285",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:51",
      "total_flops_so_far": 7.811063313795725e+16,
      "budget_used_percent": 78.11063313795725
    },
    {
      "type": "training",
      "description": "Training step 3286",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:53",
      "total_flops_so_far": 7.813439737016563e+16,
      "budget_used_percent": 78.13439737016563
    },
    {
      "type": "training",
      "description": "Training step 3287",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:54",
      "total_flops_so_far": 7.815816160237402e+16,
      "budget_used_percent": 78.15816160237402
    },
    {
      "type": "training",
      "description": "Training step 3288",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:55",
      "total_flops_so_far": 7.81819258345824e+16,
      "budget_used_percent": 78.1819258345824
    },
    {
      "type": "training",
      "description": "Training step 3289",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:57",
      "total_flops_so_far": 7.820569006679078e+16,
      "budget_used_percent": 78.2056900667908
    },
    {
      "type": "training",
      "description": "Training step 3290",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:58",
      "total_flops_so_far": 7.822945429899917e+16,
      "budget_used_percent": 78.22945429899917
    },
    {
      "type": "training",
      "description": "Training step 3291",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:24:59",
      "total_flops_so_far": 7.825321853120755e+16,
      "budget_used_percent": 78.25321853120755
    },
    {
      "type": "training",
      "description": "Training step 3292",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:01",
      "total_flops_so_far": 7.827698276341594e+16,
      "budget_used_percent": 78.27698276341593
    },
    {
      "type": "training",
      "description": "Training step 3293",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:02",
      "total_flops_so_far": 7.830074699562432e+16,
      "budget_used_percent": 78.30074699562431
    },
    {
      "type": "training",
      "description": "Training step 3294",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:03",
      "total_flops_so_far": 7.83245112278327e+16,
      "budget_used_percent": 78.3245112278327
    },
    {
      "type": "training",
      "description": "Training step 3295",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:05",
      "total_flops_so_far": 7.834827546004109e+16,
      "budget_used_percent": 78.34827546004108
    },
    {
      "type": "training",
      "description": "Training step 3296",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:06",
      "total_flops_so_far": 7.837203969224947e+16,
      "budget_used_percent": 78.37203969224947
    },
    {
      "type": "training",
      "description": "Training step 3297",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:07",
      "total_flops_so_far": 7.839580392445786e+16,
      "budget_used_percent": 78.39580392445785
    },
    {
      "type": "training",
      "description": "Training step 3298",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:09",
      "total_flops_so_far": 7.841956815666624e+16,
      "budget_used_percent": 78.41956815666624
    },
    {
      "type": "training",
      "description": "Training step 3299",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:11",
      "total_flops_so_far": 7.844333238887462e+16,
      "budget_used_percent": 78.44333238887462
    },
    {
      "type": "training",
      "description": "Training step 3300",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:11",
      "total_flops_so_far": 7.8467096621083e+16,
      "budget_used_percent": 78.46709662108302
    },
    {
      "type": "training",
      "description": "Training step 3301",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:12",
      "total_flops_so_far": 7.84908608532914e+16,
      "budget_used_percent": 78.4908608532914
    },
    {
      "type": "training",
      "description": "Training step 3302",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:14",
      "total_flops_so_far": 7.851462508549978e+16,
      "budget_used_percent": 78.51462508549977
    },
    {
      "type": "training",
      "description": "Training step 3303",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:15",
      "total_flops_so_far": 7.853838931770816e+16,
      "budget_used_percent": 78.53838931770815
    },
    {
      "type": "training",
      "description": "Training step 3304",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:17",
      "total_flops_so_far": 7.856215354991654e+16,
      "budget_used_percent": 78.56215354991654
    },
    {
      "type": "training",
      "description": "Training step 3305",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:18",
      "total_flops_so_far": 7.858591778212493e+16,
      "budget_used_percent": 78.58591778212492
    },
    {
      "type": "training",
      "description": "Training step 3306",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:19",
      "total_flops_so_far": 7.860968201433331e+16,
      "budget_used_percent": 78.60968201433332
    },
    {
      "type": "training",
      "description": "Training step 3307",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:21",
      "total_flops_so_far": 7.86334462465417e+16,
      "budget_used_percent": 78.6334462465417
    },
    {
      "type": "training",
      "description": "Training step 3308",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:22",
      "total_flops_so_far": 7.865721047875008e+16,
      "budget_used_percent": 78.65721047875009
    },
    {
      "type": "training",
      "description": "Training step 3309",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:23",
      "total_flops_so_far": 7.868097471095846e+16,
      "budget_used_percent": 78.68097471095847
    },
    {
      "type": "training",
      "description": "Training step 3310",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:25",
      "total_flops_so_far": 7.870473894316685e+16,
      "budget_used_percent": 78.70473894316686
    },
    {
      "type": "training",
      "description": "Training step 3311",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:26",
      "total_flops_so_far": 7.872850317537523e+16,
      "budget_used_percent": 78.72850317537524
    },
    {
      "type": "training",
      "description": "Training step 3312",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:27",
      "total_flops_so_far": 7.875226740758362e+16,
      "budget_used_percent": 78.75226740758362
    },
    {
      "type": "training",
      "description": "Training step 3313",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:29",
      "total_flops_so_far": 7.8776031639792e+16,
      "budget_used_percent": 78.776031639792
    },
    {
      "type": "training",
      "description": "Training step 3314",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:30",
      "total_flops_so_far": 7.879979587200038e+16,
      "budget_used_percent": 78.79979587200039
    },
    {
      "type": "training",
      "description": "Training step 3315",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:31",
      "total_flops_so_far": 7.882356010420877e+16,
      "budget_used_percent": 78.82356010420877
    },
    {
      "type": "training",
      "description": "Training step 3316",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:33",
      "total_flops_so_far": 7.884732433641715e+16,
      "budget_used_percent": 78.84732433641716
    },
    {
      "type": "training",
      "description": "Training step 3317",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:34",
      "total_flops_so_far": 7.887108856862554e+16,
      "budget_used_percent": 78.87108856862554
    },
    {
      "type": "training",
      "description": "Training step 3318",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:35",
      "total_flops_so_far": 7.889485280083392e+16,
      "budget_used_percent": 78.89485280083392
    },
    {
      "type": "training",
      "description": "Training step 3319",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:37",
      "total_flops_so_far": 7.89186170330423e+16,
      "budget_used_percent": 78.91861703304231
    },
    {
      "type": "training",
      "description": "Training step 3320",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:38",
      "total_flops_so_far": 7.894238126525069e+16,
      "budget_used_percent": 78.94238126525069
    },
    {
      "type": "training",
      "description": "Training step 3321",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:40",
      "total_flops_so_far": 7.896614549745907e+16,
      "budget_used_percent": 78.96614549745907
    },
    {
      "type": "training",
      "description": "Training step 3322",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:41",
      "total_flops_so_far": 7.898990972966746e+16,
      "budget_used_percent": 78.98990972966745
    },
    {
      "type": "training",
      "description": "Training step 3323",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:42",
      "total_flops_so_far": 7.901367396187584e+16,
      "budget_used_percent": 79.01367396187584
    },
    {
      "type": "training",
      "description": "Training step 3324",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:44",
      "total_flops_so_far": 7.903743819408422e+16,
      "budget_used_percent": 79.03743819408422
    },
    {
      "type": "training",
      "description": "Training step 3325",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:45",
      "total_flops_so_far": 7.90612024262926e+16,
      "budget_used_percent": 79.06120242629261
    },
    {
      "type": "training",
      "description": "Training step 3326",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:46",
      "total_flops_so_far": 7.9084966658501e+16,
      "budget_used_percent": 79.08496665850099
    },
    {
      "type": "training",
      "description": "Training step 3327",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:48",
      "total_flops_so_far": 7.910873089070938e+16,
      "budget_used_percent": 79.10873089070938
    },
    {
      "type": "training",
      "description": "Training step 3328",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:49",
      "total_flops_so_far": 7.913249512291776e+16,
      "budget_used_percent": 79.13249512291776
    },
    {
      "type": "training",
      "description": "Training step 3329",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:50",
      "total_flops_so_far": 7.915625935512614e+16,
      "budget_used_percent": 79.15625935512615
    },
    {
      "type": "training",
      "description": "Training step 3330",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:52",
      "total_flops_so_far": 7.918002358733453e+16,
      "budget_used_percent": 79.18002358733453
    },
    {
      "type": "training",
      "description": "Training step 3331",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:53",
      "total_flops_so_far": 7.920378781954291e+16,
      "budget_used_percent": 79.20378781954291
    },
    {
      "type": "training",
      "description": "Training step 3332",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:54",
      "total_flops_so_far": 7.92275520517513e+16,
      "budget_used_percent": 79.22755205175129
    },
    {
      "type": "training",
      "description": "Training step 3333",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:56",
      "total_flops_so_far": 7.925131628395968e+16,
      "budget_used_percent": 79.25131628395968
    },
    {
      "type": "training",
      "description": "Training step 3334",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:57",
      "total_flops_so_far": 7.927508051616806e+16,
      "budget_used_percent": 79.27508051616806
    },
    {
      "type": "training",
      "description": "Training step 3335",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:25:59",
      "total_flops_so_far": 7.929884474837645e+16,
      "budget_used_percent": 79.29884474837645
    },
    {
      "type": "training",
      "description": "Training step 3336",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:00",
      "total_flops_so_far": 7.932260898058483e+16,
      "budget_used_percent": 79.32260898058483
    },
    {
      "type": "training",
      "description": "Training step 3337",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:01",
      "total_flops_so_far": 7.934637321279322e+16,
      "budget_used_percent": 79.34637321279322
    },
    {
      "type": "training",
      "description": "Training step 3338",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:03",
      "total_flops_so_far": 7.93701374450016e+16,
      "budget_used_percent": 79.3701374450016
    },
    {
      "type": "training",
      "description": "Training step 3339",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:04",
      "total_flops_so_far": 7.939390167720998e+16,
      "budget_used_percent": 79.39390167720998
    },
    {
      "type": "training",
      "description": "Training step 3340",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:05",
      "total_flops_so_far": 7.941766590941837e+16,
      "budget_used_percent": 79.41766590941836
    },
    {
      "type": "training",
      "description": "Training step 3341",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:07",
      "total_flops_so_far": 7.944143014162675e+16,
      "budget_used_percent": 79.44143014162675
    },
    {
      "type": "training",
      "description": "Training step 3342",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:08",
      "total_flops_so_far": 7.946519437383514e+16,
      "budget_used_percent": 79.46519437383513
    },
    {
      "type": "training",
      "description": "Training step 3343",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:09",
      "total_flops_so_far": 7.948895860604352e+16,
      "budget_used_percent": 79.48895860604352
    },
    {
      "type": "training",
      "description": "Training step 3344",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:11",
      "total_flops_so_far": 7.95127228382519e+16,
      "budget_used_percent": 79.5127228382519
    },
    {
      "type": "training",
      "description": "Training step 3345",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:12",
      "total_flops_so_far": 7.953648707046029e+16,
      "budget_used_percent": 79.5364870704603
    },
    {
      "type": "training",
      "description": "Training step 3346",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:13",
      "total_flops_so_far": 7.956025130266867e+16,
      "budget_used_percent": 79.56025130266868
    },
    {
      "type": "training",
      "description": "Training step 3347",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:15",
      "total_flops_so_far": 7.958401553487706e+16,
      "budget_used_percent": 79.58401553487705
    },
    {
      "type": "training",
      "description": "Training step 3348",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:16",
      "total_flops_so_far": 7.960777976708544e+16,
      "budget_used_percent": 79.60777976708545
    },
    {
      "type": "training",
      "description": "Training step 3349",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:18",
      "total_flops_so_far": 7.963154399929382e+16,
      "budget_used_percent": 79.63154399929383
    },
    {
      "type": "training",
      "description": "Training step 3350",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:19",
      "total_flops_so_far": 7.96553082315022e+16,
      "budget_used_percent": 79.6553082315022
    },
    {
      "type": "training",
      "description": "Training step 3351",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:20",
      "total_flops_so_far": 7.96790724637106e+16,
      "budget_used_percent": 79.67907246371058
    },
    {
      "type": "training",
      "description": "Training step 3352",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:22",
      "total_flops_so_far": 7.970283669591898e+16,
      "budget_used_percent": 79.70283669591898
    },
    {
      "type": "training",
      "description": "Training step 3353",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:23",
      "total_flops_so_far": 7.972660092812736e+16,
      "budget_used_percent": 79.72660092812735
    },
    {
      "type": "training",
      "description": "Training step 3354",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:24",
      "total_flops_so_far": 7.975036516033574e+16,
      "budget_used_percent": 79.75036516033575
    },
    {
      "type": "training",
      "description": "Training step 3355",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:26",
      "total_flops_so_far": 7.977412939254413e+16,
      "budget_used_percent": 79.77412939254413
    },
    {
      "type": "training",
      "description": "Training step 3356",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:27",
      "total_flops_so_far": 7.979789362475251e+16,
      "budget_used_percent": 79.79789362475252
    },
    {
      "type": "training",
      "description": "Training step 3357",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:29",
      "total_flops_so_far": 7.98216578569609e+16,
      "budget_used_percent": 79.8216578569609
    },
    {
      "type": "training",
      "description": "Training step 3358",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:30",
      "total_flops_so_far": 7.984542208916928e+16,
      "budget_used_percent": 79.84542208916928
    },
    {
      "type": "training",
      "description": "Training step 3359",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:31",
      "total_flops_so_far": 7.986918632137766e+16,
      "budget_used_percent": 79.86918632137765
    },
    {
      "type": "training",
      "description": "Training step 3360",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:33",
      "total_flops_so_far": 7.989295055358605e+16,
      "budget_used_percent": 79.89295055358605
    },
    {
      "type": "training",
      "description": "Training step 3361",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:34",
      "total_flops_so_far": 7.991671478579443e+16,
      "budget_used_percent": 79.91671478579443
    },
    {
      "type": "training",
      "description": "Training step 3362",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:35",
      "total_flops_so_far": 7.994047901800282e+16,
      "budget_used_percent": 79.94047901800282
    },
    {
      "type": "training",
      "description": "Training step 3363",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:37",
      "total_flops_so_far": 7.99642432502112e+16,
      "budget_used_percent": 79.9642432502112
    },
    {
      "type": "training",
      "description": "Training step 3364",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:38",
      "total_flops_so_far": 7.998800748241958e+16,
      "budget_used_percent": 79.98800748241959
    },
    {
      "type": "training",
      "description": "Training step 3365",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:39",
      "total_flops_so_far": 8.001177171462797e+16,
      "budget_used_percent": 80.01177171462797
    },
    {
      "type": "training",
      "description": "Training step 3366",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:41",
      "total_flops_so_far": 8.003553594683635e+16,
      "budget_used_percent": 80.03553594683636
    },
    {
      "type": "training",
      "description": "Training step 3367",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:42",
      "total_flops_so_far": 8.005930017904474e+16,
      "budget_used_percent": 80.05930017904474
    },
    {
      "type": "training",
      "description": "Training step 3368",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:43",
      "total_flops_so_far": 8.008306441125312e+16,
      "budget_used_percent": 80.08306441125312
    },
    {
      "type": "training",
      "description": "Training step 3369",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:45",
      "total_flops_so_far": 8.01068286434615e+16,
      "budget_used_percent": 80.1068286434615
    },
    {
      "type": "training",
      "description": "Training step 3370",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:46",
      "total_flops_so_far": 8.013059287566989e+16,
      "budget_used_percent": 80.13059287566989
    },
    {
      "type": "training",
      "description": "Training step 3371",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:47",
      "total_flops_so_far": 8.015435710787827e+16,
      "budget_used_percent": 80.15435710787827
    },
    {
      "type": "training",
      "description": "Training step 3372",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:49",
      "total_flops_so_far": 8.017812134008666e+16,
      "budget_used_percent": 80.17812134008666
    },
    {
      "type": "training",
      "description": "Training step 3373",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:50",
      "total_flops_so_far": 8.020188557229504e+16,
      "budget_used_percent": 80.20188557229504
    },
    {
      "type": "training",
      "description": "Training step 3374",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:52",
      "total_flops_so_far": 8.022564980450342e+16,
      "budget_used_percent": 80.22564980450343
    },
    {
      "type": "training",
      "description": "Training step 3375",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:53",
      "total_flops_so_far": 8.02494140367118e+16,
      "budget_used_percent": 80.24941403671181
    },
    {
      "type": "training",
      "description": "Training step 3376",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:54",
      "total_flops_so_far": 8.02731782689202e+16,
      "budget_used_percent": 80.27317826892019
    },
    {
      "type": "training",
      "description": "Training step 3377",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:56",
      "total_flops_so_far": 8.029694250112858e+16,
      "budget_used_percent": 80.29694250112857
    },
    {
      "type": "training",
      "description": "Training step 3378",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:57",
      "total_flops_so_far": 8.032070673333696e+16,
      "budget_used_percent": 80.32070673333695
    },
    {
      "type": "training",
      "description": "Training step 3379",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:26:58",
      "total_flops_so_far": 8.034447096554534e+16,
      "budget_used_percent": 80.34447096554534
    },
    {
      "type": "training",
      "description": "Training step 3380",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:00",
      "total_flops_so_far": 8.036823519775373e+16,
      "budget_used_percent": 80.36823519775372
    },
    {
      "type": "training",
      "description": "Training step 3381",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:01",
      "total_flops_so_far": 8.039199942996211e+16,
      "budget_used_percent": 80.39199942996211
    },
    {
      "type": "training",
      "description": "Training step 3382",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:02",
      "total_flops_so_far": 8.04157636621705e+16,
      "budget_used_percent": 80.41576366217049
    },
    {
      "type": "training",
      "description": "Training step 3383",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:04",
      "total_flops_so_far": 8.043952789437888e+16,
      "budget_used_percent": 80.43952789437888
    },
    {
      "type": "training",
      "description": "Training step 3384",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:05",
      "total_flops_so_far": 8.046329212658726e+16,
      "budget_used_percent": 80.46329212658726
    },
    {
      "type": "training",
      "description": "Training step 3385",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:06",
      "total_flops_so_far": 8.048705635879565e+16,
      "budget_used_percent": 80.48705635879566
    },
    {
      "type": "training",
      "description": "Training step 3386",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:08",
      "total_flops_so_far": 8.051082059100403e+16,
      "budget_used_percent": 80.51082059100403
    },
    {
      "type": "training",
      "description": "Training step 3387",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:09",
      "total_flops_so_far": 8.053458482321242e+16,
      "budget_used_percent": 80.53458482321241
    },
    {
      "type": "training",
      "description": "Training step 3388",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:11",
      "total_flops_so_far": 8.05583490554208e+16,
      "budget_used_percent": 80.55834905542079
    },
    {
      "type": "training",
      "description": "Training step 3389",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:12",
      "total_flops_so_far": 8.058211328762918e+16,
      "budget_used_percent": 80.58211328762918
    },
    {
      "type": "training",
      "description": "Training step 3390",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:13",
      "total_flops_so_far": 8.060587751983757e+16,
      "budget_used_percent": 80.60587751983756
    },
    {
      "type": "training",
      "description": "Training step 3391",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:15",
      "total_flops_so_far": 8.062964175204595e+16,
      "budget_used_percent": 80.62964175204596
    },
    {
      "type": "training",
      "description": "Training step 3392",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:16",
      "total_flops_so_far": 8.065340598425434e+16,
      "budget_used_percent": 80.65340598425433
    },
    {
      "type": "training",
      "description": "Training step 3393",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:17",
      "total_flops_so_far": 8.067717021646272e+16,
      "budget_used_percent": 80.67717021646273
    },
    {
      "type": "training",
      "description": "Training step 3394",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:19",
      "total_flops_so_far": 8.07009344486711e+16,
      "budget_used_percent": 80.7009344486711
    },
    {
      "type": "training",
      "description": "Training step 3395",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:20",
      "total_flops_so_far": 8.072469868087949e+16,
      "budget_used_percent": 80.7246986808795
    },
    {
      "type": "training",
      "description": "Training step 3396",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:22",
      "total_flops_so_far": 8.074846291308787e+16,
      "budget_used_percent": 80.74846291308788
    },
    {
      "type": "training",
      "description": "Training step 3397",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:23",
      "total_flops_so_far": 8.077222714529626e+16,
      "budget_used_percent": 80.77222714529626
    },
    {
      "type": "training",
      "description": "Training step 3398",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:24",
      "total_flops_so_far": 8.079599137750464e+16,
      "budget_used_percent": 80.79599137750463
    },
    {
      "type": "training",
      "description": "Training step 3399",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:26",
      "total_flops_so_far": 8.081975560971302e+16,
      "budget_used_percent": 80.81975560971303
    },
    {
      "type": "training",
      "description": "Training step 3400",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:27",
      "total_flops_so_far": 8.08435198419214e+16,
      "budget_used_percent": 80.8435198419214
    },
    {
      "type": "training",
      "description": "Training step 3401",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:28",
      "total_flops_so_far": 8.08672840741298e+16,
      "budget_used_percent": 80.8672840741298
    },
    {
      "type": "training",
      "description": "Training step 3402",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:30",
      "total_flops_so_far": 8.089104830633818e+16,
      "budget_used_percent": 80.89104830633818
    },
    {
      "type": "training",
      "description": "Training step 3403",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:31",
      "total_flops_so_far": 8.091481253854656e+16,
      "budget_used_percent": 80.91481253854657
    },
    {
      "type": "training",
      "description": "Training step 3404",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:32",
      "total_flops_so_far": 8.093857677075494e+16,
      "budget_used_percent": 80.93857677075495
    },
    {
      "type": "training",
      "description": "Training step 3405",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:34",
      "total_flops_so_far": 8.096234100296333e+16,
      "budget_used_percent": 80.96234100296333
    },
    {
      "type": "training",
      "description": "Training step 3406",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:35",
      "total_flops_so_far": 8.098610523517171e+16,
      "budget_used_percent": 80.9861052351717
    },
    {
      "type": "training",
      "description": "Training step 3407",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:37",
      "total_flops_so_far": 8.10098694673801e+16,
      "budget_used_percent": 81.00986946738008
    },
    {
      "type": "training",
      "description": "Training step 3408",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:38",
      "total_flops_so_far": 8.103363369958848e+16,
      "budget_used_percent": 81.03363369958848
    },
    {
      "type": "training",
      "description": "Training step 3409",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:39",
      "total_flops_so_far": 8.105739793179686e+16,
      "budget_used_percent": 81.05739793179686
    },
    {
      "type": "training",
      "description": "Training step 3410",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:41",
      "total_flops_so_far": 8.108116216400525e+16,
      "budget_used_percent": 81.08116216400525
    },
    {
      "type": "training",
      "description": "Training step 3411",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:42",
      "total_flops_so_far": 8.110492639621363e+16,
      "budget_used_percent": 81.10492639621363
    },
    {
      "type": "training",
      "description": "Training step 3412",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:43",
      "total_flops_so_far": 8.112869062842202e+16,
      "budget_used_percent": 81.12869062842202
    },
    {
      "type": "training",
      "description": "Training step 3413",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:45",
      "total_flops_so_far": 8.11524548606304e+16,
      "budget_used_percent": 81.1524548606304
    },
    {
      "type": "training",
      "description": "Training step 3414",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:46",
      "total_flops_so_far": 8.117621909283878e+16,
      "budget_used_percent": 81.17621909283879
    },
    {
      "type": "training",
      "description": "Training step 3415",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:47",
      "total_flops_so_far": 8.119998332504717e+16,
      "budget_used_percent": 81.19998332504717
    },
    {
      "type": "training",
      "description": "Training step 3416",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:49",
      "total_flops_so_far": 8.122374755725555e+16,
      "budget_used_percent": 81.22374755725555
    },
    {
      "type": "training",
      "description": "Training step 3417",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:50",
      "total_flops_so_far": 8.124751178946394e+16,
      "budget_used_percent": 81.24751178946393
    },
    {
      "type": "training",
      "description": "Training step 3418",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:51",
      "total_flops_so_far": 8.127127602167232e+16,
      "budget_used_percent": 81.27127602167232
    },
    {
      "type": "training",
      "description": "Training step 3419",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:53",
      "total_flops_so_far": 8.12950402538807e+16,
      "budget_used_percent": 81.2950402538807
    },
    {
      "type": "training",
      "description": "Training step 3420",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:54",
      "total_flops_so_far": 8.131880448608909e+16,
      "budget_used_percent": 81.31880448608909
    },
    {
      "type": "training",
      "description": "Training step 3421",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:56",
      "total_flops_so_far": 8.134256871829747e+16,
      "budget_used_percent": 81.34256871829747
    },
    {
      "type": "training",
      "description": "Training step 3422",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:57",
      "total_flops_so_far": 8.136633295050586e+16,
      "budget_used_percent": 81.36633295050586
    },
    {
      "type": "training",
      "description": "Training step 3423",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:27:58",
      "total_flops_so_far": 8.139009718271424e+16,
      "budget_used_percent": 81.39009718271424
    },
    {
      "type": "training",
      "description": "Training step 3424",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:00",
      "total_flops_so_far": 8.141386141492262e+16,
      "budget_used_percent": 81.41386141492262
    },
    {
      "type": "training",
      "description": "Training step 3425",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:01",
      "total_flops_so_far": 8.1437625647131e+16,
      "budget_used_percent": 81.437625647131
    },
    {
      "type": "training",
      "description": "Training step 3426",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:02",
      "total_flops_so_far": 8.14613898793394e+16,
      "budget_used_percent": 81.4613898793394
    },
    {
      "type": "training",
      "description": "Training step 3427",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:04",
      "total_flops_so_far": 8.148515411154778e+16,
      "budget_used_percent": 81.48515411154777
    },
    {
      "type": "training",
      "description": "Training step 3428",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:05",
      "total_flops_so_far": 8.150891834375616e+16,
      "budget_used_percent": 81.50891834375616
    },
    {
      "type": "training",
      "description": "Training step 3429",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:06",
      "total_flops_so_far": 8.153268257596454e+16,
      "budget_used_percent": 81.53268257596454
    },
    {
      "type": "training",
      "description": "Training step 3430",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:08",
      "total_flops_so_far": 8.155644680817293e+16,
      "budget_used_percent": 81.55644680817294
    },
    {
      "type": "training",
      "description": "Training step 3431",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:09",
      "total_flops_so_far": 8.158021104038131e+16,
      "budget_used_percent": 81.58021104038131
    },
    {
      "type": "training",
      "description": "Training step 3432",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:09",
      "total_flops_so_far": 8.16039752725897e+16,
      "budget_used_percent": 81.6039752725897
    },
    {
      "type": "training",
      "description": "Training step 3433",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:11",
      "total_flops_so_far": 8.162773950479808e+16,
      "budget_used_percent": 81.62773950479809
    },
    {
      "type": "training",
      "description": "Training step 3434",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:12",
      "total_flops_so_far": 8.165150373700646e+16,
      "budget_used_percent": 81.65150373700646
    },
    {
      "type": "training",
      "description": "Training step 3435",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:14",
      "total_flops_so_far": 8.167526796921485e+16,
      "budget_used_percent": 81.67526796921484
    },
    {
      "type": "training",
      "description": "Training step 3436",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:15",
      "total_flops_so_far": 8.169903220142323e+16,
      "budget_used_percent": 81.69903220142322
    },
    {
      "type": "training",
      "description": "Training step 3437",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:16",
      "total_flops_so_far": 8.172279643363162e+16,
      "budget_used_percent": 81.72279643363161
    },
    {
      "type": "training",
      "description": "Training step 3438",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:18",
      "total_flops_so_far": 8.174656066584e+16,
      "budget_used_percent": 81.74656066584
    },
    {
      "type": "training",
      "description": "Training step 3439",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:19",
      "total_flops_so_far": 8.177032489804838e+16,
      "budget_used_percent": 81.77032489804839
    },
    {
      "type": "training",
      "description": "Training step 3440",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:21",
      "total_flops_so_far": 8.179408913025677e+16,
      "budget_used_percent": 81.79408913025676
    },
    {
      "type": "training",
      "description": "Training step 3441",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:22",
      "total_flops_so_far": 8.181785336246515e+16,
      "budget_used_percent": 81.81785336246516
    },
    {
      "type": "training",
      "description": "Training step 3442",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:23",
      "total_flops_so_far": 8.184161759467354e+16,
      "budget_used_percent": 81.84161759467354
    },
    {
      "type": "training",
      "description": "Training step 3443",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:25",
      "total_flops_so_far": 8.186538182688192e+16,
      "budget_used_percent": 81.86538182688193
    },
    {
      "type": "training",
      "description": "Training step 3444",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:26",
      "total_flops_so_far": 8.18891460590903e+16,
      "budget_used_percent": 81.88914605909031
    },
    {
      "type": "training",
      "description": "Training step 3445",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:27",
      "total_flops_so_far": 8.191291029129869e+16,
      "budget_used_percent": 81.91291029129869
    },
    {
      "type": "training",
      "description": "Training step 3446",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:29",
      "total_flops_so_far": 8.193667452350707e+16,
      "budget_used_percent": 81.93667452350707
    },
    {
      "type": "training",
      "description": "Training step 3447",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:30",
      "total_flops_so_far": 8.196043875571546e+16,
      "budget_used_percent": 81.96043875571546
    },
    {
      "type": "training",
      "description": "Training step 3448",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:31",
      "total_flops_so_far": 8.198420298792384e+16,
      "budget_used_percent": 81.98420298792384
    },
    {
      "type": "training",
      "description": "Training step 3449",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:33",
      "total_flops_so_far": 8.200796722013222e+16,
      "budget_used_percent": 82.00796722013223
    },
    {
      "type": "training",
      "description": "Training step 3450",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:34",
      "total_flops_so_far": 8.20317314523406e+16,
      "budget_used_percent": 82.03173145234061
    },
    {
      "type": "training",
      "description": "Training step 3451",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:35",
      "total_flops_so_far": 8.2055495684549e+16,
      "budget_used_percent": 82.055495684549
    },
    {
      "type": "training",
      "description": "Training step 3452",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:37",
      "total_flops_so_far": 8.207925991675738e+16,
      "budget_used_percent": 82.07925991675738
    },
    {
      "type": "training",
      "description": "Training step 3453",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:38",
      "total_flops_so_far": 8.210302414896576e+16,
      "budget_used_percent": 82.10302414896576
    },
    {
      "type": "training",
      "description": "Training step 3454",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:39",
      "total_flops_so_far": 8.212678838117414e+16,
      "budget_used_percent": 82.12678838117414
    },
    {
      "type": "training",
      "description": "Training step 3455",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:41",
      "total_flops_so_far": 8.215055261338253e+16,
      "budget_used_percent": 82.15055261338253
    },
    {
      "type": "training",
      "description": "Training step 3456",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:42",
      "total_flops_so_far": 8.217431684559091e+16,
      "budget_used_percent": 82.17431684559091
    },
    {
      "type": "training",
      "description": "Training step 3457",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:44",
      "total_flops_so_far": 8.21980810777993e+16,
      "budget_used_percent": 82.1980810777993
    },
    {
      "type": "training",
      "description": "Training step 3458",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:45",
      "total_flops_so_far": 8.222184531000768e+16,
      "budget_used_percent": 82.22184531000768
    },
    {
      "type": "training",
      "description": "Training step 3459",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:46",
      "total_flops_so_far": 8.224560954221606e+16,
      "budget_used_percent": 82.24560954221607
    },
    {
      "type": "training",
      "description": "Training step 3460",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:48",
      "total_flops_so_far": 8.226937377442445e+16,
      "budget_used_percent": 82.26937377442445
    },
    {
      "type": "training",
      "description": "Training step 3461",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:49",
      "total_flops_so_far": 8.229313800663283e+16,
      "budget_used_percent": 82.29313800663283
    },
    {
      "type": "training",
      "description": "Training step 3462",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:50",
      "total_flops_so_far": 8.231690223884122e+16,
      "budget_used_percent": 82.31690223884122
    },
    {
      "type": "training",
      "description": "Training step 3463",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:52",
      "total_flops_so_far": 8.23406664710496e+16,
      "budget_used_percent": 82.3406664710496
    },
    {
      "type": "training",
      "description": "Training step 3464",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:53",
      "total_flops_so_far": 8.236443070325798e+16,
      "budget_used_percent": 82.36443070325798
    },
    {
      "type": "training",
      "description": "Training step 3465",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:54",
      "total_flops_so_far": 8.238819493546637e+16,
      "budget_used_percent": 82.38819493546636
    },
    {
      "type": "training",
      "description": "Training step 3466",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:56",
      "total_flops_so_far": 8.241195916767475e+16,
      "budget_used_percent": 82.41195916767475
    },
    {
      "type": "training",
      "description": "Training step 3467",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:57",
      "total_flops_so_far": 8.243572339988314e+16,
      "budget_used_percent": 82.43572339988313
    },
    {
      "type": "training",
      "description": "Training step 3468",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:28:58",
      "total_flops_so_far": 8.245948763209152e+16,
      "budget_used_percent": 82.45948763209152
    },
    {
      "type": "training",
      "description": "Training step 3469",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:00",
      "total_flops_so_far": 8.24832518642999e+16,
      "budget_used_percent": 82.4832518642999
    },
    {
      "type": "training",
      "description": "Training step 3470",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:01",
      "total_flops_so_far": 8.250701609650829e+16,
      "budget_used_percent": 82.5070160965083
    },
    {
      "type": "training",
      "description": "Training step 3471",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:03",
      "total_flops_so_far": 8.253078032871667e+16,
      "budget_used_percent": 82.53078032871667
    },
    {
      "type": "training",
      "description": "Training step 3472",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:04",
      "total_flops_so_far": 8.255454456092506e+16,
      "budget_used_percent": 82.55454456092505
    },
    {
      "type": "training",
      "description": "Training step 3473",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:05",
      "total_flops_so_far": 8.257830879313344e+16,
      "budget_used_percent": 82.57830879313343
    },
    {
      "type": "training",
      "description": "Training step 3474",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:07",
      "total_flops_so_far": 8.260207302534182e+16,
      "budget_used_percent": 82.60207302534182
    },
    {
      "type": "training",
      "description": "Training step 3475",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:08",
      "total_flops_so_far": 8.26258372575502e+16,
      "budget_used_percent": 82.6258372575502
    },
    {
      "type": "training",
      "description": "Training step 3476",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:09",
      "total_flops_so_far": 8.26496014897586e+16,
      "budget_used_percent": 82.6496014897586
    },
    {
      "type": "training",
      "description": "Training step 3477",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:11",
      "total_flops_so_far": 8.267336572196698e+16,
      "budget_used_percent": 82.67336572196697
    },
    {
      "type": "training",
      "description": "Training step 3478",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:12",
      "total_flops_so_far": 8.269712995417536e+16,
      "budget_used_percent": 82.69712995417537
    },
    {
      "type": "training",
      "description": "Training step 3479",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:13",
      "total_flops_so_far": 8.272089418638374e+16,
      "budget_used_percent": 82.72089418638375
    },
    {
      "type": "training",
      "description": "Training step 3480",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:15",
      "total_flops_so_far": 8.274465841859213e+16,
      "budget_used_percent": 82.74465841859214
    },
    {
      "type": "training",
      "description": "Training step 3481",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:16",
      "total_flops_so_far": 8.276842265080051e+16,
      "budget_used_percent": 82.76842265080052
    },
    {
      "type": "training",
      "description": "Training step 3482",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:18",
      "total_flops_so_far": 8.27921868830089e+16,
      "budget_used_percent": 82.7921868830089
    },
    {
      "type": "training",
      "description": "Training step 3483",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:19",
      "total_flops_so_far": 8.281595111521728e+16,
      "budget_used_percent": 82.81595111521727
    },
    {
      "type": "training",
      "description": "Training step 3484",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:20",
      "total_flops_so_far": 8.283971534742566e+16,
      "budget_used_percent": 82.83971534742567
    },
    {
      "type": "training",
      "description": "Training step 3485",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:22",
      "total_flops_so_far": 8.286347957963405e+16,
      "budget_used_percent": 82.86347957963405
    },
    {
      "type": "training",
      "description": "Training step 3486",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:23",
      "total_flops_so_far": 8.288724381184243e+16,
      "budget_used_percent": 82.88724381184244
    },
    {
      "type": "training",
      "description": "Training step 3487",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:24",
      "total_flops_so_far": 8.291100804405082e+16,
      "budget_used_percent": 82.91100804405082
    },
    {
      "type": "training",
      "description": "Training step 3488",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:26",
      "total_flops_so_far": 8.29347722762592e+16,
      "budget_used_percent": 82.93477227625921
    },
    {
      "type": "training",
      "description": "Training step 3489",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:27",
      "total_flops_so_far": 8.295853650846758e+16,
      "budget_used_percent": 82.95853650846759
    },
    {
      "type": "training",
      "description": "Training step 3490",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:28",
      "total_flops_so_far": 8.298230074067597e+16,
      "budget_used_percent": 82.98230074067597
    },
    {
      "type": "training",
      "description": "Training step 3491",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:30",
      "total_flops_so_far": 8.300606497288435e+16,
      "budget_used_percent": 83.00606497288435
    },
    {
      "type": "training",
      "description": "Training step 3492",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:31",
      "total_flops_so_far": 8.302982920509274e+16,
      "budget_used_percent": 83.02982920509272
    },
    {
      "type": "training",
      "description": "Training step 3493",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:32",
      "total_flops_so_far": 8.305359343730112e+16,
      "budget_used_percent": 83.05359343730112
    },
    {
      "type": "training",
      "description": "Training step 3494",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:34",
      "total_flops_so_far": 8.30773576695095e+16,
      "budget_used_percent": 83.0773576695095
    },
    {
      "type": "training",
      "description": "Training step 3495",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:35",
      "total_flops_so_far": 8.310112190171789e+16,
      "budget_used_percent": 83.10112190171789
    },
    {
      "type": "training",
      "description": "Training step 3496",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:37",
      "total_flops_so_far": 8.312488613392627e+16,
      "budget_used_percent": 83.12488613392627
    },
    {
      "type": "training",
      "description": "Training step 3497",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:38",
      "total_flops_so_far": 8.314865036613466e+16,
      "budget_used_percent": 83.14865036613466
    },
    {
      "type": "training",
      "description": "Training step 3498",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:39",
      "total_flops_so_far": 8.317241459834304e+16,
      "budget_used_percent": 83.17241459834304
    },
    {
      "type": "training",
      "description": "Training step 3499",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:41",
      "total_flops_so_far": 8.319617883055142e+16,
      "budget_used_percent": 83.19617883055143
    },
    {
      "type": "training",
      "description": "Training step 3500",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:42",
      "total_flops_so_far": 8.32199430627598e+16,
      "budget_used_percent": 83.21994306275981
    },
    {
      "type": "training",
      "description": "Training step 3501",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:43",
      "total_flops_so_far": 8.32437072949682e+16,
      "budget_used_percent": 83.24370729496819
    },
    {
      "type": "training",
      "description": "Training step 3502",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:45",
      "total_flops_so_far": 8.326747152717658e+16,
      "budget_used_percent": 83.26747152717657
    },
    {
      "type": "training",
      "description": "Training step 3503",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:46",
      "total_flops_so_far": 8.329123575938496e+16,
      "budget_used_percent": 83.29123575938496
    },
    {
      "type": "training",
      "description": "Training step 3504",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:48",
      "total_flops_so_far": 8.331499999159334e+16,
      "budget_used_percent": 83.31499999159334
    },
    {
      "type": "training",
      "description": "Training step 3505",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:49",
      "total_flops_so_far": 8.333876422380173e+16,
      "budget_used_percent": 83.33876422380173
    },
    {
      "type": "training",
      "description": "Training step 3506",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:50",
      "total_flops_so_far": 8.336252845601011e+16,
      "budget_used_percent": 83.36252845601011
    },
    {
      "type": "training",
      "description": "Training step 3507",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:52",
      "total_flops_so_far": 8.33862926882185e+16,
      "budget_used_percent": 83.3862926882185
    },
    {
      "type": "training",
      "description": "Training step 3508",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:53",
      "total_flops_so_far": 8.341005692042688e+16,
      "budget_used_percent": 83.41005692042688
    },
    {
      "type": "training",
      "description": "Training step 3509",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:54",
      "total_flops_so_far": 8.343382115263526e+16,
      "budget_used_percent": 83.43382115263526
    },
    {
      "type": "training",
      "description": "Training step 3510",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:56",
      "total_flops_so_far": 8.345758538484365e+16,
      "budget_used_percent": 83.45758538484364
    },
    {
      "type": "training",
      "description": "Training step 3511",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:57",
      "total_flops_so_far": 8.348134961705203e+16,
      "budget_used_percent": 83.48134961705203
    },
    {
      "type": "training",
      "description": "Training step 3512",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:29:58",
      "total_flops_so_far": 8.350511384926042e+16,
      "budget_used_percent": 83.50511384926041
    },
    {
      "type": "training",
      "description": "Training step 3513",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:00",
      "total_flops_so_far": 8.35288780814688e+16,
      "budget_used_percent": 83.5288780814688
    },
    {
      "type": "training",
      "description": "Training step 3514",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:01",
      "total_flops_so_far": 8.355264231367718e+16,
      "budget_used_percent": 83.55264231367718
    },
    {
      "type": "training",
      "description": "Training step 3515",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:03",
      "total_flops_so_far": 8.357640654588557e+16,
      "budget_used_percent": 83.57640654588558
    },
    {
      "type": "training",
      "description": "Training step 3516",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:05",
      "total_flops_so_far": 8.360017077809395e+16,
      "budget_used_percent": 83.60017077809395
    },
    {
      "type": "training",
      "description": "Training step 3517",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:06",
      "total_flops_so_far": 8.362393501030234e+16,
      "budget_used_percent": 83.62393501030233
    },
    {
      "type": "training",
      "description": "Training step 3518",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:08",
      "total_flops_so_far": 8.364769924251072e+16,
      "budget_used_percent": 83.64769924251073
    },
    {
      "type": "training",
      "description": "Training step 3519",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:09",
      "total_flops_so_far": 8.36714634747191e+16,
      "budget_used_percent": 83.6714634747191
    },
    {
      "type": "training",
      "description": "Training step 3520",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:10",
      "total_flops_so_far": 8.369522770692749e+16,
      "budget_used_percent": 83.69522770692748
    },
    {
      "type": "training",
      "description": "Training step 3521",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:12",
      "total_flops_so_far": 8.371899193913587e+16,
      "budget_used_percent": 83.71899193913586
    },
    {
      "type": "training",
      "description": "Training step 3522",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:13",
      "total_flops_so_far": 8.374275617134426e+16,
      "budget_used_percent": 83.74275617134425
    },
    {
      "type": "training",
      "description": "Training step 3523",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:14",
      "total_flops_so_far": 8.376652040355264e+16,
      "budget_used_percent": 83.76652040355263
    },
    {
      "type": "training",
      "description": "Training step 3524",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:16",
      "total_flops_so_far": 8.379028463576102e+16,
      "budget_used_percent": 83.79028463576103
    },
    {
      "type": "training",
      "description": "Training step 3525",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:17",
      "total_flops_so_far": 8.38140488679694e+16,
      "budget_used_percent": 83.8140488679694
    },
    {
      "type": "training",
      "description": "Training step 3526",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:18",
      "total_flops_so_far": 8.38378131001778e+16,
      "budget_used_percent": 83.8378131001778
    },
    {
      "type": "training",
      "description": "Training step 3527",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:20",
      "total_flops_so_far": 8.386157733238618e+16,
      "budget_used_percent": 83.86157733238618
    },
    {
      "type": "training",
      "description": "Training step 3528",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:21",
      "total_flops_so_far": 8.388534156459456e+16,
      "budget_used_percent": 83.88534156459457
    },
    {
      "type": "training",
      "description": "Training step 3529",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:23",
      "total_flops_so_far": 8.390910579680294e+16,
      "budget_used_percent": 83.90910579680295
    },
    {
      "type": "training",
      "description": "Training step 3530",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:24",
      "total_flops_so_far": 8.393287002901133e+16,
      "budget_used_percent": 83.93287002901133
    },
    {
      "type": "training",
      "description": "Training step 3531",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:25",
      "total_flops_so_far": 8.395663426121971e+16,
      "budget_used_percent": 83.9566342612197
    },
    {
      "type": "training",
      "description": "Training step 3532",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:27",
      "total_flops_so_far": 8.39803984934281e+16,
      "budget_used_percent": 83.9803984934281
    },
    {
      "type": "training",
      "description": "Training step 3533",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:28",
      "total_flops_so_far": 8.400416272563648e+16,
      "budget_used_percent": 84.00416272563648
    },
    {
      "type": "training",
      "description": "Training step 3534",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:29",
      "total_flops_so_far": 8.402792695784486e+16,
      "budget_used_percent": 84.02792695784487
    },
    {
      "type": "training",
      "description": "Training step 3535",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:31",
      "total_flops_so_far": 8.405169119005325e+16,
      "budget_used_percent": 84.05169119005325
    },
    {
      "type": "training",
      "description": "Training step 3536",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:32",
      "total_flops_so_far": 8.407545542226163e+16,
      "budget_used_percent": 84.07545542226164
    },
    {
      "type": "training",
      "description": "Training step 3537",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:34",
      "total_flops_so_far": 8.409921965447002e+16,
      "budget_used_percent": 84.09921965447002
    },
    {
      "type": "training",
      "description": "Training step 3538",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:35",
      "total_flops_so_far": 8.41229838866784e+16,
      "budget_used_percent": 84.1229838866784
    },
    {
      "type": "training",
      "description": "Training step 3539",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:36",
      "total_flops_so_far": 8.414674811888678e+16,
      "budget_used_percent": 84.14674811888678
    },
    {
      "type": "training",
      "description": "Training step 3540",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:38",
      "total_flops_so_far": 8.417051235109517e+16,
      "budget_used_percent": 84.17051235109517
    },
    {
      "type": "training",
      "description": "Training step 3541",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:39",
      "total_flops_so_far": 8.419427658330355e+16,
      "budget_used_percent": 84.19427658330355
    },
    {
      "type": "training",
      "description": "Training step 3542",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:40",
      "total_flops_so_far": 8.421804081551194e+16,
      "budget_used_percent": 84.21804081551194
    },
    {
      "type": "training",
      "description": "Training step 3543",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:42",
      "total_flops_so_far": 8.424180504772032e+16,
      "budget_used_percent": 84.24180504772032
    },
    {
      "type": "training",
      "description": "Training step 3544",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:43",
      "total_flops_so_far": 8.42655692799287e+16,
      "budget_used_percent": 84.26556927992871
    },
    {
      "type": "training",
      "description": "Training step 3545",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:44",
      "total_flops_so_far": 8.428933351213709e+16,
      "budget_used_percent": 84.28933351213709
    },
    {
      "type": "training",
      "description": "Training step 3546",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:46",
      "total_flops_so_far": 8.431309774434547e+16,
      "budget_used_percent": 84.31309774434547
    },
    {
      "type": "training",
      "description": "Training step 3547",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:47",
      "total_flops_so_far": 8.433686197655386e+16,
      "budget_used_percent": 84.33686197655386
    },
    {
      "type": "training",
      "description": "Training step 3548",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:49",
      "total_flops_so_far": 8.436062620876224e+16,
      "budget_used_percent": 84.36062620876224
    },
    {
      "type": "training",
      "description": "Training step 3549",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:50",
      "total_flops_so_far": 8.438439044097062e+16,
      "budget_used_percent": 84.38439044097062
    },
    {
      "type": "training",
      "description": "Training step 3550",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:51",
      "total_flops_so_far": 8.4408154673179e+16,
      "budget_used_percent": 84.408154673179
    },
    {
      "type": "training",
      "description": "Training step 3551",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:53",
      "total_flops_so_far": 8.44319189053874e+16,
      "budget_used_percent": 84.43191890538739
    },
    {
      "type": "training",
      "description": "Training step 3552",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:54",
      "total_flops_so_far": 8.445568313759578e+16,
      "budget_used_percent": 84.45568313759577
    },
    {
      "type": "training",
      "description": "Training step 3553",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:55",
      "total_flops_so_far": 8.447944736980416e+16,
      "budget_used_percent": 84.47944736980416
    },
    {
      "type": "training",
      "description": "Training step 3554",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:57",
      "total_flops_so_far": 8.450321160201254e+16,
      "budget_used_percent": 84.50321160201254
    },
    {
      "type": "training",
      "description": "Training step 3555",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:58",
      "total_flops_so_far": 8.452697583422093e+16,
      "budget_used_percent": 84.52697583422093
    },
    {
      "type": "training",
      "description": "Training step 3556",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:30:59",
      "total_flops_so_far": 8.455074006642931e+16,
      "budget_used_percent": 84.55074006642931
    },
    {
      "type": "training",
      "description": "Training step 3557",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:01",
      "total_flops_so_far": 8.45745042986377e+16,
      "budget_used_percent": 84.57450429863769
    },
    {
      "type": "training",
      "description": "Training step 3558",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:02",
      "total_flops_so_far": 8.459826853084608e+16,
      "budget_used_percent": 84.59826853084607
    },
    {
      "type": "training",
      "description": "Training step 3559",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:03",
      "total_flops_so_far": 8.462203276305446e+16,
      "budget_used_percent": 84.62203276305446
    },
    {
      "type": "training",
      "description": "Training step 3560",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:05",
      "total_flops_so_far": 8.464579699526285e+16,
      "budget_used_percent": 84.64579699526284
    },
    {
      "type": "training",
      "description": "Training step 3561",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:06",
      "total_flops_so_far": 8.466956122747123e+16,
      "budget_used_percent": 84.66956122747123
    },
    {
      "type": "training",
      "description": "Training step 3562",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:08",
      "total_flops_so_far": 8.469332545967962e+16,
      "budget_used_percent": 84.69332545967961
    },
    {
      "type": "training",
      "description": "Training step 3563",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:09",
      "total_flops_so_far": 8.4717089691888e+16,
      "budget_used_percent": 84.717089691888
    },
    {
      "type": "training",
      "description": "Training step 3564",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:09",
      "total_flops_so_far": 8.474085392409638e+16,
      "budget_used_percent": 84.74085392409638
    },
    {
      "type": "training",
      "description": "Training step 3565",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:11",
      "total_flops_so_far": 8.476461815630477e+16,
      "budget_used_percent": 84.76461815630478
    },
    {
      "type": "training",
      "description": "Training step 3566",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:12",
      "total_flops_so_far": 8.478838238851315e+16,
      "budget_used_percent": 84.78838238851316
    },
    {
      "type": "training",
      "description": "Training step 3567",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:13",
      "total_flops_so_far": 8.481214662072154e+16,
      "budget_used_percent": 84.81214662072153
    },
    {
      "type": "training",
      "description": "Training step 3568",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:15",
      "total_flops_so_far": 8.483591085292992e+16,
      "budget_used_percent": 84.83591085292991
    },
    {
      "type": "training",
      "description": "Training step 3569",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:16",
      "total_flops_so_far": 8.48596750851383e+16,
      "budget_used_percent": 84.8596750851383
    },
    {
      "type": "training",
      "description": "Training step 3570",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:18",
      "total_flops_so_far": 8.488343931734669e+16,
      "budget_used_percent": 84.88343931734669
    },
    {
      "type": "training",
      "description": "Training step 3571",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:19",
      "total_flops_so_far": 8.490720354955507e+16,
      "budget_used_percent": 84.90720354955508
    },
    {
      "type": "training",
      "description": "Training step 3572",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:20",
      "total_flops_so_far": 8.493096778176346e+16,
      "budget_used_percent": 84.93096778176346
    },
    {
      "type": "training",
      "description": "Training step 3573",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:22",
      "total_flops_so_far": 8.495473201397184e+16,
      "budget_used_percent": 84.95473201397185
    },
    {
      "type": "training",
      "description": "Training step 3574",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:23",
      "total_flops_so_far": 8.497849624618022e+16,
      "budget_used_percent": 84.97849624618023
    },
    {
      "type": "training",
      "description": "Training step 3575",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:24",
      "total_flops_so_far": 8.50022604783886e+16,
      "budget_used_percent": 85.0022604783886
    },
    {
      "type": "training",
      "description": "Training step 3576",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:26",
      "total_flops_so_far": 8.5026024710597e+16,
      "budget_used_percent": 85.026024710597
    },
    {
      "type": "training",
      "description": "Training step 3577",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:27",
      "total_flops_so_far": 8.504978894280538e+16,
      "budget_used_percent": 85.04978894280538
    },
    {
      "type": "training",
      "description": "Training step 3578",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:29",
      "total_flops_so_far": 8.507355317501376e+16,
      "budget_used_percent": 85.07355317501376
    },
    {
      "type": "training",
      "description": "Training step 3579",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:30",
      "total_flops_so_far": 8.509731740722214e+16,
      "budget_used_percent": 85.09731740722214
    },
    {
      "type": "training",
      "description": "Training step 3580",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:31",
      "total_flops_so_far": 8.512108163943053e+16,
      "budget_used_percent": 85.12108163943053
    },
    {
      "type": "training",
      "description": "Training step 3581",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:33",
      "total_flops_so_far": 8.514484587163891e+16,
      "budget_used_percent": 85.1448458716389
    },
    {
      "type": "training",
      "description": "Training step 3582",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:34",
      "total_flops_so_far": 8.51686101038473e+16,
      "budget_used_percent": 85.1686101038473
    },
    {
      "type": "training",
      "description": "Training step 3583",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:35",
      "total_flops_so_far": 8.519237433605568e+16,
      "budget_used_percent": 85.19237433605568
    },
    {
      "type": "training",
      "description": "Training step 3584",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:37",
      "total_flops_so_far": 8.521613856826406e+16,
      "budget_used_percent": 85.21613856826407
    },
    {
      "type": "training",
      "description": "Training step 3585",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:38",
      "total_flops_so_far": 8.523990280047245e+16,
      "budget_used_percent": 85.23990280047245
    },
    {
      "type": "training",
      "description": "Training step 3586",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:39",
      "total_flops_so_far": 8.526366703268083e+16,
      "budget_used_percent": 85.26366703268083
    },
    {
      "type": "training",
      "description": "Training step 3587",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:41",
      "total_flops_so_far": 8.528743126488922e+16,
      "budget_used_percent": 85.28743126488921
    },
    {
      "type": "training",
      "description": "Training step 3588",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:42",
      "total_flops_so_far": 8.53111954970976e+16,
      "budget_used_percent": 85.3111954970976
    },
    {
      "type": "training",
      "description": "Training step 3589",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:44",
      "total_flops_so_far": 8.533495972930598e+16,
      "budget_used_percent": 85.33495972930598
    },
    {
      "type": "training",
      "description": "Training step 3590",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:45",
      "total_flops_so_far": 8.535872396151437e+16,
      "budget_used_percent": 85.35872396151437
    },
    {
      "type": "training",
      "description": "Training step 3591",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:46",
      "total_flops_so_far": 8.538248819372275e+16,
      "budget_used_percent": 85.38248819372275
    },
    {
      "type": "training",
      "description": "Training step 3592",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:48",
      "total_flops_so_far": 8.540625242593114e+16,
      "budget_used_percent": 85.40625242593114
    },
    {
      "type": "training",
      "description": "Training step 3593",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:49",
      "total_flops_so_far": 8.543001665813952e+16,
      "budget_used_percent": 85.43001665813952
    },
    {
      "type": "training",
      "description": "Training step 3594",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:50",
      "total_flops_so_far": 8.54537808903479e+16,
      "budget_used_percent": 85.45378089034791
    },
    {
      "type": "training",
      "description": "Training step 3595",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:52",
      "total_flops_so_far": 8.547754512255629e+16,
      "budget_used_percent": 85.4775451225563
    },
    {
      "type": "training",
      "description": "Training step 3596",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:53",
      "total_flops_so_far": 8.550130935476467e+16,
      "budget_used_percent": 85.50130935476467
    },
    {
      "type": "training",
      "description": "Training step 3597",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:54",
      "total_flops_so_far": 8.552507358697306e+16,
      "budget_used_percent": 85.52507358697305
    },
    {
      "type": "training",
      "description": "Training step 3598",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:56",
      "total_flops_so_far": 8.554883781918144e+16,
      "budget_used_percent": 85.54883781918144
    },
    {
      "type": "training",
      "description": "Training step 3599",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:57",
      "total_flops_so_far": 8.557260205138982e+16,
      "budget_used_percent": 85.57260205138982
    },
    {
      "type": "training",
      "description": "Training step 3600",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:31:58",
      "total_flops_so_far": 8.55963662835982e+16,
      "budget_used_percent": 85.59636628359821
    },
    {
      "type": "training",
      "description": "Training step 3601",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:00",
      "total_flops_so_far": 8.56201305158066e+16,
      "budget_used_percent": 85.6201305158066
    },
    {
      "type": "training",
      "description": "Training step 3602",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:01",
      "total_flops_so_far": 8.564389474801498e+16,
      "budget_used_percent": 85.64389474801499
    },
    {
      "type": "training",
      "description": "Training step 3603",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:03",
      "total_flops_so_far": 8.566765898022336e+16,
      "budget_used_percent": 85.66765898022337
    },
    {
      "type": "training",
      "description": "Training step 3604",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:04",
      "total_flops_so_far": 8.569142321243174e+16,
      "budget_used_percent": 85.69142321243174
    },
    {
      "type": "training",
      "description": "Training step 3605",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:05",
      "total_flops_so_far": 8.571518744464013e+16,
      "budget_used_percent": 85.71518744464012
    },
    {
      "type": "training",
      "description": "Training step 3606",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:07",
      "total_flops_so_far": 8.573895167684851e+16,
      "budget_used_percent": 85.7389516768485
    },
    {
      "type": "training",
      "description": "Training step 3607",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:08",
      "total_flops_so_far": 8.57627159090569e+16,
      "budget_used_percent": 85.7627159090569
    },
    {
      "type": "training",
      "description": "Training step 3608",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:09",
      "total_flops_so_far": 8.578648014126528e+16,
      "budget_used_percent": 85.78648014126527
    },
    {
      "type": "training",
      "description": "Training step 3609",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:11",
      "total_flops_so_far": 8.581024437347366e+16,
      "budget_used_percent": 85.81024437347367
    },
    {
      "type": "training",
      "description": "Training step 3610",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:12",
      "total_flops_so_far": 8.583400860568205e+16,
      "budget_used_percent": 85.83400860568204
    },
    {
      "type": "training",
      "description": "Training step 3611",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:13",
      "total_flops_so_far": 8.585777283789043e+16,
      "budget_used_percent": 85.85777283789044
    },
    {
      "type": "training",
      "description": "Training step 3612",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:15",
      "total_flops_so_far": 8.588153707009882e+16,
      "budget_used_percent": 85.88153707009882
    },
    {
      "type": "training",
      "description": "Training step 3613",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:16",
      "total_flops_so_far": 8.59053013023072e+16,
      "budget_used_percent": 85.90530130230721
    },
    {
      "type": "training",
      "description": "Training step 3614",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:18",
      "total_flops_so_far": 8.592906553451558e+16,
      "budget_used_percent": 85.92906553451559
    },
    {
      "type": "training",
      "description": "Training step 3615",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:19",
      "total_flops_so_far": 8.595282976672397e+16,
      "budget_used_percent": 85.95282976672397
    },
    {
      "type": "training",
      "description": "Training step 3616",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:20",
      "total_flops_so_far": 8.597659399893235e+16,
      "budget_used_percent": 85.97659399893234
    },
    {
      "type": "training",
      "description": "Training step 3617",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:22",
      "total_flops_so_far": 8.600035823114074e+16,
      "budget_used_percent": 86.00035823114074
    },
    {
      "type": "training",
      "description": "Training step 3618",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:23",
      "total_flops_so_far": 8.602412246334912e+16,
      "budget_used_percent": 86.02412246334912
    },
    {
      "type": "training",
      "description": "Training step 3619",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:24",
      "total_flops_so_far": 8.60478866955575e+16,
      "budget_used_percent": 86.04788669555751
    },
    {
      "type": "training",
      "description": "Training step 3620",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:26",
      "total_flops_so_far": 8.607165092776589e+16,
      "budget_used_percent": 86.07165092776589
    },
    {
      "type": "training",
      "description": "Training step 3621",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:27",
      "total_flops_so_far": 8.609541515997427e+16,
      "budget_used_percent": 86.09541515997428
    },
    {
      "type": "training",
      "description": "Training step 3622",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:29",
      "total_flops_so_far": 8.611917939218266e+16,
      "budget_used_percent": 86.11917939218266
    },
    {
      "type": "training",
      "description": "Training step 3623",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:30",
      "total_flops_so_far": 8.614294362439104e+16,
      "budget_used_percent": 86.14294362439104
    },
    {
      "type": "training",
      "description": "Training step 3624",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:31",
      "total_flops_so_far": 8.616670785659942e+16,
      "budget_used_percent": 86.16670785659942
    },
    {
      "type": "training",
      "description": "Training step 3625",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:33",
      "total_flops_so_far": 8.61904720888078e+16,
      "budget_used_percent": 86.19047208880781
    },
    {
      "type": "training",
      "description": "Training step 3626",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:34",
      "total_flops_so_far": 8.62142363210162e+16,
      "budget_used_percent": 86.21423632101619
    },
    {
      "type": "training",
      "description": "Training step 3627",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:35",
      "total_flops_so_far": 8.623800055322458e+16,
      "budget_used_percent": 86.23800055322458
    },
    {
      "type": "training",
      "description": "Training step 3628",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:37",
      "total_flops_so_far": 8.626176478543296e+16,
      "budget_used_percent": 86.26176478543296
    },
    {
      "type": "training",
      "description": "Training step 3629",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:38",
      "total_flops_so_far": 8.628552901764134e+16,
      "budget_used_percent": 86.28552901764135
    },
    {
      "type": "training",
      "description": "Training step 3630",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:39",
      "total_flops_so_far": 8.630929324984973e+16,
      "budget_used_percent": 86.30929324984973
    },
    {
      "type": "training",
      "description": "Training step 3631",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:41",
      "total_flops_so_far": 8.633305748205811e+16,
      "budget_used_percent": 86.33305748205811
    },
    {
      "type": "training",
      "description": "Training step 3632",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:42",
      "total_flops_so_far": 8.63568217142665e+16,
      "budget_used_percent": 86.3568217142665
    },
    {
      "type": "training",
      "description": "Training step 3633",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:44",
      "total_flops_so_far": 8.638058594647488e+16,
      "budget_used_percent": 86.38058594647488
    },
    {
      "type": "training",
      "description": "Training step 3634",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:45",
      "total_flops_so_far": 8.640435017868326e+16,
      "budget_used_percent": 86.40435017868326
    },
    {
      "type": "training",
      "description": "Training step 3635",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:46",
      "total_flops_so_far": 8.642811441089165e+16,
      "budget_used_percent": 86.42811441089164
    },
    {
      "type": "training",
      "description": "Training step 3636",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:48",
      "total_flops_so_far": 8.645187864310003e+16,
      "budget_used_percent": 86.45187864310003
    },
    {
      "type": "training",
      "description": "Training step 3637",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:49",
      "total_flops_so_far": 8.647564287530842e+16,
      "budget_used_percent": 86.47564287530841
    },
    {
      "type": "training",
      "description": "Training step 3638",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:50",
      "total_flops_so_far": 8.64994071075168e+16,
      "budget_used_percent": 86.4994071075168
    },
    {
      "type": "training",
      "description": "Training step 3639",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:52",
      "total_flops_so_far": 8.652317133972518e+16,
      "budget_used_percent": 86.52317133972518
    },
    {
      "type": "training",
      "description": "Training step 3640",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:53",
      "total_flops_so_far": 8.654693557193357e+16,
      "budget_used_percent": 86.54693557193357
    },
    {
      "type": "training",
      "description": "Training step 3641",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:54",
      "total_flops_so_far": 8.657069980414195e+16,
      "budget_used_percent": 86.57069980414195
    },
    {
      "type": "training",
      "description": "Training step 3642",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:56",
      "total_flops_so_far": 8.659446403635034e+16,
      "budget_used_percent": 86.59446403635033
    },
    {
      "type": "training",
      "description": "Training step 3643",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:57",
      "total_flops_so_far": 8.661822826855872e+16,
      "budget_used_percent": 86.61822826855871
    },
    {
      "type": "training",
      "description": "Training step 3644",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:32:59",
      "total_flops_so_far": 8.66419925007671e+16,
      "budget_used_percent": 86.6419925007671
    },
    {
      "type": "training",
      "description": "Training step 3645",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:00",
      "total_flops_so_far": 8.666575673297549e+16,
      "budget_used_percent": 86.66575673297548
    },
    {
      "type": "training",
      "description": "Training step 3646",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:01",
      "total_flops_so_far": 8.668952096518387e+16,
      "budget_used_percent": 86.68952096518387
    },
    {
      "type": "training",
      "description": "Training step 3647",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:03",
      "total_flops_so_far": 8.671328519739226e+16,
      "budget_used_percent": 86.71328519739225
    },
    {
      "type": "training",
      "description": "Training step 3648",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:04",
      "total_flops_so_far": 8.673704942960064e+16,
      "budget_used_percent": 86.73704942960065
    },
    {
      "type": "training",
      "description": "Training step 3649",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:05",
      "total_flops_so_far": 8.676081366180902e+16,
      "budget_used_percent": 86.76081366180902
    },
    {
      "type": "training",
      "description": "Training step 3650",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:07",
      "total_flops_so_far": 8.67845778940174e+16,
      "budget_used_percent": 86.78457789401742
    },
    {
      "type": "training",
      "description": "Training step 3651",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:08",
      "total_flops_so_far": 8.68083421262258e+16,
      "budget_used_percent": 86.8083421262258
    },
    {
      "type": "training",
      "description": "Training step 3652",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:09",
      "total_flops_so_far": 8.683210635843418e+16,
      "budget_used_percent": 86.83210635843417
    },
    {
      "type": "training",
      "description": "Training step 3653",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:11",
      "total_flops_so_far": 8.685587059064256e+16,
      "budget_used_percent": 86.85587059064255
    },
    {
      "type": "training",
      "description": "Training step 3654",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:12",
      "total_flops_so_far": 8.687963482285094e+16,
      "budget_used_percent": 86.87963482285095
    },
    {
      "type": "training",
      "description": "Training step 3655",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:13",
      "total_flops_so_far": 8.690339905505933e+16,
      "budget_used_percent": 86.90339905505932
    },
    {
      "type": "training",
      "description": "Training step 3656",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:15",
      "total_flops_so_far": 8.692716328726771e+16,
      "budget_used_percent": 86.92716328726772
    },
    {
      "type": "training",
      "description": "Training step 3657",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:16",
      "total_flops_so_far": 8.69509275194761e+16,
      "budget_used_percent": 86.9509275194761
    },
    {
      "type": "training",
      "description": "Training step 3658",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:18",
      "total_flops_so_far": 8.697469175168448e+16,
      "budget_used_percent": 86.97469175168449
    },
    {
      "type": "training",
      "description": "Training step 3659",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:19",
      "total_flops_so_far": 8.699845598389286e+16,
      "budget_used_percent": 86.99845598389287
    },
    {
      "type": "training",
      "description": "Training step 3660",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:20",
      "total_flops_so_far": 8.702222021610125e+16,
      "budget_used_percent": 87.02222021610125
    },
    {
      "type": "training",
      "description": "Training step 3661",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:22",
      "total_flops_so_far": 8.704598444830963e+16,
      "budget_used_percent": 87.04598444830964
    },
    {
      "type": "training",
      "description": "Training step 3662",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:23",
      "total_flops_so_far": 8.706974868051802e+16,
      "budget_used_percent": 87.06974868051802
    },
    {
      "type": "training",
      "description": "Training step 3663",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:25",
      "total_flops_so_far": 8.70935129127264e+16,
      "budget_used_percent": 87.0935129127264
    },
    {
      "type": "training",
      "description": "Training step 3664",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:26",
      "total_flops_so_far": 8.711727714493478e+16,
      "budget_used_percent": 87.11727714493477
    },
    {
      "type": "training",
      "description": "Training step 3665",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:27",
      "total_flops_so_far": 8.714104137714317e+16,
      "budget_used_percent": 87.14104137714317
    },
    {
      "type": "training",
      "description": "Training step 3666",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:29",
      "total_flops_so_far": 8.716480560935155e+16,
      "budget_used_percent": 87.16480560935155
    },
    {
      "type": "training",
      "description": "Training step 3667",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:30",
      "total_flops_so_far": 8.718856984155994e+16,
      "budget_used_percent": 87.18856984155994
    },
    {
      "type": "training",
      "description": "Training step 3668",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:31",
      "total_flops_so_far": 8.721233407376832e+16,
      "budget_used_percent": 87.21233407376832
    },
    {
      "type": "training",
      "description": "Training step 3669",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:33",
      "total_flops_so_far": 8.72360983059767e+16,
      "budget_used_percent": 87.23609830597671
    },
    {
      "type": "training",
      "description": "Training step 3670",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:34",
      "total_flops_so_far": 8.725986253818509e+16,
      "budget_used_percent": 87.25986253818509
    },
    {
      "type": "training",
      "description": "Training step 3671",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:35",
      "total_flops_so_far": 8.728362677039347e+16,
      "budget_used_percent": 87.28362677039347
    },
    {
      "type": "training",
      "description": "Training step 3672",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:37",
      "total_flops_so_far": 8.730739100260186e+16,
      "budget_used_percent": 87.30739100260185
    },
    {
      "type": "training",
      "description": "Training step 3673",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:38",
      "total_flops_so_far": 8.733115523481024e+16,
      "budget_used_percent": 87.33115523481024
    },
    {
      "type": "training",
      "description": "Training step 3674",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:40",
      "total_flops_so_far": 8.735491946701862e+16,
      "budget_used_percent": 87.35491946701862
    },
    {
      "type": "training",
      "description": "Training step 3675",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:41",
      "total_flops_so_far": 8.7378683699227e+16,
      "budget_used_percent": 87.37868369922701
    },
    {
      "type": "training",
      "description": "Training step 3676",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:42",
      "total_flops_so_far": 8.74024479314354e+16,
      "budget_used_percent": 87.40244793143539
    },
    {
      "type": "training",
      "description": "Training step 3677",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:44",
      "total_flops_so_far": 8.742621216364378e+16,
      "budget_used_percent": 87.42621216364378
    },
    {
      "type": "training",
      "description": "Training step 3678",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:45",
      "total_flops_so_far": 8.744997639585216e+16,
      "budget_used_percent": 87.44997639585216
    },
    {
      "type": "training",
      "description": "Training step 3679",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:46",
      "total_flops_so_far": 8.747374062806054e+16,
      "budget_used_percent": 87.47374062806055
    },
    {
      "type": "training",
      "description": "Training step 3680",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:48",
      "total_flops_so_far": 8.749750486026893e+16,
      "budget_used_percent": 87.49750486026893
    },
    {
      "type": "training",
      "description": "Training step 3681",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:49",
      "total_flops_so_far": 8.752126909247731e+16,
      "budget_used_percent": 87.52126909247731
    },
    {
      "type": "training",
      "description": "Training step 3682",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:50",
      "total_flops_so_far": 8.75450333246857e+16,
      "budget_used_percent": 87.54503332468569
    },
    {
      "type": "training",
      "description": "Training step 3683",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:52",
      "total_flops_so_far": 8.756879755689408e+16,
      "budget_used_percent": 87.56879755689408
    },
    {
      "type": "training",
      "description": "Training step 3684",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:53",
      "total_flops_so_far": 8.759256178910246e+16,
      "budget_used_percent": 87.59256178910246
    },
    {
      "type": "training",
      "description": "Training step 3685",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:55",
      "total_flops_so_far": 8.761632602131085e+16,
      "budget_used_percent": 87.61632602131085
    },
    {
      "type": "training",
      "description": "Training step 3686",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:56",
      "total_flops_so_far": 8.764009025351923e+16,
      "budget_used_percent": 87.64009025351923
    },
    {
      "type": "training",
      "description": "Training step 3687",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:57",
      "total_flops_so_far": 8.766385448572762e+16,
      "budget_used_percent": 87.66385448572763
    },
    {
      "type": "training",
      "description": "Training step 3688",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:33:59",
      "total_flops_so_far": 8.7687618717936e+16,
      "budget_used_percent": 87.687618717936
    },
    {
      "type": "training",
      "description": "Training step 3689",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:00",
      "total_flops_so_far": 8.771138295014438e+16,
      "budget_used_percent": 87.71138295014438
    },
    {
      "type": "training",
      "description": "Training step 3690",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:01",
      "total_flops_so_far": 8.773514718235277e+16,
      "budget_used_percent": 87.73514718235276
    },
    {
      "type": "training",
      "description": "Training step 3691",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:03",
      "total_flops_so_far": 8.775891141456115e+16,
      "budget_used_percent": 87.75891141456114
    },
    {
      "type": "training",
      "description": "Training step 3692",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:04",
      "total_flops_so_far": 8.778267564676954e+16,
      "budget_used_percent": 87.78267564676953
    },
    {
      "type": "training",
      "description": "Training step 3693",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:05",
      "total_flops_so_far": 8.780643987897792e+16,
      "budget_used_percent": 87.80643987897791
    },
    {
      "type": "training",
      "description": "Training step 3694",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:07",
      "total_flops_so_far": 8.78302041111863e+16,
      "budget_used_percent": 87.8302041111863
    },
    {
      "type": "training",
      "description": "Training step 3695",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:08",
      "total_flops_so_far": 8.785396834339469e+16,
      "budget_used_percent": 87.85396834339468
    },
    {
      "type": "training",
      "description": "Training step 3696",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:09",
      "total_flops_so_far": 8.787773257560307e+16,
      "budget_used_percent": 87.87773257560308
    },
    {
      "type": "training",
      "description": "Training step 3697",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:10",
      "total_flops_so_far": 8.790149680781146e+16,
      "budget_used_percent": 87.90149680781145
    },
    {
      "type": "training",
      "description": "Training step 3698",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:11",
      "total_flops_so_far": 8.792526104001984e+16,
      "budget_used_percent": 87.92526104001985
    },
    {
      "type": "training",
      "description": "Training step 3699",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:13",
      "total_flops_so_far": 8.794902527222822e+16,
      "budget_used_percent": 87.94902527222823
    },
    {
      "type": "training",
      "description": "Training step 3700",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:14",
      "total_flops_so_far": 8.79727895044366e+16,
      "budget_used_percent": 87.9727895044366
    },
    {
      "type": "training",
      "description": "Training step 3701",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:16",
      "total_flops_so_far": 8.7996553736645e+16,
      "budget_used_percent": 87.99655373664498
    },
    {
      "type": "training",
      "description": "Training step 3702",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:17",
      "total_flops_so_far": 8.802031796885338e+16,
      "budget_used_percent": 88.02031796885338
    },
    {
      "type": "training",
      "description": "Training step 3703",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:18",
      "total_flops_so_far": 8.804408220106176e+16,
      "budget_used_percent": 88.04408220106176
    },
    {
      "type": "training",
      "description": "Training step 3704",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:20",
      "total_flops_so_far": 8.806784643327014e+16,
      "budget_used_percent": 88.06784643327015
    },
    {
      "type": "training",
      "description": "Training step 3705",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:21",
      "total_flops_so_far": 8.809161066547853e+16,
      "budget_used_percent": 88.09161066547853
    },
    {
      "type": "training",
      "description": "Training step 3706",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:22",
      "total_flops_so_far": 8.811537489768691e+16,
      "budget_used_percent": 88.11537489768692
    },
    {
      "type": "training",
      "description": "Training step 3707",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:24",
      "total_flops_so_far": 8.81391391298953e+16,
      "budget_used_percent": 88.1391391298953
    },
    {
      "type": "training",
      "description": "Training step 3708",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:25",
      "total_flops_so_far": 8.816290336210368e+16,
      "budget_used_percent": 88.16290336210369
    },
    {
      "type": "training",
      "description": "Training step 3709",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:26",
      "total_flops_so_far": 8.818666759431206e+16,
      "budget_used_percent": 88.18666759431207
    },
    {
      "type": "training",
      "description": "Training step 3710",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:28",
      "total_flops_so_far": 8.821043182652045e+16,
      "budget_used_percent": 88.21043182652045
    },
    {
      "type": "training",
      "description": "Training step 3711",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:29",
      "total_flops_so_far": 8.823419605872883e+16,
      "budget_used_percent": 88.23419605872883
    },
    {
      "type": "training",
      "description": "Training step 3712",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:31",
      "total_flops_so_far": 8.825796029093722e+16,
      "budget_used_percent": 88.25796029093722
    },
    {
      "type": "training",
      "description": "Training step 3713",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:32",
      "total_flops_so_far": 8.82817245231456e+16,
      "budget_used_percent": 88.2817245231456
    },
    {
      "type": "training",
      "description": "Training step 3714",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:33",
      "total_flops_so_far": 8.830548875535398e+16,
      "budget_used_percent": 88.30548875535399
    },
    {
      "type": "training",
      "description": "Training step 3715",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:35",
      "total_flops_so_far": 8.832925298756237e+16,
      "budget_used_percent": 88.32925298756237
    },
    {
      "type": "training",
      "description": "Training step 3716",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:36",
      "total_flops_so_far": 8.835301721977075e+16,
      "budget_used_percent": 88.35301721977076
    },
    {
      "type": "training",
      "description": "Training step 3717",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:37",
      "total_flops_so_far": 8.837678145197914e+16,
      "budget_used_percent": 88.37678145197914
    },
    {
      "type": "training",
      "description": "Training step 3718",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:39",
      "total_flops_so_far": 8.840054568418752e+16,
      "budget_used_percent": 88.40054568418752
    },
    {
      "type": "training",
      "description": "Training step 3719",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:40",
      "total_flops_so_far": 8.84243099163959e+16,
      "budget_used_percent": 88.4243099163959
    },
    {
      "type": "training",
      "description": "Training step 3720",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:41",
      "total_flops_so_far": 8.844807414860429e+16,
      "budget_used_percent": 88.44807414860428
    },
    {
      "type": "training",
      "description": "Training step 3721",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:43",
      "total_flops_so_far": 8.847183838081267e+16,
      "budget_used_percent": 88.47183838081267
    },
    {
      "type": "training",
      "description": "Training step 3722",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:44",
      "total_flops_so_far": 8.849560261302106e+16,
      "budget_used_percent": 88.49560261302105
    },
    {
      "type": "training",
      "description": "Training step 3723",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:46",
      "total_flops_so_far": 8.851936684522944e+16,
      "budget_used_percent": 88.51936684522944
    },
    {
      "type": "training",
      "description": "Training step 3724",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:47",
      "total_flops_so_far": 8.854313107743782e+16,
      "budget_used_percent": 88.54313107743782
    },
    {
      "type": "training",
      "description": "Training step 3725",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:48",
      "total_flops_so_far": 8.85668953096462e+16,
      "budget_used_percent": 88.56689530964621
    },
    {
      "type": "training",
      "description": "Training step 3726",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:50",
      "total_flops_so_far": 8.85906595418546e+16,
      "budget_used_percent": 88.59065954185459
    },
    {
      "type": "training",
      "description": "Training step 3727",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:51",
      "total_flops_so_far": 8.861442377406298e+16,
      "budget_used_percent": 88.61442377406298
    },
    {
      "type": "training",
      "description": "Training step 3728",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:52",
      "total_flops_so_far": 8.863818800627136e+16,
      "budget_used_percent": 88.63818800627136
    },
    {
      "type": "training",
      "description": "Training step 3729",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:54",
      "total_flops_so_far": 8.866195223847974e+16,
      "budget_used_percent": 88.66195223847974
    },
    {
      "type": "training",
      "description": "Training step 3730",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:55",
      "total_flops_so_far": 8.868571647068813e+16,
      "budget_used_percent": 88.68571647068812
    },
    {
      "type": "training",
      "description": "Training step 3731",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:56",
      "total_flops_so_far": 8.870948070289651e+16,
      "budget_used_percent": 88.70948070289651
    },
    {
      "type": "training",
      "description": "Training step 3732",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:58",
      "total_flops_so_far": 8.87332449351049e+16,
      "budget_used_percent": 88.73324493510489
    },
    {
      "type": "training",
      "description": "Training step 3733",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:34:59",
      "total_flops_so_far": 8.875700916731328e+16,
      "budget_used_percent": 88.75700916731329
    },
    {
      "type": "training",
      "description": "Training step 3734",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:01",
      "total_flops_so_far": 8.878077339952166e+16,
      "budget_used_percent": 88.78077339952166
    },
    {
      "type": "training",
      "description": "Training step 3735",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:02",
      "total_flops_so_far": 8.880453763173005e+16,
      "budget_used_percent": 88.80453763173006
    },
    {
      "type": "training",
      "description": "Training step 3736",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:03",
      "total_flops_so_far": 8.882830186393843e+16,
      "budget_used_percent": 88.82830186393844
    },
    {
      "type": "training",
      "description": "Training step 3737",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:06",
      "total_flops_so_far": 8.885206609614682e+16,
      "budget_used_percent": 88.85206609614681
    },
    {
      "type": "training",
      "description": "Training step 3738",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:07",
      "total_flops_so_far": 8.88758303283552e+16,
      "budget_used_percent": 88.87583032835519
    },
    {
      "type": "training",
      "description": "Training step 3739",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:09",
      "total_flops_so_far": 8.889959456056358e+16,
      "budget_used_percent": 88.89959456056359
    },
    {
      "type": "training",
      "description": "Training step 3740",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:10",
      "total_flops_so_far": 8.892335879277197e+16,
      "budget_used_percent": 88.92335879277196
    },
    {
      "type": "training",
      "description": "Training step 3741",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:11",
      "total_flops_so_far": 8.894712302498035e+16,
      "budget_used_percent": 88.94712302498036
    },
    {
      "type": "training",
      "description": "Training step 3742",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:13",
      "total_flops_so_far": 8.897088725718874e+16,
      "budget_used_percent": 88.97088725718874
    },
    {
      "type": "training",
      "description": "Training step 3743",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:14",
      "total_flops_so_far": 8.899465148939712e+16,
      "budget_used_percent": 88.99465148939713
    },
    {
      "type": "training",
      "description": "Training step 3744",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:16",
      "total_flops_so_far": 8.90184157216055e+16,
      "budget_used_percent": 89.0184157216055
    },
    {
      "type": "training",
      "description": "Training step 3745",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:17",
      "total_flops_so_far": 8.904217995381389e+16,
      "budget_used_percent": 89.04217995381389
    },
    {
      "type": "training",
      "description": "Training step 3746",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:18",
      "total_flops_so_far": 8.906594418602227e+16,
      "budget_used_percent": 89.06594418602228
    },
    {
      "type": "training",
      "description": "Training step 3747",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:20",
      "total_flops_so_far": 8.908970841823066e+16,
      "budget_used_percent": 89.08970841823066
    },
    {
      "type": "training",
      "description": "Training step 3748",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:21",
      "total_flops_so_far": 8.911347265043904e+16,
      "budget_used_percent": 89.11347265043904
    },
    {
      "type": "training",
      "description": "Training step 3749",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:22",
      "total_flops_so_far": 8.913723688264742e+16,
      "budget_used_percent": 89.13723688264741
    },
    {
      "type": "training",
      "description": "Training step 3750",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:24",
      "total_flops_so_far": 8.91610011148558e+16,
      "budget_used_percent": 89.16100111485581
    },
    {
      "type": "training",
      "description": "Training step 3751",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:25",
      "total_flops_so_far": 8.91847653470642e+16,
      "budget_used_percent": 89.18476534706419
    },
    {
      "type": "training",
      "description": "Training step 3752",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:26",
      "total_flops_so_far": 8.920852957927258e+16,
      "budget_used_percent": 89.20852957927258
    },
    {
      "type": "training",
      "description": "Training step 3753",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:28",
      "total_flops_so_far": 8.923229381148096e+16,
      "budget_used_percent": 89.23229381148096
    },
    {
      "type": "training",
      "description": "Training step 3754",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:29",
      "total_flops_so_far": 8.925605804368934e+16,
      "budget_used_percent": 89.25605804368935
    },
    {
      "type": "training",
      "description": "Training step 3755",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:31",
      "total_flops_so_far": 8.927982227589773e+16,
      "budget_used_percent": 89.27982227589773
    },
    {
      "type": "training",
      "description": "Training step 3756",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:32",
      "total_flops_so_far": 8.930358650810611e+16,
      "budget_used_percent": 89.30358650810611
    },
    {
      "type": "training",
      "description": "Training step 3757",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:33",
      "total_flops_so_far": 8.93273507403145e+16,
      "budget_used_percent": 89.32735074031449
    },
    {
      "type": "training",
      "description": "Training step 3758",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:35",
      "total_flops_so_far": 8.935111497252288e+16,
      "budget_used_percent": 89.35111497252288
    },
    {
      "type": "training",
      "description": "Training step 3759",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:36",
      "total_flops_so_far": 8.937487920473126e+16,
      "budget_used_percent": 89.37487920473126
    },
    {
      "type": "training",
      "description": "Training step 3760",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:37",
      "total_flops_so_far": 8.939864343693965e+16,
      "budget_used_percent": 89.39864343693965
    },
    {
      "type": "training",
      "description": "Training step 3761",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:39",
      "total_flops_so_far": 8.942240766914803e+16,
      "budget_used_percent": 89.42240766914803
    },
    {
      "type": "training",
      "description": "Training step 3762",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:40",
      "total_flops_so_far": 8.944617190135642e+16,
      "budget_used_percent": 89.44617190135642
    },
    {
      "type": "training",
      "description": "Training step 3763",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:41",
      "total_flops_so_far": 8.94699361335648e+16,
      "budget_used_percent": 89.4699361335648
    },
    {
      "type": "training",
      "description": "Training step 3764",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:43",
      "total_flops_so_far": 8.949370036577318e+16,
      "budget_used_percent": 89.4937003657732
    },
    {
      "type": "training",
      "description": "Training step 3765",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:44",
      "total_flops_so_far": 8.951746459798157e+16,
      "budget_used_percent": 89.51746459798157
    },
    {
      "type": "training",
      "description": "Training step 3766",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:46",
      "total_flops_so_far": 8.954122883018995e+16,
      "budget_used_percent": 89.54122883018995
    },
    {
      "type": "training",
      "description": "Training step 3767",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:47",
      "total_flops_so_far": 8.956499306239834e+16,
      "budget_used_percent": 89.56499306239833
    },
    {
      "type": "training",
      "description": "Training step 3768",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:48",
      "total_flops_so_far": 8.958875729460672e+16,
      "budget_used_percent": 89.58875729460672
    },
    {
      "type": "training",
      "description": "Training step 3769",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:50",
      "total_flops_so_far": 8.96125215268151e+16,
      "budget_used_percent": 89.6125215268151
    },
    {
      "type": "training",
      "description": "Training step 3770",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:51",
      "total_flops_so_far": 8.963628575902349e+16,
      "budget_used_percent": 89.6362857590235
    },
    {
      "type": "training",
      "description": "Training step 3771",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:52",
      "total_flops_so_far": 8.966004999123187e+16,
      "budget_used_percent": 89.66004999123187
    },
    {
      "type": "training",
      "description": "Training step 3772",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:54",
      "total_flops_so_far": 8.968381422344026e+16,
      "budget_used_percent": 89.68381422344027
    },
    {
      "type": "training",
      "description": "Training step 3773",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:55",
      "total_flops_so_far": 8.970757845564864e+16,
      "budget_used_percent": 89.70757845564864
    },
    {
      "type": "training",
      "description": "Training step 3774",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:56",
      "total_flops_so_far": 8.973134268785702e+16,
      "budget_used_percent": 89.73134268785702
    },
    {
      "type": "training",
      "description": "Training step 3775",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:58",
      "total_flops_so_far": 8.97551069200654e+16,
      "budget_used_percent": 89.7551069200654
    },
    {
      "type": "training",
      "description": "Training step 3776",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:35:59",
      "total_flops_so_far": 8.97788711522738e+16,
      "budget_used_percent": 89.77887115227378
    },
    {
      "type": "training",
      "description": "Training step 3777",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:36:01",
      "total_flops_so_far": 8.980263538448218e+16,
      "budget_used_percent": 89.80263538448217
    },
    {
      "type": "training",
      "description": "Training step 3778",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:36:02",
      "total_flops_so_far": 8.982639961669056e+16,
      "budget_used_percent": 89.82639961669055
    },
    {
      "type": "training",
      "description": "Training step 3779",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:36:03",
      "total_flops_so_far": 8.985016384889894e+16,
      "budget_used_percent": 89.85016384889894
    },
    {
      "type": "training",
      "description": "Training step 3780",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:36:05",
      "total_flops_so_far": 8.987392808110733e+16,
      "budget_used_percent": 89.87392808110732
    },
    {
      "type": "training",
      "description": "Training step 3781",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:36:06",
      "total_flops_so_far": 8.989769231331571e+16,
      "budget_used_percent": 89.89769231331572
    },
    {
      "type": "training",
      "description": "Training step 3782",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:36:08",
      "total_flops_so_far": 8.99214565455241e+16,
      "budget_used_percent": 89.9214565455241
    },
    {
      "type": "training",
      "description": "Training step 3783",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:36:09",
      "total_flops_so_far": 8.994522077773248e+16,
      "budget_used_percent": 89.94522077773249
    },
    {
      "type": "training",
      "description": "Training step 3784",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:36:10",
      "total_flops_so_far": 8.996898500994086e+16,
      "budget_used_percent": 89.96898500994087
    },
    {
      "type": "training",
      "description": "Training step 3785",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:36:12",
      "total_flops_so_far": 8.999274924214925e+16,
      "budget_used_percent": 89.99274924214924
    },
    {
      "type": "training",
      "description": "Training step 3786",
      "seq_len": 512,
      "batch_size": 16,
      "forward_flops": 7921410736128.0,
      "backward_flops": 15842821472256.0,
      "flops": 23764232208384.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:36:13",
      "total_flops_so_far": 9.001651347435763e+16,
      "budget_used_percent": 90.01651347435762
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:36:19",
      "total_flops_so_far": 9.001722410349549e+16,
      "budget_used_percent": 90.01722410349548
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 714333709232.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:36:24",
      "total_flops_so_far": 9.001793843720472e+16,
      "budget_used_percent": 90.01793843720472
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:36:29",
      "total_flops_so_far": 9.001865091826806e+16,
      "budget_used_percent": 90.01865091826807
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:36:35",
      "total_flops_so_far": 9.001936154740592e+16,
      "budget_used_percent": 90.01936154740592
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 713407296244.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:36:40",
      "total_flops_so_far": 9.002007495470216e+16,
      "budget_used_percent": 90.02007495470215
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 5",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:36:45",
      "total_flops_so_far": 9.002078558384002e+16,
      "budget_used_percent": 90.02078558384001
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 6",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:36:50",
      "total_flops_so_far": 9.002149806490336e+16,
      "budget_used_percent": 90.02149806490335
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 7",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:36:55",
      "total_flops_so_far": 9.002221054596672e+16,
      "budget_used_percent": 90.02221054596671
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 8",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:37:00",
      "total_flops_so_far": 9.002292302703008e+16,
      "budget_used_percent": 90.02292302703007
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 9",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-20 08:37:05",
      "total_flops_so_far": 9.002363550809344e+16,
      "budget_used_percent": 90.02363550809343
    }
  ],
  "total_flops": 9.002363550809344e+16,
  "budget_used_percent": 90.02363550809343
}