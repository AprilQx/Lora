{
  "experiment_name": "default_experiment",
  "model_config": {
    "hidden_size": 896,
    "num_attention_heads": 14,
    "num_hidden_layers": 24,
    "intermediate_size": 4864,
    "head_dim": 64,
    "vocab_size": 151936,
    "lora_r": 8,
    "lora_target_modules": [
      "q_proj",
      "v_proj"
    ]
  },
  "max_budget": 1e+17,
  "start_time": "2025-03-16 22:04:00",
  "operations": [
    {
      "type": "training",
      "description": "Training step 0",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:15",
      "total_flops_so_far": 11882116104192.0,
      "budget_used_percent": 0.011882116104192
    },
    {
      "type": "training",
      "description": "Training step 1",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:17",
      "total_flops_so_far": 23764232208384.0,
      "budget_used_percent": 0.023764232208384
    },
    {
      "type": "training",
      "description": "Training step 2",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:17",
      "total_flops_so_far": 35646348312576.0,
      "budget_used_percent": 0.035646348312576
    },
    {
      "type": "training",
      "description": "Training step 3",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:18",
      "total_flops_so_far": 47528464416768.0,
      "budget_used_percent": 0.047528464416768
    },
    {
      "type": "training",
      "description": "Training step 4",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:18",
      "total_flops_so_far": 59410580520960.0,
      "budget_used_percent": 0.05941058052096
    },
    {
      "type": "training",
      "description": "Training step 5",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:19",
      "total_flops_so_far": 71292696625152.0,
      "budget_used_percent": 0.071292696625152
    },
    {
      "type": "training",
      "description": "Training step 6",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:20",
      "total_flops_so_far": 83174812729344.0,
      "budget_used_percent": 0.083174812729344
    },
    {
      "type": "training",
      "description": "Training step 7",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:20",
      "total_flops_so_far": 95056928833536.0,
      "budget_used_percent": 0.095056928833536
    },
    {
      "type": "training",
      "description": "Training step 8",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:21",
      "total_flops_so_far": 106939044937728.0,
      "budget_used_percent": 0.10693904493772802
    },
    {
      "type": "training",
      "description": "Training step 9",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:21",
      "total_flops_so_far": 118821161041920.0,
      "budget_used_percent": 0.11882116104192
    },
    {
      "type": "training",
      "description": "Training step 10",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:22",
      "total_flops_so_far": 130703277146112.0,
      "budget_used_percent": 0.13070327714611202
    },
    {
      "type": "training",
      "description": "Training step 11",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:22",
      "total_flops_so_far": 142585393250304.0,
      "budget_used_percent": 0.142585393250304
    },
    {
      "type": "training",
      "description": "Training step 12",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:23",
      "total_flops_so_far": 154467509354496.0,
      "budget_used_percent": 0.15446750935449602
    },
    {
      "type": "training",
      "description": "Training step 13",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:24",
      "total_flops_so_far": 166349625458688.0,
      "budget_used_percent": 0.166349625458688
    },
    {
      "type": "training",
      "description": "Training step 14",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:24",
      "total_flops_so_far": 178231741562880.0,
      "budget_used_percent": 0.17823174156288002
    },
    {
      "type": "training",
      "description": "Training step 15",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:25",
      "total_flops_so_far": 190113857667072.0,
      "budget_used_percent": 0.190113857667072
    },
    {
      "type": "training",
      "description": "Training step 16",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:25",
      "total_flops_so_far": 201995973771264.0,
      "budget_used_percent": 0.20199597377126402
    },
    {
      "type": "training",
      "description": "Training step 17",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:26",
      "total_flops_so_far": 213878089875456.0,
      "budget_used_percent": 0.21387808987545603
    },
    {
      "type": "training",
      "description": "Training step 18",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:26",
      "total_flops_so_far": 225760205979648.0,
      "budget_used_percent": 0.225760205979648
    },
    {
      "type": "training",
      "description": "Training step 19",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:27",
      "total_flops_so_far": 237642322083840.0,
      "budget_used_percent": 0.23764232208384
    },
    {
      "type": "training",
      "description": "Training step 20",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:28",
      "total_flops_so_far": 249524438188032.0,
      "budget_used_percent": 0.24952443818803202
    },
    {
      "type": "training",
      "description": "Training step 21",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:28",
      "total_flops_so_far": 261406554292224.0,
      "budget_used_percent": 0.26140655429222404
    },
    {
      "type": "training",
      "description": "Training step 22",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:29",
      "total_flops_so_far": 273288670396416.0,
      "budget_used_percent": 0.27328867039641597
    },
    {
      "type": "training",
      "description": "Training step 23",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:29",
      "total_flops_so_far": 285170786500608.0,
      "budget_used_percent": 0.285170786500608
    },
    {
      "type": "training",
      "description": "Training step 24",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:30",
      "total_flops_so_far": 297052902604800.0,
      "budget_used_percent": 0.2970529026048
    },
    {
      "type": "training",
      "description": "Training step 25",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:30",
      "total_flops_so_far": 308935018708992.0,
      "budget_used_percent": 0.30893501870899204
    },
    {
      "type": "training",
      "description": "Training step 26",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:31",
      "total_flops_so_far": 320817134813184.0,
      "budget_used_percent": 0.32081713481318397
    },
    {
      "type": "training",
      "description": "Training step 27",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:32",
      "total_flops_so_far": 332699250917376.0,
      "budget_used_percent": 0.332699250917376
    },
    {
      "type": "training",
      "description": "Training step 28",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:32",
      "total_flops_so_far": 344581367021568.0,
      "budget_used_percent": 0.344581367021568
    },
    {
      "type": "training",
      "description": "Training step 29",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:33",
      "total_flops_so_far": 356463483125760.0,
      "budget_used_percent": 0.35646348312576004
    },
    {
      "type": "training",
      "description": "Training step 30",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:33",
      "total_flops_so_far": 368345599229952.0,
      "budget_used_percent": 0.36834559922995197
    },
    {
      "type": "training",
      "description": "Training step 31",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:34",
      "total_flops_so_far": 380227715334144.0,
      "budget_used_percent": 0.380227715334144
    },
    {
      "type": "training",
      "description": "Training step 32",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:34",
      "total_flops_so_far": 392109831438336.0,
      "budget_used_percent": 0.392109831438336
    },
    {
      "type": "training",
      "description": "Training step 33",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:35",
      "total_flops_so_far": 403991947542528.0,
      "budget_used_percent": 0.40399194754252804
    },
    {
      "type": "training",
      "description": "Training step 34",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:36",
      "total_flops_so_far": 415874063646720.0,
      "budget_used_percent": 0.41587406364672
    },
    {
      "type": "training",
      "description": "Training step 35",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:36",
      "total_flops_so_far": 427756179750912.0,
      "budget_used_percent": 0.42775617975091207
    },
    {
      "type": "training",
      "description": "Training step 36",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:37",
      "total_flops_so_far": 439638295855104.0,
      "budget_used_percent": 0.43963829585510394
    },
    {
      "type": "training",
      "description": "Training step 37",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:37",
      "total_flops_so_far": 451520411959296.0,
      "budget_used_percent": 0.451520411959296
    },
    {
      "type": "training",
      "description": "Training step 38",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:38",
      "total_flops_so_far": 463402528063488.0,
      "budget_used_percent": 0.463402528063488
    },
    {
      "type": "training",
      "description": "Training step 39",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:38",
      "total_flops_so_far": 475284644167680.0,
      "budget_used_percent": 0.47528464416768
    },
    {
      "type": "training",
      "description": "Training step 40",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:39",
      "total_flops_so_far": 487166760271872.0,
      "budget_used_percent": 0.487166760271872
    },
    {
      "type": "training",
      "description": "Training step 41",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:40",
      "total_flops_so_far": 499048876376064.0,
      "budget_used_percent": 0.49904887637606404
    },
    {
      "type": "training",
      "description": "Training step 42",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:40",
      "total_flops_so_far": 510930992480256.0,
      "budget_used_percent": 0.5109309924802561
    },
    {
      "type": "training",
      "description": "Training step 43",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:41",
      "total_flops_so_far": 522813108584448.0,
      "budget_used_percent": 0.5228131085844481
    },
    {
      "type": "training",
      "description": "Training step 44",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:41",
      "total_flops_so_far": 534695224688640.0,
      "budget_used_percent": 0.53469522468864
    },
    {
      "type": "training",
      "description": "Training step 45",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:42",
      "total_flops_so_far": 546577340792832.0,
      "budget_used_percent": 0.5465773407928319
    },
    {
      "type": "training",
      "description": "Training step 46",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:43",
      "total_flops_so_far": 558459456897024.0,
      "budget_used_percent": 0.558459456897024
    },
    {
      "type": "training",
      "description": "Training step 47",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:43",
      "total_flops_so_far": 570341573001216.0,
      "budget_used_percent": 0.570341573001216
    },
    {
      "type": "training",
      "description": "Training step 48",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:44",
      "total_flops_so_far": 582223689105408.0,
      "budget_used_percent": 0.582223689105408
    },
    {
      "type": "training",
      "description": "Training step 49",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:44",
      "total_flops_so_far": 594105805209600.0,
      "budget_used_percent": 0.5941058052096
    },
    {
      "type": "training",
      "description": "Training step 50",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:45",
      "total_flops_so_far": 605987921313792.0,
      "budget_used_percent": 0.605987921313792
    },
    {
      "type": "training",
      "description": "Training step 51",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:45",
      "total_flops_so_far": 617870037417984.0,
      "budget_used_percent": 0.6178700374179841
    },
    {
      "type": "training",
      "description": "Training step 52",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:46",
      "total_flops_so_far": 629752153522176.0,
      "budget_used_percent": 0.629752153522176
    },
    {
      "type": "training",
      "description": "Training step 53",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:47",
      "total_flops_so_far": 641634269626368.0,
      "budget_used_percent": 0.6416342696263679
    },
    {
      "type": "training",
      "description": "Training step 54",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:47",
      "total_flops_so_far": 653516385730560.0,
      "budget_used_percent": 0.65351638573056
    },
    {
      "type": "training",
      "description": "Training step 55",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:48",
      "total_flops_so_far": 665398501834752.0,
      "budget_used_percent": 0.665398501834752
    },
    {
      "type": "training",
      "description": "Training step 56",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:48",
      "total_flops_so_far": 677280617938944.0,
      "budget_used_percent": 0.677280617938944
    },
    {
      "type": "training",
      "description": "Training step 57",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:49",
      "total_flops_so_far": 689162734043136.0,
      "budget_used_percent": 0.689162734043136
    },
    {
      "type": "training",
      "description": "Training step 58",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:49",
      "total_flops_so_far": 701044850147328.0,
      "budget_used_percent": 0.701044850147328
    },
    {
      "type": "training",
      "description": "Training step 59",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:50",
      "total_flops_so_far": 712926966251520.0,
      "budget_used_percent": 0.7129269662515201
    },
    {
      "type": "training",
      "description": "Training step 60",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:51",
      "total_flops_so_far": 724809082355712.0,
      "budget_used_percent": 0.724809082355712
    },
    {
      "type": "training",
      "description": "Training step 61",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:51",
      "total_flops_so_far": 736691198459904.0,
      "budget_used_percent": 0.7366911984599039
    },
    {
      "type": "training",
      "description": "Training step 62",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:52",
      "total_flops_so_far": 748573314564096.0,
      "budget_used_percent": 0.748573314564096
    },
    {
      "type": "training",
      "description": "Training step 63",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:52",
      "total_flops_so_far": 760455430668288.0,
      "budget_used_percent": 0.760455430668288
    },
    {
      "type": "training",
      "description": "Training step 64",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:53",
      "total_flops_so_far": 772337546772480.0,
      "budget_used_percent": 0.77233754677248
    },
    {
      "type": "training",
      "description": "Training step 65",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:53",
      "total_flops_so_far": 784219662876672.0,
      "budget_used_percent": 0.784219662876672
    },
    {
      "type": "training",
      "description": "Training step 66",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:54",
      "total_flops_so_far": 796101778980864.0,
      "budget_used_percent": 0.796101778980864
    },
    {
      "type": "training",
      "description": "Training step 67",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:55",
      "total_flops_so_far": 807983895085056.0,
      "budget_used_percent": 0.8079838950850561
    },
    {
      "type": "training",
      "description": "Training step 68",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:55",
      "total_flops_so_far": 819866011189248.0,
      "budget_used_percent": 0.819866011189248
    },
    {
      "type": "training",
      "description": "Training step 69",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:56",
      "total_flops_so_far": 831748127293440.0,
      "budget_used_percent": 0.83174812729344
    },
    {
      "type": "training",
      "description": "Training step 70",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:56",
      "total_flops_so_far": 843630243397632.0,
      "budget_used_percent": 0.8436302433976319
    },
    {
      "type": "training",
      "description": "Training step 71",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:57",
      "total_flops_so_far": 855512359501824.0,
      "budget_used_percent": 0.8555123595018241
    },
    {
      "type": "training",
      "description": "Training step 72",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:57",
      "total_flops_so_far": 867394475606016.0,
      "budget_used_percent": 0.867394475606016
    },
    {
      "type": "training",
      "description": "Training step 73",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:58",
      "total_flops_so_far": 879276591710208.0,
      "budget_used_percent": 0.8792765917102079
    },
    {
      "type": "training",
      "description": "Training step 74",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:59",
      "total_flops_so_far": 891158707814400.0,
      "budget_used_percent": 0.8911587078144
    },
    {
      "type": "training",
      "description": "Training step 75",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:04:59",
      "total_flops_so_far": 903040823918592.0,
      "budget_used_percent": 0.903040823918592
    },
    {
      "type": "training",
      "description": "Training step 76",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:00",
      "total_flops_so_far": 914922940022784.0,
      "budget_used_percent": 0.9149229400227841
    },
    {
      "type": "training",
      "description": "Training step 77",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:00",
      "total_flops_so_far": 926805056126976.0,
      "budget_used_percent": 0.926805056126976
    },
    {
      "type": "training",
      "description": "Training step 78",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:01",
      "total_flops_so_far": 938687172231168.0,
      "budget_used_percent": 0.938687172231168
    },
    {
      "type": "training",
      "description": "Training step 79",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:01",
      "total_flops_so_far": 950569288335360.0,
      "budget_used_percent": 0.95056928833536
    },
    {
      "type": "training",
      "description": "Training step 80",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:02",
      "total_flops_so_far": 962451404439552.0,
      "budget_used_percent": 0.9624514044395519
    },
    {
      "type": "training",
      "description": "Training step 81",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:03",
      "total_flops_so_far": 974333520543744.0,
      "budget_used_percent": 0.974333520543744
    },
    {
      "type": "training",
      "description": "Training step 82",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:03",
      "total_flops_so_far": 986215636647936.0,
      "budget_used_percent": 0.986215636647936
    },
    {
      "type": "training",
      "description": "Training step 83",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:04",
      "total_flops_so_far": 998097752752128.0,
      "budget_used_percent": 0.9980977527521281
    },
    {
      "type": "training",
      "description": "Training step 84",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:04",
      "total_flops_so_far": 1009979868856320.0,
      "budget_used_percent": 1.00997986885632
    },
    {
      "type": "training",
      "description": "Training step 85",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:05",
      "total_flops_so_far": 1021861984960512.0,
      "budget_used_percent": 1.0218619849605122
    },
    {
      "type": "training",
      "description": "Training step 86",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:06",
      "total_flops_so_far": 1033744101064704.0,
      "budget_used_percent": 1.033744101064704
    },
    {
      "type": "training",
      "description": "Training step 87",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:06",
      "total_flops_so_far": 1045626217168896.0,
      "budget_used_percent": 1.0456262171688961
    },
    {
      "type": "training",
      "description": "Training step 88",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:07",
      "total_flops_so_far": 1057508333273088.0,
      "budget_used_percent": 1.057508333273088
    },
    {
      "type": "training",
      "description": "Training step 89",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:07",
      "total_flops_so_far": 1069390449377280.0,
      "budget_used_percent": 1.06939044937728
    },
    {
      "type": "training",
      "description": "Training step 90",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:08",
      "total_flops_so_far": 1081272565481472.0,
      "budget_used_percent": 1.081272565481472
    },
    {
      "type": "training",
      "description": "Training step 91",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:08",
      "total_flops_so_far": 1093154681585664.0,
      "budget_used_percent": 1.0931546815856639
    },
    {
      "type": "training",
      "description": "Training step 92",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:09",
      "total_flops_so_far": 1105036797689856.0,
      "budget_used_percent": 1.105036797689856
    },
    {
      "type": "training",
      "description": "Training step 93",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:10",
      "total_flops_so_far": 1116918913794048.0,
      "budget_used_percent": 1.116918913794048
    },
    {
      "type": "training",
      "description": "Training step 94",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:10",
      "total_flops_so_far": 1128801029898240.0,
      "budget_used_percent": 1.12880102989824
    },
    {
      "type": "training",
      "description": "Training step 95",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:11",
      "total_flops_so_far": 1140683146002432.0,
      "budget_used_percent": 1.140683146002432
    },
    {
      "type": "training",
      "description": "Training step 96",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:11",
      "total_flops_so_far": 1152565262106624.0,
      "budget_used_percent": 1.152565262106624
    },
    {
      "type": "training",
      "description": "Training step 97",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:12",
      "total_flops_so_far": 1164447378210816.0,
      "budget_used_percent": 1.164447378210816
    },
    {
      "type": "training",
      "description": "Training step 98",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:12",
      "total_flops_so_far": 1176329494315008.0,
      "budget_used_percent": 1.176329494315008
    },
    {
      "type": "training",
      "description": "Training step 99",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:13",
      "total_flops_so_far": 1188211610419200.0,
      "budget_used_percent": 1.1882116104192
    },
    {
      "type": "training",
      "description": "Training step 100",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:14",
      "total_flops_so_far": 1200093726523392.0,
      "budget_used_percent": 1.200093726523392
    },
    {
      "type": "training",
      "description": "Training step 101",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:14",
      "total_flops_so_far": 1211975842627584.0,
      "budget_used_percent": 1.211975842627584
    },
    {
      "type": "training",
      "description": "Training step 102",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:15",
      "total_flops_so_far": 1223857958731776.0,
      "budget_used_percent": 1.223857958731776
    },
    {
      "type": "training",
      "description": "Training step 103",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:15",
      "total_flops_so_far": 1235740074835968.0,
      "budget_used_percent": 1.2357400748359681
    },
    {
      "type": "training",
      "description": "Training step 104",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:16",
      "total_flops_so_far": 1247622190940160.0,
      "budget_used_percent": 1.24762219094016
    },
    {
      "type": "training",
      "description": "Training step 105",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:16",
      "total_flops_so_far": 1259504307044352.0,
      "budget_used_percent": 1.259504307044352
    },
    {
      "type": "training",
      "description": "Training step 106",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:17",
      "total_flops_so_far": 1271386423148544.0,
      "budget_used_percent": 1.271386423148544
    },
    {
      "type": "training",
      "description": "Training step 107",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:18",
      "total_flops_so_far": 1283268539252736.0,
      "budget_used_percent": 1.2832685392527359
    },
    {
      "type": "training",
      "description": "Training step 108",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:18",
      "total_flops_so_far": 1295150655356928.0,
      "budget_used_percent": 1.295150655356928
    },
    {
      "type": "training",
      "description": "Training step 109",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:19",
      "total_flops_so_far": 1307032771461120.0,
      "budget_used_percent": 1.30703277146112
    },
    {
      "type": "training",
      "description": "Training step 110",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:19",
      "total_flops_so_far": 1318914887565312.0,
      "budget_used_percent": 1.318914887565312
    },
    {
      "type": "training",
      "description": "Training step 111",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:20",
      "total_flops_so_far": 1330797003669504.0,
      "budget_used_percent": 1.330797003669504
    },
    {
      "type": "training",
      "description": "Training step 112",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:20",
      "total_flops_so_far": 1342679119773696.0,
      "budget_used_percent": 1.3426791197736958
    },
    {
      "type": "training",
      "description": "Training step 113",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:21",
      "total_flops_so_far": 1354561235877888.0,
      "budget_used_percent": 1.354561235877888
    },
    {
      "type": "training",
      "description": "Training step 114",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:22",
      "total_flops_so_far": 1366443351982080.0,
      "budget_used_percent": 1.36644335198208
    },
    {
      "type": "training",
      "description": "Training step 115",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:22",
      "total_flops_so_far": 1378325468086272.0,
      "budget_used_percent": 1.378325468086272
    },
    {
      "type": "training",
      "description": "Training step 116",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:23",
      "total_flops_so_far": 1390207584190464.0,
      "budget_used_percent": 1.390207584190464
    },
    {
      "type": "training",
      "description": "Training step 117",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:23",
      "total_flops_so_far": 1402089700294656.0,
      "budget_used_percent": 1.402089700294656
    },
    {
      "type": "training",
      "description": "Training step 118",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:24",
      "total_flops_so_far": 1413971816398848.0,
      "budget_used_percent": 1.413971816398848
    },
    {
      "type": "training",
      "description": "Training step 119",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:25",
      "total_flops_so_far": 1425853932503040.0,
      "budget_used_percent": 1.4258539325030402
    },
    {
      "type": "training",
      "description": "Training step 120",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:25",
      "total_flops_so_far": 1437736048607232.0,
      "budget_used_percent": 1.437736048607232
    },
    {
      "type": "training",
      "description": "Training step 121",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:26",
      "total_flops_so_far": 1449618164711424.0,
      "budget_used_percent": 1.449618164711424
    },
    {
      "type": "training",
      "description": "Training step 122",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:26",
      "total_flops_so_far": 1461500280815616.0,
      "budget_used_percent": 1.4615002808156161
    },
    {
      "type": "training",
      "description": "Training step 123",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:27",
      "total_flops_so_far": 1473382396919808.0,
      "budget_used_percent": 1.4733823969198079
    },
    {
      "type": "training",
      "description": "Training step 124",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:27",
      "total_flops_so_far": 1485264513024000.0,
      "budget_used_percent": 1.485264513024
    },
    {
      "type": "training",
      "description": "Training step 125",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:28",
      "total_flops_so_far": 1497146629128192.0,
      "budget_used_percent": 1.497146629128192
    },
    {
      "type": "training",
      "description": "Training step 126",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:29",
      "total_flops_so_far": 1509028745232384.0,
      "budget_used_percent": 1.509028745232384
    },
    {
      "type": "training",
      "description": "Training step 127",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:29",
      "total_flops_so_far": 1520910861336576.0,
      "budget_used_percent": 1.520910861336576
    },
    {
      "type": "training",
      "description": "Training step 128",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:30",
      "total_flops_so_far": 1532792977440768.0,
      "budget_used_percent": 1.5327929774407678
    },
    {
      "type": "training",
      "description": "Training step 129",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:30",
      "total_flops_so_far": 1544675093544960.0,
      "budget_used_percent": 1.54467509354496
    },
    {
      "type": "training",
      "description": "Training step 130",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:31",
      "total_flops_so_far": 1556557209649152.0,
      "budget_used_percent": 1.556557209649152
    },
    {
      "type": "training",
      "description": "Training step 131",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:31",
      "total_flops_so_far": 1568439325753344.0,
      "budget_used_percent": 1.568439325753344
    },
    {
      "type": "training",
      "description": "Training step 132",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:32",
      "total_flops_so_far": 1580321441857536.0,
      "budget_used_percent": 1.5803214418575362
    },
    {
      "type": "training",
      "description": "Training step 133",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:33",
      "total_flops_so_far": 1592203557961728.0,
      "budget_used_percent": 1.592203557961728
    },
    {
      "type": "training",
      "description": "Training step 134",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:33",
      "total_flops_so_far": 1604085674065920.0,
      "budget_used_percent": 1.60408567406592
    },
    {
      "type": "training",
      "description": "Training step 135",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:34",
      "total_flops_so_far": 1615967790170112.0,
      "budget_used_percent": 1.6159677901701122
    },
    {
      "type": "training",
      "description": "Training step 136",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:34",
      "total_flops_so_far": 1627849906274304.0,
      "budget_used_percent": 1.627849906274304
    },
    {
      "type": "training",
      "description": "Training step 137",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:35",
      "total_flops_so_far": 1639732022378496.0,
      "budget_used_percent": 1.639732022378496
    },
    {
      "type": "training",
      "description": "Training step 138",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:36",
      "total_flops_so_far": 1651614138482688.0,
      "budget_used_percent": 1.6516141384826881
    },
    {
      "type": "training",
      "description": "Training step 139",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:36",
      "total_flops_so_far": 1663496254586880.0,
      "budget_used_percent": 1.66349625458688
    },
    {
      "type": "training",
      "description": "Training step 140",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:37",
      "total_flops_so_far": 1675378370691072.0,
      "budget_used_percent": 1.6753783706910719
    },
    {
      "type": "training",
      "description": "Training step 141",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:37",
      "total_flops_so_far": 1687260486795264.0,
      "budget_used_percent": 1.6872604867952639
    },
    {
      "type": "training",
      "description": "Training step 142",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:38",
      "total_flops_so_far": 1699142602899456.0,
      "budget_used_percent": 1.699142602899456
    },
    {
      "type": "training",
      "description": "Training step 143",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:38",
      "total_flops_so_far": 1711024719003648.0,
      "budget_used_percent": 1.7110247190036483
    },
    {
      "type": "training",
      "description": "Training step 144",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:39",
      "total_flops_so_far": 1722906835107840.0,
      "budget_used_percent": 1.7229068351078398
    },
    {
      "type": "training",
      "description": "Training step 145",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:40",
      "total_flops_so_far": 1734788951212032.0,
      "budget_used_percent": 1.734788951212032
    },
    {
      "type": "training",
      "description": "Training step 146",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:40",
      "total_flops_so_far": 1746671067316224.0,
      "budget_used_percent": 1.746671067316224
    },
    {
      "type": "training",
      "description": "Training step 147",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:41",
      "total_flops_so_far": 1758553183420416.0,
      "budget_used_percent": 1.7585531834204158
    },
    {
      "type": "training",
      "description": "Training step 148",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:41",
      "total_flops_so_far": 1770435299524608.0,
      "budget_used_percent": 1.770435299524608
    },
    {
      "type": "training",
      "description": "Training step 149",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:42",
      "total_flops_so_far": 1782317415628800.0,
      "budget_used_percent": 1.7823174156288
    },
    {
      "type": "training",
      "description": "Training step 150",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:42",
      "total_flops_so_far": 1794199531732992.0,
      "budget_used_percent": 1.7941995317329922
    },
    {
      "type": "training",
      "description": "Training step 151",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:43",
      "total_flops_so_far": 1806081647837184.0,
      "budget_used_percent": 1.806081647837184
    },
    {
      "type": "training",
      "description": "Training step 152",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:44",
      "total_flops_so_far": 1817963763941376.0,
      "budget_used_percent": 1.817963763941376
    },
    {
      "type": "training",
      "description": "Training step 153",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:44",
      "total_flops_so_far": 1829845880045568.0,
      "budget_used_percent": 1.8298458800455681
    },
    {
      "type": "training",
      "description": "Training step 154",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:45",
      "total_flops_so_far": 1841727996149760.0,
      "budget_used_percent": 1.84172799614976
    },
    {
      "type": "training",
      "description": "Training step 155",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:45",
      "total_flops_so_far": 1853610112253952.0,
      "budget_used_percent": 1.853610112253952
    },
    {
      "type": "training",
      "description": "Training step 156",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:46",
      "total_flops_so_far": 1865492228358144.0,
      "budget_used_percent": 1.865492228358144
    },
    {
      "type": "training",
      "description": "Training step 157",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:46",
      "total_flops_so_far": 1877374344462336.0,
      "budget_used_percent": 1.877374344462336
    },
    {
      "type": "training",
      "description": "Training step 158",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:47",
      "total_flops_so_far": 1889256460566528.0,
      "budget_used_percent": 1.8892564605665279
    },
    {
      "type": "training",
      "description": "Training step 159",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:48",
      "total_flops_so_far": 1901138576670720.0,
      "budget_used_percent": 1.90113857667072
    },
    {
      "type": "training",
      "description": "Training step 160",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:48",
      "total_flops_so_far": 1913020692774912.0,
      "budget_used_percent": 1.913020692774912
    },
    {
      "type": "training",
      "description": "Training step 161",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:49",
      "total_flops_so_far": 1924902808879104.0,
      "budget_used_percent": 1.9249028088791038
    },
    {
      "type": "training",
      "description": "Training step 162",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:49",
      "total_flops_so_far": 1936784924983296.0,
      "budget_used_percent": 1.936784924983296
    },
    {
      "type": "training",
      "description": "Training step 163",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:50",
      "total_flops_so_far": 1948667041087488.0,
      "budget_used_percent": 1.948667041087488
    },
    {
      "type": "training",
      "description": "Training step 164",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:51",
      "total_flops_so_far": 1960549157191680.0,
      "budget_used_percent": 1.9605491571916802
    },
    {
      "type": "training",
      "description": "Training step 165",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:51",
      "total_flops_so_far": 1972431273295872.0,
      "budget_used_percent": 1.972431273295872
    },
    {
      "type": "training",
      "description": "Training step 166",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:52",
      "total_flops_so_far": 1984313389400064.0,
      "budget_used_percent": 1.984313389400064
    },
    {
      "type": "training",
      "description": "Training step 167",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:52",
      "total_flops_so_far": 1996195505504256.0,
      "budget_used_percent": 1.9961955055042562
    },
    {
      "type": "training",
      "description": "Training step 168",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:53",
      "total_flops_so_far": 2008077621608448.0,
      "budget_used_percent": 2.008077621608448
    },
    {
      "type": "training",
      "description": "Training step 169",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:53",
      "total_flops_so_far": 2019959737712640.0,
      "budget_used_percent": 2.01995973771264
    },
    {
      "type": "training",
      "description": "Training step 170",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:54",
      "total_flops_so_far": 2031841853816832.0,
      "budget_used_percent": 2.031841853816832
    },
    {
      "type": "training",
      "description": "Training step 171",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:55",
      "total_flops_so_far": 2043723969921024.0,
      "budget_used_percent": 2.0437239699210243
    },
    {
      "type": "training",
      "description": "Training step 172",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:55",
      "total_flops_so_far": 2055606086025216.0,
      "budget_used_percent": 2.055606086025216
    },
    {
      "type": "training",
      "description": "Training step 173",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:56",
      "total_flops_so_far": 2067488202129408.0,
      "budget_used_percent": 2.067488202129408
    },
    {
      "type": "training",
      "description": "Training step 174",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:56",
      "total_flops_so_far": 2079370318233600.0,
      "budget_used_percent": 2.0793703182336
    },
    {
      "type": "training",
      "description": "Training step 175",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:57",
      "total_flops_so_far": 2091252434337792.0,
      "budget_used_percent": 2.0912524343377923
    },
    {
      "type": "training",
      "description": "Training step 176",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:57",
      "total_flops_so_far": 2103134550441984.0,
      "budget_used_percent": 2.103134550441984
    },
    {
      "type": "training",
      "description": "Training step 177",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:58",
      "total_flops_so_far": 2115016666546176.0,
      "budget_used_percent": 2.115016666546176
    },
    {
      "type": "training",
      "description": "Training step 178",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:59",
      "total_flops_so_far": 2126898782650368.0,
      "budget_used_percent": 2.1268987826503682
    },
    {
      "type": "training",
      "description": "Training step 179",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:05:59",
      "total_flops_so_far": 2138780898754560.0,
      "budget_used_percent": 2.13878089875456
    },
    {
      "type": "training",
      "description": "Training step 180",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:00",
      "total_flops_so_far": 2150663014858752.0,
      "budget_used_percent": 2.1506630148587518
    },
    {
      "type": "training",
      "description": "Training step 181",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:00",
      "total_flops_so_far": 2162545130962944.0,
      "budget_used_percent": 2.162545130962944
    },
    {
      "type": "training",
      "description": "Training step 182",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:01",
      "total_flops_so_far": 2174427247067136.0,
      "budget_used_percent": 2.174427247067136
    },
    {
      "type": "training",
      "description": "Training step 183",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:02",
      "total_flops_so_far": 2186309363171328.0,
      "budget_used_percent": 2.1863093631713277
    },
    {
      "type": "training",
      "description": "Training step 184",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:02",
      "total_flops_so_far": 2198191479275520.0,
      "budget_used_percent": 2.19819147927552
    },
    {
      "type": "training",
      "description": "Training step 185",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:03",
      "total_flops_so_far": 2210073595379712.0,
      "budget_used_percent": 2.210073595379712
    },
    {
      "type": "training",
      "description": "Training step 186",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:03",
      "total_flops_so_far": 2221955711483904.0,
      "budget_used_percent": 2.2219557114839037
    },
    {
      "type": "training",
      "description": "Training step 187",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:04",
      "total_flops_so_far": 2233837827588096.0,
      "budget_used_percent": 2.233837827588096
    },
    {
      "type": "training",
      "description": "Training step 188",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:04",
      "total_flops_so_far": 2245719943692288.0,
      "budget_used_percent": 2.245719943692288
    },
    {
      "type": "training",
      "description": "Training step 189",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:05",
      "total_flops_so_far": 2257602059796480.0,
      "budget_used_percent": 2.25760205979648
    },
    {
      "type": "training",
      "description": "Training step 190",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:06",
      "total_flops_so_far": 2269484175900672.0,
      "budget_used_percent": 2.269484175900672
    },
    {
      "type": "training",
      "description": "Training step 191",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:06",
      "total_flops_so_far": 2281366292004864.0,
      "budget_used_percent": 2.281366292004864
    },
    {
      "type": "training",
      "description": "Training step 192",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:07",
      "total_flops_so_far": 2293248408109056.0,
      "budget_used_percent": 2.293248408109056
    },
    {
      "type": "training",
      "description": "Training step 193",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:07",
      "total_flops_so_far": 2305130524213248.0,
      "budget_used_percent": 2.305130524213248
    },
    {
      "type": "training",
      "description": "Training step 194",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:08",
      "total_flops_so_far": 2317012640317440.0,
      "budget_used_percent": 2.31701264031744
    },
    {
      "type": "training",
      "description": "Training step 195",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:08",
      "total_flops_so_far": 2328894756421632.0,
      "budget_used_percent": 2.328894756421632
    },
    {
      "type": "training",
      "description": "Training step 196",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:09",
      "total_flops_so_far": 2340776872525824.0,
      "budget_used_percent": 2.340776872525824
    },
    {
      "type": "training",
      "description": "Training step 197",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:10",
      "total_flops_so_far": 2352658988630016.0,
      "budget_used_percent": 2.352658988630016
    },
    {
      "type": "training",
      "description": "Training step 198",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:10",
      "total_flops_so_far": 2364541104734208.0,
      "budget_used_percent": 2.364541104734208
    },
    {
      "type": "training",
      "description": "Training step 199",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:11",
      "total_flops_so_far": 2376423220838400.0,
      "budget_used_percent": 2.3764232208384
    },
    {
      "type": "training",
      "description": "Training step 200",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:11",
      "total_flops_so_far": 2388305336942592.0,
      "budget_used_percent": 2.388305336942592
    },
    {
      "type": "training",
      "description": "Training step 201",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:12",
      "total_flops_so_far": 2400187453046784.0,
      "budget_used_percent": 2.400187453046784
    },
    {
      "type": "training",
      "description": "Training step 202",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:13",
      "total_flops_so_far": 2412069569150976.0,
      "budget_used_percent": 2.412069569150976
    },
    {
      "type": "training",
      "description": "Training step 203",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:13",
      "total_flops_so_far": 2423951685255168.0,
      "budget_used_percent": 2.423951685255168
    },
    {
      "type": "training",
      "description": "Training step 204",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:14",
      "total_flops_so_far": 2435833801359360.0,
      "budget_used_percent": 2.43583380135936
    },
    {
      "type": "training",
      "description": "Training step 205",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:14",
      "total_flops_so_far": 2447715917463552.0,
      "budget_used_percent": 2.447715917463552
    },
    {
      "type": "training",
      "description": "Training step 206",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:15",
      "total_flops_so_far": 2459598033567744.0,
      "budget_used_percent": 2.459598033567744
    },
    {
      "type": "training",
      "description": "Training step 207",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:15",
      "total_flops_so_far": 2471480149671936.0,
      "budget_used_percent": 2.4714801496719363
    },
    {
      "type": "training",
      "description": "Training step 208",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:16",
      "total_flops_so_far": 2483362265776128.0,
      "budget_used_percent": 2.483362265776128
    },
    {
      "type": "training",
      "description": "Training step 209",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:17",
      "total_flops_so_far": 2495244381880320.0,
      "budget_used_percent": 2.49524438188032
    },
    {
      "type": "training",
      "description": "Training step 210",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:17",
      "total_flops_so_far": 2507126497984512.0,
      "budget_used_percent": 2.5071264979845123
    },
    {
      "type": "training",
      "description": "Training step 211",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:18",
      "total_flops_so_far": 2519008614088704.0,
      "budget_used_percent": 2.519008614088704
    },
    {
      "type": "training",
      "description": "Training step 212",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:18",
      "total_flops_so_far": 2530890730192896.0,
      "budget_used_percent": 2.530890730192896
    },
    {
      "type": "training",
      "description": "Training step 213",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:19",
      "total_flops_so_far": 2542772846297088.0,
      "budget_used_percent": 2.542772846297088
    },
    {
      "type": "training",
      "description": "Training step 214",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:19",
      "total_flops_so_far": 2554654962401280.0,
      "budget_used_percent": 2.55465496240128
    },
    {
      "type": "training",
      "description": "Training step 215",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:20",
      "total_flops_so_far": 2566537078505472.0,
      "budget_used_percent": 2.5665370785054717
    },
    {
      "type": "training",
      "description": "Training step 216",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:21",
      "total_flops_so_far": 2578419194609664.0,
      "budget_used_percent": 2.578419194609664
    },
    {
      "type": "training",
      "description": "Training step 217",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:21",
      "total_flops_so_far": 2590301310713856.0,
      "budget_used_percent": 2.590301310713856
    },
    {
      "type": "training",
      "description": "Training step 218",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:22",
      "total_flops_so_far": 2602183426818048.0,
      "budget_used_percent": 2.6021834268180477
    },
    {
      "type": "training",
      "description": "Training step 219",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:22",
      "total_flops_so_far": 2614065542922240.0,
      "budget_used_percent": 2.61406554292224
    },
    {
      "type": "training",
      "description": "Training step 220",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:23",
      "total_flops_so_far": 2625947659026432.0,
      "budget_used_percent": 2.625947659026432
    },
    {
      "type": "training",
      "description": "Training step 221",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:24",
      "total_flops_so_far": 2637829775130624.0,
      "budget_used_percent": 2.637829775130624
    },
    {
      "type": "training",
      "description": "Training step 222",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:24",
      "total_flops_so_far": 2649711891234816.0,
      "budget_used_percent": 2.649711891234816
    },
    {
      "type": "training",
      "description": "Training step 223",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:25",
      "total_flops_so_far": 2661594007339008.0,
      "budget_used_percent": 2.661594007339008
    },
    {
      "type": "training",
      "description": "Training step 224",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:25",
      "total_flops_so_far": 2673476123443200.0,
      "budget_used_percent": 2.6734761234432
    },
    {
      "type": "training",
      "description": "Training step 225",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:26",
      "total_flops_so_far": 2685358239547392.0,
      "budget_used_percent": 2.6853582395473916
    },
    {
      "type": "training",
      "description": "Training step 226",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:26",
      "total_flops_so_far": 2697240355651584.0,
      "budget_used_percent": 2.697240355651584
    },
    {
      "type": "training",
      "description": "Training step 227",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:27",
      "total_flops_so_far": 2709122471755776.0,
      "budget_used_percent": 2.709122471755776
    },
    {
      "type": "training",
      "description": "Training step 228",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:28",
      "total_flops_so_far": 2721004587859968.0,
      "budget_used_percent": 2.721004587859968
    },
    {
      "type": "training",
      "description": "Training step 229",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:28",
      "total_flops_so_far": 2732886703964160.0,
      "budget_used_percent": 2.73288670396416
    },
    {
      "type": "training",
      "description": "Training step 230",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:29",
      "total_flops_so_far": 2744768820068352.0,
      "budget_used_percent": 2.744768820068352
    },
    {
      "type": "training",
      "description": "Training step 231",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:29",
      "total_flops_so_far": 2756650936172544.0,
      "budget_used_percent": 2.756650936172544
    },
    {
      "type": "training",
      "description": "Training step 232",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:30",
      "total_flops_so_far": 2768533052276736.0,
      "budget_used_percent": 2.768533052276736
    },
    {
      "type": "training",
      "description": "Training step 233",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:31",
      "total_flops_so_far": 2780415168380928.0,
      "budget_used_percent": 2.780415168380928
    },
    {
      "type": "training",
      "description": "Training step 234",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:31",
      "total_flops_so_far": 2792297284485120.0,
      "budget_used_percent": 2.79229728448512
    },
    {
      "type": "training",
      "description": "Training step 235",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:32",
      "total_flops_so_far": 2804179400589312.0,
      "budget_used_percent": 2.804179400589312
    },
    {
      "type": "training",
      "description": "Training step 236",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:32",
      "total_flops_so_far": 2816061516693504.0,
      "budget_used_percent": 2.816061516693504
    },
    {
      "type": "training",
      "description": "Training step 237",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:33",
      "total_flops_so_far": 2827943632797696.0,
      "budget_used_percent": 2.827943632797696
    },
    {
      "type": "training",
      "description": "Training step 238",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:33",
      "total_flops_so_far": 2839825748901888.0,
      "budget_used_percent": 2.839825748901888
    },
    {
      "type": "training",
      "description": "Training step 239",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:34",
      "total_flops_so_far": 2851707865006080.0,
      "budget_used_percent": 2.8517078650060803
    },
    {
      "type": "training",
      "description": "Training step 240",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:35",
      "total_flops_so_far": 2863589981110272.0,
      "budget_used_percent": 2.863589981110272
    },
    {
      "type": "training",
      "description": "Training step 241",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:35",
      "total_flops_so_far": 2875472097214464.0,
      "budget_used_percent": 2.875472097214464
    },
    {
      "type": "training",
      "description": "Training step 242",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:36",
      "total_flops_so_far": 2887354213318656.0,
      "budget_used_percent": 2.8873542133186563
    },
    {
      "type": "training",
      "description": "Training step 243",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:36",
      "total_flops_so_far": 2899236329422848.0,
      "budget_used_percent": 2.899236329422848
    },
    {
      "type": "training",
      "description": "Training step 244",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:37",
      "total_flops_so_far": 2911118445527040.0,
      "budget_used_percent": 2.91111844552704
    },
    {
      "type": "training",
      "description": "Training step 245",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:37",
      "total_flops_so_far": 2923000561631232.0,
      "budget_used_percent": 2.9230005616312322
    },
    {
      "type": "training",
      "description": "Training step 246",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:38",
      "total_flops_so_far": 2934882677735424.0,
      "budget_used_percent": 2.934882677735424
    },
    {
      "type": "training",
      "description": "Training step 247",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:39",
      "total_flops_so_far": 2946764793839616.0,
      "budget_used_percent": 2.9467647938396158
    },
    {
      "type": "training",
      "description": "Training step 248",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:39",
      "total_flops_so_far": 2958646909943808.0,
      "budget_used_percent": 2.958646909943808
    },
    {
      "type": "training",
      "description": "Training step 249",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:40",
      "total_flops_so_far": 2970529026048000.0,
      "budget_used_percent": 2.970529026048
    },
    {
      "type": "training",
      "description": "Training step 250",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:40",
      "total_flops_so_far": 2982411142152192.0,
      "budget_used_percent": 2.9824111421521917
    },
    {
      "type": "training",
      "description": "Training step 251",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:41",
      "total_flops_so_far": 2994293258256384.0,
      "budget_used_percent": 2.994293258256384
    },
    {
      "type": "training",
      "description": "Training step 252",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:42",
      "total_flops_so_far": 3006175374360576.0,
      "budget_used_percent": 3.006175374360576
    },
    {
      "type": "training",
      "description": "Training step 253",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:42",
      "total_flops_so_far": 3018057490464768.0,
      "budget_used_percent": 3.018057490464768
    },
    {
      "type": "training",
      "description": "Training step 254",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:43",
      "total_flops_so_far": 3029939606568960.0,
      "budget_used_percent": 3.02993960656896
    },
    {
      "type": "training",
      "description": "Training step 255",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:43",
      "total_flops_so_far": 3041821722673152.0,
      "budget_used_percent": 3.041821722673152
    },
    {
      "type": "training",
      "description": "Training step 256",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:44",
      "total_flops_so_far": 3053703838777344.0,
      "budget_used_percent": 3.053703838777344
    },
    {
      "type": "training",
      "description": "Training step 257",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:44",
      "total_flops_so_far": 3065585954881536.0,
      "budget_used_percent": 3.0655859548815356
    },
    {
      "type": "training",
      "description": "Training step 258",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:45",
      "total_flops_so_far": 3077468070985728.0,
      "budget_used_percent": 3.077468070985728
    },
    {
      "type": "training",
      "description": "Training step 259",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:46",
      "total_flops_so_far": 3089350187089920.0,
      "budget_used_percent": 3.08935018708992
    },
    {
      "type": "training",
      "description": "Training step 260",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:46",
      "total_flops_so_far": 3101232303194112.0,
      "budget_used_percent": 3.101232303194112
    },
    {
      "type": "training",
      "description": "Training step 261",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:47",
      "total_flops_so_far": 3113114419298304.0,
      "budget_used_percent": 3.113114419298304
    },
    {
      "type": "training",
      "description": "Training step 262",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:47",
      "total_flops_so_far": 3124996535402496.0,
      "budget_used_percent": 3.124996535402496
    },
    {
      "type": "training",
      "description": "Training step 263",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:48",
      "total_flops_so_far": 3136878651506688.0,
      "budget_used_percent": 3.136878651506688
    },
    {
      "type": "training",
      "description": "Training step 264",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:48",
      "total_flops_so_far": 3148760767610880.0,
      "budget_used_percent": 3.14876076761088
    },
    {
      "type": "training",
      "description": "Training step 265",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:49",
      "total_flops_so_far": 3160642883715072.0,
      "budget_used_percent": 3.1606428837150724
    },
    {
      "type": "training",
      "description": "Training step 266",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:49",
      "total_flops_so_far": 3172524999819264.0,
      "budget_used_percent": 3.1725249998192635
    },
    {
      "type": "training",
      "description": "Training step 267",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:50",
      "total_flops_so_far": 3184407115923456.0,
      "budget_used_percent": 3.184407115923456
    },
    {
      "type": "training",
      "description": "Training step 268",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:51",
      "total_flops_so_far": 3196289232027648.0,
      "budget_used_percent": 3.196289232027648
    },
    {
      "type": "training",
      "description": "Training step 269",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:51",
      "total_flops_so_far": 3208171348131840.0,
      "budget_used_percent": 3.20817134813184
    },
    {
      "type": "training",
      "description": "Training step 270",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:52",
      "total_flops_so_far": 3220053464236032.0,
      "budget_used_percent": 3.220053464236032
    },
    {
      "type": "training",
      "description": "Training step 271",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:52",
      "total_flops_so_far": 3231935580340224.0,
      "budget_used_percent": 3.2319355803402243
    },
    {
      "type": "training",
      "description": "Training step 272",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:53",
      "total_flops_so_far": 3243817696444416.0,
      "budget_used_percent": 3.2438176964444163
    },
    {
      "type": "training",
      "description": "Training step 273",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:53",
      "total_flops_so_far": 3255699812548608.0,
      "budget_used_percent": 3.255699812548608
    },
    {
      "type": "training",
      "description": "Training step 274",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:54",
      "total_flops_so_far": 3267581928652800.0,
      "budget_used_percent": 3.2675819286528
    },
    {
      "type": "training",
      "description": "Training step 275",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:55",
      "total_flops_so_far": 3279464044756992.0,
      "budget_used_percent": 3.279464044756992
    },
    {
      "type": "training",
      "description": "Training step 276",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:55",
      "total_flops_so_far": 3291346160861184.0,
      "budget_used_percent": 3.291346160861184
    },
    {
      "type": "training",
      "description": "Training step 277",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:56",
      "total_flops_so_far": 3303228276965376.0,
      "budget_used_percent": 3.3032282769653762
    },
    {
      "type": "training",
      "description": "Training step 278",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:56",
      "total_flops_so_far": 3315110393069568.0,
      "budget_used_percent": 3.3151103930695682
    },
    {
      "type": "training",
      "description": "Training step 279",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:57",
      "total_flops_so_far": 3326992509173760.0,
      "budget_used_percent": 3.32699250917376
    },
    {
      "type": "training",
      "description": "Training step 280",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:58",
      "total_flops_so_far": 3338874625277952.0,
      "budget_used_percent": 3.338874625277952
    },
    {
      "type": "training",
      "description": "Training step 281",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:58",
      "total_flops_so_far": 3350756741382144.0,
      "budget_used_percent": 3.3507567413821437
    },
    {
      "type": "training",
      "description": "Training step 282",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:59",
      "total_flops_so_far": 3362638857486336.0,
      "budget_used_percent": 3.3626388574863357
    },
    {
      "type": "training",
      "description": "Training step 283",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:06:59",
      "total_flops_so_far": 3374520973590528.0,
      "budget_used_percent": 3.3745209735905277
    },
    {
      "type": "training",
      "description": "Training step 284",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:00",
      "total_flops_so_far": 3386403089694720.0,
      "budget_used_percent": 3.38640308969472
    },
    {
      "type": "training",
      "description": "Training step 285",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:00",
      "total_flops_so_far": 3398285205798912.0,
      "budget_used_percent": 3.398285205798912
    },
    {
      "type": "training",
      "description": "Training step 286",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:01",
      "total_flops_so_far": 3410167321903104.0,
      "budget_used_percent": 3.410167321903104
    },
    {
      "type": "training",
      "description": "Training step 287",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:02",
      "total_flops_so_far": 3422049438007296.0,
      "budget_used_percent": 3.4220494380072966
    },
    {
      "type": "training",
      "description": "Training step 288",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:02",
      "total_flops_so_far": 3433931554111488.0,
      "budget_used_percent": 3.4339315541114876
    },
    {
      "type": "training",
      "description": "Training step 289",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:03",
      "total_flops_so_far": 3445813670215680.0,
      "budget_used_percent": 3.4458136702156796
    },
    {
      "type": "training",
      "description": "Training step 290",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:03",
      "total_flops_so_far": 3457695786319872.0,
      "budget_used_percent": 3.457695786319872
    },
    {
      "type": "training",
      "description": "Training step 291",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:04",
      "total_flops_so_far": 3469577902424064.0,
      "budget_used_percent": 3.469577902424064
    },
    {
      "type": "training",
      "description": "Training step 292",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:05",
      "total_flops_so_far": 3481460018528256.0,
      "budget_used_percent": 3.481460018528256
    },
    {
      "type": "training",
      "description": "Training step 293",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:05",
      "total_flops_so_far": 3493342134632448.0,
      "budget_used_percent": 3.493342134632448
    },
    {
      "type": "training",
      "description": "Training step 294",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:06",
      "total_flops_so_far": 3505224250736640.0,
      "budget_used_percent": 3.5052242507366405
    },
    {
      "type": "training",
      "description": "Training step 295",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:06",
      "total_flops_so_far": 3517106366840832.0,
      "budget_used_percent": 3.5171063668408316
    },
    {
      "type": "training",
      "description": "Training step 296",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:07",
      "total_flops_so_far": 3528988482945024.0,
      "budget_used_percent": 3.528988482945024
    },
    {
      "type": "training",
      "description": "Training step 297",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:07",
      "total_flops_so_far": 3540870599049216.0,
      "budget_used_percent": 3.540870599049216
    },
    {
      "type": "training",
      "description": "Training step 298",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:08",
      "total_flops_so_far": 3552752715153408.0,
      "budget_used_percent": 3.552752715153408
    },
    {
      "type": "training",
      "description": "Training step 299",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:09",
      "total_flops_so_far": 3564634831257600.0,
      "budget_used_percent": 3.5646348312576
    },
    {
      "type": "training",
      "description": "Training step 300",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:09",
      "total_flops_so_far": 3576516947361792.0,
      "budget_used_percent": 3.5765169473617924
    },
    {
      "type": "training",
      "description": "Training step 301",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:10",
      "total_flops_so_far": 3588399063465984.0,
      "budget_used_percent": 3.5883990634659844
    },
    {
      "type": "training",
      "description": "Training step 302",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:10",
      "total_flops_so_far": 3600281179570176.0,
      "budget_used_percent": 3.600281179570176
    },
    {
      "type": "training",
      "description": "Training step 303",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:11",
      "total_flops_so_far": 3612163295674368.0,
      "budget_used_percent": 3.612163295674368
    },
    {
      "type": "training",
      "description": "Training step 304",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:12",
      "total_flops_so_far": 3624045411778560.0,
      "budget_used_percent": 3.62404541177856
    },
    {
      "type": "training",
      "description": "Training step 305",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:12",
      "total_flops_so_far": 3635927527882752.0,
      "budget_used_percent": 3.635927527882752
    },
    {
      "type": "training",
      "description": "Training step 306",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:13",
      "total_flops_so_far": 3647809643986944.0,
      "budget_used_percent": 3.6478096439869443
    },
    {
      "type": "training",
      "description": "Training step 307",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:13",
      "total_flops_so_far": 3659691760091136.0,
      "budget_used_percent": 3.6596917600911363
    },
    {
      "type": "training",
      "description": "Training step 308",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:14",
      "total_flops_so_far": 3671573876195328.0,
      "budget_used_percent": 3.6715738761953283
    },
    {
      "type": "training",
      "description": "Training step 309",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:14",
      "total_flops_so_far": 3683455992299520.0,
      "budget_used_percent": 3.68345599229952
    },
    {
      "type": "training",
      "description": "Training step 310",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:15",
      "total_flops_so_far": 3695338108403712.0,
      "budget_used_percent": 3.695338108403712
    },
    {
      "type": "training",
      "description": "Training step 311",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:16",
      "total_flops_so_far": 3707220224507904.0,
      "budget_used_percent": 3.707220224507904
    },
    {
      "type": "training",
      "description": "Training step 312",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:16",
      "total_flops_so_far": 3719102340612096.0,
      "budget_used_percent": 3.719102340612096
    },
    {
      "type": "training",
      "description": "Training step 313",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:17",
      "total_flops_so_far": 3730984456716288.0,
      "budget_used_percent": 3.730984456716288
    },
    {
      "type": "training",
      "description": "Training step 314",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:17",
      "total_flops_so_far": 3742866572820480.0,
      "budget_used_percent": 3.74286657282048
    },
    {
      "type": "training",
      "description": "Training step 315",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:18",
      "total_flops_so_far": 3754748688924672.0,
      "budget_used_percent": 3.754748688924672
    },
    {
      "type": "training",
      "description": "Training step 316",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:19",
      "total_flops_so_far": 3766630805028864.0,
      "budget_used_percent": 3.7666308050288637
    },
    {
      "type": "training",
      "description": "Training step 317",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:19",
      "total_flops_so_far": 3778512921133056.0,
      "budget_used_percent": 3.7785129211330557
    },
    {
      "type": "training",
      "description": "Training step 318",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:20",
      "total_flops_so_far": 3790395037237248.0,
      "budget_used_percent": 3.7903950372372477
    },
    {
      "type": "training",
      "description": "Training step 319",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:20",
      "total_flops_so_far": 3802277153341440.0,
      "budget_used_percent": 3.80227715334144
    },
    {
      "type": "training",
      "description": "Training step 320",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:21",
      "total_flops_so_far": 3814159269445632.0,
      "budget_used_percent": 3.814159269445632
    },
    {
      "type": "training",
      "description": "Training step 321",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:21",
      "total_flops_so_far": 3826041385549824.0,
      "budget_used_percent": 3.826041385549824
    },
    {
      "type": "training",
      "description": "Training step 322",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:22",
      "total_flops_so_far": 3837923501654016.0,
      "budget_used_percent": 3.837923501654016
    },
    {
      "type": "training",
      "description": "Training step 323",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:23",
      "total_flops_so_far": 3849805617758208.0,
      "budget_used_percent": 3.8498056177582076
    },
    {
      "type": "training",
      "description": "Training step 324",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:23",
      "total_flops_so_far": 3861687733862400.0,
      "budget_used_percent": 3.8616877338623996
    },
    {
      "type": "training",
      "description": "Training step 325",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:24",
      "total_flops_so_far": 3873569849966592.0,
      "budget_used_percent": 3.873569849966592
    },
    {
      "type": "training",
      "description": "Training step 326",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:24",
      "total_flops_so_far": 3885451966070784.0,
      "budget_used_percent": 3.885451966070784
    },
    {
      "type": "training",
      "description": "Training step 327",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:25",
      "total_flops_so_far": 3897334082174976.0,
      "budget_used_percent": 3.897334082174976
    },
    {
      "type": "training",
      "description": "Training step 328",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:25",
      "total_flops_so_far": 3909216198279168.0,
      "budget_used_percent": 3.909216198279168
    },
    {
      "type": "training",
      "description": "Training step 329",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:26",
      "total_flops_so_far": 3921098314383360.0,
      "budget_used_percent": 3.9210983143833604
    },
    {
      "type": "training",
      "description": "Training step 330",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:27",
      "total_flops_so_far": 3932980430487552.0,
      "budget_used_percent": 3.9329804304875515
    },
    {
      "type": "training",
      "description": "Training step 331",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:27",
      "total_flops_so_far": 3944862546591744.0,
      "budget_used_percent": 3.944862546591744
    },
    {
      "type": "training",
      "description": "Training step 332",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:28",
      "total_flops_so_far": 3956744662695936.0,
      "budget_used_percent": 3.956744662695936
    },
    {
      "type": "training",
      "description": "Training step 333",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:28",
      "total_flops_so_far": 3968626778800128.0,
      "budget_used_percent": 3.968626778800128
    },
    {
      "type": "training",
      "description": "Training step 334",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:29",
      "total_flops_so_far": 3980508894904320.0,
      "budget_used_percent": 3.98050889490432
    },
    {
      "type": "training",
      "description": "Training step 335",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:30",
      "total_flops_so_far": 3992391011008512.0,
      "budget_used_percent": 3.9923910110085123
    },
    {
      "type": "training",
      "description": "Training step 336",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:30",
      "total_flops_so_far": 4004273127112704.0,
      "budget_used_percent": 4.004273127112704
    },
    {
      "type": "training",
      "description": "Training step 337",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:31",
      "total_flops_so_far": 4016155243216896.0,
      "budget_used_percent": 4.016155243216896
    },
    {
      "type": "training",
      "description": "Training step 338",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:31",
      "total_flops_so_far": 4028037359321088.0,
      "budget_used_percent": 4.028037359321088
    },
    {
      "type": "training",
      "description": "Training step 339",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:32",
      "total_flops_so_far": 4039919475425280.0,
      "budget_used_percent": 4.03991947542528
    },
    {
      "type": "training",
      "description": "Training step 340",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:32",
      "total_flops_so_far": 4051801591529472.0,
      "budget_used_percent": 4.051801591529472
    },
    {
      "type": "training",
      "description": "Training step 341",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:33",
      "total_flops_so_far": 4063683707633664.0,
      "budget_used_percent": 4.063683707633664
    },
    {
      "type": "training",
      "description": "Training step 342",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:34",
      "total_flops_so_far": 4075565823737856.0,
      "budget_used_percent": 4.075565823737856
    },
    {
      "type": "training",
      "description": "Training step 343",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:34",
      "total_flops_so_far": 4087447939842048.0,
      "budget_used_percent": 4.087447939842049
    },
    {
      "type": "training",
      "description": "Training step 344",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:35",
      "total_flops_so_far": 4099330055946240.0,
      "budget_used_percent": 4.099330055946241
    },
    {
      "type": "training",
      "description": "Training step 345",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:35",
      "total_flops_so_far": 4111212172050432.0,
      "budget_used_percent": 4.111212172050432
    },
    {
      "type": "training",
      "description": "Training step 346",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:36",
      "total_flops_so_far": 4123094288154624.0,
      "budget_used_percent": 4.123094288154624
    },
    {
      "type": "training",
      "description": "Training step 347",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:37",
      "total_flops_so_far": 4134976404258816.0,
      "budget_used_percent": 4.134976404258816
    },
    {
      "type": "training",
      "description": "Training step 348",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:37",
      "total_flops_so_far": 4146858520363008.0,
      "budget_used_percent": 4.146858520363008
    },
    {
      "type": "training",
      "description": "Training step 349",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:38",
      "total_flops_so_far": 4158740636467200.0,
      "budget_used_percent": 4.1587406364672
    },
    {
      "type": "training",
      "description": "Training step 350",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:38",
      "total_flops_so_far": 4170622752571392.0,
      "budget_used_percent": 4.170622752571393
    },
    {
      "type": "training",
      "description": "Training step 351",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:39",
      "total_flops_so_far": 4182504868675584.0,
      "budget_used_percent": 4.182504868675585
    },
    {
      "type": "training",
      "description": "Training step 352",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:39",
      "total_flops_so_far": 4194386984779776.0,
      "budget_used_percent": 4.194386984779776
    },
    {
      "type": "training",
      "description": "Training step 353",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:40",
      "total_flops_so_far": 4206269100883968.0,
      "budget_used_percent": 4.206269100883968
    },
    {
      "type": "training",
      "description": "Training step 354",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:41",
      "total_flops_so_far": 4218151216988160.0,
      "budget_used_percent": 4.21815121698816
    },
    {
      "type": "training",
      "description": "Training step 355",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:41",
      "total_flops_so_far": 4230033333092352.0,
      "budget_used_percent": 4.230033333092352
    },
    {
      "type": "training",
      "description": "Training step 356",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:42",
      "total_flops_so_far": 4241915449196544.0,
      "budget_used_percent": 4.2419154491965445
    },
    {
      "type": "training",
      "description": "Training step 357",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:42",
      "total_flops_so_far": 4253797565300736.0,
      "budget_used_percent": 4.2537975653007365
    },
    {
      "type": "training",
      "description": "Training step 358",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:43",
      "total_flops_so_far": 4265679681404928.0,
      "budget_used_percent": 4.2656796814049285
    },
    {
      "type": "training",
      "description": "Training step 359",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:44",
      "total_flops_so_far": 4277561797509120.0,
      "budget_used_percent": 4.27756179750912
    },
    {
      "type": "training",
      "description": "Training step 360",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:44",
      "total_flops_so_far": 4289443913613312.0,
      "budget_used_percent": 4.289443913613312
    },
    {
      "type": "training",
      "description": "Training step 361",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:45",
      "total_flops_so_far": 4301326029717504.0,
      "budget_used_percent": 4.3013260297175036
    },
    {
      "type": "training",
      "description": "Training step 362",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:45",
      "total_flops_so_far": 4313208145821696.0,
      "budget_used_percent": 4.313208145821696
    },
    {
      "type": "training",
      "description": "Training step 363",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:46",
      "total_flops_so_far": 4325090261925888.0,
      "budget_used_percent": 4.325090261925888
    },
    {
      "type": "training",
      "description": "Training step 364",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:46",
      "total_flops_so_far": 4336972378030080.0,
      "budget_used_percent": 4.33697237803008
    },
    {
      "type": "training",
      "description": "Training step 365",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:47",
      "total_flops_so_far": 4348854494134272.0,
      "budget_used_percent": 4.348854494134272
    },
    {
      "type": "training",
      "description": "Training step 366",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:48",
      "total_flops_so_far": 4360736610238464.0,
      "budget_used_percent": 4.3607366102384635
    },
    {
      "type": "training",
      "description": "Training step 367",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:48",
      "total_flops_so_far": 4372618726342656.0,
      "budget_used_percent": 4.3726187263426555
    },
    {
      "type": "training",
      "description": "Training step 368",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:49",
      "total_flops_so_far": 4384500842446848.0,
      "budget_used_percent": 4.3845008424468475
    },
    {
      "type": "training",
      "description": "Training step 369",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:49",
      "total_flops_so_far": 4396382958551040.0,
      "budget_used_percent": 4.39638295855104
    },
    {
      "type": "training",
      "description": "Training step 370",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:50",
      "total_flops_so_far": 4408265074655232.0,
      "budget_used_percent": 4.408265074655232
    },
    {
      "type": "training",
      "description": "Training step 371",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:51",
      "total_flops_so_far": 4420147190759424.0,
      "budget_used_percent": 4.420147190759424
    },
    {
      "type": "training",
      "description": "Training step 372",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:51",
      "total_flops_so_far": 4432029306863616.0,
      "budget_used_percent": 4.432029306863616
    },
    {
      "type": "training",
      "description": "Training step 373",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:52",
      "total_flops_so_far": 4443911422967808.0,
      "budget_used_percent": 4.443911422967807
    },
    {
      "type": "training",
      "description": "Training step 374",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:52",
      "total_flops_so_far": 4455793539072000.0,
      "budget_used_percent": 4.455793539071999
    },
    {
      "type": "training",
      "description": "Training step 375",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:53",
      "total_flops_so_far": 4467675655176192.0,
      "budget_used_percent": 4.467675655176192
    },
    {
      "type": "training",
      "description": "Training step 376",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:54",
      "total_flops_so_far": 4479557771280384.0,
      "budget_used_percent": 4.479557771280384
    },
    {
      "type": "training",
      "description": "Training step 377",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:54",
      "total_flops_so_far": 4491439887384576.0,
      "budget_used_percent": 4.491439887384576
    },
    {
      "type": "training",
      "description": "Training step 378",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:55",
      "total_flops_so_far": 4503322003488768.0,
      "budget_used_percent": 4.503322003488768
    },
    {
      "type": "training",
      "description": "Training step 379",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:55",
      "total_flops_so_far": 4515204119592960.0,
      "budget_used_percent": 4.51520411959296
    },
    {
      "type": "training",
      "description": "Training step 380",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:56",
      "total_flops_so_far": 4527086235697152.0,
      "budget_used_percent": 4.527086235697151
    },
    {
      "type": "training",
      "description": "Training step 381",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:56",
      "total_flops_so_far": 4538968351801344.0,
      "budget_used_percent": 4.538968351801344
    },
    {
      "type": "training",
      "description": "Training step 382",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:57",
      "total_flops_so_far": 4550850467905536.0,
      "budget_used_percent": 4.550850467905536
    },
    {
      "type": "training",
      "description": "Training step 383",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:58",
      "total_flops_so_far": 4562732584009728.0,
      "budget_used_percent": 4.562732584009728
    },
    {
      "type": "training",
      "description": "Training step 384",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:58",
      "total_flops_so_far": 4574614700113920.0,
      "budget_used_percent": 4.57461470011392
    },
    {
      "type": "training",
      "description": "Training step 385",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:59",
      "total_flops_so_far": 4586496816218112.0,
      "budget_used_percent": 4.586496816218112
    },
    {
      "type": "training",
      "description": "Training step 386",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:07:59",
      "total_flops_so_far": 4598378932322304.0,
      "budget_used_percent": 4.598378932322304
    },
    {
      "type": "training",
      "description": "Training step 387",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:00",
      "total_flops_so_far": 4610261048426496.0,
      "budget_used_percent": 4.610261048426496
    },
    {
      "type": "training",
      "description": "Training step 388",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:01",
      "total_flops_so_far": 4622143164530688.0,
      "budget_used_percent": 4.622143164530688
    },
    {
      "type": "training",
      "description": "Training step 389",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:01",
      "total_flops_so_far": 4634025280634880.0,
      "budget_used_percent": 4.63402528063488
    },
    {
      "type": "training",
      "description": "Training step 390",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:02",
      "total_flops_so_far": 4645907396739072.0,
      "budget_used_percent": 4.645907396739072
    },
    {
      "type": "training",
      "description": "Training step 391",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:02",
      "total_flops_so_far": 4657789512843264.0,
      "budget_used_percent": 4.657789512843264
    },
    {
      "type": "training",
      "description": "Training step 392",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:03",
      "total_flops_so_far": 4669671628947456.0,
      "budget_used_percent": 4.669671628947456
    },
    {
      "type": "training",
      "description": "Training step 393",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:03",
      "total_flops_so_far": 4681553745051648.0,
      "budget_used_percent": 4.681553745051648
    },
    {
      "type": "training",
      "description": "Training step 394",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:04",
      "total_flops_so_far": 4693435861155840.0,
      "budget_used_percent": 4.69343586115584
    },
    {
      "type": "training",
      "description": "Training step 395",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:05",
      "total_flops_so_far": 4705317977260032.0,
      "budget_used_percent": 4.705317977260032
    },
    {
      "type": "training",
      "description": "Training step 396",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:05",
      "total_flops_so_far": 4717200093364224.0,
      "budget_used_percent": 4.717200093364224
    },
    {
      "type": "training",
      "description": "Training step 397",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:06",
      "total_flops_so_far": 4729082209468416.0,
      "budget_used_percent": 4.729082209468416
    },
    {
      "type": "training",
      "description": "Training step 398",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:06",
      "total_flops_so_far": 4740964325572608.0,
      "budget_used_percent": 4.740964325572608
    },
    {
      "type": "training",
      "description": "Training step 399",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:07",
      "total_flops_so_far": 4752846441676800.0,
      "budget_used_percent": 4.7528464416768
    },
    {
      "type": "training",
      "description": "Training step 400",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:08",
      "total_flops_so_far": 4764728557780992.0,
      "budget_used_percent": 4.764728557780992
    },
    {
      "type": "training",
      "description": "Training step 401",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:08",
      "total_flops_so_far": 4776610673885184.0,
      "budget_used_percent": 4.776610673885184
    },
    {
      "type": "training",
      "description": "Training step 402",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:09",
      "total_flops_so_far": 4788492789989376.0,
      "budget_used_percent": 4.788492789989376
    },
    {
      "type": "training",
      "description": "Training step 403",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:09",
      "total_flops_so_far": 4800374906093568.0,
      "budget_used_percent": 4.800374906093568
    },
    {
      "type": "training",
      "description": "Training step 404",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:10",
      "total_flops_so_far": 4812257022197760.0,
      "budget_used_percent": 4.81225702219776
    },
    {
      "type": "training",
      "description": "Training step 405",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:10",
      "total_flops_so_far": 4824139138301952.0,
      "budget_used_percent": 4.824139138301952
    },
    {
      "type": "training",
      "description": "Training step 406",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:11",
      "total_flops_so_far": 4836021254406144.0,
      "budget_used_percent": 4.836021254406144
    },
    {
      "type": "training",
      "description": "Training step 407",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:12",
      "total_flops_so_far": 4847903370510336.0,
      "budget_used_percent": 4.847903370510336
    },
    {
      "type": "training",
      "description": "Training step 408",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:12",
      "total_flops_so_far": 4859785486614528.0,
      "budget_used_percent": 4.859785486614529
    },
    {
      "type": "training",
      "description": "Training step 409",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:13",
      "total_flops_so_far": 4871667602718720.0,
      "budget_used_percent": 4.87166760271872
    },
    {
      "type": "training",
      "description": "Training step 410",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:13",
      "total_flops_so_far": 4883549718822912.0,
      "budget_used_percent": 4.883549718822912
    },
    {
      "type": "training",
      "description": "Training step 411",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:14",
      "total_flops_so_far": 4895431834927104.0,
      "budget_used_percent": 4.895431834927104
    },
    {
      "type": "training",
      "description": "Training step 412",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:15",
      "total_flops_so_far": 4907313951031296.0,
      "budget_used_percent": 4.907313951031296
    },
    {
      "type": "training",
      "description": "Training step 413",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:15",
      "total_flops_so_far": 4919196067135488.0,
      "budget_used_percent": 4.919196067135488
    },
    {
      "type": "training",
      "description": "Training step 414",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:16",
      "total_flops_so_far": 4931078183239680.0,
      "budget_used_percent": 4.931078183239681
    },
    {
      "type": "training",
      "description": "Training step 415",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:16",
      "total_flops_so_far": 4942960299343872.0,
      "budget_used_percent": 4.942960299343873
    },
    {
      "type": "training",
      "description": "Training step 416",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:17",
      "total_flops_so_far": 4954842415448064.0,
      "budget_used_percent": 4.954842415448064
    },
    {
      "type": "training",
      "description": "Training step 417",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:17",
      "total_flops_so_far": 4966724531552256.0,
      "budget_used_percent": 4.966724531552256
    },
    {
      "type": "training",
      "description": "Training step 418",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:18",
      "total_flops_so_far": 4978606647656448.0,
      "budget_used_percent": 4.978606647656448
    },
    {
      "type": "training",
      "description": "Training step 419",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:19",
      "total_flops_so_far": 4990488763760640.0,
      "budget_used_percent": 4.99048876376064
    },
    {
      "type": "training",
      "description": "Training step 420",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:19",
      "total_flops_so_far": 5002370879864832.0,
      "budget_used_percent": 5.0023708798648325
    },
    {
      "type": "training",
      "description": "Training step 421",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:20",
      "total_flops_so_far": 5014252995969024.0,
      "budget_used_percent": 5.0142529959690245
    },
    {
      "type": "training",
      "description": "Training step 422",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:20",
      "total_flops_so_far": 5026135112073216.0,
      "budget_used_percent": 5.0261351120732165
    },
    {
      "type": "training",
      "description": "Training step 423",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:21",
      "total_flops_so_far": 5038017228177408.0,
      "budget_used_percent": 5.038017228177408
    },
    {
      "type": "training",
      "description": "Training step 424",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:22",
      "total_flops_so_far": 5049899344281600.0,
      "budget_used_percent": 5.0498993442816
    },
    {
      "type": "training",
      "description": "Training step 425",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:22",
      "total_flops_so_far": 5061781460385792.0,
      "budget_used_percent": 5.061781460385792
    },
    {
      "type": "training",
      "description": "Training step 426",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:23",
      "total_flops_so_far": 5073663576489984.0,
      "budget_used_percent": 5.0736635764899845
    },
    {
      "type": "training",
      "description": "Training step 427",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:23",
      "total_flops_so_far": 5085545692594176.0,
      "budget_used_percent": 5.085545692594176
    },
    {
      "type": "training",
      "description": "Training step 428",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:24",
      "total_flops_so_far": 5097427808698368.0,
      "budget_used_percent": 5.097427808698368
    },
    {
      "type": "training",
      "description": "Training step 429",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:24",
      "total_flops_so_far": 5109309924802560.0,
      "budget_used_percent": 5.10930992480256
    },
    {
      "type": "training",
      "description": "Training step 430",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:25",
      "total_flops_so_far": 5121192040906752.0,
      "budget_used_percent": 5.1211920409067515
    },
    {
      "type": "training",
      "description": "Training step 431",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:26",
      "total_flops_so_far": 5133074157010944.0,
      "budget_used_percent": 5.1330741570109435
    },
    {
      "type": "training",
      "description": "Training step 432",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:26",
      "total_flops_so_far": 5144956273115136.0,
      "budget_used_percent": 5.1449562731151355
    },
    {
      "type": "training",
      "description": "Training step 433",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:27",
      "total_flops_so_far": 5156838389219328.0,
      "budget_used_percent": 5.156838389219328
    },
    {
      "type": "training",
      "description": "Training step 434",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:27",
      "total_flops_so_far": 5168720505323520.0,
      "budget_used_percent": 5.16872050532352
    },
    {
      "type": "training",
      "description": "Training step 435",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:28",
      "total_flops_so_far": 5180602621427712.0,
      "budget_used_percent": 5.180602621427712
    },
    {
      "type": "training",
      "description": "Training step 436",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:29",
      "total_flops_so_far": 5192484737531904.0,
      "budget_used_percent": 5.192484737531904
    },
    {
      "type": "training",
      "description": "Training step 437",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:29",
      "total_flops_so_far": 5204366853636096.0,
      "budget_used_percent": 5.204366853636095
    },
    {
      "type": "training",
      "description": "Training step 438",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:30",
      "total_flops_so_far": 5216248969740288.0,
      "budget_used_percent": 5.216248969740287
    },
    {
      "type": "training",
      "description": "Training step 439",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:30",
      "total_flops_so_far": 5228131085844480.0,
      "budget_used_percent": 5.22813108584448
    },
    {
      "type": "training",
      "description": "Training step 440",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:31",
      "total_flops_so_far": 5240013201948672.0,
      "budget_used_percent": 5.240013201948672
    },
    {
      "type": "training",
      "description": "Training step 441",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:32",
      "total_flops_so_far": 5251895318052864.0,
      "budget_used_percent": 5.251895318052864
    },
    {
      "type": "training",
      "description": "Training step 442",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:32",
      "total_flops_so_far": 5263777434157056.0,
      "budget_used_percent": 5.263777434157056
    },
    {
      "type": "training",
      "description": "Training step 443",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:33",
      "total_flops_so_far": 5275659550261248.0,
      "budget_used_percent": 5.275659550261248
    },
    {
      "type": "training",
      "description": "Training step 444",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:33",
      "total_flops_so_far": 5287541666365440.0,
      "budget_used_percent": 5.287541666365439
    },
    {
      "type": "training",
      "description": "Training step 445",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:34",
      "total_flops_so_far": 5299423782469632.0,
      "budget_used_percent": 5.299423782469632
    },
    {
      "type": "training",
      "description": "Training step 446",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:34",
      "total_flops_so_far": 5311305898573824.0,
      "budget_used_percent": 5.311305898573824
    },
    {
      "type": "training",
      "description": "Training step 447",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:35",
      "total_flops_so_far": 5323188014678016.0,
      "budget_used_percent": 5.323188014678016
    },
    {
      "type": "training",
      "description": "Training step 448",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:36",
      "total_flops_so_far": 5335070130782208.0,
      "budget_used_percent": 5.335070130782208
    },
    {
      "type": "training",
      "description": "Training step 449",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:36",
      "total_flops_so_far": 5346952246886400.0,
      "budget_used_percent": 5.3469522468864
    },
    {
      "type": "training",
      "description": "Training step 450",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:37",
      "total_flops_so_far": 5358834362990592.0,
      "budget_used_percent": 5.358834362990592
    },
    {
      "type": "training",
      "description": "Training step 451",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:37",
      "total_flops_so_far": 5370716479094784.0,
      "budget_used_percent": 5.370716479094783
    },
    {
      "type": "training",
      "description": "Training step 452",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:38",
      "total_flops_so_far": 5382598595198976.0,
      "budget_used_percent": 5.382598595198976
    },
    {
      "type": "training",
      "description": "Training step 453",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:39",
      "total_flops_so_far": 5394480711303168.0,
      "budget_used_percent": 5.394480711303168
    },
    {
      "type": "training",
      "description": "Training step 454",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:39",
      "total_flops_so_far": 5406362827407360.0,
      "budget_used_percent": 5.40636282740736
    },
    {
      "type": "training",
      "description": "Training step 455",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:40",
      "total_flops_so_far": 5418244943511552.0,
      "budget_used_percent": 5.418244943511552
    },
    {
      "type": "training",
      "description": "Training step 456",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:40",
      "total_flops_so_far": 5430127059615744.0,
      "budget_used_percent": 5.430127059615744
    },
    {
      "type": "training",
      "description": "Training step 457",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:41",
      "total_flops_so_far": 5442009175719936.0,
      "budget_used_percent": 5.442009175719936
    },
    {
      "type": "training",
      "description": "Training step 458",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:41",
      "total_flops_so_far": 5453891291824128.0,
      "budget_used_percent": 5.453891291824128
    },
    {
      "type": "training",
      "description": "Training step 459",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:42",
      "total_flops_so_far": 5465773407928320.0,
      "budget_used_percent": 5.46577340792832
    },
    {
      "type": "training",
      "description": "Training step 460",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:43",
      "total_flops_so_far": 5477655524032512.0,
      "budget_used_percent": 5.477655524032512
    },
    {
      "type": "training",
      "description": "Training step 461",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:43",
      "total_flops_so_far": 5489537640136704.0,
      "budget_used_percent": 5.489537640136704
    },
    {
      "type": "training",
      "description": "Training step 462",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:44",
      "total_flops_so_far": 5501419756240896.0,
      "budget_used_percent": 5.501419756240896
    },
    {
      "type": "training",
      "description": "Training step 463",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:44",
      "total_flops_so_far": 5513301872345088.0,
      "budget_used_percent": 5.513301872345088
    },
    {
      "type": "training",
      "description": "Training step 464",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:45",
      "total_flops_so_far": 5525183988449280.0,
      "budget_used_percent": 5.52518398844928
    },
    {
      "type": "training",
      "description": "Training step 465",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:46",
      "total_flops_so_far": 5537066104553472.0,
      "budget_used_percent": 5.537066104553472
    },
    {
      "type": "training",
      "description": "Training step 466",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:46",
      "total_flops_so_far": 5548948220657664.0,
      "budget_used_percent": 5.548948220657664
    },
    {
      "type": "training",
      "description": "Training step 467",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:47",
      "total_flops_so_far": 5560830336761856.0,
      "budget_used_percent": 5.560830336761856
    },
    {
      "type": "training",
      "description": "Training step 468",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:47",
      "total_flops_so_far": 5572712452866048.0,
      "budget_used_percent": 5.572712452866048
    },
    {
      "type": "training",
      "description": "Training step 469",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:48",
      "total_flops_so_far": 5584594568970240.0,
      "budget_used_percent": 5.58459456897024
    },
    {
      "type": "training",
      "description": "Training step 470",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:48",
      "total_flops_so_far": 5596476685074432.0,
      "budget_used_percent": 5.596476685074432
    },
    {
      "type": "training",
      "description": "Training step 471",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:49",
      "total_flops_so_far": 5608358801178624.0,
      "budget_used_percent": 5.608358801178624
    },
    {
      "type": "training",
      "description": "Training step 472",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:50",
      "total_flops_so_far": 5620240917282816.0,
      "budget_used_percent": 5.620240917282817
    },
    {
      "type": "training",
      "description": "Training step 473",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:50",
      "total_flops_so_far": 5632123033387008.0,
      "budget_used_percent": 5.632123033387008
    },
    {
      "type": "training",
      "description": "Training step 474",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:51",
      "total_flops_so_far": 5644005149491200.0,
      "budget_used_percent": 5.6440051494912
    },
    {
      "type": "training",
      "description": "Training step 475",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:51",
      "total_flops_so_far": 5655887265595392.0,
      "budget_used_percent": 5.655887265595392
    },
    {
      "type": "training",
      "description": "Training step 476",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:52",
      "total_flops_so_far": 5667769381699584.0,
      "budget_used_percent": 5.667769381699584
    },
    {
      "type": "training",
      "description": "Training step 477",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:53",
      "total_flops_so_far": 5679651497803776.0,
      "budget_used_percent": 5.679651497803776
    },
    {
      "type": "training",
      "description": "Training step 478",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:53",
      "total_flops_so_far": 5691533613907968.0,
      "budget_used_percent": 5.691533613907969
    },
    {
      "type": "training",
      "description": "Training step 479",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:54",
      "total_flops_so_far": 5703415730012160.0,
      "budget_used_percent": 5.703415730012161
    },
    {
      "type": "training",
      "description": "Training step 480",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:54",
      "total_flops_so_far": 5715297846116352.0,
      "budget_used_percent": 5.715297846116352
    },
    {
      "type": "training",
      "description": "Training step 481",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:55",
      "total_flops_so_far": 5727179962220544.0,
      "budget_used_percent": 5.727179962220544
    },
    {
      "type": "training",
      "description": "Training step 482",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:56",
      "total_flops_so_far": 5739062078324736.0,
      "budget_used_percent": 5.739062078324736
    },
    {
      "type": "training",
      "description": "Training step 483",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:56",
      "total_flops_so_far": 5750944194428928.0,
      "budget_used_percent": 5.750944194428928
    },
    {
      "type": "training",
      "description": "Training step 484",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:57",
      "total_flops_so_far": 5762826310533120.0,
      "budget_used_percent": 5.76282631053312
    },
    {
      "type": "training",
      "description": "Training step 485",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:57",
      "total_flops_so_far": 5774708426637312.0,
      "budget_used_percent": 5.7747084266373125
    },
    {
      "type": "training",
      "description": "Training step 486",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:58",
      "total_flops_so_far": 5786590542741504.0,
      "budget_used_percent": 5.7865905427415045
    },
    {
      "type": "training",
      "description": "Training step 487",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:58",
      "total_flops_so_far": 5798472658845696.0,
      "budget_used_percent": 5.798472658845696
    },
    {
      "type": "training",
      "description": "Training step 488",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:08:59",
      "total_flops_so_far": 5810354774949888.0,
      "budget_used_percent": 5.810354774949888
    },
    {
      "type": "training",
      "description": "Training step 489",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:09:00",
      "total_flops_so_far": 5822236891054080.0,
      "budget_used_percent": 5.82223689105408
    },
    {
      "type": "training",
      "description": "Training step 490",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:09:00",
      "total_flops_so_far": 5834119007158272.0,
      "budget_used_percent": 5.834119007158272
    },
    {
      "type": "training",
      "description": "Training step 491",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:09:01",
      "total_flops_so_far": 5846001123262464.0,
      "budget_used_percent": 5.8460011232624645
    },
    {
      "type": "training",
      "description": "Training step 492",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:09:01",
      "total_flops_so_far": 5857883239366656.0,
      "budget_used_percent": 5.8578832393666564
    },
    {
      "type": "training",
      "description": "Training step 493",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:09:02",
      "total_flops_so_far": 5869765355470848.0,
      "budget_used_percent": 5.869765355470848
    },
    {
      "type": "training",
      "description": "Training step 494",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:09:03",
      "total_flops_so_far": 5881647471575040.0,
      "budget_used_percent": 5.8816474715750395
    },
    {
      "type": "training",
      "description": "Training step 495",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:09:03",
      "total_flops_so_far": 5893529587679232.0,
      "budget_used_percent": 5.8935295876792315
    },
    {
      "type": "training",
      "description": "Training step 496",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:09:04",
      "total_flops_so_far": 5905411703783424.0,
      "budget_used_percent": 5.9054117037834235
    },
    {
      "type": "training",
      "description": "Training step 497",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:09:04",
      "total_flops_so_far": 5917293819887616.0,
      "budget_used_percent": 5.917293819887616
    },
    {
      "type": "training",
      "description": "Training step 498",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:09:05",
      "total_flops_so_far": 5929175935991808.0,
      "budget_used_percent": 5.929175935991808
    },
    {
      "type": "training",
      "description": "Training step 499",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:09:06",
      "total_flops_so_far": 5941058052096000.0,
      "budget_used_percent": 5.941058052096
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:09:14",
      "total_flops_so_far": 5941768681233856.0,
      "budget_used_percent": 5.941768681233857
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 714333709232.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:09:22",
      "total_flops_so_far": 5942483014943088.0,
      "budget_used_percent": 5.942483014943088
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:09:29",
      "total_flops_so_far": 5943195496006440.0,
      "budget_used_percent": 5.94319549600644
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:09:37",
      "total_flops_so_far": 5943906125144296.0,
      "budget_used_percent": 5.943906125144296
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 713407296244.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:09:44",
      "total_flops_so_far": 5944619532440540.0,
      "budget_used_percent": 5.94461953244054
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 5",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:09:51",
      "total_flops_so_far": 5945330161578396.0,
      "budget_used_percent": 5.945330161578396
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 6",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:09:59",
      "total_flops_so_far": 5946042642641748.0,
      "budget_used_percent": 5.946042642641748
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 7",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:06",
      "total_flops_so_far": 5946755123705100.0,
      "budget_used_percent": 5.9467551237051
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 8",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:13",
      "total_flops_so_far": 5947467604768452.0,
      "budget_used_percent": 5.947467604768452
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 9",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:21",
      "total_flops_so_far": 5948180085831804.0,
      "budget_used_percent": 5.948180085831805
    },
    {
      "type": "training",
      "description": "Training step 500",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:21",
      "total_flops_so_far": 5960062201935996.0,
      "budget_used_percent": 5.960062201935996
    },
    {
      "type": "training",
      "description": "Training step 501",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:22",
      "total_flops_so_far": 5971944318040188.0,
      "budget_used_percent": 5.971944318040188
    },
    {
      "type": "training",
      "description": "Training step 502",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:22",
      "total_flops_so_far": 5983826434144380.0,
      "budget_used_percent": 5.98382643414438
    },
    {
      "type": "training",
      "description": "Training step 503",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:23",
      "total_flops_so_far": 5995708550248572.0,
      "budget_used_percent": 5.995708550248572
    },
    {
      "type": "training",
      "description": "Training step 504",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:23",
      "total_flops_so_far": 6007590666352764.0,
      "budget_used_percent": 6.0075906663527645
    },
    {
      "type": "training",
      "description": "Training step 505",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:24",
      "total_flops_so_far": 6019472782456956.0,
      "budget_used_percent": 6.0194727824569565
    },
    {
      "type": "training",
      "description": "Training step 506",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:24",
      "total_flops_so_far": 6031354898561148.0,
      "budget_used_percent": 6.0313548985611485
    },
    {
      "type": "training",
      "description": "Training step 507",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:25",
      "total_flops_so_far": 6043237014665340.0,
      "budget_used_percent": 6.04323701466534
    },
    {
      "type": "training",
      "description": "Training step 508",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:26",
      "total_flops_so_far": 6055119130769532.0,
      "budget_used_percent": 6.055119130769532
    },
    {
      "type": "training",
      "description": "Training step 509",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:26",
      "total_flops_so_far": 6067001246873724.0,
      "budget_used_percent": 6.0670012468737236
    },
    {
      "type": "training",
      "description": "Training step 510",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:27",
      "total_flops_so_far": 6078883362977916.0,
      "budget_used_percent": 6.0788833629779155
    },
    {
      "type": "training",
      "description": "Training step 511",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:27",
      "total_flops_so_far": 6090765479082108.0,
      "budget_used_percent": 6.090765479082108
    },
    {
      "type": "training",
      "description": "Training step 512",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:28",
      "total_flops_so_far": 6102647595186300.0,
      "budget_used_percent": 6.1026475951863
    },
    {
      "type": "training",
      "description": "Training step 513",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:29",
      "total_flops_so_far": 6114529711290492.0,
      "budget_used_percent": 6.114529711290492
    },
    {
      "type": "training",
      "description": "Training step 514",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:29",
      "total_flops_so_far": 6126411827394684.0,
      "budget_used_percent": 6.1264118273946835
    },
    {
      "type": "training",
      "description": "Training step 515",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:30",
      "total_flops_so_far": 6138293943498876.0,
      "budget_used_percent": 6.1382939434988755
    },
    {
      "type": "training",
      "description": "Training step 516",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:30",
      "total_flops_so_far": 6150176059603068.0,
      "budget_used_percent": 6.1501760596030675
    },
    {
      "type": "training",
      "description": "Training step 517",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:31",
      "total_flops_so_far": 6162058175707260.0,
      "budget_used_percent": 6.16205817570726
    },
    {
      "type": "training",
      "description": "Training step 518",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:31",
      "total_flops_so_far": 6173940291811452.0,
      "budget_used_percent": 6.173940291811452
    },
    {
      "type": "training",
      "description": "Training step 519",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:32",
      "total_flops_so_far": 6185822407915644.0,
      "budget_used_percent": 6.185822407915644
    },
    {
      "type": "training",
      "description": "Training step 520",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:33",
      "total_flops_so_far": 6197704524019836.0,
      "budget_used_percent": 6.197704524019836
    },
    {
      "type": "training",
      "description": "Training step 521",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:33",
      "total_flops_so_far": 6209586640124028.0,
      "budget_used_percent": 6.209586640124027
    },
    {
      "type": "training",
      "description": "Training step 522",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:34",
      "total_flops_so_far": 6221468756228220.0,
      "budget_used_percent": 6.221468756228219
    },
    {
      "type": "training",
      "description": "Training step 523",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:34",
      "total_flops_so_far": 6233350872332412.0,
      "budget_used_percent": 6.233350872332412
    },
    {
      "type": "training",
      "description": "Training step 524",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:35",
      "total_flops_so_far": 6245232988436604.0,
      "budget_used_percent": 6.245232988436604
    },
    {
      "type": "training",
      "description": "Training step 525",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:36",
      "total_flops_so_far": 6257115104540796.0,
      "budget_used_percent": 6.257115104540796
    },
    {
      "type": "training",
      "description": "Training step 526",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:36",
      "total_flops_so_far": 6268997220644988.0,
      "budget_used_percent": 6.268997220644988
    },
    {
      "type": "training",
      "description": "Training step 527",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:36",
      "total_flops_so_far": 6280879336749180.0,
      "budget_used_percent": 6.28087933674918
    },
    {
      "type": "training",
      "description": "Training step 528",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:37",
      "total_flops_so_far": 6292761452853372.0,
      "budget_used_percent": 6.292761452853372
    },
    {
      "type": "training",
      "description": "Training step 529",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:38",
      "total_flops_so_far": 6304643568957564.0,
      "budget_used_percent": 6.304643568957564
    },
    {
      "type": "training",
      "description": "Training step 530",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:38",
      "total_flops_so_far": 6316525685061756.0,
      "budget_used_percent": 6.316525685061756
    },
    {
      "type": "training",
      "description": "Training step 531",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:39",
      "total_flops_so_far": 6328407801165948.0,
      "budget_used_percent": 6.328407801165949
    },
    {
      "type": "training",
      "description": "Training step 532",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:39",
      "total_flops_so_far": 6340289917270140.0,
      "budget_used_percent": 6.340289917270139
    },
    {
      "type": "training",
      "description": "Training step 533",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:40",
      "total_flops_so_far": 6352172033374332.0,
      "budget_used_percent": 6.352172033374331
    },
    {
      "type": "training",
      "description": "Training step 534",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:41",
      "total_flops_so_far": 6364054149478524.0,
      "budget_used_percent": 6.364054149478523
    },
    {
      "type": "training",
      "description": "Training step 535",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:41",
      "total_flops_so_far": 6375936265582716.0,
      "budget_used_percent": 6.375936265582716
    },
    {
      "type": "training",
      "description": "Training step 536",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:42",
      "total_flops_so_far": 6387818381686908.0,
      "budget_used_percent": 6.387818381686908
    },
    {
      "type": "training",
      "description": "Training step 537",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:42",
      "total_flops_so_far": 6399700497791100.0,
      "budget_used_percent": 6.3997004977911
    },
    {
      "type": "training",
      "description": "Training step 538",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:43",
      "total_flops_so_far": 6411582613895292.0,
      "budget_used_percent": 6.411582613895292
    },
    {
      "type": "training",
      "description": "Training step 539",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:43",
      "total_flops_so_far": 6423464729999484.0,
      "budget_used_percent": 6.423464729999484
    },
    {
      "type": "training",
      "description": "Training step 540",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:44",
      "total_flops_so_far": 6435346846103676.0,
      "budget_used_percent": 6.435346846103676
    },
    {
      "type": "training",
      "description": "Training step 541",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:45",
      "total_flops_so_far": 6447228962207868.0,
      "budget_used_percent": 6.447228962207868
    },
    {
      "type": "training",
      "description": "Training step 542",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:45",
      "total_flops_so_far": 6459111078312060.0,
      "budget_used_percent": 6.45911107831206
    },
    {
      "type": "training",
      "description": "Training step 543",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:46",
      "total_flops_so_far": 6470993194416252.0,
      "budget_used_percent": 6.470993194416252
    },
    {
      "type": "training",
      "description": "Training step 544",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:46",
      "total_flops_so_far": 6482875310520444.0,
      "budget_used_percent": 6.482875310520445
    },
    {
      "type": "training",
      "description": "Training step 545",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:47",
      "total_flops_so_far": 6494757426624636.0,
      "budget_used_percent": 6.494757426624637
    },
    {
      "type": "training",
      "description": "Training step 546",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:48",
      "total_flops_so_far": 6506639542728828.0,
      "budget_used_percent": 6.506639542728827
    },
    {
      "type": "training",
      "description": "Training step 547",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:48",
      "total_flops_so_far": 6518521658833020.0,
      "budget_used_percent": 6.518521658833019
    },
    {
      "type": "training",
      "description": "Training step 548",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:49",
      "total_flops_so_far": 6530403774937212.0,
      "budget_used_percent": 6.530403774937212
    },
    {
      "type": "training",
      "description": "Training step 549",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:49",
      "total_flops_so_far": 6542285891041404.0,
      "budget_used_percent": 6.542285891041404
    },
    {
      "type": "training",
      "description": "Training step 550",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:50",
      "total_flops_so_far": 6554168007145596.0,
      "budget_used_percent": 6.554168007145596
    },
    {
      "type": "training",
      "description": "Training step 551",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:51",
      "total_flops_so_far": 6566050123249788.0,
      "budget_used_percent": 6.566050123249788
    },
    {
      "type": "training",
      "description": "Training step 552",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:51",
      "total_flops_so_far": 6577932239353980.0,
      "budget_used_percent": 6.57793223935398
    },
    {
      "type": "training",
      "description": "Training step 553",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:52",
      "total_flops_so_far": 6589814355458172.0,
      "budget_used_percent": 6.589814355458172
    },
    {
      "type": "training",
      "description": "Training step 554",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:52",
      "total_flops_so_far": 6601696471562364.0,
      "budget_used_percent": 6.601696471562364
    },
    {
      "type": "training",
      "description": "Training step 555",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:53",
      "total_flops_so_far": 6613578587666556.0,
      "budget_used_percent": 6.613578587666556
    },
    {
      "type": "training",
      "description": "Training step 556",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:53",
      "total_flops_so_far": 6625460703770748.0,
      "budget_used_percent": 6.625460703770749
    },
    {
      "type": "training",
      "description": "Training step 557",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:54",
      "total_flops_so_far": 6637342819874940.0,
      "budget_used_percent": 6.637342819874941
    },
    {
      "type": "training",
      "description": "Training step 558",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:55",
      "total_flops_so_far": 6649224935979132.0,
      "budget_used_percent": 6.649224935979133
    },
    {
      "type": "training",
      "description": "Training step 559",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:55",
      "total_flops_so_far": 6661107052083324.0,
      "budget_used_percent": 6.661107052083325
    },
    {
      "type": "training",
      "description": "Training step 560",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:56",
      "total_flops_so_far": 6672989168187516.0,
      "budget_used_percent": 6.672989168187515
    },
    {
      "type": "training",
      "description": "Training step 561",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:56",
      "total_flops_so_far": 6684871284291708.0,
      "budget_used_percent": 6.684871284291708
    },
    {
      "type": "training",
      "description": "Training step 562",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:57",
      "total_flops_so_far": 6696753400395900.0,
      "budget_used_percent": 6.6967534003959
    },
    {
      "type": "training",
      "description": "Training step 563",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:58",
      "total_flops_so_far": 6708635516500092.0,
      "budget_used_percent": 6.708635516500092
    },
    {
      "type": "training",
      "description": "Training step 564",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:58",
      "total_flops_so_far": 6720517632604284.0,
      "budget_used_percent": 6.720517632604284
    },
    {
      "type": "training",
      "description": "Training step 565",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:59",
      "total_flops_so_far": 6732399748708476.0,
      "budget_used_percent": 6.732399748708476
    },
    {
      "type": "training",
      "description": "Training step 566",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:10:59",
      "total_flops_so_far": 6744281864812668.0,
      "budget_used_percent": 6.744281864812668
    },
    {
      "type": "training",
      "description": "Training step 567",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:00",
      "total_flops_so_far": 6756163980916860.0,
      "budget_used_percent": 6.75616398091686
    },
    {
      "type": "training",
      "description": "Training step 568",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:01",
      "total_flops_so_far": 6768046097021052.0,
      "budget_used_percent": 6.7680460970210525
    },
    {
      "type": "training",
      "description": "Training step 569",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:01",
      "total_flops_so_far": 6779928213125244.0,
      "budget_used_percent": 6.7799282131252445
    },
    {
      "type": "training",
      "description": "Training step 570",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:02",
      "total_flops_so_far": 6791810329229436.0,
      "budget_used_percent": 6.7918103292294365
    },
    {
      "type": "training",
      "description": "Training step 571",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:02",
      "total_flops_so_far": 6803692445333628.0,
      "budget_used_percent": 6.8036924453336285
    },
    {
      "type": "training",
      "description": "Training step 572",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:03",
      "total_flops_so_far": 6815574561437820.0,
      "budget_used_percent": 6.8155745614378205
    },
    {
      "type": "training",
      "description": "Training step 573",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:03",
      "total_flops_so_far": 6827456677542012.0,
      "budget_used_percent": 6.8274566775420125
    },
    {
      "type": "training",
      "description": "Training step 574",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:04",
      "total_flops_so_far": 6839338793646204.0,
      "budget_used_percent": 6.8393387936462045
    },
    {
      "type": "training",
      "description": "Training step 575",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:05",
      "total_flops_so_far": 6851220909750396.0,
      "budget_used_percent": 6.8512209097503955
    },
    {
      "type": "training",
      "description": "Training step 576",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:05",
      "total_flops_so_far": 6863103025854588.0,
      "budget_used_percent": 6.8631030258545875
    },
    {
      "type": "training",
      "description": "Training step 577",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:06",
      "total_flops_so_far": 6874985141958780.0,
      "budget_used_percent": 6.8749851419587795
    },
    {
      "type": "training",
      "description": "Training step 578",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:06",
      "total_flops_so_far": 6886867258062972.0,
      "budget_used_percent": 6.8868672580629715
    },
    {
      "type": "training",
      "description": "Training step 579",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:07",
      "total_flops_so_far": 6898749374167164.0,
      "budget_used_percent": 6.8987493741671635
    },
    {
      "type": "training",
      "description": "Training step 580",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:08",
      "total_flops_so_far": 6910631490271356.0,
      "budget_used_percent": 6.9106314902713555
    },
    {
      "type": "training",
      "description": "Training step 581",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:08",
      "total_flops_so_far": 6922513606375548.0,
      "budget_used_percent": 6.922513606375548
    },
    {
      "type": "training",
      "description": "Training step 582",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:09",
      "total_flops_so_far": 6934395722479740.0,
      "budget_used_percent": 6.93439572247974
    },
    {
      "type": "training",
      "description": "Training step 583",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:09",
      "total_flops_so_far": 6946277838583932.0,
      "budget_used_percent": 6.946277838583932
    },
    {
      "type": "training",
      "description": "Training step 584",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:10",
      "total_flops_so_far": 6958159954688124.0,
      "budget_used_percent": 6.958159954688124
    },
    {
      "type": "training",
      "description": "Training step 585",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:10",
      "total_flops_so_far": 6970042070792316.0,
      "budget_used_percent": 6.970042070792316
    },
    {
      "type": "training",
      "description": "Training step 586",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:11",
      "total_flops_so_far": 6981924186896508.0,
      "budget_used_percent": 6.981924186896508
    },
    {
      "type": "training",
      "description": "Training step 587",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:12",
      "total_flops_so_far": 6993806303000700.0,
      "budget_used_percent": 6.9938063030007
    },
    {
      "type": "training",
      "description": "Training step 588",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:12",
      "total_flops_so_far": 7005688419104892.0,
      "budget_used_percent": 7.005688419104892
    },
    {
      "type": "training",
      "description": "Training step 589",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:13",
      "total_flops_so_far": 7017570535209084.0,
      "budget_used_percent": 7.017570535209083
    },
    {
      "type": "training",
      "description": "Training step 590",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:13",
      "total_flops_so_far": 7029452651313276.0,
      "budget_used_percent": 7.029452651313275
    },
    {
      "type": "training",
      "description": "Training step 591",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:14",
      "total_flops_so_far": 7041334767417468.0,
      "budget_used_percent": 7.041334767417467
    },
    {
      "type": "training",
      "description": "Training step 592",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:15",
      "total_flops_so_far": 7053216883521660.0,
      "budget_used_percent": 7.053216883521659
    },
    {
      "type": "training",
      "description": "Training step 593",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:15",
      "total_flops_so_far": 7065098999625852.0,
      "budget_used_percent": 7.065098999625851
    },
    {
      "type": "training",
      "description": "Training step 594",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:16",
      "total_flops_so_far": 7076981115730044.0,
      "budget_used_percent": 7.076981115730044
    },
    {
      "type": "training",
      "description": "Training step 595",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:16",
      "total_flops_so_far": 7088863231834236.0,
      "budget_used_percent": 7.088863231834236
    },
    {
      "type": "training",
      "description": "Training step 596",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:17",
      "total_flops_so_far": 7100745347938428.0,
      "budget_used_percent": 7.100745347938428
    },
    {
      "type": "training",
      "description": "Training step 597",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:18",
      "total_flops_so_far": 7112627464042620.0,
      "budget_used_percent": 7.11262746404262
    },
    {
      "type": "training",
      "description": "Training step 598",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:18",
      "total_flops_so_far": 7124509580146812.0,
      "budget_used_percent": 7.124509580146812
    },
    {
      "type": "training",
      "description": "Training step 599",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:19",
      "total_flops_so_far": 7136391696251004.0,
      "budget_used_percent": 7.136391696251004
    },
    {
      "type": "training",
      "description": "Training step 600",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:19",
      "total_flops_so_far": 7148273812355196.0,
      "budget_used_percent": 7.148273812355196
    },
    {
      "type": "training",
      "description": "Training step 601",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:20",
      "total_flops_so_far": 7160155928459388.0,
      "budget_used_percent": 7.160155928459389
    },
    {
      "type": "training",
      "description": "Training step 602",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:20",
      "total_flops_so_far": 7172038044563580.0,
      "budget_used_percent": 7.172038044563581
    },
    {
      "type": "training",
      "description": "Training step 603",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:21",
      "total_flops_so_far": 7183920160667772.0,
      "budget_used_percent": 7.183920160667771
    },
    {
      "type": "training",
      "description": "Training step 604",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:22",
      "total_flops_so_far": 7195802276771964.0,
      "budget_used_percent": 7.195802276771963
    },
    {
      "type": "training",
      "description": "Training step 605",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:22",
      "total_flops_so_far": 7207684392876156.0,
      "budget_used_percent": 7.207684392876155
    },
    {
      "type": "training",
      "description": "Training step 606",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:23",
      "total_flops_so_far": 7219566508980348.0,
      "budget_used_percent": 7.219566508980348
    },
    {
      "type": "training",
      "description": "Training step 607",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:23",
      "total_flops_so_far": 7231448625084540.0,
      "budget_used_percent": 7.23144862508454
    },
    {
      "type": "training",
      "description": "Training step 608",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:24",
      "total_flops_so_far": 7243330741188732.0,
      "budget_used_percent": 7.243330741188732
    },
    {
      "type": "training",
      "description": "Training step 609",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:25",
      "total_flops_so_far": 7255212857292924.0,
      "budget_used_percent": 7.255212857292924
    },
    {
      "type": "training",
      "description": "Training step 610",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:25",
      "total_flops_so_far": 7267094973397116.0,
      "budget_used_percent": 7.267094973397116
    },
    {
      "type": "training",
      "description": "Training step 611",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:26",
      "total_flops_so_far": 7278977089501308.0,
      "budget_used_percent": 7.278977089501308
    },
    {
      "type": "training",
      "description": "Training step 612",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:26",
      "total_flops_so_far": 7290859205605500.0,
      "budget_used_percent": 7.2908592056055
    },
    {
      "type": "training",
      "description": "Training step 613",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:27",
      "total_flops_so_far": 7302741321709692.0,
      "budget_used_percent": 7.302741321709692
    },
    {
      "type": "training",
      "description": "Training step 614",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:28",
      "total_flops_so_far": 7314623437813884.0,
      "budget_used_percent": 7.314623437813885
    },
    {
      "type": "training",
      "description": "Training step 615",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:28",
      "total_flops_so_far": 7326505553918076.0,
      "budget_used_percent": 7.326505553918077
    },
    {
      "type": "training",
      "description": "Training step 616",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:29",
      "total_flops_so_far": 7338387670022268.0,
      "budget_used_percent": 7.338387670022269
    },
    {
      "type": "training",
      "description": "Training step 617",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:29",
      "total_flops_so_far": 7350269786126460.0,
      "budget_used_percent": 7.350269786126459
    },
    {
      "type": "training",
      "description": "Training step 618",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:30",
      "total_flops_so_far": 7362151902230652.0,
      "budget_used_percent": 7.362151902230652
    },
    {
      "type": "training",
      "description": "Training step 619",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:30",
      "total_flops_so_far": 7374034018334844.0,
      "budget_used_percent": 7.374034018334844
    },
    {
      "type": "training",
      "description": "Training step 620",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:31",
      "total_flops_so_far": 7385916134439036.0,
      "budget_used_percent": 7.385916134439036
    },
    {
      "type": "training",
      "description": "Training step 621",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:32",
      "total_flops_so_far": 7397798250543228.0,
      "budget_used_percent": 7.397798250543228
    },
    {
      "type": "training",
      "description": "Training step 622",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:32",
      "total_flops_so_far": 7409680366647420.0,
      "budget_used_percent": 7.40968036664742
    },
    {
      "type": "training",
      "description": "Training step 623",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:33",
      "total_flops_so_far": 7421562482751612.0,
      "budget_used_percent": 7.421562482751612
    },
    {
      "type": "training",
      "description": "Training step 624",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:33",
      "total_flops_so_far": 7433444598855804.0,
      "budget_used_percent": 7.433444598855804
    },
    {
      "type": "training",
      "description": "Training step 625",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:34",
      "total_flops_so_far": 7445326714959996.0,
      "budget_used_percent": 7.445326714959996
    },
    {
      "type": "training",
      "description": "Training step 626",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:35",
      "total_flops_so_far": 7457208831064188.0,
      "budget_used_percent": 7.457208831064188
    },
    {
      "type": "training",
      "description": "Training step 627",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:35",
      "total_flops_so_far": 7469090947168380.0,
      "budget_used_percent": 7.469090947168381
    },
    {
      "type": "training",
      "description": "Training step 628",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:36",
      "total_flops_so_far": 7480973063272572.0,
      "budget_used_percent": 7.480973063272573
    },
    {
      "type": "training",
      "description": "Training step 629",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:36",
      "total_flops_so_far": 7492855179376764.0,
      "budget_used_percent": 7.492855179376765
    },
    {
      "type": "training",
      "description": "Training step 630",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:37",
      "total_flops_so_far": 7504737295480956.0,
      "budget_used_percent": 7.504737295480957
    },
    {
      "type": "training",
      "description": "Training step 631",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:38",
      "total_flops_so_far": 7516619411585148.0,
      "budget_used_percent": 7.516619411585149
    },
    {
      "type": "training",
      "description": "Training step 632",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:38",
      "total_flops_so_far": 7528501527689340.0,
      "budget_used_percent": 7.52850152768934
    },
    {
      "type": "training",
      "description": "Training step 633",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:39",
      "total_flops_so_far": 7540383643793532.0,
      "budget_used_percent": 7.540383643793532
    },
    {
      "type": "training",
      "description": "Training step 634",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:39",
      "total_flops_so_far": 7552265759897724.0,
      "budget_used_percent": 7.552265759897724
    },
    {
      "type": "training",
      "description": "Training step 635",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:40",
      "total_flops_so_far": 7564147876001916.0,
      "budget_used_percent": 7.564147876001916
    },
    {
      "type": "training",
      "description": "Training step 636",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:40",
      "total_flops_so_far": 7576029992106108.0,
      "budget_used_percent": 7.576029992106108
    },
    {
      "type": "training",
      "description": "Training step 637",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:41",
      "total_flops_so_far": 7587912108210300.0,
      "budget_used_percent": 7.5879121082103
    },
    {
      "type": "training",
      "description": "Training step 638",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:42",
      "total_flops_so_far": 7599794224314492.0,
      "budget_used_percent": 7.599794224314492
    },
    {
      "type": "training",
      "description": "Training step 639",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:42",
      "total_flops_so_far": 7611676340418684.0,
      "budget_used_percent": 7.6116763404186845
    },
    {
      "type": "training",
      "description": "Training step 640",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:43",
      "total_flops_so_far": 7623558456522876.0,
      "budget_used_percent": 7.6235584565228764
    },
    {
      "type": "training",
      "description": "Training step 641",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:43",
      "total_flops_so_far": 7635440572627068.0,
      "budget_used_percent": 7.635440572627068
    },
    {
      "type": "training",
      "description": "Training step 642",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:44",
      "total_flops_so_far": 7647322688731260.0,
      "budget_used_percent": 7.64732268873126
    },
    {
      "type": "training",
      "description": "Training step 643",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:45",
      "total_flops_so_far": 7659204804835452.0,
      "budget_used_percent": 7.659204804835452
    },
    {
      "type": "training",
      "description": "Training step 644",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:45",
      "total_flops_so_far": 7671086920939644.0,
      "budget_used_percent": 7.671086920939644
    },
    {
      "type": "training",
      "description": "Training step 645",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:46",
      "total_flops_so_far": 7682969037043836.0,
      "budget_used_percent": 7.682969037043836
    },
    {
      "type": "training",
      "description": "Training step 646",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:46",
      "total_flops_so_far": 7694851153148028.0,
      "budget_used_percent": 7.6948511531480275
    },
    {
      "type": "training",
      "description": "Training step 647",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:47",
      "total_flops_so_far": 7706733269252220.0,
      "budget_used_percent": 7.7067332692522195
    },
    {
      "type": "training",
      "description": "Training step 648",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:48",
      "total_flops_so_far": 7718615385356412.0,
      "budget_used_percent": 7.7186153853564115
    },
    {
      "type": "training",
      "description": "Training step 649",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:48",
      "total_flops_so_far": 7730497501460604.0,
      "budget_used_percent": 7.730497501460603
    },
    {
      "type": "training",
      "description": "Training step 650",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:49",
      "total_flops_so_far": 7742379617564796.0,
      "budget_used_percent": 7.742379617564795
    },
    {
      "type": "training",
      "description": "Training step 651",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:49",
      "total_flops_so_far": 7754261733668988.0,
      "budget_used_percent": 7.754261733668988
    },
    {
      "type": "training",
      "description": "Training step 652",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:50",
      "total_flops_so_far": 7766143849773180.0,
      "budget_used_percent": 7.76614384977318
    },
    {
      "type": "training",
      "description": "Training step 653",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:51",
      "total_flops_so_far": 7778025965877372.0,
      "budget_used_percent": 7.778025965877372
    },
    {
      "type": "training",
      "description": "Training step 654",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:51",
      "total_flops_so_far": 7789908081981564.0,
      "budget_used_percent": 7.789908081981564
    },
    {
      "type": "training",
      "description": "Training step 655",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:52",
      "total_flops_so_far": 7801790198085756.0,
      "budget_used_percent": 7.801790198085756
    },
    {
      "type": "training",
      "description": "Training step 656",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:52",
      "total_flops_so_far": 7813672314189948.0,
      "budget_used_percent": 7.813672314189948
    },
    {
      "type": "training",
      "description": "Training step 657",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:53",
      "total_flops_so_far": 7825554430294140.0,
      "budget_used_percent": 7.82555443029414
    },
    {
      "type": "training",
      "description": "Training step 658",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:53",
      "total_flops_so_far": 7837436546398332.0,
      "budget_used_percent": 7.837436546398332
    },
    {
      "type": "training",
      "description": "Training step 659",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:54",
      "total_flops_so_far": 7849318662502524.0,
      "budget_used_percent": 7.849318662502524
    },
    {
      "type": "training",
      "description": "Training step 660",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:55",
      "total_flops_so_far": 7861200778606716.0,
      "budget_used_percent": 7.861200778606715
    },
    {
      "type": "training",
      "description": "Training step 661",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:55",
      "total_flops_so_far": 7873082894710908.0,
      "budget_used_percent": 7.873082894710907
    },
    {
      "type": "training",
      "description": "Training step 662",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:56",
      "total_flops_so_far": 7884965010815100.0,
      "budget_used_percent": 7.884965010815099
    },
    {
      "type": "training",
      "description": "Training step 663",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:56",
      "total_flops_so_far": 7896847126919292.0,
      "budget_used_percent": 7.896847126919291
    },
    {
      "type": "training",
      "description": "Training step 664",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:57",
      "total_flops_so_far": 7908729243023484.0,
      "budget_used_percent": 7.908729243023484
    },
    {
      "type": "training",
      "description": "Training step 665",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:58",
      "total_flops_so_far": 7920611359127676.0,
      "budget_used_percent": 7.920611359127676
    },
    {
      "type": "training",
      "description": "Training step 666",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:58",
      "total_flops_so_far": 7932493475231868.0,
      "budget_used_percent": 7.932493475231868
    },
    {
      "type": "training",
      "description": "Training step 667",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:59",
      "total_flops_so_far": 7944375591336060.0,
      "budget_used_percent": 7.94437559133606
    },
    {
      "type": "training",
      "description": "Training step 668",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:11:59",
      "total_flops_so_far": 7956257707440252.0,
      "budget_used_percent": 7.956257707440252
    },
    {
      "type": "training",
      "description": "Training step 669",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:00",
      "total_flops_so_far": 7968139823544444.0,
      "budget_used_percent": 7.968139823544444
    },
    {
      "type": "training",
      "description": "Training step 670",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:01",
      "total_flops_so_far": 7980021939648636.0,
      "budget_used_percent": 7.980021939648636
    },
    {
      "type": "training",
      "description": "Training step 671",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:01",
      "total_flops_so_far": 7991904055752828.0,
      "budget_used_percent": 7.991904055752828
    },
    {
      "type": "training",
      "description": "Training step 672",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:02",
      "total_flops_so_far": 8003786171857020.0,
      "budget_used_percent": 8.003786171857021
    },
    {
      "type": "training",
      "description": "Training step 673",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:02",
      "total_flops_so_far": 8015668287961212.0,
      "budget_used_percent": 8.015668287961212
    },
    {
      "type": "training",
      "description": "Training step 674",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:03",
      "total_flops_so_far": 8027550404065404.0,
      "budget_used_percent": 8.027550404065403
    },
    {
      "type": "training",
      "description": "Training step 675",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:03",
      "total_flops_so_far": 8039432520169596.0,
      "budget_used_percent": 8.039432520169596
    },
    {
      "type": "training",
      "description": "Training step 676",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:04",
      "total_flops_so_far": 8051314636273788.0,
      "budget_used_percent": 8.051314636273787
    },
    {
      "type": "training",
      "description": "Training step 677",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:05",
      "total_flops_so_far": 8063196752377980.0,
      "budget_used_percent": 8.06319675237798
    },
    {
      "type": "training",
      "description": "Training step 678",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:05",
      "total_flops_so_far": 8075078868482172.0,
      "budget_used_percent": 8.075078868482171
    },
    {
      "type": "training",
      "description": "Training step 679",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:06",
      "total_flops_so_far": 8086960984586364.0,
      "budget_used_percent": 8.086960984586364
    },
    {
      "type": "training",
      "description": "Training step 680",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:06",
      "total_flops_so_far": 8098843100690556.0,
      "budget_used_percent": 8.098843100690555
    },
    {
      "type": "training",
      "description": "Training step 681",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:07",
      "total_flops_so_far": 8110725216794748.0,
      "budget_used_percent": 8.110725216794748
    },
    {
      "type": "training",
      "description": "Training step 682",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:08",
      "total_flops_so_far": 8122607332898940.0,
      "budget_used_percent": 8.12260733289894
    },
    {
      "type": "training",
      "description": "Training step 683",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:08",
      "total_flops_so_far": 8134489449003132.0,
      "budget_used_percent": 8.134489449003132
    },
    {
      "type": "training",
      "description": "Training step 684",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:09",
      "total_flops_so_far": 8146371565107324.0,
      "budget_used_percent": 8.146371565107325
    },
    {
      "type": "training",
      "description": "Training step 685",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:09",
      "total_flops_so_far": 8158253681211516.0,
      "budget_used_percent": 8.158253681211516
    },
    {
      "type": "training",
      "description": "Training step 686",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:10",
      "total_flops_so_far": 8170135797315708.0,
      "budget_used_percent": 8.170135797315709
    },
    {
      "type": "training",
      "description": "Training step 687",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:11",
      "total_flops_so_far": 8182017913419900.0,
      "budget_used_percent": 8.1820179134199
    },
    {
      "type": "training",
      "description": "Training step 688",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:11",
      "total_flops_so_far": 8193900029524092.0,
      "budget_used_percent": 8.193900029524093
    },
    {
      "type": "training",
      "description": "Training step 689",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:12",
      "total_flops_so_far": 8205782145628284.0,
      "budget_used_percent": 8.205782145628284
    },
    {
      "type": "training",
      "description": "Training step 690",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:12",
      "total_flops_so_far": 8217664261732476.0,
      "budget_used_percent": 8.217664261732475
    },
    {
      "type": "training",
      "description": "Training step 691",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:13",
      "total_flops_so_far": 8229546377836668.0,
      "budget_used_percent": 8.229546377836668
    },
    {
      "type": "training",
      "description": "Training step 692",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:14",
      "total_flops_so_far": 8241428493940860.0,
      "budget_used_percent": 8.241428493940859
    },
    {
      "type": "training",
      "description": "Training step 693",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:14",
      "total_flops_so_far": 8253310610045052.0,
      "budget_used_percent": 8.253310610045052
    },
    {
      "type": "training",
      "description": "Training step 694",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:15",
      "total_flops_so_far": 8265192726149244.0,
      "budget_used_percent": 8.265192726149245
    },
    {
      "type": "training",
      "description": "Training step 695",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:15",
      "total_flops_so_far": 8277074842253436.0,
      "budget_used_percent": 8.277074842253436
    },
    {
      "type": "training",
      "description": "Training step 696",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:16",
      "total_flops_so_far": 8288956958357628.0,
      "budget_used_percent": 8.288956958357629
    },
    {
      "type": "training",
      "description": "Training step 697",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:16",
      "total_flops_so_far": 8300839074461820.0,
      "budget_used_percent": 8.30083907446182
    },
    {
      "type": "training",
      "description": "Training step 698",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:17",
      "total_flops_so_far": 8312721190566012.0,
      "budget_used_percent": 8.312721190566013
    },
    {
      "type": "training",
      "description": "Training step 699",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:18",
      "total_flops_so_far": 8324603306670204.0,
      "budget_used_percent": 8.324603306670204
    },
    {
      "type": "training",
      "description": "Training step 700",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:18",
      "total_flops_so_far": 8336485422774396.0,
      "budget_used_percent": 8.336485422774397
    },
    {
      "type": "training",
      "description": "Training step 701",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:19",
      "total_flops_so_far": 8348367538878588.0,
      "budget_used_percent": 8.34836753887859
    },
    {
      "type": "training",
      "description": "Training step 702",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:19",
      "total_flops_so_far": 8360249654982780.0,
      "budget_used_percent": 8.36024965498278
    },
    {
      "type": "training",
      "description": "Training step 703",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:20",
      "total_flops_so_far": 8372131771086972.0,
      "budget_used_percent": 8.372131771086972
    },
    {
      "type": "training",
      "description": "Training step 704",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:21",
      "total_flops_so_far": 8384013887191164.0,
      "budget_used_percent": 8.384013887191163
    },
    {
      "type": "training",
      "description": "Training step 705",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:21",
      "total_flops_so_far": 8395896003295356.0,
      "budget_used_percent": 8.395896003295356
    },
    {
      "type": "training",
      "description": "Training step 706",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:22",
      "total_flops_so_far": 8407778119399548.0,
      "budget_used_percent": 8.407778119399548
    },
    {
      "type": "training",
      "description": "Training step 707",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:22",
      "total_flops_so_far": 8419660235503740.0,
      "budget_used_percent": 8.41966023550374
    },
    {
      "type": "training",
      "description": "Training step 708",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:23",
      "total_flops_so_far": 8431542351607932.0,
      "budget_used_percent": 8.431542351607932
    },
    {
      "type": "training",
      "description": "Training step 709",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:24",
      "total_flops_so_far": 8443424467712124.0,
      "budget_used_percent": 8.443424467712124
    },
    {
      "type": "training",
      "description": "Training step 710",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:24",
      "total_flops_so_far": 8455306583816316.0,
      "budget_used_percent": 8.455306583816316
    },
    {
      "type": "training",
      "description": "Training step 711",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:25",
      "total_flops_so_far": 8467188699920508.0,
      "budget_used_percent": 8.467188699920507
    },
    {
      "type": "training",
      "description": "Training step 712",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:25",
      "total_flops_so_far": 8479070816024700.0,
      "budget_used_percent": 8.4790708160247
    },
    {
      "type": "training",
      "description": "Training step 713",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:26",
      "total_flops_so_far": 8490952932128892.0,
      "budget_used_percent": 8.490952932128891
    },
    {
      "type": "training",
      "description": "Training step 714",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:27",
      "total_flops_so_far": 8502835048233084.0,
      "budget_used_percent": 8.502835048233084
    },
    {
      "type": "training",
      "description": "Training step 715",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:27",
      "total_flops_so_far": 8514717164337276.0,
      "budget_used_percent": 8.514717164337277
    },
    {
      "type": "training",
      "description": "Training step 716",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:28",
      "total_flops_so_far": 8526599280441468.0,
      "budget_used_percent": 8.526599280441468
    },
    {
      "type": "training",
      "description": "Training step 717",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:28",
      "total_flops_so_far": 8538481396545660.0,
      "budget_used_percent": 8.53848139654566
    },
    {
      "type": "training",
      "description": "Training step 718",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:29",
      "total_flops_so_far": 8550363512649852.0,
      "budget_used_percent": 8.550363512649852
    },
    {
      "type": "training",
      "description": "Training step 719",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:29",
      "total_flops_so_far": 8562245628754044.0,
      "budget_used_percent": 8.562245628754043
    },
    {
      "type": "training",
      "description": "Training step 720",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:30",
      "total_flops_so_far": 8574127744858236.0,
      "budget_used_percent": 8.574127744858236
    },
    {
      "type": "training",
      "description": "Training step 721",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:31",
      "total_flops_so_far": 8586009860962428.0,
      "budget_used_percent": 8.586009860962427
    },
    {
      "type": "training",
      "description": "Training step 722",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:31",
      "total_flops_so_far": 8597891977066620.0,
      "budget_used_percent": 8.59789197706662
    },
    {
      "type": "training",
      "description": "Training step 723",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:32",
      "total_flops_so_far": 8609774093170812.0,
      "budget_used_percent": 8.609774093170811
    },
    {
      "type": "training",
      "description": "Training step 724",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:32",
      "total_flops_so_far": 8621656209275004.0,
      "budget_used_percent": 8.621656209275004
    },
    {
      "type": "training",
      "description": "Training step 725",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:33",
      "total_flops_so_far": 8633538325379196.0,
      "budget_used_percent": 8.633538325379195
    },
    {
      "type": "training",
      "description": "Training step 726",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:34",
      "total_flops_so_far": 8645420441483388.0,
      "budget_used_percent": 8.645420441483388
    },
    {
      "type": "training",
      "description": "Training step 727",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:34",
      "total_flops_so_far": 8657302557587580.0,
      "budget_used_percent": 8.657302557587581
    },
    {
      "type": "training",
      "description": "Training step 728",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:35",
      "total_flops_so_far": 8669184673691772.0,
      "budget_used_percent": 8.669184673691772
    },
    {
      "type": "training",
      "description": "Training step 729",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:35",
      "total_flops_so_far": 8681066789795964.0,
      "budget_used_percent": 8.681066789795965
    },
    {
      "type": "training",
      "description": "Training step 730",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:36",
      "total_flops_so_far": 8692948905900156.0,
      "budget_used_percent": 8.692948905900156
    },
    {
      "type": "training",
      "description": "Training step 731",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:37",
      "total_flops_so_far": 8704831022004348.0,
      "budget_used_percent": 8.704831022004347
    },
    {
      "type": "training",
      "description": "Training step 732",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:37",
      "total_flops_so_far": 8716713138108540.0,
      "budget_used_percent": 8.71671313810854
    },
    {
      "type": "training",
      "description": "Training step 733",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:38",
      "total_flops_so_far": 8728595254212732.0,
      "budget_used_percent": 8.728595254212731
    },
    {
      "type": "training",
      "description": "Training step 734",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:38",
      "total_flops_so_far": 8740477370316924.0,
      "budget_used_percent": 8.740477370316924
    },
    {
      "type": "training",
      "description": "Training step 735",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:39",
      "total_flops_so_far": 8752359486421116.0,
      "budget_used_percent": 8.752359486421115
    },
    {
      "type": "training",
      "description": "Training step 736",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:40",
      "total_flops_so_far": 8764241602525308.0,
      "budget_used_percent": 8.764241602525308
    },
    {
      "type": "training",
      "description": "Training step 737",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:40",
      "total_flops_so_far": 8776123718629500.0,
      "budget_used_percent": 8.7761237186295
    },
    {
      "type": "training",
      "description": "Training step 738",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:41",
      "total_flops_so_far": 8788005834733692.0,
      "budget_used_percent": 8.788005834733692
    },
    {
      "type": "training",
      "description": "Training step 739",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:41",
      "total_flops_so_far": 8799887950837884.0,
      "budget_used_percent": 8.799887950837885
    },
    {
      "type": "training",
      "description": "Training step 740",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:42",
      "total_flops_so_far": 8811770066942076.0,
      "budget_used_percent": 8.811770066942076
    },
    {
      "type": "training",
      "description": "Training step 741",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:42",
      "total_flops_so_far": 8823652183046268.0,
      "budget_used_percent": 8.823652183046269
    },
    {
      "type": "training",
      "description": "Training step 742",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:43",
      "total_flops_so_far": 8835534299150460.0,
      "budget_used_percent": 8.83553429915046
    },
    {
      "type": "training",
      "description": "Training step 743",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:44",
      "total_flops_so_far": 8847416415254652.0,
      "budget_used_percent": 8.847416415254653
    },
    {
      "type": "training",
      "description": "Training step 744",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:44",
      "total_flops_so_far": 8859298531358844.0,
      "budget_used_percent": 8.859298531358844
    },
    {
      "type": "training",
      "description": "Training step 745",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:45",
      "total_flops_so_far": 8871180647463036.0,
      "budget_used_percent": 8.871180647463035
    },
    {
      "type": "training",
      "description": "Training step 746",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:45",
      "total_flops_so_far": 8883062763567228.0,
      "budget_used_percent": 8.883062763567228
    },
    {
      "type": "training",
      "description": "Training step 747",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:46",
      "total_flops_so_far": 8894944879671420.0,
      "budget_used_percent": 8.894944879671419
    },
    {
      "type": "training",
      "description": "Training step 748",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:47",
      "total_flops_so_far": 8906826995775612.0,
      "budget_used_percent": 8.906826995775612
    },
    {
      "type": "training",
      "description": "Training step 749",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:47",
      "total_flops_so_far": 8918709111879804.0,
      "budget_used_percent": 8.918709111879803
    },
    {
      "type": "training",
      "description": "Training step 750",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:48",
      "total_flops_so_far": 8930591227983996.0,
      "budget_used_percent": 8.930591227983996
    },
    {
      "type": "training",
      "description": "Training step 751",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:48",
      "total_flops_so_far": 8942473344088188.0,
      "budget_used_percent": 8.942473344088189
    },
    {
      "type": "training",
      "description": "Training step 752",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:49",
      "total_flops_so_far": 8954355460192380.0,
      "budget_used_percent": 8.95435546019238
    },
    {
      "type": "training",
      "description": "Training step 753",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:50",
      "total_flops_so_far": 8966237576296572.0,
      "budget_used_percent": 8.966237576296573
    },
    {
      "type": "training",
      "description": "Training step 754",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:50",
      "total_flops_so_far": 8978119692400764.0,
      "budget_used_percent": 8.978119692400764
    },
    {
      "type": "training",
      "description": "Training step 755",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:51",
      "total_flops_so_far": 8990001808504956.0,
      "budget_used_percent": 8.990001808504957
    },
    {
      "type": "training",
      "description": "Training step 756",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:51",
      "total_flops_so_far": 9001883924609148.0,
      "budget_used_percent": 9.001883924609148
    },
    {
      "type": "training",
      "description": "Training step 757",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:52",
      "total_flops_so_far": 9013766040713340.0,
      "budget_used_percent": 9.01376604071334
    },
    {
      "type": "training",
      "description": "Training step 758",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:53",
      "total_flops_so_far": 9025648156817532.0,
      "budget_used_percent": 9.025648156817532
    },
    {
      "type": "training",
      "description": "Training step 759",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:53",
      "total_flops_so_far": 9037530272921724.0,
      "budget_used_percent": 9.037530272921725
    },
    {
      "type": "training",
      "description": "Training step 760",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:54",
      "total_flops_so_far": 9049412389025916.0,
      "budget_used_percent": 9.049412389025916
    },
    {
      "type": "training",
      "description": "Training step 761",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:54",
      "total_flops_so_far": 9061294505130108.0,
      "budget_used_percent": 9.061294505130107
    },
    {
      "type": "training",
      "description": "Training step 762",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:55",
      "total_flops_so_far": 9073176621234300.0,
      "budget_used_percent": 9.0731766212343
    },
    {
      "type": "training",
      "description": "Training step 763",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:56",
      "total_flops_so_far": 9085058737338492.0,
      "budget_used_percent": 9.085058737338493
    },
    {
      "type": "training",
      "description": "Training step 764",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:56",
      "total_flops_so_far": 9096940853442684.0,
      "budget_used_percent": 9.096940853442684
    },
    {
      "type": "training",
      "description": "Training step 765",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:57",
      "total_flops_so_far": 9108822969546876.0,
      "budget_used_percent": 9.108822969546877
    },
    {
      "type": "training",
      "description": "Training step 766",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:57",
      "total_flops_so_far": 9120705085651068.0,
      "budget_used_percent": 9.120705085651068
    },
    {
      "type": "training",
      "description": "Training step 767",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:58",
      "total_flops_so_far": 9132587201755260.0,
      "budget_used_percent": 9.13258720175526
    },
    {
      "type": "training",
      "description": "Training step 768",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:58",
      "total_flops_so_far": 9144469317859452.0,
      "budget_used_percent": 9.144469317859452
    },
    {
      "type": "training",
      "description": "Training step 769",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:12:59",
      "total_flops_so_far": 9156351433963644.0,
      "budget_used_percent": 9.156351433963644
    },
    {
      "type": "training",
      "description": "Training step 770",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:00",
      "total_flops_so_far": 9168233550067836.0,
      "budget_used_percent": 9.168233550067836
    },
    {
      "type": "training",
      "description": "Training step 771",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:00",
      "total_flops_so_far": 9180115666172028.0,
      "budget_used_percent": 9.180115666172028
    },
    {
      "type": "training",
      "description": "Training step 772",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:01",
      "total_flops_so_far": 9191997782276220.0,
      "budget_used_percent": 9.191997782276221
    },
    {
      "type": "training",
      "description": "Training step 773",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:01",
      "total_flops_so_far": 9203879898380412.0,
      "budget_used_percent": 9.203879898380412
    },
    {
      "type": "training",
      "description": "Training step 774",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:02",
      "total_flops_so_far": 9215762014484604.0,
      "budget_used_percent": 9.215762014484604
    },
    {
      "type": "training",
      "description": "Training step 775",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:03",
      "total_flops_so_far": 9227644130588796.0,
      "budget_used_percent": 9.227644130588795
    },
    {
      "type": "training",
      "description": "Training step 776",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:03",
      "total_flops_so_far": 9239526246692988.0,
      "budget_used_percent": 9.239526246692987
    },
    {
      "type": "training",
      "description": "Training step 777",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:04",
      "total_flops_so_far": 9251408362797180.0,
      "budget_used_percent": 9.25140836279718
    },
    {
      "type": "training",
      "description": "Training step 778",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:04",
      "total_flops_so_far": 9263290478901372.0,
      "budget_used_percent": 9.263290478901371
    },
    {
      "type": "training",
      "description": "Training step 779",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:05",
      "total_flops_so_far": 9275172595005564.0,
      "budget_used_percent": 9.275172595005564
    },
    {
      "type": "training",
      "description": "Training step 780",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:06",
      "total_flops_so_far": 9287054711109756.0,
      "budget_used_percent": 9.287054711109755
    },
    {
      "type": "training",
      "description": "Training step 781",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:06",
      "total_flops_so_far": 9298936827213948.0,
      "budget_used_percent": 9.298936827213948
    },
    {
      "type": "training",
      "description": "Training step 782",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:07",
      "total_flops_so_far": 9310818943318140.0,
      "budget_used_percent": 9.31081894331814
    },
    {
      "type": "training",
      "description": "Training step 783",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:07",
      "total_flops_so_far": 9322701059422332.0,
      "budget_used_percent": 9.322701059422332
    },
    {
      "type": "training",
      "description": "Training step 784",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:08",
      "total_flops_so_far": 9334583175526524.0,
      "budget_used_percent": 9.334583175526525
    },
    {
      "type": "training",
      "description": "Training step 785",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:09",
      "total_flops_so_far": 9346465291630716.0,
      "budget_used_percent": 9.346465291630716
    },
    {
      "type": "training",
      "description": "Training step 786",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:09",
      "total_flops_so_far": 9358347407734908.0,
      "budget_used_percent": 9.35834740773491
    },
    {
      "type": "training",
      "description": "Training step 787",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:10",
      "total_flops_so_far": 9370229523839100.0,
      "budget_used_percent": 9.3702295238391
    },
    {
      "type": "training",
      "description": "Training step 788",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:10",
      "total_flops_so_far": 9382111639943292.0,
      "budget_used_percent": 9.382111639943291
    },
    {
      "type": "training",
      "description": "Training step 789",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:11",
      "total_flops_so_far": 9393993756047484.0,
      "budget_used_percent": 9.393993756047484
    },
    {
      "type": "training",
      "description": "Training step 790",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:11",
      "total_flops_so_far": 9405875872151676.0,
      "budget_used_percent": 9.405875872151675
    },
    {
      "type": "training",
      "description": "Training step 791",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:12",
      "total_flops_so_far": 9417757988255868.0,
      "budget_used_percent": 9.417757988255868
    },
    {
      "type": "training",
      "description": "Training step 792",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:12",
      "total_flops_so_far": 9429640104360060.0,
      "budget_used_percent": 9.42964010436006
    },
    {
      "type": "training",
      "description": "Training step 793",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:13",
      "total_flops_so_far": 9441522220464252.0,
      "budget_used_percent": 9.441522220464252
    },
    {
      "type": "training",
      "description": "Training step 794",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:14",
      "total_flops_so_far": 9453404336568444.0,
      "budget_used_percent": 9.453404336568443
    },
    {
      "type": "training",
      "description": "Training step 795",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:14",
      "total_flops_so_far": 9465286452672636.0,
      "budget_used_percent": 9.465286452672636
    },
    {
      "type": "training",
      "description": "Training step 796",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:15",
      "total_flops_so_far": 9477168568776828.0,
      "budget_used_percent": 9.477168568776829
    },
    {
      "type": "training",
      "description": "Training step 797",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:15",
      "total_flops_so_far": 9489050684881020.0,
      "budget_used_percent": 9.48905068488102
    },
    {
      "type": "training",
      "description": "Training step 798",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:16",
      "total_flops_so_far": 9500932800985212.0,
      "budget_used_percent": 9.500932800985213
    },
    {
      "type": "training",
      "description": "Training step 799",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:17",
      "total_flops_so_far": 9512814917089404.0,
      "budget_used_percent": 9.512814917089404
    },
    {
      "type": "training",
      "description": "Training step 800",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:17",
      "total_flops_so_far": 9524697033193596.0,
      "budget_used_percent": 9.524697033193597
    },
    {
      "type": "training",
      "description": "Training step 801",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:18",
      "total_flops_so_far": 9536579149297788.0,
      "budget_used_percent": 9.536579149297788
    },
    {
      "type": "training",
      "description": "Training step 802",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:18",
      "total_flops_so_far": 9548461265401980.0,
      "budget_used_percent": 9.54846126540198
    },
    {
      "type": "training",
      "description": "Training step 803",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:19",
      "total_flops_so_far": 9560343381506172.0,
      "budget_used_percent": 9.560343381506172
    },
    {
      "type": "training",
      "description": "Training step 804",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:20",
      "total_flops_so_far": 9572225497610364.0,
      "budget_used_percent": 9.572225497610363
    },
    {
      "type": "training",
      "description": "Training step 805",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:20",
      "total_flops_so_far": 9584107613714556.0,
      "budget_used_percent": 9.584107613714556
    },
    {
      "type": "training",
      "description": "Training step 806",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:21",
      "total_flops_so_far": 9595989729818748.0,
      "budget_used_percent": 9.595989729818747
    },
    {
      "type": "training",
      "description": "Training step 807",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:21",
      "total_flops_so_far": 9607871845922940.0,
      "budget_used_percent": 9.60787184592294
    },
    {
      "type": "training",
      "description": "Training step 808",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:22",
      "total_flops_so_far": 9619753962027132.0,
      "budget_used_percent": 9.619753962027131
    },
    {
      "type": "training",
      "description": "Training step 809",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:23",
      "total_flops_so_far": 9631636078131324.0,
      "budget_used_percent": 9.631636078131324
    },
    {
      "type": "training",
      "description": "Training step 810",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:23",
      "total_flops_so_far": 9643518194235516.0,
      "budget_used_percent": 9.643518194235517
    },
    {
      "type": "training",
      "description": "Training step 811",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:24",
      "total_flops_so_far": 9655400310339708.0,
      "budget_used_percent": 9.655400310339708
    },
    {
      "type": "training",
      "description": "Training step 812",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:24",
      "total_flops_so_far": 9667282426443900.0,
      "budget_used_percent": 9.6672824264439
    },
    {
      "type": "training",
      "description": "Training step 813",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:25",
      "total_flops_so_far": 9679164542548092.0,
      "budget_used_percent": 9.679164542548092
    },
    {
      "type": "training",
      "description": "Training step 814",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:25",
      "total_flops_so_far": 9691046658652284.0,
      "budget_used_percent": 9.691046658652285
    },
    {
      "type": "training",
      "description": "Training step 815",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:26",
      "total_flops_so_far": 9702928774756476.0,
      "budget_used_percent": 9.702928774756476
    },
    {
      "type": "training",
      "description": "Training step 816",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:27",
      "total_flops_so_far": 9714810890860668.0,
      "budget_used_percent": 9.714810890860669
    },
    {
      "type": "training",
      "description": "Training step 817",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:27",
      "total_flops_so_far": 9726693006964860.0,
      "budget_used_percent": 9.72669300696486
    },
    {
      "type": "training",
      "description": "Training step 818",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:28",
      "total_flops_so_far": 9738575123069052.0,
      "budget_used_percent": 9.738575123069051
    },
    {
      "type": "training",
      "description": "Training step 819",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:28",
      "total_flops_so_far": 9750457239173244.0,
      "budget_used_percent": 9.750457239173244
    },
    {
      "type": "training",
      "description": "Training step 820",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:29",
      "total_flops_so_far": 9762339355277436.0,
      "budget_used_percent": 9.762339355277435
    },
    {
      "type": "training",
      "description": "Training step 821",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:30",
      "total_flops_so_far": 9774221471381628.0,
      "budget_used_percent": 9.774221471381628
    },
    {
      "type": "training",
      "description": "Training step 822",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:30",
      "total_flops_so_far": 9786103587485820.0,
      "budget_used_percent": 9.78610358748582
    },
    {
      "type": "training",
      "description": "Training step 823",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:31",
      "total_flops_so_far": 9797985703590012.0,
      "budget_used_percent": 9.797985703590012
    },
    {
      "type": "training",
      "description": "Training step 824",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:31",
      "total_flops_so_far": 9809867819694204.0,
      "budget_used_percent": 9.809867819694205
    },
    {
      "type": "training",
      "description": "Training step 825",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:32",
      "total_flops_so_far": 9821749935798396.0,
      "budget_used_percent": 9.821749935798396
    },
    {
      "type": "training",
      "description": "Training step 826",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:33",
      "total_flops_so_far": 9833632051902588.0,
      "budget_used_percent": 9.833632051902589
    },
    {
      "type": "training",
      "description": "Training step 827",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:33",
      "total_flops_so_far": 9845514168006780.0,
      "budget_used_percent": 9.84551416800678
    },
    {
      "type": "training",
      "description": "Training step 828",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:34",
      "total_flops_so_far": 9857396284110972.0,
      "budget_used_percent": 9.857396284110973
    },
    {
      "type": "training",
      "description": "Training step 829",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:34",
      "total_flops_so_far": 9869278400215164.0,
      "budget_used_percent": 9.869278400215165
    },
    {
      "type": "training",
      "description": "Training step 830",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:35",
      "total_flops_so_far": 9881160516319356.0,
      "budget_used_percent": 9.881160516319357
    },
    {
      "type": "training",
      "description": "Training step 831",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:36",
      "total_flops_so_far": 9893042632423548.0,
      "budget_used_percent": 9.893042632423548
    },
    {
      "type": "training",
      "description": "Training step 832",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:36",
      "total_flops_so_far": 9904924748527740.0,
      "budget_used_percent": 9.904924748527739
    },
    {
      "type": "training",
      "description": "Training step 833",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:37",
      "total_flops_so_far": 9916806864631932.0,
      "budget_used_percent": 9.916806864631932
    },
    {
      "type": "training",
      "description": "Training step 834",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:37",
      "total_flops_so_far": 9928688980736124.0,
      "budget_used_percent": 9.928688980736124
    },
    {
      "type": "training",
      "description": "Training step 835",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:38",
      "total_flops_so_far": 9940571096840316.0,
      "budget_used_percent": 9.940571096840316
    },
    {
      "type": "training",
      "description": "Training step 836",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:39",
      "total_flops_so_far": 9952453212944508.0,
      "budget_used_percent": 9.952453212944508
    },
    {
      "type": "training",
      "description": "Training step 837",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:39",
      "total_flops_so_far": 9964335329048700.0,
      "budget_used_percent": 9.9643353290487
    },
    {
      "type": "training",
      "description": "Training step 838",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:40",
      "total_flops_so_far": 9976217445152892.0,
      "budget_used_percent": 9.976217445152892
    },
    {
      "type": "training",
      "description": "Training step 839",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:40",
      "total_flops_so_far": 9988099561257084.0,
      "budget_used_percent": 9.988099561257084
    },
    {
      "type": "training",
      "description": "Training step 840",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:41",
      "total_flops_so_far": 9999981677361276.0,
      "budget_used_percent": 9.999981677361276
    },
    {
      "type": "training",
      "description": "Training step 841",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:42",
      "total_flops_so_far": 1.0011863793465468e+16,
      "budget_used_percent": 10.011863793465468
    },
    {
      "type": "training",
      "description": "Training step 842",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:42",
      "total_flops_so_far": 1.002374590956966e+16,
      "budget_used_percent": 10.02374590956966
    },
    {
      "type": "training",
      "description": "Training step 843",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:43",
      "total_flops_so_far": 1.0035628025673852e+16,
      "budget_used_percent": 10.035628025673853
    },
    {
      "type": "training",
      "description": "Training step 844",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:43",
      "total_flops_so_far": 1.0047510141778044e+16,
      "budget_used_percent": 10.047510141778044
    },
    {
      "type": "training",
      "description": "Training step 845",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:44",
      "total_flops_so_far": 1.0059392257882236e+16,
      "budget_used_percent": 10.059392257882235
    },
    {
      "type": "training",
      "description": "Training step 846",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:44",
      "total_flops_so_far": 1.0071274373986428e+16,
      "budget_used_percent": 10.071274373986427
    },
    {
      "type": "training",
      "description": "Training step 847",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:45",
      "total_flops_so_far": 1.008315649009062e+16,
      "budget_used_percent": 10.08315649009062
    },
    {
      "type": "training",
      "description": "Training step 848",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:46",
      "total_flops_so_far": 1.0095038606194812e+16,
      "budget_used_percent": 10.095038606194812
    },
    {
      "type": "training",
      "description": "Training step 849",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:46",
      "total_flops_so_far": 1.0106920722299004e+16,
      "budget_used_percent": 10.106920722299003
    },
    {
      "type": "training",
      "description": "Training step 850",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:47",
      "total_flops_so_far": 1.0118802838403196e+16,
      "budget_used_percent": 10.118802838403196
    },
    {
      "type": "training",
      "description": "Training step 851",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:47",
      "total_flops_so_far": 1.0130684954507388e+16,
      "budget_used_percent": 10.130684954507387
    },
    {
      "type": "training",
      "description": "Training step 852",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:48",
      "total_flops_so_far": 1.014256707061158e+16,
      "budget_used_percent": 10.14256707061158
    },
    {
      "type": "training",
      "description": "Training step 853",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:49",
      "total_flops_so_far": 1.0154449186715772e+16,
      "budget_used_percent": 10.154449186715771
    },
    {
      "type": "training",
      "description": "Training step 854",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:49",
      "total_flops_so_far": 1.0166331302819964e+16,
      "budget_used_percent": 10.166331302819964
    },
    {
      "type": "training",
      "description": "Training step 855",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:50",
      "total_flops_so_far": 1.0178213418924156e+16,
      "budget_used_percent": 10.178213418924157
    },
    {
      "type": "training",
      "description": "Training step 856",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:50",
      "total_flops_so_far": 1.0190095535028348e+16,
      "budget_used_percent": 10.190095535028348
    },
    {
      "type": "training",
      "description": "Training step 857",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:51",
      "total_flops_so_far": 1.020197765113254e+16,
      "budget_used_percent": 10.201977651132541
    },
    {
      "type": "training",
      "description": "Training step 858",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:52",
      "total_flops_so_far": 1.0213859767236732e+16,
      "budget_used_percent": 10.213859767236732
    },
    {
      "type": "training",
      "description": "Training step 859",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:52",
      "total_flops_so_far": 1.0225741883340924e+16,
      "budget_used_percent": 10.225741883340923
    },
    {
      "type": "training",
      "description": "Training step 860",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:53",
      "total_flops_so_far": 1.0237623999445116e+16,
      "budget_used_percent": 10.237623999445116
    },
    {
      "type": "training",
      "description": "Training step 861",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:53",
      "total_flops_so_far": 1.0249506115549308e+16,
      "budget_used_percent": 10.249506115549307
    },
    {
      "type": "training",
      "description": "Training step 862",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:54",
      "total_flops_so_far": 1.02613882316535e+16,
      "budget_used_percent": 10.2613882316535
    },
    {
      "type": "training",
      "description": "Training step 863",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:55",
      "total_flops_so_far": 1.0273270347757692e+16,
      "budget_used_percent": 10.273270347757691
    },
    {
      "type": "training",
      "description": "Training step 864",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:55",
      "total_flops_so_far": 1.0285152463861884e+16,
      "budget_used_percent": 10.285152463861884
    },
    {
      "type": "training",
      "description": "Training step 865",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:56",
      "total_flops_so_far": 1.0297034579966076e+16,
      "budget_used_percent": 10.297034579966075
    },
    {
      "type": "training",
      "description": "Training step 866",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:56",
      "total_flops_so_far": 1.0308916696070268e+16,
      "budget_used_percent": 10.308916696070268
    },
    {
      "type": "training",
      "description": "Training step 867",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:57",
      "total_flops_so_far": 1.032079881217446e+16,
      "budget_used_percent": 10.320798812174461
    },
    {
      "type": "training",
      "description": "Training step 868",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:58",
      "total_flops_so_far": 1.0332680928278652e+16,
      "budget_used_percent": 10.332680928278652
    },
    {
      "type": "training",
      "description": "Training step 869",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:58",
      "total_flops_so_far": 1.0344563044382844e+16,
      "budget_used_percent": 10.344563044382845
    },
    {
      "type": "training",
      "description": "Training step 870",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:59",
      "total_flops_so_far": 1.0356445160487036e+16,
      "budget_used_percent": 10.356445160487036
    },
    {
      "type": "training",
      "description": "Training step 871",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:13:59",
      "total_flops_so_far": 1.0368327276591228e+16,
      "budget_used_percent": 10.368327276591229
    },
    {
      "type": "training",
      "description": "Training step 872",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:00",
      "total_flops_so_far": 1.038020939269542e+16,
      "budget_used_percent": 10.38020939269542
    },
    {
      "type": "training",
      "description": "Training step 873",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:01",
      "total_flops_so_far": 1.0392091508799612e+16,
      "budget_used_percent": 10.392091508799613
    },
    {
      "type": "training",
      "description": "Training step 874",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:01",
      "total_flops_so_far": 1.0403973624903804e+16,
      "budget_used_percent": 10.403973624903804
    },
    {
      "type": "training",
      "description": "Training step 875",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:02",
      "total_flops_so_far": 1.0415855741007996e+16,
      "budget_used_percent": 10.415855741007995
    },
    {
      "type": "training",
      "description": "Training step 876",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:02",
      "total_flops_so_far": 1.0427737857112188e+16,
      "budget_used_percent": 10.427737857112188
    },
    {
      "type": "training",
      "description": "Training step 877",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:03",
      "total_flops_so_far": 1.043961997321638e+16,
      "budget_used_percent": 10.439619973216379
    },
    {
      "type": "training",
      "description": "Training step 878",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:04",
      "total_flops_so_far": 1.0451502089320572e+16,
      "budget_used_percent": 10.451502089320572
    },
    {
      "type": "training",
      "description": "Training step 879",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:04",
      "total_flops_so_far": 1.0463384205424764e+16,
      "budget_used_percent": 10.463384205424763
    },
    {
      "type": "training",
      "description": "Training step 880",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:05",
      "total_flops_so_far": 1.0475266321528956e+16,
      "budget_used_percent": 10.475266321528956
    },
    {
      "type": "training",
      "description": "Training step 881",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:05",
      "total_flops_so_far": 1.0487148437633148e+16,
      "budget_used_percent": 10.487148437633149
    },
    {
      "type": "training",
      "description": "Training step 882",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:06",
      "total_flops_so_far": 1.049903055373734e+16,
      "budget_used_percent": 10.49903055373734
    },
    {
      "type": "training",
      "description": "Training step 883",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:06",
      "total_flops_so_far": 1.0510912669841532e+16,
      "budget_used_percent": 10.510912669841533
    },
    {
      "type": "training",
      "description": "Training step 884",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:07",
      "total_flops_so_far": 1.0522794785945724e+16,
      "budget_used_percent": 10.522794785945724
    },
    {
      "type": "training",
      "description": "Training step 885",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:08",
      "total_flops_so_far": 1.0534676902049916e+16,
      "budget_used_percent": 10.534676902049917
    },
    {
      "type": "training",
      "description": "Training step 886",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:08",
      "total_flops_so_far": 1.0546559018154108e+16,
      "budget_used_percent": 10.546559018154108
    },
    {
      "type": "training",
      "description": "Training step 887",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:09",
      "total_flops_so_far": 1.05584411342583e+16,
      "budget_used_percent": 10.5584411342583
    },
    {
      "type": "training",
      "description": "Training step 888",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:09",
      "total_flops_so_far": 1.0570323250362492e+16,
      "budget_used_percent": 10.570323250362492
    },
    {
      "type": "training",
      "description": "Training step 889",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:10",
      "total_flops_so_far": 1.0582205366466684e+16,
      "budget_used_percent": 10.582205366466683
    },
    {
      "type": "training",
      "description": "Training step 890",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:11",
      "total_flops_so_far": 1.0594087482570876e+16,
      "budget_used_percent": 10.594087482570876
    },
    {
      "type": "training",
      "description": "Training step 891",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:11",
      "total_flops_so_far": 1.0605969598675068e+16,
      "budget_used_percent": 10.605969598675067
    },
    {
      "type": "training",
      "description": "Training step 892",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:12",
      "total_flops_so_far": 1.061785171477926e+16,
      "budget_used_percent": 10.61785171477926
    },
    {
      "type": "training",
      "description": "Training step 893",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:12",
      "total_flops_so_far": 1.0629733830883452e+16,
      "budget_used_percent": 10.629733830883453
    },
    {
      "type": "training",
      "description": "Training step 894",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:13",
      "total_flops_so_far": 1.0641615946987644e+16,
      "budget_used_percent": 10.641615946987644
    },
    {
      "type": "training",
      "description": "Training step 895",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:14",
      "total_flops_so_far": 1.0653498063091836e+16,
      "budget_used_percent": 10.653498063091837
    },
    {
      "type": "training",
      "description": "Training step 896",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:14",
      "total_flops_so_far": 1.0665380179196028e+16,
      "budget_used_percent": 10.665380179196028
    },
    {
      "type": "training",
      "description": "Training step 897",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:15",
      "total_flops_so_far": 1.067726229530022e+16,
      "budget_used_percent": 10.67726229530022
    },
    {
      "type": "training",
      "description": "Training step 898",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:15",
      "total_flops_so_far": 1.0689144411404412e+16,
      "budget_used_percent": 10.689144411404412
    },
    {
      "type": "training",
      "description": "Training step 899",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:16",
      "total_flops_so_far": 1.0701026527508604e+16,
      "budget_used_percent": 10.701026527508605
    },
    {
      "type": "training",
      "description": "Training step 900",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:17",
      "total_flops_so_far": 1.0712908643612796e+16,
      "budget_used_percent": 10.712908643612797
    },
    {
      "type": "training",
      "description": "Training step 901",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:17",
      "total_flops_so_far": 1.0724790759716988e+16,
      "budget_used_percent": 10.724790759716988
    },
    {
      "type": "training",
      "description": "Training step 902",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:18",
      "total_flops_so_far": 1.073667287582118e+16,
      "budget_used_percent": 10.73667287582118
    },
    {
      "type": "training",
      "description": "Training step 903",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:18",
      "total_flops_so_far": 1.0748554991925372e+16,
      "budget_used_percent": 10.74855499192537
    },
    {
      "type": "training",
      "description": "Training step 904",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:19",
      "total_flops_so_far": 1.0760437108029564e+16,
      "budget_used_percent": 10.760437108029564
    },
    {
      "type": "training",
      "description": "Training step 905",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:20",
      "total_flops_so_far": 1.0772319224133756e+16,
      "budget_used_percent": 10.772319224133756
    },
    {
      "type": "training",
      "description": "Training step 906",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:20",
      "total_flops_so_far": 1.0784201340237948e+16,
      "budget_used_percent": 10.784201340237948
    },
    {
      "type": "training",
      "description": "Training step 907",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:21",
      "total_flops_so_far": 1.079608345634214e+16,
      "budget_used_percent": 10.79608345634214
    },
    {
      "type": "training",
      "description": "Training step 908",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:21",
      "total_flops_so_far": 1.0807965572446332e+16,
      "budget_used_percent": 10.807965572446331
    },
    {
      "type": "training",
      "description": "Training step 909",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:22",
      "total_flops_so_far": 1.0819847688550524e+16,
      "budget_used_percent": 10.819847688550524
    },
    {
      "type": "training",
      "description": "Training step 910",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:23",
      "total_flops_so_far": 1.0831729804654716e+16,
      "budget_used_percent": 10.831729804654715
    },
    {
      "type": "training",
      "description": "Training step 911",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:23",
      "total_flops_so_far": 1.0843611920758908e+16,
      "budget_used_percent": 10.843611920758908
    },
    {
      "type": "training",
      "description": "Training step 912",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:24",
      "total_flops_so_far": 1.08554940368631e+16,
      "budget_used_percent": 10.8554940368631
    },
    {
      "type": "training",
      "description": "Training step 913",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:24",
      "total_flops_so_far": 1.0867376152967292e+16,
      "budget_used_percent": 10.867376152967292
    },
    {
      "type": "training",
      "description": "Training step 914",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:25",
      "total_flops_so_far": 1.0879258269071484e+16,
      "budget_used_percent": 10.879258269071485
    },
    {
      "type": "training",
      "description": "Training step 915",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:26",
      "total_flops_so_far": 1.0891140385175676e+16,
      "budget_used_percent": 10.891140385175676
    },
    {
      "type": "training",
      "description": "Training step 916",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:26",
      "total_flops_so_far": 1.0903022501279868e+16,
      "budget_used_percent": 10.903022501279867
    },
    {
      "type": "training",
      "description": "Training step 917",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:27",
      "total_flops_so_far": 1.091490461738406e+16,
      "budget_used_percent": 10.91490461738406
    },
    {
      "type": "training",
      "description": "Training step 918",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:27",
      "total_flops_so_far": 1.0926786733488252e+16,
      "budget_used_percent": 10.926786733488251
    },
    {
      "type": "training",
      "description": "Training step 919",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:28",
      "total_flops_so_far": 1.0938668849592444e+16,
      "budget_used_percent": 10.938668849592444
    },
    {
      "type": "training",
      "description": "Training step 920",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:29",
      "total_flops_so_far": 1.0950550965696636e+16,
      "budget_used_percent": 10.950550965696635
    },
    {
      "type": "training",
      "description": "Training step 921",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:29",
      "total_flops_so_far": 1.0962433081800828e+16,
      "budget_used_percent": 10.962433081800828
    },
    {
      "type": "training",
      "description": "Training step 922",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:30",
      "total_flops_so_far": 1.097431519790502e+16,
      "budget_used_percent": 10.97431519790502
    },
    {
      "type": "training",
      "description": "Training step 923",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:30",
      "total_flops_so_far": 1.0986197314009212e+16,
      "budget_used_percent": 10.986197314009212
    },
    {
      "type": "training",
      "description": "Training step 924",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:31",
      "total_flops_so_far": 1.0998079430113404e+16,
      "budget_used_percent": 10.998079430113403
    },
    {
      "type": "training",
      "description": "Training step 925",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:32",
      "total_flops_so_far": 1.1009961546217596e+16,
      "budget_used_percent": 11.009961546217596
    },
    {
      "type": "training",
      "description": "Training step 926",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:32",
      "total_flops_so_far": 1.1021843662321788e+16,
      "budget_used_percent": 11.021843662321789
    },
    {
      "type": "training",
      "description": "Training step 927",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:33",
      "total_flops_so_far": 1.103372577842598e+16,
      "budget_used_percent": 11.03372577842598
    },
    {
      "type": "training",
      "description": "Training step 928",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:33",
      "total_flops_so_far": 1.1045607894530172e+16,
      "budget_used_percent": 11.045607894530173
    },
    {
      "type": "training",
      "description": "Training step 929",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:34",
      "total_flops_so_far": 1.1057490010634364e+16,
      "budget_used_percent": 11.057490010634364
    },
    {
      "type": "training",
      "description": "Training step 930",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:35",
      "total_flops_so_far": 1.1069372126738556e+16,
      "budget_used_percent": 11.069372126738555
    },
    {
      "type": "training",
      "description": "Training step 931",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:35",
      "total_flops_so_far": 1.1081254242842748e+16,
      "budget_used_percent": 11.081254242842748
    },
    {
      "type": "training",
      "description": "Training step 932",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:36",
      "total_flops_so_far": 1.109313635894694e+16,
      "budget_used_percent": 11.09313635894694
    },
    {
      "type": "training",
      "description": "Training step 933",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:36",
      "total_flops_so_far": 1.1105018475051132e+16,
      "budget_used_percent": 11.105018475051132
    },
    {
      "type": "training",
      "description": "Training step 934",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:37",
      "total_flops_so_far": 1.1116900591155324e+16,
      "budget_used_percent": 11.116900591155323
    },
    {
      "type": "training",
      "description": "Training step 935",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:37",
      "total_flops_so_far": 1.1128782707259516e+16,
      "budget_used_percent": 11.128782707259516
    },
    {
      "type": "training",
      "description": "Training step 936",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:38",
      "total_flops_so_far": 1.1140664823363708e+16,
      "budget_used_percent": 11.140664823363707
    },
    {
      "type": "training",
      "description": "Training step 937",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:39",
      "total_flops_so_far": 1.11525469394679e+16,
      "budget_used_percent": 11.1525469394679
    },
    {
      "type": "training",
      "description": "Training step 938",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:39",
      "total_flops_so_far": 1.1164429055572092e+16,
      "budget_used_percent": 11.164429055572093
    },
    {
      "type": "training",
      "description": "Training step 939",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:40",
      "total_flops_so_far": 1.1176311171676284e+16,
      "budget_used_percent": 11.176311171676284
    },
    {
      "type": "training",
      "description": "Training step 940",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:40",
      "total_flops_so_far": 1.1188193287780476e+16,
      "budget_used_percent": 11.188193287780477
    },
    {
      "type": "training",
      "description": "Training step 941",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:41",
      "total_flops_so_far": 1.1200075403884668e+16,
      "budget_used_percent": 11.200075403884668
    },
    {
      "type": "training",
      "description": "Training step 942",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:42",
      "total_flops_so_far": 1.121195751998886e+16,
      "budget_used_percent": 11.21195751998886
    },
    {
      "type": "training",
      "description": "Training step 943",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:42",
      "total_flops_so_far": 1.1223839636093052e+16,
      "budget_used_percent": 11.223839636093052
    },
    {
      "type": "training",
      "description": "Training step 944",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:43",
      "total_flops_so_far": 1.1235721752197244e+16,
      "budget_used_percent": 11.235721752197245
    },
    {
      "type": "training",
      "description": "Training step 945",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:43",
      "total_flops_so_far": 1.1247603868301436e+16,
      "budget_used_percent": 11.247603868301436
    },
    {
      "type": "training",
      "description": "Training step 946",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:44",
      "total_flops_so_far": 1.1259485984405628e+16,
      "budget_used_percent": 11.259485984405627
    },
    {
      "type": "training",
      "description": "Training step 947",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:45",
      "total_flops_so_far": 1.127136810050982e+16,
      "budget_used_percent": 11.27136810050982
    },
    {
      "type": "training",
      "description": "Training step 948",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:45",
      "total_flops_so_far": 1.1283250216614012e+16,
      "budget_used_percent": 11.283250216614011
    },
    {
      "type": "training",
      "description": "Training step 949",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:46",
      "total_flops_so_far": 1.1295132332718204e+16,
      "budget_used_percent": 11.295132332718204
    },
    {
      "type": "training",
      "description": "Training step 950",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:46",
      "total_flops_so_far": 1.1307014448822396e+16,
      "budget_used_percent": 11.307014448822397
    },
    {
      "type": "training",
      "description": "Training step 951",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:47",
      "total_flops_so_far": 1.1318896564926588e+16,
      "budget_used_percent": 11.318896564926588
    },
    {
      "type": "training",
      "description": "Training step 952",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:48",
      "total_flops_so_far": 1.133077868103078e+16,
      "budget_used_percent": 11.33077868103078
    },
    {
      "type": "training",
      "description": "Training step 953",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:48",
      "total_flops_so_far": 1.1342660797134972e+16,
      "budget_used_percent": 11.342660797134972
    },
    {
      "type": "training",
      "description": "Training step 954",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:49",
      "total_flops_so_far": 1.1354542913239164e+16,
      "budget_used_percent": 11.354542913239165
    },
    {
      "type": "training",
      "description": "Training step 955",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:49",
      "total_flops_so_far": 1.1366425029343356e+16,
      "budget_used_percent": 11.366425029343356
    },
    {
      "type": "training",
      "description": "Training step 956",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:50",
      "total_flops_so_far": 1.1378307145447548e+16,
      "budget_used_percent": 11.378307145447549
    },
    {
      "type": "training",
      "description": "Training step 957",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:51",
      "total_flops_so_far": 1.139018926155174e+16,
      "budget_used_percent": 11.39018926155174
    },
    {
      "type": "training",
      "description": "Training step 958",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:51",
      "total_flops_so_far": 1.1402071377655932e+16,
      "budget_used_percent": 11.402071377655933
    },
    {
      "type": "training",
      "description": "Training step 959",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:52",
      "total_flops_so_far": 1.1413953493760124e+16,
      "budget_used_percent": 11.413953493760124
    },
    {
      "type": "training",
      "description": "Training step 960",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:52",
      "total_flops_so_far": 1.1425835609864316e+16,
      "budget_used_percent": 11.425835609864315
    },
    {
      "type": "training",
      "description": "Training step 961",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:53",
      "total_flops_so_far": 1.1437717725968508e+16,
      "budget_used_percent": 11.437717725968508
    },
    {
      "type": "training",
      "description": "Training step 962",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:54",
      "total_flops_so_far": 1.14495998420727e+16,
      "budget_used_percent": 11.4495998420727
    },
    {
      "type": "training",
      "description": "Training step 963",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:54",
      "total_flops_so_far": 1.1461481958176892e+16,
      "budget_used_percent": 11.461481958176892
    },
    {
      "type": "training",
      "description": "Training step 964",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:55",
      "total_flops_so_far": 1.1473364074281084e+16,
      "budget_used_percent": 11.473364074281085
    },
    {
      "type": "training",
      "description": "Training step 965",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:55",
      "total_flops_so_far": 1.1485246190385276e+16,
      "budget_used_percent": 11.485246190385276
    },
    {
      "type": "training",
      "description": "Training step 966",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:56",
      "total_flops_so_far": 1.1497128306489468e+16,
      "budget_used_percent": 11.497128306489468
    },
    {
      "type": "training",
      "description": "Training step 967",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:57",
      "total_flops_so_far": 1.150901042259366e+16,
      "budget_used_percent": 11.50901042259366
    },
    {
      "type": "training",
      "description": "Training step 968",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:57",
      "total_flops_so_far": 1.1520892538697852e+16,
      "budget_used_percent": 11.520892538697852
    },
    {
      "type": "training",
      "description": "Training step 969",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:58",
      "total_flops_so_far": 1.1532774654802044e+16,
      "budget_used_percent": 11.532774654802044
    },
    {
      "type": "training",
      "description": "Training step 970",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:58",
      "total_flops_so_far": 1.1544656770906236e+16,
      "budget_used_percent": 11.544656770906236
    },
    {
      "type": "training",
      "description": "Training step 971",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:14:59",
      "total_flops_so_far": 1.1556538887010428e+16,
      "budget_used_percent": 11.55653888701043
    },
    {
      "type": "training",
      "description": "Training step 972",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:00",
      "total_flops_so_far": 1.156842100311462e+16,
      "budget_used_percent": 11.56842100311462
    },
    {
      "type": "training",
      "description": "Training step 973",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:00",
      "total_flops_so_far": 1.1580303119218812e+16,
      "budget_used_percent": 11.580303119218812
    },
    {
      "type": "training",
      "description": "Training step 974",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:01",
      "total_flops_so_far": 1.1592185235323004e+16,
      "budget_used_percent": 11.592185235323003
    },
    {
      "type": "training",
      "description": "Training step 975",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:01",
      "total_flops_so_far": 1.1604067351427196e+16,
      "budget_used_percent": 11.604067351427195
    },
    {
      "type": "training",
      "description": "Training step 976",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:02",
      "total_flops_so_far": 1.1615949467531388e+16,
      "budget_used_percent": 11.615949467531388
    },
    {
      "type": "training",
      "description": "Training step 977",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:03",
      "total_flops_so_far": 1.162783158363558e+16,
      "budget_used_percent": 11.62783158363558
    },
    {
      "type": "training",
      "description": "Training step 978",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:03",
      "total_flops_so_far": 1.1639713699739772e+16,
      "budget_used_percent": 11.639713699739772
    },
    {
      "type": "training",
      "description": "Training step 979",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:04",
      "total_flops_so_far": 1.1651595815843964e+16,
      "budget_used_percent": 11.651595815843963
    },
    {
      "type": "training",
      "description": "Training step 980",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:04",
      "total_flops_so_far": 1.1663477931948156e+16,
      "budget_used_percent": 11.663477931948156
    },
    {
      "type": "training",
      "description": "Training step 981",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:05",
      "total_flops_so_far": 1.1675360048052348e+16,
      "budget_used_percent": 11.675360048052347
    },
    {
      "type": "training",
      "description": "Training step 982",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:06",
      "total_flops_so_far": 1.168724216415654e+16,
      "budget_used_percent": 11.68724216415654
    },
    {
      "type": "training",
      "description": "Training step 983",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:06",
      "total_flops_so_far": 1.1699124280260732e+16,
      "budget_used_percent": 11.699124280260733
    },
    {
      "type": "training",
      "description": "Training step 984",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:07",
      "total_flops_so_far": 1.1711006396364924e+16,
      "budget_used_percent": 11.711006396364924
    },
    {
      "type": "training",
      "description": "Training step 985",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:07",
      "total_flops_so_far": 1.1722888512469116e+16,
      "budget_used_percent": 11.722888512469117
    },
    {
      "type": "training",
      "description": "Training step 986",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:08",
      "total_flops_so_far": 1.1734770628573308e+16,
      "budget_used_percent": 11.734770628573308
    },
    {
      "type": "training",
      "description": "Training step 987",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:09",
      "total_flops_so_far": 1.17466527446775e+16,
      "budget_used_percent": 11.7466527446775
    },
    {
      "type": "training",
      "description": "Training step 988",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:09",
      "total_flops_so_far": 1.1758534860781692e+16,
      "budget_used_percent": 11.758534860781692
    },
    {
      "type": "training",
      "description": "Training step 989",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:10",
      "total_flops_so_far": 1.1770416976885884e+16,
      "budget_used_percent": 11.770416976885883
    },
    {
      "type": "training",
      "description": "Training step 990",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:10",
      "total_flops_so_far": 1.1782299092990076e+16,
      "budget_used_percent": 11.782299092990076
    },
    {
      "type": "training",
      "description": "Training step 991",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:11",
      "total_flops_so_far": 1.1794181209094268e+16,
      "budget_used_percent": 11.794181209094267
    },
    {
      "type": "training",
      "description": "Training step 992",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:12",
      "total_flops_so_far": 1.180606332519846e+16,
      "budget_used_percent": 11.80606332519846
    },
    {
      "type": "training",
      "description": "Training step 993",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:12",
      "total_flops_so_far": 1.1817945441302652e+16,
      "budget_used_percent": 11.817945441302651
    },
    {
      "type": "training",
      "description": "Training step 994",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:13",
      "total_flops_so_far": 1.1829827557406844e+16,
      "budget_used_percent": 11.829827557406844
    },
    {
      "type": "training",
      "description": "Training step 995",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:13",
      "total_flops_so_far": 1.1841709673511036e+16,
      "budget_used_percent": 11.841709673511037
    },
    {
      "type": "training",
      "description": "Training step 996",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:14",
      "total_flops_so_far": 1.1853591789615228e+16,
      "budget_used_percent": 11.853591789615228
    },
    {
      "type": "training",
      "description": "Training step 997",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:15",
      "total_flops_so_far": 1.186547390571942e+16,
      "budget_used_percent": 11.865473905719421
    },
    {
      "type": "training",
      "description": "Training step 998",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:15",
      "total_flops_so_far": 1.1877356021823612e+16,
      "budget_used_percent": 11.877356021823612
    },
    {
      "type": "training",
      "description": "Training step 999",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:16",
      "total_flops_so_far": 1.1889238137927804e+16,
      "budget_used_percent": 11.889238137927805
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:24",
      "total_flops_so_far": 1.188994876706566e+16,
      "budget_used_percent": 11.88994876706566
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 714333709232.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:31",
      "total_flops_so_far": 1.1890663100774892e+16,
      "budget_used_percent": 11.890663100774892
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:38",
      "total_flops_so_far": 1.1891375581838244e+16,
      "budget_used_percent": 11.891375581838243
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:46",
      "total_flops_so_far": 1.18920862109761e+16,
      "budget_used_percent": 11.8920862109761
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 713407296244.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:15:53",
      "total_flops_so_far": 1.1892799618272344e+16,
      "budget_used_percent": 11.892799618272344
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 5",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:01",
      "total_flops_so_far": 1.18935102474102e+16,
      "budget_used_percent": 11.8935102474102
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 6",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:08",
      "total_flops_so_far": 1.1894222728473552e+16,
      "budget_used_percent": 11.894222728473553
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 7",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:15",
      "total_flops_so_far": 1.1894935209536904e+16,
      "budget_used_percent": 11.894935209536904
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 8",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:23",
      "total_flops_so_far": 1.1895647690600256e+16,
      "budget_used_percent": 11.895647690600256
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 9",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:30",
      "total_flops_so_far": 1.1896360171663608e+16,
      "budget_used_percent": 11.89636017166361
    },
    {
      "type": "training",
      "description": "Training step 1000",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:30",
      "total_flops_so_far": 1.19082422877678e+16,
      "budget_used_percent": 11.9082422877678
    },
    {
      "type": "training",
      "description": "Training step 1001",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:31",
      "total_flops_so_far": 1.1920124403871992e+16,
      "budget_used_percent": 11.920124403871991
    },
    {
      "type": "training",
      "description": "Training step 1002",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:31",
      "total_flops_so_far": 1.1932006519976184e+16,
      "budget_used_percent": 11.932006519976184
    },
    {
      "type": "training",
      "description": "Training step 1003",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:32",
      "total_flops_so_far": 1.1943888636080376e+16,
      "budget_used_percent": 11.943888636080375
    },
    {
      "type": "training",
      "description": "Training step 1004",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:33",
      "total_flops_so_far": 1.1955770752184568e+16,
      "budget_used_percent": 11.955770752184568
    },
    {
      "type": "training",
      "description": "Training step 1005",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:33",
      "total_flops_so_far": 1.196765286828876e+16,
      "budget_used_percent": 11.96765286828876
    },
    {
      "type": "training",
      "description": "Training step 1006",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:34",
      "total_flops_so_far": 1.1979534984392952e+16,
      "budget_used_percent": 11.979534984392952
    },
    {
      "type": "training",
      "description": "Training step 1007",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:34",
      "total_flops_so_far": 1.1991417100497144e+16,
      "budget_used_percent": 11.991417100497143
    },
    {
      "type": "training",
      "description": "Training step 1008",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:35",
      "total_flops_so_far": 1.2003299216601336e+16,
      "budget_used_percent": 12.003299216601336
    },
    {
      "type": "training",
      "description": "Training step 1009",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:36",
      "total_flops_so_far": 1.2015181332705528e+16,
      "budget_used_percent": 12.015181332705529
    },
    {
      "type": "training",
      "description": "Training step 1010",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:36",
      "total_flops_so_far": 1.202706344880972e+16,
      "budget_used_percent": 12.02706344880972
    },
    {
      "type": "training",
      "description": "Training step 1011",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:37",
      "total_flops_so_far": 1.2038945564913912e+16,
      "budget_used_percent": 12.038945564913913
    },
    {
      "type": "training",
      "description": "Training step 1012",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:37",
      "total_flops_so_far": 1.2050827681018104e+16,
      "budget_used_percent": 12.050827681018104
    },
    {
      "type": "training",
      "description": "Training step 1013",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:38",
      "total_flops_so_far": 1.2062709797122296e+16,
      "budget_used_percent": 12.062709797122297
    },
    {
      "type": "training",
      "description": "Training step 1014",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:39",
      "total_flops_so_far": 1.2074591913226488e+16,
      "budget_used_percent": 12.074591913226488
    },
    {
      "type": "training",
      "description": "Training step 1015",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:39",
      "total_flops_so_far": 1.208647402933068e+16,
      "budget_used_percent": 12.08647402933068
    },
    {
      "type": "training",
      "description": "Training step 1016",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:40",
      "total_flops_so_far": 1.2098356145434872e+16,
      "budget_used_percent": 12.098356145434872
    },
    {
      "type": "training",
      "description": "Training step 1017",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:40",
      "total_flops_so_far": 1.2110238261539064e+16,
      "budget_used_percent": 12.110238261539063
    },
    {
      "type": "training",
      "description": "Training step 1018",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:41",
      "total_flops_so_far": 1.2122120377643256e+16,
      "budget_used_percent": 12.122120377643256
    },
    {
      "type": "training",
      "description": "Training step 1019",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:42",
      "total_flops_so_far": 1.2134002493747448e+16,
      "budget_used_percent": 12.134002493747447
    },
    {
      "type": "training",
      "description": "Training step 1020",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:42",
      "total_flops_so_far": 1.214588460985164e+16,
      "budget_used_percent": 12.14588460985164
    },
    {
      "type": "training",
      "description": "Training step 1021",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:43",
      "total_flops_so_far": 1.2157766725955832e+16,
      "budget_used_percent": 12.157766725955831
    },
    {
      "type": "training",
      "description": "Training step 1022",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:43",
      "total_flops_so_far": 1.2169648842060024e+16,
      "budget_used_percent": 12.169648842060024
    },
    {
      "type": "training",
      "description": "Training step 1023",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:44",
      "total_flops_so_far": 1.2181530958164216e+16,
      "budget_used_percent": 12.181530958164217
    },
    {
      "type": "training",
      "description": "Training step 1024",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:45",
      "total_flops_so_far": 1.2193413074268408e+16,
      "budget_used_percent": 12.193413074268408
    },
    {
      "type": "training",
      "description": "Training step 1025",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:45",
      "total_flops_so_far": 1.22052951903726e+16,
      "budget_used_percent": 12.2052951903726
    },
    {
      "type": "training",
      "description": "Training step 1026",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:46",
      "total_flops_so_far": 1.2217177306476792e+16,
      "budget_used_percent": 12.217177306476792
    },
    {
      "type": "training",
      "description": "Training step 1027",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:46",
      "total_flops_so_far": 1.2229059422580984e+16,
      "budget_used_percent": 12.229059422580985
    },
    {
      "type": "training",
      "description": "Training step 1028",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:47",
      "total_flops_so_far": 1.2240941538685176e+16,
      "budget_used_percent": 12.240941538685176
    },
    {
      "type": "training",
      "description": "Training step 1029",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:48",
      "total_flops_so_far": 1.2252823654789368e+16,
      "budget_used_percent": 12.252823654789367
    },
    {
      "type": "training",
      "description": "Training step 1030",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:48",
      "total_flops_so_far": 1.226470577089356e+16,
      "budget_used_percent": 12.26470577089356
    },
    {
      "type": "training",
      "description": "Training step 1031",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:49",
      "total_flops_so_far": 1.2276587886997752e+16,
      "budget_used_percent": 12.276587886997751
    },
    {
      "type": "training",
      "description": "Training step 1032",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:49",
      "total_flops_so_far": 1.2288470003101944e+16,
      "budget_used_percent": 12.288470003101944
    },
    {
      "type": "training",
      "description": "Training step 1033",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:50",
      "total_flops_so_far": 1.2300352119206136e+16,
      "budget_used_percent": 12.300352119206135
    },
    {
      "type": "training",
      "description": "Training step 1034",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:51",
      "total_flops_so_far": 1.2312234235310328e+16,
      "budget_used_percent": 12.312234235310328
    },
    {
      "type": "training",
      "description": "Training step 1035",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:51",
      "total_flops_so_far": 1.232411635141452e+16,
      "budget_used_percent": 12.32411635141452
    },
    {
      "type": "training",
      "description": "Training step 1036",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:52",
      "total_flops_so_far": 1.2335998467518712e+16,
      "budget_used_percent": 12.335998467518712
    },
    {
      "type": "training",
      "description": "Training step 1037",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:52",
      "total_flops_so_far": 1.2347880583622904e+16,
      "budget_used_percent": 12.347880583622905
    },
    {
      "type": "training",
      "description": "Training step 1038",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:53",
      "total_flops_so_far": 1.2359762699727096e+16,
      "budget_used_percent": 12.359762699727096
    },
    {
      "type": "training",
      "description": "Training step 1039",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:54",
      "total_flops_so_far": 1.2371644815831288e+16,
      "budget_used_percent": 12.371644815831289
    },
    {
      "type": "training",
      "description": "Training step 1040",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:54",
      "total_flops_so_far": 1.238352693193548e+16,
      "budget_used_percent": 12.38352693193548
    },
    {
      "type": "training",
      "description": "Training step 1041",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:55",
      "total_flops_so_far": 1.2395409048039672e+16,
      "budget_used_percent": 12.395409048039673
    },
    {
      "type": "training",
      "description": "Training step 1042",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:55",
      "total_flops_so_far": 1.2407291164143864e+16,
      "budget_used_percent": 12.407291164143865
    },
    {
      "type": "training",
      "description": "Training step 1043",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:56",
      "total_flops_so_far": 1.2419173280248056e+16,
      "budget_used_percent": 12.419173280248055
    },
    {
      "type": "training",
      "description": "Training step 1044",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:57",
      "total_flops_so_far": 1.2431055396352248e+16,
      "budget_used_percent": 12.431055396352248
    },
    {
      "type": "training",
      "description": "Training step 1045",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:57",
      "total_flops_so_far": 1.244293751245644e+16,
      "budget_used_percent": 12.442937512456439
    },
    {
      "type": "training",
      "description": "Training step 1046",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:58",
      "total_flops_so_far": 1.2454819628560632e+16,
      "budget_used_percent": 12.454819628560632
    },
    {
      "type": "training",
      "description": "Training step 1047",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:58",
      "total_flops_so_far": 1.2466701744664824e+16,
      "budget_used_percent": 12.466701744664825
    },
    {
      "type": "training",
      "description": "Training step 1048",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:16:59",
      "total_flops_so_far": 1.2478583860769016e+16,
      "budget_used_percent": 12.478583860769016
    },
    {
      "type": "training",
      "description": "Training step 1049",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:00",
      "total_flops_so_far": 1.2490465976873208e+16,
      "budget_used_percent": 12.490465976873208
    },
    {
      "type": "training",
      "description": "Training step 1050",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:00",
      "total_flops_so_far": 1.25023480929774e+16,
      "budget_used_percent": 12.5023480929774
    },
    {
      "type": "training",
      "description": "Training step 1051",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:01",
      "total_flops_so_far": 1.2514230209081592e+16,
      "budget_used_percent": 12.514230209081592
    },
    {
      "type": "training",
      "description": "Training step 1052",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:01",
      "total_flops_so_far": 1.2526112325185784e+16,
      "budget_used_percent": 12.526112325185784
    },
    {
      "type": "training",
      "description": "Training step 1053",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:02",
      "total_flops_so_far": 1.2537994441289976e+16,
      "budget_used_percent": 12.537994441289976
    },
    {
      "type": "training",
      "description": "Training step 1054",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:02",
      "total_flops_so_far": 1.2549876557394168e+16,
      "budget_used_percent": 12.549876557394168
    },
    {
      "type": "training",
      "description": "Training step 1055",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:03",
      "total_flops_so_far": 1.256175867349836e+16,
      "budget_used_percent": 12.56175867349836
    },
    {
      "type": "training",
      "description": "Training step 1056",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:03",
      "total_flops_so_far": 1.2573640789602552e+16,
      "budget_used_percent": 12.573640789602553
    },
    {
      "type": "training",
      "description": "Training step 1057",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:04",
      "total_flops_so_far": 1.2585522905706744e+16,
      "budget_used_percent": 12.585522905706744
    },
    {
      "type": "training",
      "description": "Training step 1058",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:05",
      "total_flops_so_far": 1.2597405021810936e+16,
      "budget_used_percent": 12.597405021810937
    },
    {
      "type": "training",
      "description": "Training step 1059",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:05",
      "total_flops_so_far": 1.2609287137915128e+16,
      "budget_used_percent": 12.609287137915128
    },
    {
      "type": "training",
      "description": "Training step 1060",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:06",
      "total_flops_so_far": 1.262116925401932e+16,
      "budget_used_percent": 12.621169254019321
    },
    {
      "type": "training",
      "description": "Training step 1061",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:06",
      "total_flops_so_far": 1.2633051370123512e+16,
      "budget_used_percent": 12.633051370123512
    },
    {
      "type": "training",
      "description": "Training step 1062",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:07",
      "total_flops_so_far": 1.2644933486227704e+16,
      "budget_used_percent": 12.644933486227705
    },
    {
      "type": "training",
      "description": "Training step 1063",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:08",
      "total_flops_so_far": 1.2656815602331896e+16,
      "budget_used_percent": 12.656815602331898
    },
    {
      "type": "training",
      "description": "Training step 1064",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:08",
      "total_flops_so_far": 1.2668697718436088e+16,
      "budget_used_percent": 12.66869771843609
    },
    {
      "type": "training",
      "description": "Training step 1065",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:09",
      "total_flops_so_far": 1.268057983454028e+16,
      "budget_used_percent": 12.680579834540278
    },
    {
      "type": "training",
      "description": "Training step 1066",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:09",
      "total_flops_so_far": 1.2692461950644472e+16,
      "budget_used_percent": 12.692461950644471
    },
    {
      "type": "training",
      "description": "Training step 1067",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:10",
      "total_flops_so_far": 1.2704344066748664e+16,
      "budget_used_percent": 12.704344066748662
    },
    {
      "type": "training",
      "description": "Training step 1068",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:11",
      "total_flops_so_far": 1.2716226182852856e+16,
      "budget_used_percent": 12.716226182852855
    },
    {
      "type": "training",
      "description": "Training step 1069",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:12",
      "total_flops_so_far": 1.2728108298957048e+16,
      "budget_used_percent": 12.728108298957046
    },
    {
      "type": "training",
      "description": "Training step 1070",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:12",
      "total_flops_so_far": 1.273999041506124e+16,
      "budget_used_percent": 12.73999041506124
    },
    {
      "type": "training",
      "description": "Training step 1071",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:13",
      "total_flops_so_far": 1.2751872531165432e+16,
      "budget_used_percent": 12.751872531165432
    },
    {
      "type": "training",
      "description": "Training step 1072",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:13",
      "total_flops_so_far": 1.2763754647269624e+16,
      "budget_used_percent": 12.763754647269623
    },
    {
      "type": "training",
      "description": "Training step 1073",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:14",
      "total_flops_so_far": 1.2775636763373816e+16,
      "budget_used_percent": 12.775636763373816
    },
    {
      "type": "training",
      "description": "Training step 1074",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:15",
      "total_flops_so_far": 1.2787518879478008e+16,
      "budget_used_percent": 12.787518879478007
    },
    {
      "type": "training",
      "description": "Training step 1075",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:15",
      "total_flops_so_far": 1.27994009955822e+16,
      "budget_used_percent": 12.7994009955822
    },
    {
      "type": "training",
      "description": "Training step 1076",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:16",
      "total_flops_so_far": 1.2811283111686392e+16,
      "budget_used_percent": 12.811283111686391
    },
    {
      "type": "training",
      "description": "Training step 1077",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:16",
      "total_flops_so_far": 1.2823165227790584e+16,
      "budget_used_percent": 12.823165227790584
    },
    {
      "type": "training",
      "description": "Training step 1078",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:17",
      "total_flops_so_far": 1.2835047343894776e+16,
      "budget_used_percent": 12.835047343894775
    },
    {
      "type": "training",
      "description": "Training step 1079",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:18",
      "total_flops_so_far": 1.2846929459998968e+16,
      "budget_used_percent": 12.846929459998968
    },
    {
      "type": "training",
      "description": "Training step 1080",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:18",
      "total_flops_so_far": 1.285881157610316e+16,
      "budget_used_percent": 12.858811576103161
    },
    {
      "type": "training",
      "description": "Training step 1081",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:19",
      "total_flops_so_far": 1.2870693692207352e+16,
      "budget_used_percent": 12.870693692207352
    },
    {
      "type": "training",
      "description": "Training step 1082",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:19",
      "total_flops_so_far": 1.2882575808311544e+16,
      "budget_used_percent": 12.882575808311545
    },
    {
      "type": "training",
      "description": "Training step 1083",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:20",
      "total_flops_so_far": 1.2894457924415736e+16,
      "budget_used_percent": 12.894457924415736
    },
    {
      "type": "training",
      "description": "Training step 1084",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:21",
      "total_flops_so_far": 1.2906340040519928e+16,
      "budget_used_percent": 12.906340040519929
    },
    {
      "type": "training",
      "description": "Training step 1085",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:21",
      "total_flops_so_far": 1.291822215662412e+16,
      "budget_used_percent": 12.91822215662412
    },
    {
      "type": "training",
      "description": "Training step 1086",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:22",
      "total_flops_so_far": 1.2930104272728312e+16,
      "budget_used_percent": 12.930104272728313
    },
    {
      "type": "training",
      "description": "Training step 1087",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:22",
      "total_flops_so_far": 1.2941986388832504e+16,
      "budget_used_percent": 12.941986388832504
    },
    {
      "type": "training",
      "description": "Training step 1088",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:23",
      "total_flops_so_far": 1.2953868504936696e+16,
      "budget_used_percent": 12.953868504936697
    },
    {
      "type": "training",
      "description": "Training step 1089",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:24",
      "total_flops_so_far": 1.2965750621040888e+16,
      "budget_used_percent": 12.96575062104089
    },
    {
      "type": "training",
      "description": "Training step 1090",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:24",
      "total_flops_so_far": 1.297763273714508e+16,
      "budget_used_percent": 12.97763273714508
    },
    {
      "type": "training",
      "description": "Training step 1091",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:25",
      "total_flops_so_far": 1.2989514853249272e+16,
      "budget_used_percent": 12.989514853249274
    },
    {
      "type": "training",
      "description": "Training step 1092",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:25",
      "total_flops_so_far": 1.3001396969353464e+16,
      "budget_used_percent": 13.001396969353465
    },
    {
      "type": "training",
      "description": "Training step 1093",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:26",
      "total_flops_so_far": 1.3013279085457656e+16,
      "budget_used_percent": 13.013279085457654
    },
    {
      "type": "training",
      "description": "Training step 1094",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:27",
      "total_flops_so_far": 1.3025161201561848e+16,
      "budget_used_percent": 13.025161201561847
    },
    {
      "type": "training",
      "description": "Training step 1095",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:27",
      "total_flops_so_far": 1.303704331766604e+16,
      "budget_used_percent": 13.037043317666038
    },
    {
      "type": "training",
      "description": "Training step 1096",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:28",
      "total_flops_so_far": 1.3048925433770232e+16,
      "budget_used_percent": 13.048925433770231
    },
    {
      "type": "training",
      "description": "Training step 1097",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:28",
      "total_flops_so_far": 1.3060807549874424e+16,
      "budget_used_percent": 13.060807549874424
    },
    {
      "type": "training",
      "description": "Training step 1098",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:29",
      "total_flops_so_far": 1.3072689665978616e+16,
      "budget_used_percent": 13.072689665978615
    },
    {
      "type": "training",
      "description": "Training step 1099",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:30",
      "total_flops_so_far": 1.3084571782082808e+16,
      "budget_used_percent": 13.084571782082808
    },
    {
      "type": "training",
      "description": "Training step 1100",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:30",
      "total_flops_so_far": 1.3096453898187e+16,
      "budget_used_percent": 13.096453898186999
    },
    {
      "type": "training",
      "description": "Training step 1101",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:31",
      "total_flops_so_far": 1.3108336014291192e+16,
      "budget_used_percent": 13.108336014291192
    },
    {
      "type": "training",
      "description": "Training step 1102",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:31",
      "total_flops_so_far": 1.3120218130395384e+16,
      "budget_used_percent": 13.120218130395383
    },
    {
      "type": "training",
      "description": "Training step 1103",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:32",
      "total_flops_so_far": 1.3132100246499576e+16,
      "budget_used_percent": 13.132100246499576
    },
    {
      "type": "training",
      "description": "Training step 1104",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:33",
      "total_flops_so_far": 1.3143982362603768e+16,
      "budget_used_percent": 13.143982362603769
    },
    {
      "type": "training",
      "description": "Training step 1105",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:33",
      "total_flops_so_far": 1.315586447870796e+16,
      "budget_used_percent": 13.15586447870796
    },
    {
      "type": "training",
      "description": "Training step 1106",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:34",
      "total_flops_so_far": 1.3167746594812152e+16,
      "budget_used_percent": 13.167746594812153
    },
    {
      "type": "training",
      "description": "Training step 1107",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:34",
      "total_flops_so_far": 1.3179628710916344e+16,
      "budget_used_percent": 13.179628710916344
    },
    {
      "type": "training",
      "description": "Training step 1108",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:35",
      "total_flops_so_far": 1.3191510827020536e+16,
      "budget_used_percent": 13.191510827020537
    },
    {
      "type": "training",
      "description": "Training step 1109",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:36",
      "total_flops_so_far": 1.3203392943124728e+16,
      "budget_used_percent": 13.203392943124728
    },
    {
      "type": "training",
      "description": "Training step 1110",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:36",
      "total_flops_so_far": 1.321527505922892e+16,
      "budget_used_percent": 13.21527505922892
    },
    {
      "type": "training",
      "description": "Training step 1111",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:37",
      "total_flops_so_far": 1.3227157175333112e+16,
      "budget_used_percent": 13.227157175333112
    },
    {
      "type": "training",
      "description": "Training step 1112",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:37",
      "total_flops_so_far": 1.3239039291437304e+16,
      "budget_used_percent": 13.239039291437305
    },
    {
      "type": "training",
      "description": "Training step 1113",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:38",
      "total_flops_so_far": 1.3250921407541496e+16,
      "budget_used_percent": 13.250921407541497
    },
    {
      "type": "training",
      "description": "Training step 1114",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:39",
      "total_flops_so_far": 1.3262803523645688e+16,
      "budget_used_percent": 13.262803523645688
    },
    {
      "type": "training",
      "description": "Training step 1115",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:39",
      "total_flops_so_far": 1.327468563974988e+16,
      "budget_used_percent": 13.274685639749881
    },
    {
      "type": "training",
      "description": "Training step 1116",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:40",
      "total_flops_so_far": 1.3286567755854072e+16,
      "budget_used_percent": 13.286567755854072
    },
    {
      "type": "training",
      "description": "Training step 1117",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:40",
      "total_flops_so_far": 1.3298449871958264e+16,
      "budget_used_percent": 13.298449871958265
    },
    {
      "type": "training",
      "description": "Training step 1118",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:41",
      "total_flops_so_far": 1.3310331988062456e+16,
      "budget_used_percent": 13.310331988062456
    },
    {
      "type": "training",
      "description": "Training step 1119",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:42",
      "total_flops_so_far": 1.3322214104166648e+16,
      "budget_used_percent": 13.32221410416665
    },
    {
      "type": "training",
      "description": "Training step 1120",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:42",
      "total_flops_so_far": 1.333409622027084e+16,
      "budget_used_percent": 13.33409622027084
    },
    {
      "type": "training",
      "description": "Training step 1121",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:43",
      "total_flops_so_far": 1.3345978336375032e+16,
      "budget_used_percent": 13.34597833637503
    },
    {
      "type": "training",
      "description": "Training step 1122",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:43",
      "total_flops_so_far": 1.3357860452479224e+16,
      "budget_used_percent": 13.357860452479223
    },
    {
      "type": "training",
      "description": "Training step 1123",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:44",
      "total_flops_so_far": 1.3369742568583416e+16,
      "budget_used_percent": 13.369742568583415
    },
    {
      "type": "training",
      "description": "Training step 1124",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:45",
      "total_flops_so_far": 1.3381624684687608e+16,
      "budget_used_percent": 13.381624684687607
    },
    {
      "type": "training",
      "description": "Training step 1125",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:45",
      "total_flops_so_far": 1.33935068007918e+16,
      "budget_used_percent": 13.3935068007918
    },
    {
      "type": "training",
      "description": "Training step 1126",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:46",
      "total_flops_so_far": 1.3405388916895992e+16,
      "budget_used_percent": 13.40538891689599
    },
    {
      "type": "training",
      "description": "Training step 1127",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:46",
      "total_flops_so_far": 1.3417271033000184e+16,
      "budget_used_percent": 13.417271033000183
    },
    {
      "type": "training",
      "description": "Training step 1128",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:47",
      "total_flops_so_far": 1.3429153149104376e+16,
      "budget_used_percent": 13.429153149104375
    },
    {
      "type": "training",
      "description": "Training step 1129",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:48",
      "total_flops_so_far": 1.3441035265208568e+16,
      "budget_used_percent": 13.441035265208567
    },
    {
      "type": "training",
      "description": "Training step 1130",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:48",
      "total_flops_so_far": 1.345291738131276e+16,
      "budget_used_percent": 13.45291738131276
    },
    {
      "type": "training",
      "description": "Training step 1131",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:49",
      "total_flops_so_far": 1.3464799497416952e+16,
      "budget_used_percent": 13.464799497416951
    },
    {
      "type": "training",
      "description": "Training step 1132",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:49",
      "total_flops_so_far": 1.3476681613521144e+16,
      "budget_used_percent": 13.476681613521144
    },
    {
      "type": "training",
      "description": "Training step 1133",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:50",
      "total_flops_so_far": 1.3488563729625336e+16,
      "budget_used_percent": 13.488563729625335
    },
    {
      "type": "training",
      "description": "Training step 1134",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:51",
      "total_flops_so_far": 1.3500445845729528e+16,
      "budget_used_percent": 13.500445845729528
    },
    {
      "type": "training",
      "description": "Training step 1135",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:51",
      "total_flops_so_far": 1.351232796183372e+16,
      "budget_used_percent": 13.51232796183372
    },
    {
      "type": "training",
      "description": "Training step 1136",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:52",
      "total_flops_so_far": 1.3524210077937912e+16,
      "budget_used_percent": 13.524210077937912
    },
    {
      "type": "training",
      "description": "Training step 1137",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:52",
      "total_flops_so_far": 1.3536092194042104e+16,
      "budget_used_percent": 13.536092194042105
    },
    {
      "type": "training",
      "description": "Training step 1138",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:53",
      "total_flops_so_far": 1.3547974310146296e+16,
      "budget_used_percent": 13.547974310146296
    },
    {
      "type": "training",
      "description": "Training step 1139",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:54",
      "total_flops_so_far": 1.3559856426250488e+16,
      "budget_used_percent": 13.559856426250489
    },
    {
      "type": "training",
      "description": "Training step 1140",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:54",
      "total_flops_so_far": 1.357173854235468e+16,
      "budget_used_percent": 13.57173854235468
    },
    {
      "type": "training",
      "description": "Training step 1141",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:55",
      "total_flops_so_far": 1.3583620658458872e+16,
      "budget_used_percent": 13.583620658458873
    },
    {
      "type": "training",
      "description": "Training step 1142",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:55",
      "total_flops_so_far": 1.3595502774563064e+16,
      "budget_used_percent": 13.595502774563064
    },
    {
      "type": "training",
      "description": "Training step 1143",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:56",
      "total_flops_so_far": 1.3607384890667256e+16,
      "budget_used_percent": 13.607384890667257
    },
    {
      "type": "training",
      "description": "Training step 1144",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:57",
      "total_flops_so_far": 1.3619267006771448e+16,
      "budget_used_percent": 13.619267006771448
    },
    {
      "type": "training",
      "description": "Training step 1145",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:57",
      "total_flops_so_far": 1.363114912287564e+16,
      "budget_used_percent": 13.631149122875641
    },
    {
      "type": "training",
      "description": "Training step 1146",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:58",
      "total_flops_so_far": 1.3643031238979832e+16,
      "budget_used_percent": 13.643031238979834
    },
    {
      "type": "training",
      "description": "Training step 1147",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:58",
      "total_flops_so_far": 1.3654913355084024e+16,
      "budget_used_percent": 13.654913355084025
    },
    {
      "type": "training",
      "description": "Training step 1148",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:17:59",
      "total_flops_so_far": 1.3666795471188216e+16,
      "budget_used_percent": 13.666795471188218
    },
    {
      "type": "training",
      "description": "Training step 1149",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:00",
      "total_flops_so_far": 1.3678677587292408e+16,
      "budget_used_percent": 13.678677587292409
    },
    {
      "type": "training",
      "description": "Training step 1150",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:00",
      "total_flops_so_far": 1.36905597033966e+16,
      "budget_used_percent": 13.690559703396598
    },
    {
      "type": "training",
      "description": "Training step 1151",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:01",
      "total_flops_so_far": 1.3702441819500792e+16,
      "budget_used_percent": 13.702441819500791
    },
    {
      "type": "training",
      "description": "Training step 1152",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:01",
      "total_flops_so_far": 1.3714323935604984e+16,
      "budget_used_percent": 13.714323935604982
    },
    {
      "type": "training",
      "description": "Training step 1153",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:02",
      "total_flops_so_far": 1.3726206051709176e+16,
      "budget_used_percent": 13.726206051709175
    },
    {
      "type": "training",
      "description": "Training step 1154",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:03",
      "total_flops_so_far": 1.3738088167813368e+16,
      "budget_used_percent": 13.738088167813366
    },
    {
      "type": "training",
      "description": "Training step 1155",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:03",
      "total_flops_so_far": 1.374997028391756e+16,
      "budget_used_percent": 13.749970283917559
    },
    {
      "type": "training",
      "description": "Training step 1156",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:04",
      "total_flops_so_far": 1.3761852400021752e+16,
      "budget_used_percent": 13.761852400021752
    },
    {
      "type": "training",
      "description": "Training step 1157",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:04",
      "total_flops_so_far": 1.3773734516125944e+16,
      "budget_used_percent": 13.773734516125943
    },
    {
      "type": "training",
      "description": "Training step 1158",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:05",
      "total_flops_so_far": 1.3785616632230136e+16,
      "budget_used_percent": 13.785616632230136
    },
    {
      "type": "training",
      "description": "Training step 1159",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:06",
      "total_flops_so_far": 1.3797498748334328e+16,
      "budget_used_percent": 13.797498748334327
    },
    {
      "type": "training",
      "description": "Training step 1160",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:06",
      "total_flops_so_far": 1.380938086443852e+16,
      "budget_used_percent": 13.80938086443852
    },
    {
      "type": "training",
      "description": "Training step 1161",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:07",
      "total_flops_so_far": 1.3821262980542712e+16,
      "budget_used_percent": 13.821262980542711
    },
    {
      "type": "training",
      "description": "Training step 1162",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:07",
      "total_flops_so_far": 1.3833145096646904e+16,
      "budget_used_percent": 13.833145096646904
    },
    {
      "type": "training",
      "description": "Training step 1163",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:08",
      "total_flops_so_far": 1.3845027212751096e+16,
      "budget_used_percent": 13.845027212751097
    },
    {
      "type": "training",
      "description": "Training step 1164",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:09",
      "total_flops_so_far": 1.3856909328855288e+16,
      "budget_used_percent": 13.856909328855288
    },
    {
      "type": "training",
      "description": "Training step 1165",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:09",
      "total_flops_so_far": 1.386879144495948e+16,
      "budget_used_percent": 13.86879144495948
    },
    {
      "type": "training",
      "description": "Training step 1166",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:10",
      "total_flops_so_far": 1.3880673561063672e+16,
      "budget_used_percent": 13.880673561063672
    },
    {
      "type": "training",
      "description": "Training step 1167",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:10",
      "total_flops_so_far": 1.3892555677167864e+16,
      "budget_used_percent": 13.892555677167865
    },
    {
      "type": "training",
      "description": "Training step 1168",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:11",
      "total_flops_so_far": 1.3904437793272056e+16,
      "budget_used_percent": 13.904437793272056
    },
    {
      "type": "training",
      "description": "Training step 1169",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:12",
      "total_flops_so_far": 1.3916319909376248e+16,
      "budget_used_percent": 13.916319909376249
    },
    {
      "type": "training",
      "description": "Training step 1170",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:12",
      "total_flops_so_far": 1.392820202548044e+16,
      "budget_used_percent": 13.928202025480442
    },
    {
      "type": "training",
      "description": "Training step 1171",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:13",
      "total_flops_so_far": 1.3940084141584632e+16,
      "budget_used_percent": 13.940084141584633
    },
    {
      "type": "training",
      "description": "Training step 1172",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:13",
      "total_flops_so_far": 1.3951966257688824e+16,
      "budget_used_percent": 13.951966257688825
    },
    {
      "type": "training",
      "description": "Training step 1173",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:14",
      "total_flops_so_far": 1.3963848373793016e+16,
      "budget_used_percent": 13.963848373793017
    },
    {
      "type": "training",
      "description": "Training step 1174",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:15",
      "total_flops_so_far": 1.3975730489897208e+16,
      "budget_used_percent": 13.97573048989721
    },
    {
      "type": "training",
      "description": "Training step 1175",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:15",
      "total_flops_so_far": 1.39876126060014e+16,
      "budget_used_percent": 13.9876126060014
    },
    {
      "type": "training",
      "description": "Training step 1176",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:16",
      "total_flops_so_far": 1.3999494722105592e+16,
      "budget_used_percent": 13.999494722105593
    },
    {
      "type": "training",
      "description": "Training step 1177",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:16",
      "total_flops_so_far": 1.4011376838209784e+16,
      "budget_used_percent": 14.011376838209785
    },
    {
      "type": "training",
      "description": "Training step 1178",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:17",
      "total_flops_so_far": 1.4023258954313976e+16,
      "budget_used_percent": 14.023258954313974
    },
    {
      "type": "training",
      "description": "Training step 1179",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:18",
      "total_flops_so_far": 1.4035141070418168e+16,
      "budget_used_percent": 14.035141070418167
    },
    {
      "type": "training",
      "description": "Training step 1180",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:18",
      "total_flops_so_far": 1.404702318652236e+16,
      "budget_used_percent": 14.04702318652236
    },
    {
      "type": "training",
      "description": "Training step 1181",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:19",
      "total_flops_so_far": 1.4058905302626552e+16,
      "budget_used_percent": 14.05890530262655
    },
    {
      "type": "training",
      "description": "Training step 1182",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:19",
      "total_flops_so_far": 1.4070787418730744e+16,
      "budget_used_percent": 14.070787418730744
    },
    {
      "type": "training",
      "description": "Training step 1183",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:20",
      "total_flops_so_far": 1.4082669534834936e+16,
      "budget_used_percent": 14.082669534834935
    },
    {
      "type": "training",
      "description": "Training step 1184",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:21",
      "total_flops_so_far": 1.4094551650939128e+16,
      "budget_used_percent": 14.094551650939128
    },
    {
      "type": "training",
      "description": "Training step 1185",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:21",
      "total_flops_so_far": 1.410643376704332e+16,
      "budget_used_percent": 14.106433767043319
    },
    {
      "type": "training",
      "description": "Training step 1186",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:22",
      "total_flops_so_far": 1.4118315883147512e+16,
      "budget_used_percent": 14.118315883147512
    },
    {
      "type": "training",
      "description": "Training step 1187",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:23",
      "total_flops_so_far": 1.4130197999251704e+16,
      "budget_used_percent": 14.130197999251703
    },
    {
      "type": "training",
      "description": "Training step 1188",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:23",
      "total_flops_so_far": 1.4142080115355896e+16,
      "budget_used_percent": 14.142080115355895
    },
    {
      "type": "training",
      "description": "Training step 1189",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:24",
      "total_flops_so_far": 1.4153962231460088e+16,
      "budget_used_percent": 14.153962231460088
    },
    {
      "type": "training",
      "description": "Training step 1190",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:24",
      "total_flops_so_far": 1.416584434756428e+16,
      "budget_used_percent": 14.16584434756428
    },
    {
      "type": "training",
      "description": "Training step 1191",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:25",
      "total_flops_so_far": 1.4177726463668472e+16,
      "budget_used_percent": 14.177726463668472
    },
    {
      "type": "training",
      "description": "Training step 1192",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:26",
      "total_flops_so_far": 1.4189608579772664e+16,
      "budget_used_percent": 14.189608579772663
    },
    {
      "type": "training",
      "description": "Training step 1193",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:26",
      "total_flops_so_far": 1.4201490695876856e+16,
      "budget_used_percent": 14.201490695876856
    },
    {
      "type": "training",
      "description": "Training step 1194",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:27",
      "total_flops_so_far": 1.4213372811981048e+16,
      "budget_used_percent": 14.213372811981047
    },
    {
      "type": "training",
      "description": "Training step 1195",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:27",
      "total_flops_so_far": 1.422525492808524e+16,
      "budget_used_percent": 14.22525492808524
    },
    {
      "type": "training",
      "description": "Training step 1196",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:28",
      "total_flops_so_far": 1.4237137044189432e+16,
      "budget_used_percent": 14.237137044189433
    },
    {
      "type": "training",
      "description": "Training step 1197",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:29",
      "total_flops_so_far": 1.4249019160293624e+16,
      "budget_used_percent": 14.249019160293624
    },
    {
      "type": "training",
      "description": "Training step 1198",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:29",
      "total_flops_so_far": 1.4260901276397816e+16,
      "budget_used_percent": 14.260901276397817
    },
    {
      "type": "training",
      "description": "Training step 1199",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:30",
      "total_flops_so_far": 1.4272783392502008e+16,
      "budget_used_percent": 14.272783392502008
    },
    {
      "type": "training",
      "description": "Training step 1200",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:30",
      "total_flops_so_far": 1.42846655086062e+16,
      "budget_used_percent": 14.284665508606201
    },
    {
      "type": "training",
      "description": "Training step 1201",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:31",
      "total_flops_so_far": 1.4296547624710392e+16,
      "budget_used_percent": 14.296547624710392
    },
    {
      "type": "training",
      "description": "Training step 1202",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:32",
      "total_flops_so_far": 1.4308429740814584e+16,
      "budget_used_percent": 14.308429740814585
    },
    {
      "type": "training",
      "description": "Training step 1203",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:32",
      "total_flops_so_far": 1.4320311856918776e+16,
      "budget_used_percent": 14.320311856918778
    },
    {
      "type": "training",
      "description": "Training step 1204",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:33",
      "total_flops_so_far": 1.4332193973022968e+16,
      "budget_used_percent": 14.332193973022969
    },
    {
      "type": "training",
      "description": "Training step 1205",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:33",
      "total_flops_so_far": 1.434407608912716e+16,
      "budget_used_percent": 14.344076089127162
    },
    {
      "type": "training",
      "description": "Training step 1206",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:34",
      "total_flops_so_far": 1.4355958205231352e+16,
      "budget_used_percent": 14.355958205231353
    },
    {
      "type": "training",
      "description": "Training step 1207",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:35",
      "total_flops_so_far": 1.4367840321335544e+16,
      "budget_used_percent": 14.367840321335542
    },
    {
      "type": "training",
      "description": "Training step 1208",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:35",
      "total_flops_so_far": 1.4379722437439736e+16,
      "budget_used_percent": 14.379722437439735
    },
    {
      "type": "training",
      "description": "Training step 1209",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:36",
      "total_flops_so_far": 1.4391604553543928e+16,
      "budget_used_percent": 14.391604553543926
    },
    {
      "type": "training",
      "description": "Training step 1210",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:36",
      "total_flops_so_far": 1.440348666964812e+16,
      "budget_used_percent": 14.40348666964812
    },
    {
      "type": "training",
      "description": "Training step 1211",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:37",
      "total_flops_so_far": 1.4415368785752312e+16,
      "budget_used_percent": 14.41536878575231
    },
    {
      "type": "training",
      "description": "Training step 1212",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:38",
      "total_flops_so_far": 1.4427250901856504e+16,
      "budget_used_percent": 14.427250901856503
    },
    {
      "type": "training",
      "description": "Training step 1213",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:38",
      "total_flops_so_far": 1.4439133017960696e+16,
      "budget_used_percent": 14.439133017960696
    },
    {
      "type": "training",
      "description": "Training step 1214",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:39",
      "total_flops_so_far": 1.4451015134064888e+16,
      "budget_used_percent": 14.451015134064887
    },
    {
      "type": "training",
      "description": "Training step 1215",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:39",
      "total_flops_so_far": 1.446289725016908e+16,
      "budget_used_percent": 14.46289725016908
    },
    {
      "type": "training",
      "description": "Training step 1216",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:40",
      "total_flops_so_far": 1.4474779366273272e+16,
      "budget_used_percent": 14.474779366273271
    },
    {
      "type": "training",
      "description": "Training step 1217",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:41",
      "total_flops_so_far": 1.4486661482377464e+16,
      "budget_used_percent": 14.486661482377464
    },
    {
      "type": "training",
      "description": "Training step 1218",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:41",
      "total_flops_so_far": 1.4498543598481656e+16,
      "budget_used_percent": 14.498543598481655
    },
    {
      "type": "training",
      "description": "Training step 1219",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:42",
      "total_flops_so_far": 1.4510425714585848e+16,
      "budget_used_percent": 14.510425714585848
    },
    {
      "type": "training",
      "description": "Training step 1220",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:42",
      "total_flops_so_far": 1.452230783069004e+16,
      "budget_used_percent": 14.522307830690039
    },
    {
      "type": "training",
      "description": "Training step 1221",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:43",
      "total_flops_so_far": 1.4534189946794232e+16,
      "budget_used_percent": 14.534189946794232
    },
    {
      "type": "training",
      "description": "Training step 1222",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:44",
      "total_flops_so_far": 1.4546072062898424e+16,
      "budget_used_percent": 14.546072062898425
    },
    {
      "type": "training",
      "description": "Training step 1223",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:44",
      "total_flops_so_far": 1.4557954179002616e+16,
      "budget_used_percent": 14.557954179002616
    },
    {
      "type": "training",
      "description": "Training step 1224",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:45",
      "total_flops_so_far": 1.4569836295106808e+16,
      "budget_used_percent": 14.569836295106809
    },
    {
      "type": "training",
      "description": "Training step 1225",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:45",
      "total_flops_so_far": 1.4581718411211e+16,
      "budget_used_percent": 14.581718411211
    },
    {
      "type": "training",
      "description": "Training step 1226",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:46",
      "total_flops_so_far": 1.4593600527315192e+16,
      "budget_used_percent": 14.593600527315193
    },
    {
      "type": "training",
      "description": "Training step 1227",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:47",
      "total_flops_so_far": 1.4605482643419384e+16,
      "budget_used_percent": 14.605482643419384
    },
    {
      "type": "training",
      "description": "Training step 1228",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:47",
      "total_flops_so_far": 1.4617364759523576e+16,
      "budget_used_percent": 14.617364759523577
    },
    {
      "type": "training",
      "description": "Training step 1229",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:48",
      "total_flops_so_far": 1.4629246875627768e+16,
      "budget_used_percent": 14.62924687562777
    },
    {
      "type": "training",
      "description": "Training step 1230",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:48",
      "total_flops_so_far": 1.464112899173196e+16,
      "budget_used_percent": 14.64112899173196
    },
    {
      "type": "training",
      "description": "Training step 1231",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:49",
      "total_flops_so_far": 1.4653011107836152e+16,
      "budget_used_percent": 14.653011107836154
    },
    {
      "type": "training",
      "description": "Training step 1232",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:50",
      "total_flops_so_far": 1.4664893223940344e+16,
      "budget_used_percent": 14.664893223940345
    },
    {
      "type": "training",
      "description": "Training step 1233",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:50",
      "total_flops_so_far": 1.4676775340044536e+16,
      "budget_used_percent": 14.676775340044538
    },
    {
      "type": "training",
      "description": "Training step 1234",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:51",
      "total_flops_so_far": 1.4688657456148728e+16,
      "budget_used_percent": 14.688657456148729
    },
    {
      "type": "training",
      "description": "Training step 1235",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:51",
      "total_flops_so_far": 1.470053957225292e+16,
      "budget_used_percent": 14.700539572252918
    },
    {
      "type": "training",
      "description": "Training step 1236",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:52",
      "total_flops_so_far": 1.4712421688357112e+16,
      "budget_used_percent": 14.71242168835711
    },
    {
      "type": "training",
      "description": "Training step 1237",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:53",
      "total_flops_so_far": 1.4724303804461304e+16,
      "budget_used_percent": 14.724303804461304
    },
    {
      "type": "training",
      "description": "Training step 1238",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:53",
      "total_flops_so_far": 1.4736185920565496e+16,
      "budget_used_percent": 14.736185920565495
    },
    {
      "type": "training",
      "description": "Training step 1239",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:54",
      "total_flops_so_far": 1.4748068036669688e+16,
      "budget_used_percent": 14.748068036669688
    },
    {
      "type": "training",
      "description": "Training step 1240",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:55",
      "total_flops_so_far": 1.475995015277388e+16,
      "budget_used_percent": 14.759950152773879
    },
    {
      "type": "training",
      "description": "Training step 1241",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:55",
      "total_flops_so_far": 1.4771832268878072e+16,
      "budget_used_percent": 14.771832268878072
    },
    {
      "type": "training",
      "description": "Training step 1242",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:56",
      "total_flops_so_far": 1.4783714384982264e+16,
      "budget_used_percent": 14.783714384982263
    },
    {
      "type": "training",
      "description": "Training step 1243",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:56",
      "total_flops_so_far": 1.4795596501086456e+16,
      "budget_used_percent": 14.795596501086456
    },
    {
      "type": "training",
      "description": "Training step 1244",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:57",
      "total_flops_so_far": 1.4807478617190648e+16,
      "budget_used_percent": 14.807478617190647
    },
    {
      "type": "training",
      "description": "Training step 1245",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:58",
      "total_flops_so_far": 1.481936073329484e+16,
      "budget_used_percent": 14.81936073329484
    },
    {
      "type": "training",
      "description": "Training step 1246",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:58",
      "total_flops_so_far": 1.4831242849399032e+16,
      "budget_used_percent": 14.831242849399032
    },
    {
      "type": "training",
      "description": "Training step 1247",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:59",
      "total_flops_so_far": 1.4843124965503224e+16,
      "budget_used_percent": 14.843124965503224
    },
    {
      "type": "training",
      "description": "Training step 1248",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:18:59",
      "total_flops_so_far": 1.4855007081607416e+16,
      "budget_used_percent": 14.855007081607416
    },
    {
      "type": "training",
      "description": "Training step 1249",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:00",
      "total_flops_so_far": 1.4866889197711608e+16,
      "budget_used_percent": 14.866889197711608
    },
    {
      "type": "training",
      "description": "Training step 1250",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:01",
      "total_flops_so_far": 1.48787713138158e+16,
      "budget_used_percent": 14.8787713138158
    },
    {
      "type": "training",
      "description": "Training step 1251",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:01",
      "total_flops_so_far": 1.4890653429919992e+16,
      "budget_used_percent": 14.890653429919992
    },
    {
      "type": "training",
      "description": "Training step 1252",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:02",
      "total_flops_so_far": 1.4902535546024184e+16,
      "budget_used_percent": 14.902535546024184
    },
    {
      "type": "training",
      "description": "Training step 1253",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:02",
      "total_flops_so_far": 1.4914417662128376e+16,
      "budget_used_percent": 14.914417662128375
    },
    {
      "type": "training",
      "description": "Training step 1254",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:03",
      "total_flops_so_far": 1.4926299778232568e+16,
      "budget_used_percent": 14.926299778232568
    },
    {
      "type": "training",
      "description": "Training step 1255",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:04",
      "total_flops_so_far": 1.493818189433676e+16,
      "budget_used_percent": 14.938181894336761
    },
    {
      "type": "training",
      "description": "Training step 1256",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:04",
      "total_flops_so_far": 1.4950064010440952e+16,
      "budget_used_percent": 14.950064010440952
    },
    {
      "type": "training",
      "description": "Training step 1257",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:05",
      "total_flops_so_far": 1.4961946126545144e+16,
      "budget_used_percent": 14.961946126545145
    },
    {
      "type": "training",
      "description": "Training step 1258",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:05",
      "total_flops_so_far": 1.4973828242649336e+16,
      "budget_used_percent": 14.973828242649336
    },
    {
      "type": "training",
      "description": "Training step 1259",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:06",
      "total_flops_so_far": 1.4985710358753528e+16,
      "budget_used_percent": 14.98571035875353
    },
    {
      "type": "training",
      "description": "Training step 1260",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:07",
      "total_flops_so_far": 1.499759247485772e+16,
      "budget_used_percent": 14.99759247485772
    },
    {
      "type": "training",
      "description": "Training step 1261",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:07",
      "total_flops_so_far": 1.5009474590961912e+16,
      "budget_used_percent": 15.009474590961913
    },
    {
      "type": "training",
      "description": "Training step 1262",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:08",
      "total_flops_so_far": 1.5021356707066104e+16,
      "budget_used_percent": 15.021356707066106
    },
    {
      "type": "training",
      "description": "Training step 1263",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:08",
      "total_flops_so_far": 1.5033238823170296e+16,
      "budget_used_percent": 15.033238823170297
    },
    {
      "type": "training",
      "description": "Training step 1264",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:09",
      "total_flops_so_far": 1.5045120939274488e+16,
      "budget_used_percent": 15.045120939274486
    },
    {
      "type": "training",
      "description": "Training step 1265",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:10",
      "total_flops_so_far": 1.505700305537868e+16,
      "budget_used_percent": 15.05700305537868
    },
    {
      "type": "training",
      "description": "Training step 1266",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:10",
      "total_flops_so_far": 1.5068885171482872e+16,
      "budget_used_percent": 15.06888517148287
    },
    {
      "type": "training",
      "description": "Training step 1267",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:11",
      "total_flops_so_far": 1.5080767287587064e+16,
      "budget_used_percent": 15.080767287587063
    },
    {
      "type": "training",
      "description": "Training step 1268",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:11",
      "total_flops_so_far": 1.5092649403691256e+16,
      "budget_used_percent": 15.092649403691254
    },
    {
      "type": "training",
      "description": "Training step 1269",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:12",
      "total_flops_so_far": 1.5104531519795448e+16,
      "budget_used_percent": 15.104531519795447
    },
    {
      "type": "training",
      "description": "Training step 1270",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:13",
      "total_flops_so_far": 1.511641363589964e+16,
      "budget_used_percent": 15.11641363589964
    },
    {
      "type": "training",
      "description": "Training step 1271",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:13",
      "total_flops_so_far": 1.5128295752003832e+16,
      "budget_used_percent": 15.128295752003831
    },
    {
      "type": "training",
      "description": "Training step 1272",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:14",
      "total_flops_so_far": 1.5140177868108024e+16,
      "budget_used_percent": 15.140177868108024
    },
    {
      "type": "training",
      "description": "Training step 1273",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:14",
      "total_flops_so_far": 1.5152059984212216e+16,
      "budget_used_percent": 15.152059984212215
    },
    {
      "type": "training",
      "description": "Training step 1274",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:15",
      "total_flops_so_far": 1.5163942100316408e+16,
      "budget_used_percent": 15.163942100316408
    },
    {
      "type": "training",
      "description": "Training step 1275",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:16",
      "total_flops_so_far": 1.51758242164206e+16,
      "budget_used_percent": 15.1758242164206
    },
    {
      "type": "training",
      "description": "Training step 1276",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:16",
      "total_flops_so_far": 1.5187706332524792e+16,
      "budget_used_percent": 15.187706332524792
    },
    {
      "type": "training",
      "description": "Training step 1277",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:17",
      "total_flops_so_far": 1.5199588448628984e+16,
      "budget_used_percent": 15.199588448628983
    },
    {
      "type": "training",
      "description": "Training step 1278",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:17",
      "total_flops_so_far": 1.5211470564733176e+16,
      "budget_used_percent": 15.211470564733176
    },
    {
      "type": "training",
      "description": "Training step 1279",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:18",
      "total_flops_so_far": 1.5223352680837368e+16,
      "budget_used_percent": 15.223352680837369
    },
    {
      "type": "training",
      "description": "Training step 1280",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:19",
      "total_flops_so_far": 1.523523479694156e+16,
      "budget_used_percent": 15.23523479694156
    },
    {
      "type": "training",
      "description": "Training step 1281",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:19",
      "total_flops_so_far": 1.5247116913045752e+16,
      "budget_used_percent": 15.247116913045753
    },
    {
      "type": "training",
      "description": "Training step 1282",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:20",
      "total_flops_so_far": 1.5258999029149944e+16,
      "budget_used_percent": 15.258999029149944
    },
    {
      "type": "training",
      "description": "Training step 1283",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:20",
      "total_flops_so_far": 1.5270881145254136e+16,
      "budget_used_percent": 15.270881145254137
    },
    {
      "type": "training",
      "description": "Training step 1284",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:21",
      "total_flops_so_far": 1.5282763261358328e+16,
      "budget_used_percent": 15.282763261358328
    },
    {
      "type": "training",
      "description": "Training step 1285",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:22",
      "total_flops_so_far": 1.529464537746252e+16,
      "budget_used_percent": 15.29464537746252
    },
    {
      "type": "training",
      "description": "Training step 1286",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:22",
      "total_flops_so_far": 1.5306527493566712e+16,
      "budget_used_percent": 15.306527493566712
    },
    {
      "type": "training",
      "description": "Training step 1287",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:23",
      "total_flops_so_far": 1.5318409609670904e+16,
      "budget_used_percent": 15.318409609670905
    },
    {
      "type": "training",
      "description": "Training step 1288",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:24",
      "total_flops_so_far": 1.5330291725775096e+16,
      "budget_used_percent": 15.330291725775098
    },
    {
      "type": "training",
      "description": "Training step 1289",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:24",
      "total_flops_so_far": 1.5342173841879288e+16,
      "budget_used_percent": 15.342173841879289
    },
    {
      "type": "training",
      "description": "Training step 1290",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:25",
      "total_flops_so_far": 1.535405595798348e+16,
      "budget_used_percent": 15.354055957983482
    },
    {
      "type": "training",
      "description": "Training step 1291",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:25",
      "total_flops_so_far": 1.5365938074087672e+16,
      "budget_used_percent": 15.365938074087673
    },
    {
      "type": "training",
      "description": "Training step 1292",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:26",
      "total_flops_so_far": 1.5377820190191864e+16,
      "budget_used_percent": 15.377820190191862
    },
    {
      "type": "training",
      "description": "Training step 1293",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:27",
      "total_flops_so_far": 1.5389702306296056e+16,
      "budget_used_percent": 15.389702306296055
    },
    {
      "type": "training",
      "description": "Training step 1294",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:27",
      "total_flops_so_far": 1.5401584422400248e+16,
      "budget_used_percent": 15.401584422400246
    },
    {
      "type": "training",
      "description": "Training step 1295",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:28",
      "total_flops_so_far": 1.541346653850444e+16,
      "budget_used_percent": 15.413466538504439
    },
    {
      "type": "training",
      "description": "Training step 1296",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:28",
      "total_flops_so_far": 1.5425348654608632e+16,
      "budget_used_percent": 15.425348654608632
    },
    {
      "type": "training",
      "description": "Training step 1297",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:29",
      "total_flops_so_far": 1.5437230770712824e+16,
      "budget_used_percent": 15.437230770712823
    },
    {
      "type": "training",
      "description": "Training step 1298",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:30",
      "total_flops_so_far": 1.5449112886817016e+16,
      "budget_used_percent": 15.449112886817016
    },
    {
      "type": "training",
      "description": "Training step 1299",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:30",
      "total_flops_so_far": 1.5460995002921208e+16,
      "budget_used_percent": 15.460995002921207
    },
    {
      "type": "training",
      "description": "Training step 1300",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:31",
      "total_flops_so_far": 1.54728771190254e+16,
      "budget_used_percent": 15.4728771190254
    },
    {
      "type": "training",
      "description": "Training step 1301",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:31",
      "total_flops_so_far": 1.5484759235129592e+16,
      "budget_used_percent": 15.48475923512959
    },
    {
      "type": "training",
      "description": "Training step 1302",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:32",
      "total_flops_so_far": 1.5496641351233784e+16,
      "budget_used_percent": 15.496641351233784
    },
    {
      "type": "training",
      "description": "Training step 1303",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:33",
      "total_flops_so_far": 1.5508523467337976e+16,
      "budget_used_percent": 15.508523467337977
    },
    {
      "type": "training",
      "description": "Training step 1304",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:33",
      "total_flops_so_far": 1.5520405583442168e+16,
      "budget_used_percent": 15.520405583442168
    },
    {
      "type": "training",
      "description": "Training step 1305",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:34",
      "total_flops_so_far": 1.553228769954636e+16,
      "budget_used_percent": 15.53228769954636
    },
    {
      "type": "training",
      "description": "Training step 1306",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:34",
      "total_flops_so_far": 1.5544169815650552e+16,
      "budget_used_percent": 15.544169815650552
    },
    {
      "type": "training",
      "description": "Training step 1307",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:35",
      "total_flops_so_far": 1.5556051931754744e+16,
      "budget_used_percent": 15.556051931754745
    },
    {
      "type": "training",
      "description": "Training step 1308",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:36",
      "total_flops_so_far": 1.5567934047858936e+16,
      "budget_used_percent": 15.567934047858936
    },
    {
      "type": "training",
      "description": "Training step 1309",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:36",
      "total_flops_so_far": 1.5579816163963128e+16,
      "budget_used_percent": 15.579816163963129
    },
    {
      "type": "training",
      "description": "Training step 1310",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:37",
      "total_flops_so_far": 1.559169828006732e+16,
      "budget_used_percent": 15.59169828006732
    },
    {
      "type": "training",
      "description": "Training step 1311",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:37",
      "total_flops_so_far": 1.5603580396171512e+16,
      "budget_used_percent": 15.603580396171512
    },
    {
      "type": "training",
      "description": "Training step 1312",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:38",
      "total_flops_so_far": 1.5615462512275704e+16,
      "budget_used_percent": 15.615462512275705
    },
    {
      "type": "training",
      "description": "Training step 1313",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:39",
      "total_flops_so_far": 1.5627344628379896e+16,
      "budget_used_percent": 15.627344628379896
    },
    {
      "type": "training",
      "description": "Training step 1314",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:39",
      "total_flops_so_far": 1.5639226744484088e+16,
      "budget_used_percent": 15.63922674448409
    },
    {
      "type": "training",
      "description": "Training step 1315",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:40",
      "total_flops_so_far": 1.565110886058828e+16,
      "budget_used_percent": 15.65110886058828
    },
    {
      "type": "training",
      "description": "Training step 1316",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:40",
      "total_flops_so_far": 1.5662990976692472e+16,
      "budget_used_percent": 15.662990976692473
    },
    {
      "type": "training",
      "description": "Training step 1317",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:41",
      "total_flops_so_far": 1.5674873092796664e+16,
      "budget_used_percent": 15.674873092796664
    },
    {
      "type": "training",
      "description": "Training step 1318",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:41",
      "total_flops_so_far": 1.5686755208900856e+16,
      "budget_used_percent": 15.686755208900857
    },
    {
      "type": "training",
      "description": "Training step 1319",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:42",
      "total_flops_so_far": 1.5698637325005048e+16,
      "budget_used_percent": 15.698637325005048
    },
    {
      "type": "training",
      "description": "Training step 1320",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:43",
      "total_flops_so_far": 1.571051944110924e+16,
      "budget_used_percent": 15.710519441109241
    },
    {
      "type": "training",
      "description": "Training step 1321",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:43",
      "total_flops_so_far": 1.5722401557213432e+16,
      "budget_used_percent": 15.72240155721343
    },
    {
      "type": "training",
      "description": "Training step 1322",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:44",
      "total_flops_so_far": 1.5734283673317624e+16,
      "budget_used_percent": 15.734283673317623
    },
    {
      "type": "training",
      "description": "Training step 1323",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:44",
      "total_flops_so_far": 1.5746165789421816e+16,
      "budget_used_percent": 15.746165789421815
    },
    {
      "type": "training",
      "description": "Training step 1324",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:45",
      "total_flops_so_far": 1.5758047905526008e+16,
      "budget_used_percent": 15.758047905526007
    },
    {
      "type": "training",
      "description": "Training step 1325",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:46",
      "total_flops_so_far": 1.57699300216302e+16,
      "budget_used_percent": 15.769930021630199
    },
    {
      "type": "training",
      "description": "Training step 1326",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:46",
      "total_flops_so_far": 1.5781812137734392e+16,
      "budget_used_percent": 15.781812137734391
    },
    {
      "type": "training",
      "description": "Training step 1327",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:47",
      "total_flops_so_far": 1.5793694253838584e+16,
      "budget_used_percent": 15.793694253838582
    },
    {
      "type": "training",
      "description": "Training step 1328",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:47",
      "total_flops_so_far": 1.5805576369942776e+16,
      "budget_used_percent": 15.805576369942775
    },
    {
      "type": "training",
      "description": "Training step 1329",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:48",
      "total_flops_so_far": 1.5817458486046968e+16,
      "budget_used_percent": 15.817458486046968
    },
    {
      "type": "training",
      "description": "Training step 1330",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:49",
      "total_flops_so_far": 1.582934060215116e+16,
      "budget_used_percent": 15.82934060215116
    },
    {
      "type": "training",
      "description": "Training step 1331",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:49",
      "total_flops_so_far": 1.5841222718255352e+16,
      "budget_used_percent": 15.841222718255352
    },
    {
      "type": "training",
      "description": "Training step 1332",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:50",
      "total_flops_so_far": 1.5853104834359544e+16,
      "budget_used_percent": 15.853104834359543
    },
    {
      "type": "training",
      "description": "Training step 1333",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:50",
      "total_flops_so_far": 1.5864986950463736e+16,
      "budget_used_percent": 15.864986950463736
    },
    {
      "type": "training",
      "description": "Training step 1334",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:51",
      "total_flops_so_far": 1.5876869066567928e+16,
      "budget_used_percent": 15.876869066567927
    },
    {
      "type": "training",
      "description": "Training step 1335",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:52",
      "total_flops_so_far": 1.588875118267212e+16,
      "budget_used_percent": 15.88875118267212
    },
    {
      "type": "training",
      "description": "Training step 1336",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:52",
      "total_flops_so_far": 1.5900633298776312e+16,
      "budget_used_percent": 15.900633298776313
    },
    {
      "type": "training",
      "description": "Training step 1337",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:53",
      "total_flops_so_far": 1.5912515414880504e+16,
      "budget_used_percent": 15.912515414880504
    },
    {
      "type": "training",
      "description": "Training step 1338",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:54",
      "total_flops_so_far": 1.5924397530984696e+16,
      "budget_used_percent": 15.924397530984697
    },
    {
      "type": "training",
      "description": "Training step 1339",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:54",
      "total_flops_so_far": 1.5936279647088888e+16,
      "budget_used_percent": 15.936279647088888
    },
    {
      "type": "training",
      "description": "Training step 1340",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:55",
      "total_flops_so_far": 1.594816176319308e+16,
      "budget_used_percent": 15.948161763193081
    },
    {
      "type": "training",
      "description": "Training step 1341",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:55",
      "total_flops_so_far": 1.5960043879297272e+16,
      "budget_used_percent": 15.960043879297272
    },
    {
      "type": "training",
      "description": "Training step 1342",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:56",
      "total_flops_so_far": 1.5971925995401464e+16,
      "budget_used_percent": 15.971925995401465
    },
    {
      "type": "training",
      "description": "Training step 1343",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:57",
      "total_flops_so_far": 1.5983808111505656e+16,
      "budget_used_percent": 15.983808111505656
    },
    {
      "type": "training",
      "description": "Training step 1344",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:57",
      "total_flops_so_far": 1.5995690227609848e+16,
      "budget_used_percent": 15.995690227609849
    },
    {
      "type": "training",
      "description": "Training step 1345",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:58",
      "total_flops_so_far": 1.600757234371404e+16,
      "budget_used_percent": 16.007572343714042
    },
    {
      "type": "training",
      "description": "Training step 1346",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:58",
      "total_flops_so_far": 1.6019454459818232e+16,
      "budget_used_percent": 16.019454459818235
    },
    {
      "type": "training",
      "description": "Training step 1347",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:19:59",
      "total_flops_so_far": 1.6031336575922424e+16,
      "budget_used_percent": 16.031336575922424
    },
    {
      "type": "training",
      "description": "Training step 1348",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:00",
      "total_flops_so_far": 1.6043218692026616e+16,
      "budget_used_percent": 16.043218692026617
    },
    {
      "type": "training",
      "description": "Training step 1349",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:00",
      "total_flops_so_far": 1.6055100808130808e+16,
      "budget_used_percent": 16.055100808130806
    },
    {
      "type": "training",
      "description": "Training step 1350",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:01",
      "total_flops_so_far": 1.6066982924235e+16,
      "budget_used_percent": 16.066982924235
    },
    {
      "type": "training",
      "description": "Training step 1351",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:01",
      "total_flops_so_far": 1.6078865040339192e+16,
      "budget_used_percent": 16.078865040339192
    },
    {
      "type": "training",
      "description": "Training step 1352",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:02",
      "total_flops_so_far": 1.6090747156443384e+16,
      "budget_used_percent": 16.09074715644338
    },
    {
      "type": "training",
      "description": "Training step 1353",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:03",
      "total_flops_so_far": 1.6102629272547576e+16,
      "budget_used_percent": 16.102629272547574
    },
    {
      "type": "training",
      "description": "Training step 1354",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:03",
      "total_flops_so_far": 1.6114511388651768e+16,
      "budget_used_percent": 16.114511388651767
    },
    {
      "type": "training",
      "description": "Training step 1355",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:04",
      "total_flops_so_far": 1.612639350475596e+16,
      "budget_used_percent": 16.12639350475596
    },
    {
      "type": "training",
      "description": "Training step 1356",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:04",
      "total_flops_so_far": 1.6138275620860152e+16,
      "budget_used_percent": 16.138275620860153
    },
    {
      "type": "training",
      "description": "Training step 1357",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:05",
      "total_flops_so_far": 1.6150157736964344e+16,
      "budget_used_percent": 16.150157736964342
    },
    {
      "type": "training",
      "description": "Training step 1358",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:06",
      "total_flops_so_far": 1.6162039853068536e+16,
      "budget_used_percent": 16.162039853068535
    },
    {
      "type": "training",
      "description": "Training step 1359",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:06",
      "total_flops_so_far": 1.6173921969172728e+16,
      "budget_used_percent": 16.173921969172728
    },
    {
      "type": "training",
      "description": "Training step 1360",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:07",
      "total_flops_so_far": 1.618580408527692e+16,
      "budget_used_percent": 16.18580408527692
    },
    {
      "type": "training",
      "description": "Training step 1361",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:07",
      "total_flops_so_far": 1.6197686201381112e+16,
      "budget_used_percent": 16.19768620138111
    },
    {
      "type": "training",
      "description": "Training step 1362",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:08",
      "total_flops_so_far": 1.6209568317485304e+16,
      "budget_used_percent": 16.209568317485303
    },
    {
      "type": "training",
      "description": "Training step 1363",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:09",
      "total_flops_so_far": 1.6221450433589496e+16,
      "budget_used_percent": 16.221450433589496
    },
    {
      "type": "training",
      "description": "Training step 1364",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:09",
      "total_flops_so_far": 1.6233332549693688e+16,
      "budget_used_percent": 16.23333254969369
    },
    {
      "type": "training",
      "description": "Training step 1365",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:10",
      "total_flops_so_far": 1.624521466579788e+16,
      "budget_used_percent": 16.24521466579788
    },
    {
      "type": "training",
      "description": "Training step 1366",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:11",
      "total_flops_so_far": 1.6257096781902072e+16,
      "budget_used_percent": 16.25709678190207
    },
    {
      "type": "training",
      "description": "Training step 1367",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:11",
      "total_flops_so_far": 1.6268978898006264e+16,
      "budget_used_percent": 16.268978898006264
    },
    {
      "type": "training",
      "description": "Training step 1368",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:12",
      "total_flops_so_far": 1.6280861014110456e+16,
      "budget_used_percent": 16.280861014110457
    },
    {
      "type": "training",
      "description": "Training step 1369",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:12",
      "total_flops_so_far": 1.6292743130214648e+16,
      "budget_used_percent": 16.29274313021465
    },
    {
      "type": "training",
      "description": "Training step 1370",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:13",
      "total_flops_so_far": 1.630462524631884e+16,
      "budget_used_percent": 16.304625246318842
    },
    {
      "type": "training",
      "description": "Training step 1371",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:14",
      "total_flops_so_far": 1.6316507362423032e+16,
      "budget_used_percent": 16.31650736242303
    },
    {
      "type": "training",
      "description": "Training step 1372",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:14",
      "total_flops_so_far": 1.6328389478527224e+16,
      "budget_used_percent": 16.328389478527225
    },
    {
      "type": "training",
      "description": "Training step 1373",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:15",
      "total_flops_so_far": 1.6340271594631416e+16,
      "budget_used_percent": 16.340271594631417
    },
    {
      "type": "training",
      "description": "Training step 1374",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:15",
      "total_flops_so_far": 1.6352153710735608e+16,
      "budget_used_percent": 16.35215371073561
    },
    {
      "type": "training",
      "description": "Training step 1375",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:16",
      "total_flops_so_far": 1.63640358268398e+16,
      "budget_used_percent": 16.3640358268398
    },
    {
      "type": "training",
      "description": "Training step 1376",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:17",
      "total_flops_so_far": 1.6375917942943992e+16,
      "budget_used_percent": 16.375917942943993
    },
    {
      "type": "training",
      "description": "Training step 1377",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:17",
      "total_flops_so_far": 1.6387800059048184e+16,
      "budget_used_percent": 16.387800059048185
    },
    {
      "type": "training",
      "description": "Training step 1378",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:18",
      "total_flops_so_far": 1.6399682175152376e+16,
      "budget_used_percent": 16.399682175152375
    },
    {
      "type": "training",
      "description": "Training step 1379",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:18",
      "total_flops_so_far": 1.6411564291256568e+16,
      "budget_used_percent": 16.411564291256568
    },
    {
      "type": "training",
      "description": "Training step 1380",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:19",
      "total_flops_so_far": 1.642344640736076e+16,
      "budget_used_percent": 16.42344640736076
    },
    {
      "type": "training",
      "description": "Training step 1381",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:20",
      "total_flops_so_far": 1.6435328523464952e+16,
      "budget_used_percent": 16.43532852346495
    },
    {
      "type": "training",
      "description": "Training step 1382",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:20",
      "total_flops_so_far": 1.6447210639569144e+16,
      "budget_used_percent": 16.447210639569143
    },
    {
      "type": "training",
      "description": "Training step 1383",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:21",
      "total_flops_so_far": 1.6459092755673336e+16,
      "budget_used_percent": 16.459092755673336
    },
    {
      "type": "training",
      "description": "Training step 1384",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:21",
      "total_flops_so_far": 1.6470974871777528e+16,
      "budget_used_percent": 16.47097487177753
    },
    {
      "type": "training",
      "description": "Training step 1385",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:22",
      "total_flops_so_far": 1.648285698788172e+16,
      "budget_used_percent": 16.482856987881718
    },
    {
      "type": "training",
      "description": "Training step 1386",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:23",
      "total_flops_so_far": 1.6494739103985912e+16,
      "budget_used_percent": 16.49473910398591
    },
    {
      "type": "training",
      "description": "Training step 1387",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:23",
      "total_flops_so_far": 1.6506621220090104e+16,
      "budget_used_percent": 16.506621220090103
    },
    {
      "type": "training",
      "description": "Training step 1388",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:24",
      "total_flops_so_far": 1.6518503336194296e+16,
      "budget_used_percent": 16.518503336194296
    },
    {
      "type": "training",
      "description": "Training step 1389",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:24",
      "total_flops_so_far": 1.6530385452298488e+16,
      "budget_used_percent": 16.53038545229849
    },
    {
      "type": "training",
      "description": "Training step 1390",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:25",
      "total_flops_so_far": 1.654226756840268e+16,
      "budget_used_percent": 16.54226756840268
    },
    {
      "type": "training",
      "description": "Training step 1391",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:26",
      "total_flops_so_far": 1.6554149684506872e+16,
      "budget_used_percent": 16.55414968450687
    },
    {
      "type": "training",
      "description": "Training step 1392",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:26",
      "total_flops_so_far": 1.6566031800611064e+16,
      "budget_used_percent": 16.566031800611064
    },
    {
      "type": "training",
      "description": "Training step 1393",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:27",
      "total_flops_so_far": 1.6577913916715256e+16,
      "budget_used_percent": 16.577913916715257
    },
    {
      "type": "training",
      "description": "Training step 1394",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:28",
      "total_flops_so_far": 1.6589796032819448e+16,
      "budget_used_percent": 16.58979603281945
    },
    {
      "type": "training",
      "description": "Training step 1395",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:28",
      "total_flops_so_far": 1.660167814892364e+16,
      "budget_used_percent": 16.60167814892364
    },
    {
      "type": "training",
      "description": "Training step 1396",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:29",
      "total_flops_so_far": 1.6613560265027832e+16,
      "budget_used_percent": 16.613560265027832
    },
    {
      "type": "training",
      "description": "Training step 1397",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:29",
      "total_flops_so_far": 1.6625442381132024e+16,
      "budget_used_percent": 16.625442381132025
    },
    {
      "type": "training",
      "description": "Training step 1398",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:30",
      "total_flops_so_far": 1.6637324497236216e+16,
      "budget_used_percent": 16.637324497236218
    },
    {
      "type": "training",
      "description": "Training step 1399",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:31",
      "total_flops_so_far": 1.6649206613340408e+16,
      "budget_used_percent": 16.649206613340407
    },
    {
      "type": "training",
      "description": "Training step 1400",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:31",
      "total_flops_so_far": 1.66610887294446e+16,
      "budget_used_percent": 16.6610887294446
    },
    {
      "type": "training",
      "description": "Training step 1401",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:32",
      "total_flops_so_far": 1.6672970845548792e+16,
      "budget_used_percent": 16.672970845548793
    },
    {
      "type": "training",
      "description": "Training step 1402",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:32",
      "total_flops_so_far": 1.6684852961652984e+16,
      "budget_used_percent": 16.684852961652986
    },
    {
      "type": "training",
      "description": "Training step 1403",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:33",
      "total_flops_so_far": 1.6696735077757176e+16,
      "budget_used_percent": 16.69673507775718
    },
    {
      "type": "training",
      "description": "Training step 1404",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:34",
      "total_flops_so_far": 1.6708617193861368e+16,
      "budget_used_percent": 16.708617193861368
    },
    {
      "type": "training",
      "description": "Training step 1405",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:34",
      "total_flops_so_far": 1.672049930996556e+16,
      "budget_used_percent": 16.72049930996556
    },
    {
      "type": "training",
      "description": "Training step 1406",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:35",
      "total_flops_so_far": 1.6732381426069752e+16,
      "budget_used_percent": 16.73238142606975
    },
    {
      "type": "training",
      "description": "Training step 1407",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:35",
      "total_flops_so_far": 1.6744263542173944e+16,
      "budget_used_percent": 16.744263542173943
    },
    {
      "type": "training",
      "description": "Training step 1408",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:36",
      "total_flops_so_far": 1.6756145658278136e+16,
      "budget_used_percent": 16.756145658278136
    },
    {
      "type": "training",
      "description": "Training step 1409",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:37",
      "total_flops_so_far": 1.6768027774382328e+16,
      "budget_used_percent": 16.768027774382325
    },
    {
      "type": "training",
      "description": "Training step 1410",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:37",
      "total_flops_so_far": 1.677990989048652e+16,
      "budget_used_percent": 16.77990989048652
    },
    {
      "type": "training",
      "description": "Training step 1411",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:38",
      "total_flops_so_far": 1.6791792006590712e+16,
      "budget_used_percent": 16.79179200659071
    },
    {
      "type": "training",
      "description": "Training step 1412",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:38",
      "total_flops_so_far": 1.6803674122694904e+16,
      "budget_used_percent": 16.803674122694904
    },
    {
      "type": "training",
      "description": "Training step 1413",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:39",
      "total_flops_so_far": 1.6815556238799096e+16,
      "budget_used_percent": 16.815556238799097
    },
    {
      "type": "training",
      "description": "Training step 1414",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:40",
      "total_flops_so_far": 1.6827438354903288e+16,
      "budget_used_percent": 16.827438354903286
    },
    {
      "type": "training",
      "description": "Training step 1415",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:40",
      "total_flops_so_far": 1.683932047100748e+16,
      "budget_used_percent": 16.83932047100748
    },
    {
      "type": "training",
      "description": "Training step 1416",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:41",
      "total_flops_so_far": 1.6851202587111672e+16,
      "budget_used_percent": 16.851202587111672
    },
    {
      "type": "training",
      "description": "Training step 1417",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:42",
      "total_flops_so_far": 1.6863084703215864e+16,
      "budget_used_percent": 16.863084703215865
    },
    {
      "type": "training",
      "description": "Training step 1418",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:42",
      "total_flops_so_far": 1.6874966819320056e+16,
      "budget_used_percent": 16.874966819320054
    },
    {
      "type": "training",
      "description": "Training step 1419",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:43",
      "total_flops_so_far": 1.6886848935424248e+16,
      "budget_used_percent": 16.886848935424247
    },
    {
      "type": "training",
      "description": "Training step 1420",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:43",
      "total_flops_so_far": 1.689873105152844e+16,
      "budget_used_percent": 16.89873105152844
    },
    {
      "type": "training",
      "description": "Training step 1421",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:44",
      "total_flops_so_far": 1.6910613167632632e+16,
      "budget_used_percent": 16.910613167632633
    },
    {
      "type": "training",
      "description": "Training step 1422",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:45",
      "total_flops_so_far": 1.6922495283736824e+16,
      "budget_used_percent": 16.922495283736826
    },
    {
      "type": "training",
      "description": "Training step 1423",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:45",
      "total_flops_so_far": 1.6934377399841016e+16,
      "budget_used_percent": 16.934377399841015
    },
    {
      "type": "training",
      "description": "Training step 1424",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:46",
      "total_flops_so_far": 1.6946259515945208e+16,
      "budget_used_percent": 16.946259515945208
    },
    {
      "type": "training",
      "description": "Training step 1425",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:46",
      "total_flops_so_far": 1.69581416320494e+16,
      "budget_used_percent": 16.9581416320494
    },
    {
      "type": "training",
      "description": "Training step 1426",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:47",
      "total_flops_so_far": 1.6970023748153592e+16,
      "budget_used_percent": 16.970023748153594
    },
    {
      "type": "training",
      "description": "Training step 1427",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:48",
      "total_flops_so_far": 1.6981905864257784e+16,
      "budget_used_percent": 16.981905864257783
    },
    {
      "type": "training",
      "description": "Training step 1428",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:48",
      "total_flops_so_far": 1.6993787980361976e+16,
      "budget_used_percent": 16.993787980361976
    },
    {
      "type": "training",
      "description": "Training step 1429",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:49",
      "total_flops_so_far": 1.7005670096466168e+16,
      "budget_used_percent": 17.00567009646617
    },
    {
      "type": "training",
      "description": "Training step 1430",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:49",
      "total_flops_so_far": 1.701755221257036e+16,
      "budget_used_percent": 17.01755221257036
    },
    {
      "type": "training",
      "description": "Training step 1431",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:50",
      "total_flops_so_far": 1.7029434328674552e+16,
      "budget_used_percent": 17.029434328674554
    },
    {
      "type": "training",
      "description": "Training step 1432",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:51",
      "total_flops_so_far": 1.7041316444778744e+16,
      "budget_used_percent": 17.041316444778744
    },
    {
      "type": "training",
      "description": "Training step 1433",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:51",
      "total_flops_so_far": 1.7053198560882936e+16,
      "budget_used_percent": 17.053198560882937
    },
    {
      "type": "training",
      "description": "Training step 1434",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:52",
      "total_flops_so_far": 1.7065080676987128e+16,
      "budget_used_percent": 17.06508067698713
    },
    {
      "type": "training",
      "description": "Training step 1435",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:52",
      "total_flops_so_far": 1.707696279309132e+16,
      "budget_used_percent": 17.07696279309132
    },
    {
      "type": "training",
      "description": "Training step 1436",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:53",
      "total_flops_so_far": 1.7088844909195512e+16,
      "budget_used_percent": 17.08884490919551
    },
    {
      "type": "training",
      "description": "Training step 1437",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:54",
      "total_flops_so_far": 1.7100727025299704e+16,
      "budget_used_percent": 17.100727025299705
    },
    {
      "type": "training",
      "description": "Training step 1438",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:54",
      "total_flops_so_far": 1.7112609141403896e+16,
      "budget_used_percent": 17.112609141403894
    },
    {
      "type": "training",
      "description": "Training step 1439",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:55",
      "total_flops_so_far": 1.7124491257508088e+16,
      "budget_used_percent": 17.124491257508087
    },
    {
      "type": "training",
      "description": "Training step 1440",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:56",
      "total_flops_so_far": 1.713637337361228e+16,
      "budget_used_percent": 17.13637337361228
    },
    {
      "type": "training",
      "description": "Training step 1441",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:56",
      "total_flops_so_far": 1.7148255489716472e+16,
      "budget_used_percent": 17.148255489716473
    },
    {
      "type": "training",
      "description": "Training step 1442",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:57",
      "total_flops_so_far": 1.7160137605820664e+16,
      "budget_used_percent": 17.160137605820662
    },
    {
      "type": "training",
      "description": "Training step 1443",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:57",
      "total_flops_so_far": 1.7172019721924856e+16,
      "budget_used_percent": 17.172019721924855
    },
    {
      "type": "training",
      "description": "Training step 1444",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:58",
      "total_flops_so_far": 1.7183901838029048e+16,
      "budget_used_percent": 17.183901838029048
    },
    {
      "type": "training",
      "description": "Training step 1445",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:59",
      "total_flops_so_far": 1.719578395413324e+16,
      "budget_used_percent": 17.19578395413324
    },
    {
      "type": "training",
      "description": "Training step 1446",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:20:59",
      "total_flops_so_far": 1.7207666070237432e+16,
      "budget_used_percent": 17.207666070237433
    },
    {
      "type": "training",
      "description": "Training step 1447",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:00",
      "total_flops_so_far": 1.7219548186341624e+16,
      "budget_used_percent": 17.219548186341623
    },
    {
      "type": "training",
      "description": "Training step 1448",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:00",
      "total_flops_so_far": 1.7231430302445816e+16,
      "budget_used_percent": 17.231430302445816
    },
    {
      "type": "training",
      "description": "Training step 1449",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:01",
      "total_flops_so_far": 1.7243312418550008e+16,
      "budget_used_percent": 17.24331241855001
    },
    {
      "type": "training",
      "description": "Training step 1450",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:02",
      "total_flops_so_far": 1.72551945346542e+16,
      "budget_used_percent": 17.2551945346542
    },
    {
      "type": "training",
      "description": "Training step 1451",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:02",
      "total_flops_so_far": 1.7267076650758392e+16,
      "budget_used_percent": 17.26707665075839
    },
    {
      "type": "training",
      "description": "Training step 1452",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:03",
      "total_flops_so_far": 1.7278958766862584e+16,
      "budget_used_percent": 17.278958766862583
    },
    {
      "type": "training",
      "description": "Training step 1453",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:03",
      "total_flops_so_far": 1.7290840882966776e+16,
      "budget_used_percent": 17.290840882966776
    },
    {
      "type": "training",
      "description": "Training step 1454",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:04",
      "total_flops_so_far": 1.7302722999070968e+16,
      "budget_used_percent": 17.30272299907097
    },
    {
      "type": "training",
      "description": "Training step 1455",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:05",
      "total_flops_so_far": 1.731460511517516e+16,
      "budget_used_percent": 17.314605115175162
    },
    {
      "type": "training",
      "description": "Training step 1456",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:05",
      "total_flops_so_far": 1.7326487231279352e+16,
      "budget_used_percent": 17.32648723127935
    },
    {
      "type": "training",
      "description": "Training step 1457",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:06",
      "total_flops_so_far": 1.7338369347383544e+16,
      "budget_used_percent": 17.338369347383544
    },
    {
      "type": "training",
      "description": "Training step 1458",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:06",
      "total_flops_so_far": 1.7350251463487736e+16,
      "budget_used_percent": 17.350251463487737
    },
    {
      "type": "training",
      "description": "Training step 1459",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:07",
      "total_flops_so_far": 1.7362133579591928e+16,
      "budget_used_percent": 17.36213357959193
    },
    {
      "type": "training",
      "description": "Training step 1460",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:08",
      "total_flops_so_far": 1.737401569569612e+16,
      "budget_used_percent": 17.374015695696123
    },
    {
      "type": "training",
      "description": "Training step 1461",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:08",
      "total_flops_so_far": 1.7385897811800312e+16,
      "budget_used_percent": 17.385897811800312
    },
    {
      "type": "training",
      "description": "Training step 1462",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:09",
      "total_flops_so_far": 1.7397779927904504e+16,
      "budget_used_percent": 17.397779927904505
    },
    {
      "type": "training",
      "description": "Training step 1463",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:10",
      "total_flops_so_far": 1.7409662044008696e+16,
      "budget_used_percent": 17.409662044008694
    },
    {
      "type": "training",
      "description": "Training step 1464",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:10",
      "total_flops_so_far": 1.7421544160112888e+16,
      "budget_used_percent": 17.421544160112887
    },
    {
      "type": "training",
      "description": "Training step 1465",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:11",
      "total_flops_so_far": 1.743342627621708e+16,
      "budget_used_percent": 17.43342627621708
    },
    {
      "type": "training",
      "description": "Training step 1466",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:11",
      "total_flops_so_far": 1.7445308392321272e+16,
      "budget_used_percent": 17.44530839232127
    },
    {
      "type": "training",
      "description": "Training step 1467",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:12",
      "total_flops_so_far": 1.7457190508425464e+16,
      "budget_used_percent": 17.457190508425462
    },
    {
      "type": "training",
      "description": "Training step 1468",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:13",
      "total_flops_so_far": 1.7469072624529656e+16,
      "budget_used_percent": 17.469072624529655
    },
    {
      "type": "training",
      "description": "Training step 1469",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:13",
      "total_flops_so_far": 1.7480954740633848e+16,
      "budget_used_percent": 17.480954740633848
    },
    {
      "type": "training",
      "description": "Training step 1470",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:14",
      "total_flops_so_far": 1.749283685673804e+16,
      "budget_used_percent": 17.49283685673804
    },
    {
      "type": "training",
      "description": "Training step 1471",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:14",
      "total_flops_so_far": 1.7504718972842232e+16,
      "budget_used_percent": 17.50471897284223
    },
    {
      "type": "training",
      "description": "Training step 1472",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:15",
      "total_flops_so_far": 1.7516601088946424e+16,
      "budget_used_percent": 17.516601088946423
    },
    {
      "type": "training",
      "description": "Training step 1473",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:16",
      "total_flops_so_far": 1.7528483205050616e+16,
      "budget_used_percent": 17.528483205050616
    },
    {
      "type": "training",
      "description": "Training step 1474",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:16",
      "total_flops_so_far": 1.7540365321154808e+16,
      "budget_used_percent": 17.54036532115481
    },
    {
      "type": "training",
      "description": "Training step 1475",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:17",
      "total_flops_so_far": 1.7552247437259e+16,
      "budget_used_percent": 17.552247437259
    },
    {
      "type": "training",
      "description": "Training step 1476",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:17",
      "total_flops_so_far": 1.7564129553363192e+16,
      "budget_used_percent": 17.56412955336319
    },
    {
      "type": "training",
      "description": "Training step 1477",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:18",
      "total_flops_so_far": 1.7576011669467384e+16,
      "budget_used_percent": 17.576011669467384
    },
    {
      "type": "training",
      "description": "Training step 1478",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:19",
      "total_flops_so_far": 1.7587893785571576e+16,
      "budget_used_percent": 17.587893785571577
    },
    {
      "type": "training",
      "description": "Training step 1479",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:19",
      "total_flops_so_far": 1.7599775901675768e+16,
      "budget_used_percent": 17.59977590167577
    },
    {
      "type": "training",
      "description": "Training step 1480",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:20",
      "total_flops_so_far": 1.761165801777996e+16,
      "budget_used_percent": 17.61165801777996
    },
    {
      "type": "training",
      "description": "Training step 1481",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:21",
      "total_flops_so_far": 1.7623540133884152e+16,
      "budget_used_percent": 17.623540133884152
    },
    {
      "type": "training",
      "description": "Training step 1482",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:21",
      "total_flops_so_far": 1.7635422249988344e+16,
      "budget_used_percent": 17.635422249988345
    },
    {
      "type": "training",
      "description": "Training step 1483",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:22",
      "total_flops_so_far": 1.7647304366092536e+16,
      "budget_used_percent": 17.647304366092538
    },
    {
      "type": "training",
      "description": "Training step 1484",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:22",
      "total_flops_so_far": 1.7659186482196728e+16,
      "budget_used_percent": 17.659186482196727
    },
    {
      "type": "training",
      "description": "Training step 1485",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:23",
      "total_flops_so_far": 1.767106859830092e+16,
      "budget_used_percent": 17.67106859830092
    },
    {
      "type": "training",
      "description": "Training step 1486",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:24",
      "total_flops_so_far": 1.7682950714405112e+16,
      "budget_used_percent": 17.682950714405113
    },
    {
      "type": "training",
      "description": "Training step 1487",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:24",
      "total_flops_so_far": 1.7694832830509304e+16,
      "budget_used_percent": 17.694832830509306
    },
    {
      "type": "training",
      "description": "Training step 1488",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:25",
      "total_flops_so_far": 1.7706714946613496e+16,
      "budget_used_percent": 17.7067149466135
    },
    {
      "type": "training",
      "description": "Training step 1489",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:25",
      "total_flops_so_far": 1.7718597062717688e+16,
      "budget_used_percent": 17.718597062717688
    },
    {
      "type": "training",
      "description": "Training step 1490",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:26",
      "total_flops_so_far": 1.773047917882188e+16,
      "budget_used_percent": 17.73047917882188
    },
    {
      "type": "training",
      "description": "Training step 1491",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:27",
      "total_flops_so_far": 1.7742361294926072e+16,
      "budget_used_percent": 17.74236129492607
    },
    {
      "type": "training",
      "description": "Training step 1492",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:27",
      "total_flops_so_far": 1.7754243411030264e+16,
      "budget_used_percent": 17.754243411030263
    },
    {
      "type": "training",
      "description": "Training step 1493",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:28",
      "total_flops_so_far": 1.7766125527134456e+16,
      "budget_used_percent": 17.766125527134456
    },
    {
      "type": "training",
      "description": "Training step 1494",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:28",
      "total_flops_so_far": 1.7778007643238648e+16,
      "budget_used_percent": 17.778007643238645
    },
    {
      "type": "training",
      "description": "Training step 1495",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:29",
      "total_flops_so_far": 1.778988975934284e+16,
      "budget_used_percent": 17.789889759342838
    },
    {
      "type": "training",
      "description": "Training step 1496",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:30",
      "total_flops_so_far": 1.7801771875447032e+16,
      "budget_used_percent": 17.80177187544703
    },
    {
      "type": "training",
      "description": "Training step 1497",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:30",
      "total_flops_so_far": 1.7813653991551224e+16,
      "budget_used_percent": 17.813653991551224
    },
    {
      "type": "training",
      "description": "Training step 1498",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:31",
      "total_flops_so_far": 1.7825536107655416e+16,
      "budget_used_percent": 17.825536107655417
    },
    {
      "type": "training",
      "description": "Training step 1499",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:32",
      "total_flops_so_far": 1.7837418223759608e+16,
      "budget_used_percent": 17.837418223759606
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:39",
      "total_flops_so_far": 1.7838128852897464e+16,
      "budget_used_percent": 17.838128852897466
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 714333709232.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:47",
      "total_flops_so_far": 1.7838843186606696e+16,
      "budget_used_percent": 17.838843186606695
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:21:54",
      "total_flops_so_far": 1.7839555667670048e+16,
      "budget_used_percent": 17.839555667670048
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:01",
      "total_flops_so_far": 1.7840266296807904e+16,
      "budget_used_percent": 17.840266296807904
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 713407296244.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:09",
      "total_flops_so_far": 1.7840979704104148e+16,
      "budget_used_percent": 17.84097970410415
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 5",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:16",
      "total_flops_so_far": 1.7841690333242004e+16,
      "budget_used_percent": 17.841690333242006
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 6",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:23",
      "total_flops_so_far": 1.7842402814305356e+16,
      "budget_used_percent": 17.842402814305355
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 7",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:31",
      "total_flops_so_far": 1.7843115295368708e+16,
      "budget_used_percent": 17.843115295368708
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 8",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:38",
      "total_flops_so_far": 1.784382777643206e+16,
      "budget_used_percent": 17.84382777643206
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 9",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:45",
      "total_flops_so_far": 1.7844540257495412e+16,
      "budget_used_percent": 17.84454025749541
    },
    {
      "type": "training",
      "description": "Training step 1500",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:45",
      "total_flops_so_far": 1.7856422373599604e+16,
      "budget_used_percent": 17.856422373599603
    },
    {
      "type": "training",
      "description": "Training step 1501",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:46",
      "total_flops_so_far": 1.7868304489703796e+16,
      "budget_used_percent": 17.868304489703796
    },
    {
      "type": "training",
      "description": "Training step 1502",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:47",
      "total_flops_so_far": 1.7880186605807988e+16,
      "budget_used_percent": 17.88018660580799
    },
    {
      "type": "training",
      "description": "Training step 1503",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:47",
      "total_flops_so_far": 1.789206872191218e+16,
      "budget_used_percent": 17.89206872191218
    },
    {
      "type": "training",
      "description": "Training step 1504",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:48",
      "total_flops_so_far": 1.7903950838016372e+16,
      "budget_used_percent": 17.90395083801637
    },
    {
      "type": "training",
      "description": "Training step 1505",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:48",
      "total_flops_so_far": 1.7915832954120564e+16,
      "budget_used_percent": 17.915832954120564
    },
    {
      "type": "training",
      "description": "Training step 1506",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:49",
      "total_flops_so_far": 1.7927715070224756e+16,
      "budget_used_percent": 17.927715070224757
    },
    {
      "type": "training",
      "description": "Training step 1507",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:50",
      "total_flops_so_far": 1.7939597186328948e+16,
      "budget_used_percent": 17.93959718632895
    },
    {
      "type": "training",
      "description": "Training step 1508",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:50",
      "total_flops_so_far": 1.795147930243314e+16,
      "budget_used_percent": 17.95147930243314
    },
    {
      "type": "training",
      "description": "Training step 1509",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:51",
      "total_flops_so_far": 1.7963361418537332e+16,
      "budget_used_percent": 17.963361418537332
    },
    {
      "type": "training",
      "description": "Training step 1510",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:51",
      "total_flops_so_far": 1.7975243534641524e+16,
      "budget_used_percent": 17.975243534641525
    },
    {
      "type": "training",
      "description": "Training step 1511",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:52",
      "total_flops_so_far": 1.7987125650745716e+16,
      "budget_used_percent": 17.987125650745718
    },
    {
      "type": "training",
      "description": "Training step 1512",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:53",
      "total_flops_so_far": 1.7999007766849908e+16,
      "budget_used_percent": 17.99900776684991
    },
    {
      "type": "training",
      "description": "Training step 1513",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:53",
      "total_flops_so_far": 1.80108898829541e+16,
      "budget_used_percent": 18.0108898829541
    },
    {
      "type": "training",
      "description": "Training step 1514",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:54",
      "total_flops_so_far": 1.8022771999058292e+16,
      "budget_used_percent": 18.022771999058293
    },
    {
      "type": "training",
      "description": "Training step 1515",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:55",
      "total_flops_so_far": 1.8034654115162484e+16,
      "budget_used_percent": 18.034654115162486
    },
    {
      "type": "training",
      "description": "Training step 1516",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:55",
      "total_flops_so_far": 1.8046536231266676e+16,
      "budget_used_percent": 18.04653623126668
    },
    {
      "type": "training",
      "description": "Training step 1517",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:56",
      "total_flops_so_far": 1.8058418347370868e+16,
      "budget_used_percent": 18.058418347370868
    },
    {
      "type": "training",
      "description": "Training step 1518",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:56",
      "total_flops_so_far": 1.807030046347506e+16,
      "budget_used_percent": 18.07030046347506
    },
    {
      "type": "training",
      "description": "Training step 1519",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:57",
      "total_flops_so_far": 1.8082182579579252e+16,
      "budget_used_percent": 18.08218257957925
    },
    {
      "type": "training",
      "description": "Training step 1520",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:58",
      "total_flops_so_far": 1.8094064695683444e+16,
      "budget_used_percent": 18.094064695683443
    },
    {
      "type": "training",
      "description": "Training step 1521",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:58",
      "total_flops_so_far": 1.8105946811787636e+16,
      "budget_used_percent": 18.105946811787636
    },
    {
      "type": "training",
      "description": "Training step 1522",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:59",
      "total_flops_so_far": 1.8117828927891828e+16,
      "budget_used_percent": 18.11782892789183
    },
    {
      "type": "training",
      "description": "Training step 1523",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:22:59",
      "total_flops_so_far": 1.812971104399602e+16,
      "budget_used_percent": 18.129711043996018
    },
    {
      "type": "training",
      "description": "Training step 1524",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:00",
      "total_flops_so_far": 1.8141593160100212e+16,
      "budget_used_percent": 18.14159316010021
    },
    {
      "type": "training",
      "description": "Training step 1525",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:01",
      "total_flops_so_far": 1.8153475276204404e+16,
      "budget_used_percent": 18.153475276204404
    },
    {
      "type": "training",
      "description": "Training step 1526",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:01",
      "total_flops_so_far": 1.8165357392308596e+16,
      "budget_used_percent": 18.165357392308596
    },
    {
      "type": "training",
      "description": "Training step 1527",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:02",
      "total_flops_so_far": 1.8177239508412788e+16,
      "budget_used_percent": 18.177239508412786
    },
    {
      "type": "training",
      "description": "Training step 1528",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:02",
      "total_flops_so_far": 1.818912162451698e+16,
      "budget_used_percent": 18.18912162451698
    },
    {
      "type": "training",
      "description": "Training step 1529",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:03",
      "total_flops_so_far": 1.8201003740621172e+16,
      "budget_used_percent": 18.20100374062117
    },
    {
      "type": "training",
      "description": "Training step 1530",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:04",
      "total_flops_so_far": 1.8212885856725364e+16,
      "budget_used_percent": 18.212885856725364
    },
    {
      "type": "training",
      "description": "Training step 1531",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:04",
      "total_flops_so_far": 1.8224767972829556e+16,
      "budget_used_percent": 18.224767972829557
    },
    {
      "type": "training",
      "description": "Training step 1532",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:05",
      "total_flops_so_far": 1.8236650088933748e+16,
      "budget_used_percent": 18.236650088933747
    },
    {
      "type": "training",
      "description": "Training step 1533",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:06",
      "total_flops_so_far": 1.824853220503794e+16,
      "budget_used_percent": 18.24853220503794
    },
    {
      "type": "training",
      "description": "Training step 1534",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:06",
      "total_flops_so_far": 1.8260414321142132e+16,
      "budget_used_percent": 18.260414321142132
    },
    {
      "type": "training",
      "description": "Training step 1535",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:07",
      "total_flops_so_far": 1.8272296437246324e+16,
      "budget_used_percent": 18.272296437246325
    },
    {
      "type": "training",
      "description": "Training step 1536",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:07",
      "total_flops_so_far": 1.8284178553350516e+16,
      "budget_used_percent": 18.284178553350515
    },
    {
      "type": "training",
      "description": "Training step 1537",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:08",
      "total_flops_so_far": 1.8296060669454708e+16,
      "budget_used_percent": 18.296060669454707
    },
    {
      "type": "training",
      "description": "Training step 1538",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:09",
      "total_flops_so_far": 1.83079427855589e+16,
      "budget_used_percent": 18.3079427855589
    },
    {
      "type": "training",
      "description": "Training step 1539",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:09",
      "total_flops_so_far": 1.8319824901663092e+16,
      "budget_used_percent": 18.319824901663093
    },
    {
      "type": "training",
      "description": "Training step 1540",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:10",
      "total_flops_so_far": 1.8331707017767284e+16,
      "budget_used_percent": 18.331707017767286
    },
    {
      "type": "training",
      "description": "Training step 1541",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:10",
      "total_flops_so_far": 1.8343589133871476e+16,
      "budget_used_percent": 18.343589133871475
    },
    {
      "type": "training",
      "description": "Training step 1542",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:11",
      "total_flops_so_far": 1.8355471249975668e+16,
      "budget_used_percent": 18.35547124997567
    },
    {
      "type": "training",
      "description": "Training step 1543",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:12",
      "total_flops_so_far": 1.836735336607986e+16,
      "budget_used_percent": 18.36735336607986
    },
    {
      "type": "training",
      "description": "Training step 1544",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:12",
      "total_flops_so_far": 1.8379235482184052e+16,
      "budget_used_percent": 18.379235482184054
    },
    {
      "type": "training",
      "description": "Training step 1545",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:13",
      "total_flops_so_far": 1.8391117598288244e+16,
      "budget_used_percent": 18.391117598288247
    },
    {
      "type": "training",
      "description": "Training step 1546",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:13",
      "total_flops_so_far": 1.8402999714392436e+16,
      "budget_used_percent": 18.402999714392436
    },
    {
      "type": "training",
      "description": "Training step 1547",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:14",
      "total_flops_so_far": 1.8414881830496628e+16,
      "budget_used_percent": 18.414881830496626
    },
    {
      "type": "training",
      "description": "Training step 1548",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:15",
      "total_flops_so_far": 1.842676394660082e+16,
      "budget_used_percent": 18.42676394660082
    },
    {
      "type": "training",
      "description": "Training step 1549",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:15",
      "total_flops_so_far": 1.8438646062705012e+16,
      "budget_used_percent": 18.43864606270501
    },
    {
      "type": "training",
      "description": "Training step 1550",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:16",
      "total_flops_so_far": 1.8450528178809204e+16,
      "budget_used_percent": 18.450528178809204
    },
    {
      "type": "training",
      "description": "Training step 1551",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:17",
      "total_flops_so_far": 1.8462410294913396e+16,
      "budget_used_percent": 18.462410294913393
    },
    {
      "type": "training",
      "description": "Training step 1552",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:17",
      "total_flops_so_far": 1.8474292411017588e+16,
      "budget_used_percent": 18.474292411017586
    },
    {
      "type": "training",
      "description": "Training step 1553",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:18",
      "total_flops_so_far": 1.848617452712178e+16,
      "budget_used_percent": 18.48617452712178
    },
    {
      "type": "training",
      "description": "Training step 1554",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:18",
      "total_flops_so_far": 1.8498056643225972e+16,
      "budget_used_percent": 18.498056643225972
    },
    {
      "type": "training",
      "description": "Training step 1555",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:19",
      "total_flops_so_far": 1.8509938759330164e+16,
      "budget_used_percent": 18.509938759330165
    },
    {
      "type": "training",
      "description": "Training step 1556",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:20",
      "total_flops_so_far": 1.8521820875434356e+16,
      "budget_used_percent": 18.521820875434354
    },
    {
      "type": "training",
      "description": "Training step 1557",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:20",
      "total_flops_so_far": 1.8533702991538548e+16,
      "budget_used_percent": 18.533702991538547
    },
    {
      "type": "training",
      "description": "Training step 1558",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:21",
      "total_flops_so_far": 1.854558510764274e+16,
      "budget_used_percent": 18.54558510764274
    },
    {
      "type": "training",
      "description": "Training step 1559",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:21",
      "total_flops_so_far": 1.8557467223746932e+16,
      "budget_used_percent": 18.557467223746933
    },
    {
      "type": "training",
      "description": "Training step 1560",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:22",
      "total_flops_so_far": 1.8569349339851124e+16,
      "budget_used_percent": 18.569349339851122
    },
    {
      "type": "training",
      "description": "Training step 1561",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:23",
      "total_flops_so_far": 1.8581231455955316e+16,
      "budget_used_percent": 18.581231455955315
    },
    {
      "type": "training",
      "description": "Training step 1562",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:23",
      "total_flops_so_far": 1.8593113572059508e+16,
      "budget_used_percent": 18.593113572059508
    },
    {
      "type": "training",
      "description": "Training step 1563",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:24",
      "total_flops_so_far": 1.86049956881637e+16,
      "budget_used_percent": 18.6049956881637
    },
    {
      "type": "training",
      "description": "Training step 1564",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:24",
      "total_flops_so_far": 1.8616877804267892e+16,
      "budget_used_percent": 18.616877804267894
    },
    {
      "type": "training",
      "description": "Training step 1565",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:25",
      "total_flops_so_far": 1.8628759920372084e+16,
      "budget_used_percent": 18.628759920372083
    },
    {
      "type": "training",
      "description": "Training step 1566",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:26",
      "total_flops_so_far": 1.8640642036476276e+16,
      "budget_used_percent": 18.640642036476276
    },
    {
      "type": "training",
      "description": "Training step 1567",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:26",
      "total_flops_so_far": 1.8652524152580468e+16,
      "budget_used_percent": 18.65252415258047
    },
    {
      "type": "training",
      "description": "Training step 1568",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:27",
      "total_flops_so_far": 1.866440626868466e+16,
      "budget_used_percent": 18.66440626868466
    },
    {
      "type": "training",
      "description": "Training step 1569",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:28",
      "total_flops_so_far": 1.8676288384788852e+16,
      "budget_used_percent": 18.676288384788855
    },
    {
      "type": "training",
      "description": "Training step 1570",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:28",
      "total_flops_so_far": 1.8688170500893044e+16,
      "budget_used_percent": 18.688170500893044
    },
    {
      "type": "training",
      "description": "Training step 1571",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:29",
      "total_flops_so_far": 1.8700052616997236e+16,
      "budget_used_percent": 18.700052616997237
    },
    {
      "type": "training",
      "description": "Training step 1572",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:29",
      "total_flops_so_far": 1.8711934733101428e+16,
      "budget_used_percent": 18.71193473310143
    },
    {
      "type": "training",
      "description": "Training step 1573",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:30",
      "total_flops_so_far": 1.872381684920562e+16,
      "budget_used_percent": 18.723816849205623
    },
    {
      "type": "training",
      "description": "Training step 1574",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:31",
      "total_flops_so_far": 1.8735698965309812e+16,
      "budget_used_percent": 18.735698965309812
    },
    {
      "type": "training",
      "description": "Training step 1575",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:31",
      "total_flops_so_far": 1.8747581081414004e+16,
      "budget_used_percent": 18.747581081414005
    },
    {
      "type": "training",
      "description": "Training step 1576",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:32",
      "total_flops_so_far": 1.8759463197518196e+16,
      "budget_used_percent": 18.759463197518194
    },
    {
      "type": "training",
      "description": "Training step 1577",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:32",
      "total_flops_so_far": 1.8771345313622388e+16,
      "budget_used_percent": 18.771345313622387
    },
    {
      "type": "training",
      "description": "Training step 1578",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:33",
      "total_flops_so_far": 1.878322742972658e+16,
      "budget_used_percent": 18.78322742972658
    },
    {
      "type": "training",
      "description": "Training step 1579",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:33",
      "total_flops_so_far": 1.8795109545830772e+16,
      "budget_used_percent": 18.795109545830773
    },
    {
      "type": "training",
      "description": "Training step 1580",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:34",
      "total_flops_so_far": 1.8806991661934964e+16,
      "budget_used_percent": 18.806991661934962
    },
    {
      "type": "training",
      "description": "Training step 1581",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:35",
      "total_flops_so_far": 1.8818873778039156e+16,
      "budget_used_percent": 18.818873778039155
    },
    {
      "type": "training",
      "description": "Training step 1582",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:35",
      "total_flops_so_far": 1.8830755894143348e+16,
      "budget_used_percent": 18.830755894143348
    },
    {
      "type": "training",
      "description": "Training step 1583",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:36",
      "total_flops_so_far": 1.884263801024754e+16,
      "budget_used_percent": 18.84263801024754
    },
    {
      "type": "training",
      "description": "Training step 1584",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:36",
      "total_flops_so_far": 1.8854520126351732e+16,
      "budget_used_percent": 18.85452012635173
    },
    {
      "type": "training",
      "description": "Training step 1585",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:37",
      "total_flops_so_far": 1.8866402242455924e+16,
      "budget_used_percent": 18.866402242455923
    },
    {
      "type": "training",
      "description": "Training step 1586",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:38",
      "total_flops_so_far": 1.8878284358560116e+16,
      "budget_used_percent": 18.878284358560116
    },
    {
      "type": "training",
      "description": "Training step 1587",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:38",
      "total_flops_so_far": 1.8890166474664308e+16,
      "budget_used_percent": 18.89016647466431
    },
    {
      "type": "training",
      "description": "Training step 1588",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:39",
      "total_flops_so_far": 1.89020485907685e+16,
      "budget_used_percent": 18.9020485907685
    },
    {
      "type": "training",
      "description": "Training step 1589",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:39",
      "total_flops_so_far": 1.8913930706872692e+16,
      "budget_used_percent": 18.91393070687269
    },
    {
      "type": "training",
      "description": "Training step 1590",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:40",
      "total_flops_so_far": 1.8925812822976884e+16,
      "budget_used_percent": 18.925812822976884
    },
    {
      "type": "training",
      "description": "Training step 1591",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:41",
      "total_flops_so_far": 1.8937694939081076e+16,
      "budget_used_percent": 18.937694939081076
    },
    {
      "type": "training",
      "description": "Training step 1592",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:41",
      "total_flops_so_far": 1.8949577055185268e+16,
      "budget_used_percent": 18.94957705518527
    },
    {
      "type": "training",
      "description": "Training step 1593",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:42",
      "total_flops_so_far": 1.896145917128946e+16,
      "budget_used_percent": 18.96145917128946
    },
    {
      "type": "training",
      "description": "Training step 1594",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:43",
      "total_flops_so_far": 1.8973341287393652e+16,
      "budget_used_percent": 18.97334128739365
    },
    {
      "type": "training",
      "description": "Training step 1595",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:43",
      "total_flops_so_far": 1.8985223403497844e+16,
      "budget_used_percent": 18.985223403497844
    },
    {
      "type": "training",
      "description": "Training step 1596",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:44",
      "total_flops_so_far": 1.8997105519602036e+16,
      "budget_used_percent": 18.997105519602037
    },
    {
      "type": "training",
      "description": "Training step 1597",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:44",
      "total_flops_so_far": 1.9008987635706228e+16,
      "budget_used_percent": 19.00898763570623
    },
    {
      "type": "training",
      "description": "Training step 1598",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:45",
      "total_flops_so_far": 1.902086975181042e+16,
      "budget_used_percent": 19.02086975181042
    },
    {
      "type": "training",
      "description": "Training step 1599",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:46",
      "total_flops_so_far": 1.9032751867914612e+16,
      "budget_used_percent": 19.032751867914612
    },
    {
      "type": "training",
      "description": "Training step 1600",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:46",
      "total_flops_so_far": 1.9044633984018804e+16,
      "budget_used_percent": 19.044633984018805
    },
    {
      "type": "training",
      "description": "Training step 1601",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:47",
      "total_flops_so_far": 1.9056516100122996e+16,
      "budget_used_percent": 19.056516100122998
    },
    {
      "type": "training",
      "description": "Training step 1602",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:48",
      "total_flops_so_far": 1.9068398216227188e+16,
      "budget_used_percent": 19.068398216227187
    },
    {
      "type": "training",
      "description": "Training step 1603",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:48",
      "total_flops_so_far": 1.908028033233138e+16,
      "budget_used_percent": 19.08028033233138
    },
    {
      "type": "training",
      "description": "Training step 1604",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:49",
      "total_flops_so_far": 1.9092162448435572e+16,
      "budget_used_percent": 19.09216244843557
    },
    {
      "type": "training",
      "description": "Training step 1605",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:50",
      "total_flops_so_far": 1.9104044564539764e+16,
      "budget_used_percent": 19.104044564539763
    },
    {
      "type": "training",
      "description": "Training step 1606",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:50",
      "total_flops_so_far": 1.9115926680643956e+16,
      "budget_used_percent": 19.115926680643955
    },
    {
      "type": "training",
      "description": "Training step 1607",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:51",
      "total_flops_so_far": 1.9127808796748148e+16,
      "budget_used_percent": 19.12780879674815
    },
    {
      "type": "training",
      "description": "Training step 1608",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:51",
      "total_flops_so_far": 1.913969091285234e+16,
      "budget_used_percent": 19.139690912852338
    },
    {
      "type": "training",
      "description": "Training step 1609",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:52",
      "total_flops_so_far": 1.9151573028956532e+16,
      "budget_used_percent": 19.15157302895653
    },
    {
      "type": "training",
      "description": "Training step 1610",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:53",
      "total_flops_so_far": 1.9163455145060724e+16,
      "budget_used_percent": 19.163455145060723
    },
    {
      "type": "training",
      "description": "Training step 1611",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:53",
      "total_flops_so_far": 1.9175337261164916e+16,
      "budget_used_percent": 19.175337261164916
    },
    {
      "type": "training",
      "description": "Training step 1612",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:54",
      "total_flops_so_far": 1.9187219377269108e+16,
      "budget_used_percent": 19.18721937726911
    },
    {
      "type": "training",
      "description": "Training step 1613",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:55",
      "total_flops_so_far": 1.91991014933733e+16,
      "budget_used_percent": 19.1991014933733
    },
    {
      "type": "training",
      "description": "Training step 1614",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:55",
      "total_flops_so_far": 1.9210983609477492e+16,
      "budget_used_percent": 19.21098360947749
    },
    {
      "type": "training",
      "description": "Training step 1615",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:56",
      "total_flops_so_far": 1.9222865725581684e+16,
      "budget_used_percent": 19.222865725581684
    },
    {
      "type": "training",
      "description": "Training step 1616",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:56",
      "total_flops_so_far": 1.9234747841685876e+16,
      "budget_used_percent": 19.234747841685877
    },
    {
      "type": "training",
      "description": "Training step 1617",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:57",
      "total_flops_so_far": 1.9246629957790068e+16,
      "budget_used_percent": 19.246629957790066
    },
    {
      "type": "training",
      "description": "Training step 1618",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:58",
      "total_flops_so_far": 1.925851207389426e+16,
      "budget_used_percent": 19.25851207389426
    },
    {
      "type": "training",
      "description": "Training step 1619",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:58",
      "total_flops_so_far": 1.9270394189998452e+16,
      "budget_used_percent": 19.270394189998452
    },
    {
      "type": "training",
      "description": "Training step 1620",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:59",
      "total_flops_so_far": 1.9282276306102644e+16,
      "budget_used_percent": 19.282276306102645
    },
    {
      "type": "training",
      "description": "Training step 1621",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:23:59",
      "total_flops_so_far": 1.9294158422206836e+16,
      "budget_used_percent": 19.294158422206838
    },
    {
      "type": "training",
      "description": "Training step 1622",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:00",
      "total_flops_so_far": 1.9306040538311028e+16,
      "budget_used_percent": 19.306040538311027
    },
    {
      "type": "training",
      "description": "Training step 1623",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:01",
      "total_flops_so_far": 1.931792265441522e+16,
      "budget_used_percent": 19.31792265441522
    },
    {
      "type": "training",
      "description": "Training step 1624",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:01",
      "total_flops_so_far": 1.9329804770519412e+16,
      "budget_used_percent": 19.329804770519413
    },
    {
      "type": "training",
      "description": "Training step 1625",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:02",
      "total_flops_so_far": 1.9341686886623604e+16,
      "budget_used_percent": 19.341686886623606
    },
    {
      "type": "training",
      "description": "Training step 1626",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:02",
      "total_flops_so_far": 1.9353569002727796e+16,
      "budget_used_percent": 19.353569002727795
    },
    {
      "type": "training",
      "description": "Training step 1627",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:03",
      "total_flops_so_far": 1.9365451118831988e+16,
      "budget_used_percent": 19.365451118831988
    },
    {
      "type": "training",
      "description": "Training step 1628",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:04",
      "total_flops_so_far": 1.937733323493618e+16,
      "budget_used_percent": 19.37733323493618
    },
    {
      "type": "training",
      "description": "Training step 1629",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:04",
      "total_flops_so_far": 1.9389215351040372e+16,
      "budget_used_percent": 19.389215351040374
    },
    {
      "type": "training",
      "description": "Training step 1630",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:05",
      "total_flops_so_far": 1.9401097467144564e+16,
      "budget_used_percent": 19.401097467144567
    },
    {
      "type": "training",
      "description": "Training step 1631",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:06",
      "total_flops_so_far": 1.9412979583248756e+16,
      "budget_used_percent": 19.412979583248756
    },
    {
      "type": "training",
      "description": "Training step 1632",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:06",
      "total_flops_so_far": 1.9424861699352948e+16,
      "budget_used_percent": 19.42486169935295
    },
    {
      "type": "training",
      "description": "Training step 1633",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:07",
      "total_flops_so_far": 1.943674381545714e+16,
      "budget_used_percent": 19.436743815457138
    },
    {
      "type": "training",
      "description": "Training step 1634",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:07",
      "total_flops_so_far": 1.9448625931561332e+16,
      "budget_used_percent": 19.44862593156133
    },
    {
      "type": "training",
      "description": "Training step 1635",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:08",
      "total_flops_so_far": 1.9460508047665524e+16,
      "budget_used_percent": 19.460508047665524
    },
    {
      "type": "training",
      "description": "Training step 1636",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:09",
      "total_flops_so_far": 1.9472390163769716e+16,
      "budget_used_percent": 19.472390163769717
    },
    {
      "type": "training",
      "description": "Training step 1637",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:09",
      "total_flops_so_far": 1.9484272279873908e+16,
      "budget_used_percent": 19.484272279873906
    },
    {
      "type": "training",
      "description": "Training step 1638",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:10",
      "total_flops_so_far": 1.94961543959781e+16,
      "budget_used_percent": 19.4961543959781
    },
    {
      "type": "training",
      "description": "Training step 1639",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:10",
      "total_flops_so_far": 1.9508036512082292e+16,
      "budget_used_percent": 19.508036512082292
    },
    {
      "type": "training",
      "description": "Training step 1640",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:11",
      "total_flops_so_far": 1.9519918628186484e+16,
      "budget_used_percent": 19.519918628186485
    },
    {
      "type": "training",
      "description": "Training step 1641",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:12",
      "total_flops_so_far": 1.9531800744290676e+16,
      "budget_used_percent": 19.531800744290674
    },
    {
      "type": "training",
      "description": "Training step 1642",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:12",
      "total_flops_so_far": 1.9543682860394868e+16,
      "budget_used_percent": 19.543682860394867
    },
    {
      "type": "training",
      "description": "Training step 1643",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:13",
      "total_flops_so_far": 1.955556497649906e+16,
      "budget_used_percent": 19.55556497649906
    },
    {
      "type": "training",
      "description": "Training step 1644",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:14",
      "total_flops_so_far": 1.9567447092603252e+16,
      "budget_used_percent": 19.567447092603253
    },
    {
      "type": "training",
      "description": "Training step 1645",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:14",
      "total_flops_so_far": 1.9579329208707444e+16,
      "budget_used_percent": 19.579329208707446
    },
    {
      "type": "training",
      "description": "Training step 1646",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:15",
      "total_flops_so_far": 1.9591211324811636e+16,
      "budget_used_percent": 19.591211324811635
    },
    {
      "type": "training",
      "description": "Training step 1647",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:15",
      "total_flops_so_far": 1.9603093440915828e+16,
      "budget_used_percent": 19.603093440915828
    },
    {
      "type": "training",
      "description": "Training step 1648",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:16",
      "total_flops_so_far": 1.961497555702002e+16,
      "budget_used_percent": 19.61497555702002
    },
    {
      "type": "training",
      "description": "Training step 1649",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:17",
      "total_flops_so_far": 1.9626857673124212e+16,
      "budget_used_percent": 19.626857673124213
    },
    {
      "type": "training",
      "description": "Training step 1650",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:17",
      "total_flops_so_far": 1.9638739789228404e+16,
      "budget_used_percent": 19.638739789228403
    },
    {
      "type": "training",
      "description": "Training step 1651",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:18",
      "total_flops_so_far": 1.9650621905332596e+16,
      "budget_used_percent": 19.650621905332596
    },
    {
      "type": "training",
      "description": "Training step 1652",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:18",
      "total_flops_so_far": 1.9662504021436788e+16,
      "budget_used_percent": 19.66250402143679
    },
    {
      "type": "training",
      "description": "Training step 1653",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:19",
      "total_flops_so_far": 1.967438613754098e+16,
      "budget_used_percent": 19.67438613754098
    },
    {
      "type": "training",
      "description": "Training step 1654",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:20",
      "total_flops_so_far": 1.9686268253645172e+16,
      "budget_used_percent": 19.686268253645174
    },
    {
      "type": "training",
      "description": "Training step 1655",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:20",
      "total_flops_so_far": 1.9698150369749364e+16,
      "budget_used_percent": 19.698150369749364
    },
    {
      "type": "training",
      "description": "Training step 1656",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:21",
      "total_flops_so_far": 1.9710032485853556e+16,
      "budget_used_percent": 19.710032485853556
    },
    {
      "type": "training",
      "description": "Training step 1657",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:21",
      "total_flops_so_far": 1.9721914601957748e+16,
      "budget_used_percent": 19.72191460195775
    },
    {
      "type": "training",
      "description": "Training step 1658",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:22",
      "total_flops_so_far": 1.973379671806194e+16,
      "budget_used_percent": 19.733796718061942
    },
    {
      "type": "training",
      "description": "Training step 1659",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:23",
      "total_flops_so_far": 1.9745678834166132e+16,
      "budget_used_percent": 19.74567883416613
    },
    {
      "type": "training",
      "description": "Training step 1660",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:23",
      "total_flops_so_far": 1.9757560950270324e+16,
      "budget_used_percent": 19.757560950270324
    },
    {
      "type": "training",
      "description": "Training step 1661",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:24",
      "total_flops_so_far": 1.9769443066374516e+16,
      "budget_used_percent": 19.769443066374514
    },
    {
      "type": "training",
      "description": "Training step 1662",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:25",
      "total_flops_so_far": 1.9781325182478708e+16,
      "budget_used_percent": 19.781325182478707
    },
    {
      "type": "training",
      "description": "Training step 1663",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:25",
      "total_flops_so_far": 1.97932072985829e+16,
      "budget_used_percent": 19.7932072985829
    },
    {
      "type": "training",
      "description": "Training step 1664",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:26",
      "total_flops_so_far": 1.9805089414687092e+16,
      "budget_used_percent": 19.805089414687092
    },
    {
      "type": "training",
      "description": "Training step 1665",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:26",
      "total_flops_so_far": 1.9816971530791284e+16,
      "budget_used_percent": 19.81697153079128
    },
    {
      "type": "training",
      "description": "Training step 1666",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:27",
      "total_flops_so_far": 1.9828853646895476e+16,
      "budget_used_percent": 19.828853646895475
    },
    {
      "type": "training",
      "description": "Training step 1667",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:28",
      "total_flops_so_far": 1.9840735762999668e+16,
      "budget_used_percent": 19.840735762999667
    },
    {
      "type": "training",
      "description": "Training step 1668",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:28",
      "total_flops_so_far": 1.985261787910386e+16,
      "budget_used_percent": 19.85261787910386
    },
    {
      "type": "training",
      "description": "Training step 1669",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:29",
      "total_flops_so_far": 1.9864499995208052e+16,
      "budget_used_percent": 19.86449999520805
    },
    {
      "type": "training",
      "description": "Training step 1670",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:29",
      "total_flops_so_far": 1.9876382111312244e+16,
      "budget_used_percent": 19.876382111312243
    },
    {
      "type": "training",
      "description": "Training step 1671",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:30",
      "total_flops_so_far": 1.9888264227416436e+16,
      "budget_used_percent": 19.888264227416435
    },
    {
      "type": "training",
      "description": "Training step 1672",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:31",
      "total_flops_so_far": 1.9900146343520628e+16,
      "budget_used_percent": 19.90014634352063
    },
    {
      "type": "training",
      "description": "Training step 1673",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:31",
      "total_flops_so_far": 1.991202845962482e+16,
      "budget_used_percent": 19.91202845962482
    },
    {
      "type": "training",
      "description": "Training step 1674",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:32",
      "total_flops_so_far": 1.9923910575729012e+16,
      "budget_used_percent": 19.92391057572901
    },
    {
      "type": "training",
      "description": "Training step 1675",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:33",
      "total_flops_so_far": 1.9935792691833204e+16,
      "budget_used_percent": 19.935792691833203
    },
    {
      "type": "training",
      "description": "Training step 1676",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:33",
      "total_flops_so_far": 1.9947674807937396e+16,
      "budget_used_percent": 19.947674807937396
    },
    {
      "type": "training",
      "description": "Training step 1677",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:34",
      "total_flops_so_far": 1.9959556924041588e+16,
      "budget_used_percent": 19.95955692404159
    },
    {
      "type": "training",
      "description": "Training step 1678",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:34",
      "total_flops_so_far": 1.997143904014578e+16,
      "budget_used_percent": 19.971439040145782
    },
    {
      "type": "training",
      "description": "Training step 1679",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:35",
      "total_flops_so_far": 1.9983321156249972e+16,
      "budget_used_percent": 19.98332115624997
    },
    {
      "type": "training",
      "description": "Training step 1680",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:36",
      "total_flops_so_far": 1.9995203272354164e+16,
      "budget_used_percent": 19.995203272354164
    },
    {
      "type": "training",
      "description": "Training step 1681",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:36",
      "total_flops_so_far": 2.0007085388458356e+16,
      "budget_used_percent": 20.007085388458357
    },
    {
      "type": "training",
      "description": "Training step 1682",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:37",
      "total_flops_so_far": 2.0018967504562548e+16,
      "budget_used_percent": 20.01896750456255
    },
    {
      "type": "training",
      "description": "Training step 1683",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:37",
      "total_flops_so_far": 2.003084962066674e+16,
      "budget_used_percent": 20.03084962066674
    },
    {
      "type": "training",
      "description": "Training step 1684",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:38",
      "total_flops_so_far": 2.0042731736770932e+16,
      "budget_used_percent": 20.042731736770932
    },
    {
      "type": "training",
      "description": "Training step 1685",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:39",
      "total_flops_so_far": 2.0054613852875124e+16,
      "budget_used_percent": 20.054613852875125
    },
    {
      "type": "training",
      "description": "Training step 1686",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:39",
      "total_flops_so_far": 2.0066495968979316e+16,
      "budget_used_percent": 20.066495968979318
    },
    {
      "type": "training",
      "description": "Training step 1687",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:40",
      "total_flops_so_far": 2.0078378085083508e+16,
      "budget_used_percent": 20.07837808508351
    },
    {
      "type": "training",
      "description": "Training step 1688",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:41",
      "total_flops_so_far": 2.00902602011877e+16,
      "budget_used_percent": 20.0902602011877
    },
    {
      "type": "training",
      "description": "Training step 1689",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:41",
      "total_flops_so_far": 2.0102142317291892e+16,
      "budget_used_percent": 20.102142317291893
    },
    {
      "type": "training",
      "description": "Training step 1690",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:42",
      "total_flops_so_far": 2.0114024433396084e+16,
      "budget_used_percent": 20.114024433396082
    },
    {
      "type": "training",
      "description": "Training step 1691",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:42",
      "total_flops_so_far": 2.0125906549500276e+16,
      "budget_used_percent": 20.125906549500275
    },
    {
      "type": "training",
      "description": "Training step 1692",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:43",
      "total_flops_so_far": 2.0137788665604468e+16,
      "budget_used_percent": 20.137788665604468
    },
    {
      "type": "training",
      "description": "Training step 1693",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:44",
      "total_flops_so_far": 2.014967078170866e+16,
      "budget_used_percent": 20.149670781708657
    },
    {
      "type": "training",
      "description": "Training step 1694",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:44",
      "total_flops_so_far": 2.0161552897812852e+16,
      "budget_used_percent": 20.16155289781285
    },
    {
      "type": "training",
      "description": "Training step 1695",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:45",
      "total_flops_so_far": 2.0173435013917044e+16,
      "budget_used_percent": 20.173435013917043
    },
    {
      "type": "training",
      "description": "Training step 1696",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:45",
      "total_flops_so_far": 2.0185317130021236e+16,
      "budget_used_percent": 20.185317130021236
    },
    {
      "type": "training",
      "description": "Training step 1697",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:46",
      "total_flops_so_far": 2.0197199246125428e+16,
      "budget_used_percent": 20.19719924612543
    },
    {
      "type": "training",
      "description": "Training step 1698",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:47",
      "total_flops_so_far": 2.020908136222962e+16,
      "budget_used_percent": 20.209081362229618
    },
    {
      "type": "training",
      "description": "Training step 1699",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:47",
      "total_flops_so_far": 2.0220963478333812e+16,
      "budget_used_percent": 20.22096347833381
    },
    {
      "type": "training",
      "description": "Training step 1700",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:48",
      "total_flops_so_far": 2.0232845594438004e+16,
      "budget_used_percent": 20.232845594438004
    },
    {
      "type": "training",
      "description": "Training step 1701",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:49",
      "total_flops_so_far": 2.0244727710542196e+16,
      "budget_used_percent": 20.244727710542197
    },
    {
      "type": "training",
      "description": "Training step 1702",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:49",
      "total_flops_so_far": 2.0256609826646388e+16,
      "budget_used_percent": 20.25660982664639
    },
    {
      "type": "training",
      "description": "Training step 1703",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:50",
      "total_flops_so_far": 2.026849194275058e+16,
      "budget_used_percent": 20.26849194275058
    },
    {
      "type": "training",
      "description": "Training step 1704",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:50",
      "total_flops_so_far": 2.0280374058854772e+16,
      "budget_used_percent": 20.280374058854772
    },
    {
      "type": "training",
      "description": "Training step 1705",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:51",
      "total_flops_so_far": 2.0292256174958964e+16,
      "budget_used_percent": 20.292256174958965
    },
    {
      "type": "training",
      "description": "Training step 1706",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:52",
      "total_flops_so_far": 2.0304138291063156e+16,
      "budget_used_percent": 20.304138291063158
    },
    {
      "type": "training",
      "description": "Training step 1707",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:52",
      "total_flops_so_far": 2.0316020407167348e+16,
      "budget_used_percent": 20.316020407167347
    },
    {
      "type": "training",
      "description": "Training step 1708",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:53",
      "total_flops_so_far": 2.032790252327154e+16,
      "budget_used_percent": 20.32790252327154
    },
    {
      "type": "training",
      "description": "Training step 1709",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:53",
      "total_flops_so_far": 2.0339784639375732e+16,
      "budget_used_percent": 20.339784639375733
    },
    {
      "type": "training",
      "description": "Training step 1710",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:54",
      "total_flops_so_far": 2.0351666755479924e+16,
      "budget_used_percent": 20.351666755479926
    },
    {
      "type": "training",
      "description": "Training step 1711",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:55",
      "total_flops_so_far": 2.0363548871584116e+16,
      "budget_used_percent": 20.36354887158412
    },
    {
      "type": "training",
      "description": "Training step 1712",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:55",
      "total_flops_so_far": 2.0375430987688308e+16,
      "budget_used_percent": 20.375430987688308
    },
    {
      "type": "training",
      "description": "Training step 1713",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:56",
      "total_flops_so_far": 2.03873131037925e+16,
      "budget_used_percent": 20.3873131037925
    },
    {
      "type": "training",
      "description": "Training step 1714",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:57",
      "total_flops_so_far": 2.0399195219896692e+16,
      "budget_used_percent": 20.399195219896693
    },
    {
      "type": "training",
      "description": "Training step 1715",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:57",
      "total_flops_so_far": 2.0411077336000884e+16,
      "budget_used_percent": 20.411077336000886
    },
    {
      "type": "training",
      "description": "Training step 1716",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:58",
      "total_flops_so_far": 2.0422959452105076e+16,
      "budget_used_percent": 20.422959452105076
    },
    {
      "type": "training",
      "description": "Training step 1717",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:58",
      "total_flops_so_far": 2.0434841568209268e+16,
      "budget_used_percent": 20.43484156820927
    },
    {
      "type": "training",
      "description": "Training step 1718",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:24:59",
      "total_flops_so_far": 2.044672368431346e+16,
      "budget_used_percent": 20.446723684313458
    },
    {
      "type": "training",
      "description": "Training step 1719",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:00",
      "total_flops_so_far": 2.0458605800417652e+16,
      "budget_used_percent": 20.45860580041765
    },
    {
      "type": "training",
      "description": "Training step 1720",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:00",
      "total_flops_so_far": 2.0470487916521844e+16,
      "budget_used_percent": 20.470487916521844
    },
    {
      "type": "training",
      "description": "Training step 1721",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:01",
      "total_flops_so_far": 2.0482370032626036e+16,
      "budget_used_percent": 20.482370032626037
    },
    {
      "type": "training",
      "description": "Training step 1722",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:01",
      "total_flops_so_far": 2.0494252148730228e+16,
      "budget_used_percent": 20.494252148730226
    },
    {
      "type": "training",
      "description": "Training step 1723",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:02",
      "total_flops_so_far": 2.050613426483442e+16,
      "budget_used_percent": 20.50613426483442
    },
    {
      "type": "training",
      "description": "Training step 1724",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:03",
      "total_flops_so_far": 2.0518016380938612e+16,
      "budget_used_percent": 20.51801638093861
    },
    {
      "type": "training",
      "description": "Training step 1725",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:03",
      "total_flops_so_far": 2.0529898497042804e+16,
      "budget_used_percent": 20.529898497042804
    },
    {
      "type": "training",
      "description": "Training step 1726",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:04",
      "total_flops_so_far": 2.0541780613146996e+16,
      "budget_used_percent": 20.541780613146994
    },
    {
      "type": "training",
      "description": "Training step 1727",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:05",
      "total_flops_so_far": 2.0553662729251188e+16,
      "budget_used_percent": 20.553662729251187
    },
    {
      "type": "training",
      "description": "Training step 1728",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:05",
      "total_flops_so_far": 2.056554484535538e+16,
      "budget_used_percent": 20.56554484535538
    },
    {
      "type": "training",
      "description": "Training step 1729",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:06",
      "total_flops_so_far": 2.0577426961459572e+16,
      "budget_used_percent": 20.577426961459572
    },
    {
      "type": "training",
      "description": "Training step 1730",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:06",
      "total_flops_so_far": 2.0589309077563764e+16,
      "budget_used_percent": 20.589309077563765
    },
    {
      "type": "training",
      "description": "Training step 1731",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:07",
      "total_flops_so_far": 2.0601191193667956e+16,
      "budget_used_percent": 20.601191193667955
    },
    {
      "type": "training",
      "description": "Training step 1732",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:08",
      "total_flops_so_far": 2.0613073309772148e+16,
      "budget_used_percent": 20.613073309772147
    },
    {
      "type": "training",
      "description": "Training step 1733",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:08",
      "total_flops_so_far": 2.062495542587634e+16,
      "budget_used_percent": 20.62495542587634
    },
    {
      "type": "training",
      "description": "Training step 1734",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:09",
      "total_flops_so_far": 2.0636837541980532e+16,
      "budget_used_percent": 20.636837541980533
    },
    {
      "type": "training",
      "description": "Training step 1735",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:09",
      "total_flops_so_far": 2.0648719658084724e+16,
      "budget_used_percent": 20.648719658084723
    },
    {
      "type": "training",
      "description": "Training step 1736",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:10",
      "total_flops_so_far": 2.0660601774188916e+16,
      "budget_used_percent": 20.660601774188915
    },
    {
      "type": "training",
      "description": "Training step 1737",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:11",
      "total_flops_so_far": 2.0672483890293108e+16,
      "budget_used_percent": 20.67248389029311
    },
    {
      "type": "training",
      "description": "Training step 1738",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:11",
      "total_flops_so_far": 2.06843660063973e+16,
      "budget_used_percent": 20.6843660063973
    },
    {
      "type": "training",
      "description": "Training step 1739",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:12",
      "total_flops_so_far": 2.0696248122501492e+16,
      "budget_used_percent": 20.696248122501494
    },
    {
      "type": "training",
      "description": "Training step 1740",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:13",
      "total_flops_so_far": 2.0708130238605684e+16,
      "budget_used_percent": 20.708130238605683
    },
    {
      "type": "training",
      "description": "Training step 1741",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:13",
      "total_flops_so_far": 2.0720012354709876e+16,
      "budget_used_percent": 20.720012354709876
    },
    {
      "type": "training",
      "description": "Training step 1742",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:14",
      "total_flops_so_far": 2.0731894470814068e+16,
      "budget_used_percent": 20.73189447081407
    },
    {
      "type": "training",
      "description": "Training step 1743",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:14",
      "total_flops_so_far": 2.074377658691826e+16,
      "budget_used_percent": 20.743776586918262
    },
    {
      "type": "training",
      "description": "Training step 1744",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:15",
      "total_flops_so_far": 2.0755658703022452e+16,
      "budget_used_percent": 20.755658703022455
    },
    {
      "type": "training",
      "description": "Training step 1745",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:16",
      "total_flops_so_far": 2.0767540819126644e+16,
      "budget_used_percent": 20.767540819126644
    },
    {
      "type": "training",
      "description": "Training step 1746",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:16",
      "total_flops_so_far": 2.0779422935230836e+16,
      "budget_used_percent": 20.779422935230834
    },
    {
      "type": "training",
      "description": "Training step 1747",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:17",
      "total_flops_so_far": 2.0791305051335028e+16,
      "budget_used_percent": 20.791305051335026
    },
    {
      "type": "training",
      "description": "Training step 1748",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:17",
      "total_flops_so_far": 2.080318716743922e+16,
      "budget_used_percent": 20.80318716743922
    },
    {
      "type": "training",
      "description": "Training step 1749",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:18",
      "total_flops_so_far": 2.0815069283543412e+16,
      "budget_used_percent": 20.815069283543412
    },
    {
      "type": "training",
      "description": "Training step 1750",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:19",
      "total_flops_so_far": 2.0826951399647604e+16,
      "budget_used_percent": 20.8269513996476
    },
    {
      "type": "training",
      "description": "Training step 1751",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:19",
      "total_flops_so_far": 2.0838833515751796e+16,
      "budget_used_percent": 20.838833515751794
    },
    {
      "type": "training",
      "description": "Training step 1752",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:20",
      "total_flops_so_far": 2.0850715631855988e+16,
      "budget_used_percent": 20.850715631855987
    },
    {
      "type": "training",
      "description": "Training step 1753",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:21",
      "total_flops_so_far": 2.086259774796018e+16,
      "budget_used_percent": 20.86259774796018
    },
    {
      "type": "training",
      "description": "Training step 1754",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:21",
      "total_flops_so_far": 2.0874479864064372e+16,
      "budget_used_percent": 20.874479864064373
    },
    {
      "type": "training",
      "description": "Training step 1755",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:22",
      "total_flops_so_far": 2.0886361980168564e+16,
      "budget_used_percent": 20.886361980168562
    },
    {
      "type": "training",
      "description": "Training step 1756",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:22",
      "total_flops_so_far": 2.0898244096272756e+16,
      "budget_used_percent": 20.898244096272755
    },
    {
      "type": "training",
      "description": "Training step 1757",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:23",
      "total_flops_so_far": 2.0910126212376948e+16,
      "budget_used_percent": 20.910126212376948
    },
    {
      "type": "training",
      "description": "Training step 1758",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:24",
      "total_flops_so_far": 2.092200832848114e+16,
      "budget_used_percent": 20.92200832848114
    },
    {
      "type": "training",
      "description": "Training step 1759",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:24",
      "total_flops_so_far": 2.0933890444585332e+16,
      "budget_used_percent": 20.93389044458533
    },
    {
      "type": "training",
      "description": "Training step 1760",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:25",
      "total_flops_so_far": 2.0945772560689524e+16,
      "budget_used_percent": 20.945772560689523
    },
    {
      "type": "training",
      "description": "Training step 1761",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:26",
      "total_flops_so_far": 2.0957654676793716e+16,
      "budget_used_percent": 20.957654676793716
    },
    {
      "type": "training",
      "description": "Training step 1762",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:26",
      "total_flops_so_far": 2.0969536792897908e+16,
      "budget_used_percent": 20.96953679289791
    },
    {
      "type": "training",
      "description": "Training step 1763",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:27",
      "total_flops_so_far": 2.09814189090021e+16,
      "budget_used_percent": 20.9814189090021
    },
    {
      "type": "training",
      "description": "Training step 1764",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:27",
      "total_flops_so_far": 2.0993301025106292e+16,
      "budget_used_percent": 20.99330102510629
    },
    {
      "type": "training",
      "description": "Training step 1765",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:28",
      "total_flops_so_far": 2.1005183141210484e+16,
      "budget_used_percent": 21.005183141210484
    },
    {
      "type": "training",
      "description": "Training step 1766",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:29",
      "total_flops_so_far": 2.1017065257314676e+16,
      "budget_used_percent": 21.017065257314677
    },
    {
      "type": "training",
      "description": "Training step 1767",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:29",
      "total_flops_so_far": 2.1028947373418868e+16,
      "budget_used_percent": 21.02894737341887
    },
    {
      "type": "training",
      "description": "Training step 1768",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:30",
      "total_flops_so_far": 2.104082948952306e+16,
      "budget_used_percent": 21.040829489523063
    },
    {
      "type": "training",
      "description": "Training step 1769",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:30",
      "total_flops_so_far": 2.1052711605627252e+16,
      "budget_used_percent": 21.052711605627252
    },
    {
      "type": "training",
      "description": "Training step 1770",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:31",
      "total_flops_so_far": 2.1064593721731444e+16,
      "budget_used_percent": 21.064593721731445
    },
    {
      "type": "training",
      "description": "Training step 1771",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:32",
      "total_flops_so_far": 2.1076475837835636e+16,
      "budget_used_percent": 21.076475837835638
    },
    {
      "type": "training",
      "description": "Training step 1772",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:32",
      "total_flops_so_far": 2.1088357953939828e+16,
      "budget_used_percent": 21.08835795393983
    },
    {
      "type": "training",
      "description": "Training step 1773",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:33",
      "total_flops_so_far": 2.110024007004402e+16,
      "budget_used_percent": 21.10024007004402
    },
    {
      "type": "training",
      "description": "Training step 1774",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:34",
      "total_flops_so_far": 2.1112122186148212e+16,
      "budget_used_percent": 21.112122186148213
    },
    {
      "type": "training",
      "description": "Training step 1775",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:34",
      "total_flops_so_far": 2.1124004302252404e+16,
      "budget_used_percent": 21.124004302252402
    },
    {
      "type": "training",
      "description": "Training step 1776",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:35",
      "total_flops_so_far": 2.1135886418356596e+16,
      "budget_used_percent": 21.135886418356595
    },
    {
      "type": "training",
      "description": "Training step 1777",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:35",
      "total_flops_so_far": 2.1147768534460788e+16,
      "budget_used_percent": 21.147768534460788
    },
    {
      "type": "training",
      "description": "Training step 1778",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:36",
      "total_flops_so_far": 2.115965065056498e+16,
      "budget_used_percent": 21.15965065056498
    },
    {
      "type": "training",
      "description": "Training step 1779",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:37",
      "total_flops_so_far": 2.1171532766669172e+16,
      "budget_used_percent": 21.17153276666917
    },
    {
      "type": "training",
      "description": "Training step 1780",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:37",
      "total_flops_so_far": 2.1183414882773364e+16,
      "budget_used_percent": 21.183414882773363
    },
    {
      "type": "training",
      "description": "Training step 1781",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:38",
      "total_flops_so_far": 2.1195296998877556e+16,
      "budget_used_percent": 21.195296998877556
    },
    {
      "type": "training",
      "description": "Training step 1782",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:38",
      "total_flops_so_far": 2.1207179114981748e+16,
      "budget_used_percent": 21.20717911498175
    },
    {
      "type": "training",
      "description": "Training step 1783",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:39",
      "total_flops_so_far": 2.121906123108594e+16,
      "budget_used_percent": 21.219061231085938
    },
    {
      "type": "training",
      "description": "Training step 1784",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:40",
      "total_flops_so_far": 2.1230943347190132e+16,
      "budget_used_percent": 21.23094334719013
    },
    {
      "type": "training",
      "description": "Training step 1785",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:40",
      "total_flops_so_far": 2.1242825463294324e+16,
      "budget_used_percent": 21.242825463294324
    },
    {
      "type": "training",
      "description": "Training step 1786",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:41",
      "total_flops_so_far": 2.1254707579398516e+16,
      "budget_used_percent": 21.254707579398517
    },
    {
      "type": "training",
      "description": "Training step 1787",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:42",
      "total_flops_so_far": 2.1266589695502708e+16,
      "budget_used_percent": 21.26658969550271
    },
    {
      "type": "training",
      "description": "Training step 1788",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:42",
      "total_flops_so_far": 2.12784718116069e+16,
      "budget_used_percent": 21.2784718116069
    },
    {
      "type": "training",
      "description": "Training step 1789",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:43",
      "total_flops_so_far": 2.1290353927711092e+16,
      "budget_used_percent": 21.29035392771109
    },
    {
      "type": "training",
      "description": "Training step 1790",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:43",
      "total_flops_so_far": 2.1302236043815284e+16,
      "budget_used_percent": 21.302236043815284
    },
    {
      "type": "training",
      "description": "Training step 1791",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:44",
      "total_flops_so_far": 2.1314118159919476e+16,
      "budget_used_percent": 21.314118159919477
    },
    {
      "type": "training",
      "description": "Training step 1792",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:45",
      "total_flops_so_far": 2.1326000276023668e+16,
      "budget_used_percent": 21.326000276023667
    },
    {
      "type": "training",
      "description": "Training step 1793",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:45",
      "total_flops_so_far": 2.133788239212786e+16,
      "budget_used_percent": 21.33788239212786
    },
    {
      "type": "training",
      "description": "Training step 1794",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:46",
      "total_flops_so_far": 2.1349764508232052e+16,
      "budget_used_percent": 21.349764508232052
    },
    {
      "type": "training",
      "description": "Training step 1795",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:46",
      "total_flops_so_far": 2.1361646624336244e+16,
      "budget_used_percent": 21.361646624336245
    },
    {
      "type": "training",
      "description": "Training step 1796",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:47",
      "total_flops_so_far": 2.1373528740440436e+16,
      "budget_used_percent": 21.373528740440438
    },
    {
      "type": "training",
      "description": "Training step 1797",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:48",
      "total_flops_so_far": 2.1385410856544628e+16,
      "budget_used_percent": 21.385410856544627
    },
    {
      "type": "training",
      "description": "Training step 1798",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:48",
      "total_flops_so_far": 2.139729297264882e+16,
      "budget_used_percent": 21.39729297264882
    },
    {
      "type": "training",
      "description": "Training step 1799",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:49",
      "total_flops_so_far": 2.1409175088753012e+16,
      "budget_used_percent": 21.409175088753013
    },
    {
      "type": "training",
      "description": "Training step 1800",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:50",
      "total_flops_so_far": 2.1421057204857204e+16,
      "budget_used_percent": 21.421057204857206
    },
    {
      "type": "training",
      "description": "Training step 1801",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:51",
      "total_flops_so_far": 2.1432939320961396e+16,
      "budget_used_percent": 21.432939320961395
    },
    {
      "type": "training",
      "description": "Training step 1802",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:51",
      "total_flops_so_far": 2.1444821437065588e+16,
      "budget_used_percent": 21.44482143706559
    },
    {
      "type": "training",
      "description": "Training step 1803",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:52",
      "total_flops_so_far": 2.145670355316978e+16,
      "budget_used_percent": 21.456703553169778
    },
    {
      "type": "training",
      "description": "Training step 1804",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:52",
      "total_flops_so_far": 2.1468585669273972e+16,
      "budget_used_percent": 21.46858566927397
    },
    {
      "type": "training",
      "description": "Training step 1805",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:53",
      "total_flops_so_far": 2.1480467785378164e+16,
      "budget_used_percent": 21.480467785378163
    },
    {
      "type": "training",
      "description": "Training step 1806",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:54",
      "total_flops_so_far": 2.1492349901482356e+16,
      "budget_used_percent": 21.492349901482356
    },
    {
      "type": "training",
      "description": "Training step 1807",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:54",
      "total_flops_so_far": 2.1504232017586548e+16,
      "budget_used_percent": 21.504232017586546
    },
    {
      "type": "training",
      "description": "Training step 1808",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:55",
      "total_flops_so_far": 2.151611413369074e+16,
      "budget_used_percent": 21.51611413369074
    },
    {
      "type": "training",
      "description": "Training step 1809",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:55",
      "total_flops_so_far": 2.1527996249794932e+16,
      "budget_used_percent": 21.52799624979493
    },
    {
      "type": "training",
      "description": "Training step 1810",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:56",
      "total_flops_so_far": 2.1539878365899124e+16,
      "budget_used_percent": 21.539878365899124
    },
    {
      "type": "training",
      "description": "Training step 1811",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:57",
      "total_flops_so_far": 2.1551760482003316e+16,
      "budget_used_percent": 21.551760482003317
    },
    {
      "type": "training",
      "description": "Training step 1812",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:57",
      "total_flops_so_far": 2.1563642598107508e+16,
      "budget_used_percent": 21.563642598107506
    },
    {
      "type": "training",
      "description": "Training step 1813",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:58",
      "total_flops_so_far": 2.15755247142117e+16,
      "budget_used_percent": 21.5755247142117
    },
    {
      "type": "training",
      "description": "Training step 1814",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:59",
      "total_flops_so_far": 2.1587406830315892e+16,
      "budget_used_percent": 21.587406830315892
    },
    {
      "type": "training",
      "description": "Training step 1815",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:25:59",
      "total_flops_so_far": 2.1599288946420084e+16,
      "budget_used_percent": 21.599288946420085
    },
    {
      "type": "training",
      "description": "Training step 1816",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:00",
      "total_flops_so_far": 2.1611171062524276e+16,
      "budget_used_percent": 21.611171062524274
    },
    {
      "type": "training",
      "description": "Training step 1817",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:00",
      "total_flops_so_far": 2.1623053178628468e+16,
      "budget_used_percent": 21.623053178628467
    },
    {
      "type": "training",
      "description": "Training step 1818",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:01",
      "total_flops_so_far": 2.163493529473266e+16,
      "budget_used_percent": 21.63493529473266
    },
    {
      "type": "training",
      "description": "Training step 1819",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:02",
      "total_flops_so_far": 2.1646817410836852e+16,
      "budget_used_percent": 21.646817410836853
    },
    {
      "type": "training",
      "description": "Training step 1820",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:02",
      "total_flops_so_far": 2.1658699526941044e+16,
      "budget_used_percent": 21.658699526941046
    },
    {
      "type": "training",
      "description": "Training step 1821",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:03",
      "total_flops_so_far": 2.1670581643045236e+16,
      "budget_used_percent": 21.670581643045235
    },
    {
      "type": "training",
      "description": "Training step 1822",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:04",
      "total_flops_so_far": 2.1682463759149428e+16,
      "budget_used_percent": 21.682463759149428
    },
    {
      "type": "training",
      "description": "Training step 1823",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:04",
      "total_flops_so_far": 2.169434587525362e+16,
      "budget_used_percent": 21.69434587525362
    },
    {
      "type": "training",
      "description": "Training step 1824",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:05",
      "total_flops_so_far": 2.1706227991357812e+16,
      "budget_used_percent": 21.706227991357814
    },
    {
      "type": "training",
      "description": "Training step 1825",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:05",
      "total_flops_so_far": 2.1718110107462004e+16,
      "budget_used_percent": 21.718110107462003
    },
    {
      "type": "training",
      "description": "Training step 1826",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:06",
      "total_flops_so_far": 2.1729992223566196e+16,
      "budget_used_percent": 21.729992223566196
    },
    {
      "type": "training",
      "description": "Training step 1827",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:07",
      "total_flops_so_far": 2.1741874339670388e+16,
      "budget_used_percent": 21.74187433967039
    },
    {
      "type": "training",
      "description": "Training step 1828",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:07",
      "total_flops_so_far": 2.175375645577458e+16,
      "budget_used_percent": 21.75375645577458
    },
    {
      "type": "training",
      "description": "Training step 1829",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:08",
      "total_flops_so_far": 2.1765638571878772e+16,
      "budget_used_percent": 21.765638571878775
    },
    {
      "type": "training",
      "description": "Training step 1830",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:08",
      "total_flops_so_far": 2.1777520687982964e+16,
      "budget_used_percent": 21.777520687982964
    },
    {
      "type": "training",
      "description": "Training step 1831",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:09",
      "total_flops_so_far": 2.1789402804087156e+16,
      "budget_used_percent": 21.789402804087157
    },
    {
      "type": "training",
      "description": "Training step 1832",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:10",
      "total_flops_so_far": 2.1801284920191348e+16,
      "budget_used_percent": 21.801284920191346
    },
    {
      "type": "training",
      "description": "Training step 1833",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:10",
      "total_flops_so_far": 2.181316703629554e+16,
      "budget_used_percent": 21.81316703629554
    },
    {
      "type": "training",
      "description": "Training step 1834",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:11",
      "total_flops_so_far": 2.1825049152399732e+16,
      "budget_used_percent": 21.825049152399732
    },
    {
      "type": "training",
      "description": "Training step 1835",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:12",
      "total_flops_so_far": 2.1836931268503924e+16,
      "budget_used_percent": 21.836931268503925
    },
    {
      "type": "training",
      "description": "Training step 1836",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:12",
      "total_flops_so_far": 2.1848813384608116e+16,
      "budget_used_percent": 21.848813384608114
    },
    {
      "type": "training",
      "description": "Training step 1837",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:13",
      "total_flops_so_far": 2.1860695500712308e+16,
      "budget_used_percent": 21.860695500712307
    },
    {
      "type": "training",
      "description": "Training step 1838",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:13",
      "total_flops_so_far": 2.18725776168165e+16,
      "budget_used_percent": 21.8725776168165
    },
    {
      "type": "training",
      "description": "Training step 1839",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:14",
      "total_flops_so_far": 2.1884459732920692e+16,
      "budget_used_percent": 21.884459732920693
    },
    {
      "type": "training",
      "description": "Training step 1840",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:15",
      "total_flops_so_far": 2.1896341849024884e+16,
      "budget_used_percent": 21.896341849024882
    },
    {
      "type": "training",
      "description": "Training step 1841",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:15",
      "total_flops_so_far": 2.1908223965129076e+16,
      "budget_used_percent": 21.908223965129075
    },
    {
      "type": "training",
      "description": "Training step 1842",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:16",
      "total_flops_so_far": 2.1920106081233268e+16,
      "budget_used_percent": 21.920106081233268
    },
    {
      "type": "training",
      "description": "Training step 1843",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:16",
      "total_flops_so_far": 2.193198819733746e+16,
      "budget_used_percent": 21.93198819733746
    },
    {
      "type": "training",
      "description": "Training step 1844",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:17",
      "total_flops_so_far": 2.1943870313441652e+16,
      "budget_used_percent": 21.943870313441654
    },
    {
      "type": "training",
      "description": "Training step 1845",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:17",
      "total_flops_so_far": 2.1955752429545844e+16,
      "budget_used_percent": 21.955752429545843
    },
    {
      "type": "training",
      "description": "Training step 1846",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:18",
      "total_flops_so_far": 2.1967634545650036e+16,
      "budget_used_percent": 21.967634545650036
    },
    {
      "type": "training",
      "description": "Training step 1847",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:19",
      "total_flops_so_far": 2.1979516661754228e+16,
      "budget_used_percent": 21.97951666175423
    },
    {
      "type": "training",
      "description": "Training step 1848",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:19",
      "total_flops_so_far": 2.199139877785842e+16,
      "budget_used_percent": 21.99139877785842
    },
    {
      "type": "training",
      "description": "Training step 1849",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:20",
      "total_flops_so_far": 2.2003280893962612e+16,
      "budget_used_percent": 22.00328089396261
    },
    {
      "type": "training",
      "description": "Training step 1850",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:21",
      "total_flops_so_far": 2.2015163010066804e+16,
      "budget_used_percent": 22.015163010066804
    },
    {
      "type": "training",
      "description": "Training step 1851",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:21",
      "total_flops_so_far": 2.2027045126170996e+16,
      "budget_used_percent": 22.027045126170997
    },
    {
      "type": "training",
      "description": "Training step 1852",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:22",
      "total_flops_so_far": 2.2038927242275188e+16,
      "budget_used_percent": 22.03892724227519
    },
    {
      "type": "training",
      "description": "Training step 1853",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:22",
      "total_flops_so_far": 2.205080935837938e+16,
      "budget_used_percent": 22.050809358379382
    },
    {
      "type": "training",
      "description": "Training step 1854",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:23",
      "total_flops_so_far": 2.2062691474483572e+16,
      "budget_used_percent": 22.06269147448357
    },
    {
      "type": "training",
      "description": "Training step 1855",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:24",
      "total_flops_so_far": 2.2074573590587764e+16,
      "budget_used_percent": 22.074573590587764
    },
    {
      "type": "training",
      "description": "Training step 1856",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:24",
      "total_flops_so_far": 2.2086455706691956e+16,
      "budget_used_percent": 22.086455706691957
    },
    {
      "type": "training",
      "description": "Training step 1857",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:25",
      "total_flops_so_far": 2.2098337822796148e+16,
      "budget_used_percent": 22.09833782279615
    },
    {
      "type": "training",
      "description": "Training step 1858",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:26",
      "total_flops_so_far": 2.211021993890034e+16,
      "budget_used_percent": 22.11021993890034
    },
    {
      "type": "training",
      "description": "Training step 1859",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:26",
      "total_flops_so_far": 2.2122102055004532e+16,
      "budget_used_percent": 22.122102055004532
    },
    {
      "type": "training",
      "description": "Training step 1860",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:27",
      "total_flops_so_far": 2.2133984171108724e+16,
      "budget_used_percent": 22.13398417110872
    },
    {
      "type": "training",
      "description": "Training step 1861",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:27",
      "total_flops_so_far": 2.2145866287212916e+16,
      "budget_used_percent": 22.145866287212915
    },
    {
      "type": "training",
      "description": "Training step 1862",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:28",
      "total_flops_so_far": 2.2157748403317108e+16,
      "budget_used_percent": 22.157748403317107
    },
    {
      "type": "training",
      "description": "Training step 1863",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:29",
      "total_flops_so_far": 2.21696305194213e+16,
      "budget_used_percent": 22.1696305194213
    },
    {
      "type": "training",
      "description": "Training step 1864",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:29",
      "total_flops_so_far": 2.2181512635525492e+16,
      "budget_used_percent": 22.18151263552549
    },
    {
      "type": "training",
      "description": "Training step 1865",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:30",
      "total_flops_so_far": 2.2193394751629684e+16,
      "budget_used_percent": 22.193394751629683
    },
    {
      "type": "training",
      "description": "Training step 1866",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:30",
      "total_flops_so_far": 2.2205276867733876e+16,
      "budget_used_percent": 22.205276867733875
    },
    {
      "type": "training",
      "description": "Training step 1867",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:31",
      "total_flops_so_far": 2.2217158983838068e+16,
      "budget_used_percent": 22.21715898383807
    },
    {
      "type": "training",
      "description": "Training step 1868",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:32",
      "total_flops_so_far": 2.222904109994226e+16,
      "budget_used_percent": 22.229041099942258
    },
    {
      "type": "training",
      "description": "Training step 1869",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:32",
      "total_flops_so_far": 2.2240923216046452e+16,
      "budget_used_percent": 22.24092321604645
    },
    {
      "type": "training",
      "description": "Training step 1870",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:33",
      "total_flops_so_far": 2.2252805332150644e+16,
      "budget_used_percent": 22.252805332150643
    },
    {
      "type": "training",
      "description": "Training step 1871",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:34",
      "total_flops_so_far": 2.2264687448254836e+16,
      "budget_used_percent": 22.264687448254836
    },
    {
      "type": "training",
      "description": "Training step 1872",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:34",
      "total_flops_so_far": 2.2276569564359028e+16,
      "budget_used_percent": 22.27656956435903
    },
    {
      "type": "training",
      "description": "Training step 1873",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:35",
      "total_flops_so_far": 2.228845168046322e+16,
      "budget_used_percent": 22.28845168046322
    },
    {
      "type": "training",
      "description": "Training step 1874",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:35",
      "total_flops_so_far": 2.2300333796567412e+16,
      "budget_used_percent": 22.30033379656741
    },
    {
      "type": "training",
      "description": "Training step 1875",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:36",
      "total_flops_so_far": 2.2312215912671604e+16,
      "budget_used_percent": 22.312215912671604
    },
    {
      "type": "training",
      "description": "Training step 1876",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:37",
      "total_flops_so_far": 2.2324098028775796e+16,
      "budget_used_percent": 22.324098028775797
    },
    {
      "type": "training",
      "description": "Training step 1877",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:37",
      "total_flops_so_far": 2.2335980144879988e+16,
      "budget_used_percent": 22.33598014487999
    },
    {
      "type": "training",
      "description": "Training step 1878",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:38",
      "total_flops_so_far": 2.234786226098418e+16,
      "budget_used_percent": 22.34786226098418
    },
    {
      "type": "training",
      "description": "Training step 1879",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:39",
      "total_flops_so_far": 2.2359744377088372e+16,
      "budget_used_percent": 22.359744377088372
    },
    {
      "type": "training",
      "description": "Training step 1880",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:39",
      "total_flops_so_far": 2.2371626493192564e+16,
      "budget_used_percent": 22.371626493192565
    },
    {
      "type": "training",
      "description": "Training step 1881",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:40",
      "total_flops_so_far": 2.2383508609296756e+16,
      "budget_used_percent": 22.383508609296758
    },
    {
      "type": "training",
      "description": "Training step 1882",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:40",
      "total_flops_so_far": 2.2395390725400948e+16,
      "budget_used_percent": 22.395390725400947
    },
    {
      "type": "training",
      "description": "Training step 1883",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:41",
      "total_flops_so_far": 2.240727284150514e+16,
      "budget_used_percent": 22.40727284150514
    },
    {
      "type": "training",
      "description": "Training step 1884",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:42",
      "total_flops_so_far": 2.2419154957609332e+16,
      "budget_used_percent": 22.419154957609333
    },
    {
      "type": "training",
      "description": "Training step 1885",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:42",
      "total_flops_so_far": 2.2431037073713524e+16,
      "budget_used_percent": 22.431037073713526
    },
    {
      "type": "training",
      "description": "Training step 1886",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:43",
      "total_flops_so_far": 2.2442919189817716e+16,
      "budget_used_percent": 22.44291918981772
    },
    {
      "type": "training",
      "description": "Training step 1887",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:44",
      "total_flops_so_far": 2.2454801305921908e+16,
      "budget_used_percent": 22.454801305921908
    },
    {
      "type": "training",
      "description": "Training step 1888",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:44",
      "total_flops_so_far": 2.24666834220261e+16,
      "budget_used_percent": 22.4666834220261
    },
    {
      "type": "training",
      "description": "Training step 1889",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:45",
      "total_flops_so_far": 2.2478565538130292e+16,
      "budget_used_percent": 22.47856553813029
    },
    {
      "type": "training",
      "description": "Training step 1890",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:45",
      "total_flops_so_far": 2.2490447654234484e+16,
      "budget_used_percent": 22.490447654234483
    },
    {
      "type": "training",
      "description": "Training step 1891",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:46",
      "total_flops_so_far": 2.2502329770338676e+16,
      "budget_used_percent": 22.502329770338676
    },
    {
      "type": "training",
      "description": "Training step 1892",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:47",
      "total_flops_so_far": 2.2514211886442868e+16,
      "budget_used_percent": 22.514211886442865
    },
    {
      "type": "training",
      "description": "Training step 1893",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:47",
      "total_flops_so_far": 2.252609400254706e+16,
      "budget_used_percent": 22.526094002547058
    },
    {
      "type": "training",
      "description": "Training step 1894",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:48",
      "total_flops_so_far": 2.2537976118651252e+16,
      "budget_used_percent": 22.53797611865125
    },
    {
      "type": "training",
      "description": "Training step 1895",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:48",
      "total_flops_so_far": 2.2549858234755444e+16,
      "budget_used_percent": 22.549858234755444
    },
    {
      "type": "training",
      "description": "Training step 1896",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:49",
      "total_flops_so_far": 2.2561740350859636e+16,
      "budget_used_percent": 22.561740350859637
    },
    {
      "type": "training",
      "description": "Training step 1897",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:50",
      "total_flops_so_far": 2.2573622466963828e+16,
      "budget_used_percent": 22.573622466963826
    },
    {
      "type": "training",
      "description": "Training step 1898",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:50",
      "total_flops_so_far": 2.258550458306802e+16,
      "budget_used_percent": 22.58550458306802
    },
    {
      "type": "training",
      "description": "Training step 1899",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:51",
      "total_flops_so_far": 2.2597386699172212e+16,
      "budget_used_percent": 22.597386699172212
    },
    {
      "type": "training",
      "description": "Training step 1900",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:52",
      "total_flops_so_far": 2.2609268815276404e+16,
      "budget_used_percent": 22.609268815276405
    },
    {
      "type": "training",
      "description": "Training step 1901",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:52",
      "total_flops_so_far": 2.2621150931380596e+16,
      "budget_used_percent": 22.621150931380598
    },
    {
      "type": "training",
      "description": "Training step 1902",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:53",
      "total_flops_so_far": 2.2633033047484788e+16,
      "budget_used_percent": 22.633033047484787
    },
    {
      "type": "training",
      "description": "Training step 1903",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:53",
      "total_flops_so_far": 2.264491516358898e+16,
      "budget_used_percent": 22.64491516358898
    },
    {
      "type": "training",
      "description": "Training step 1904",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:54",
      "total_flops_so_far": 2.2656797279693172e+16,
      "budget_used_percent": 22.656797279693173
    },
    {
      "type": "training",
      "description": "Training step 1905",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:55",
      "total_flops_so_far": 2.2668679395797364e+16,
      "budget_used_percent": 22.668679395797366
    },
    {
      "type": "training",
      "description": "Training step 1906",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:55",
      "total_flops_so_far": 2.2680561511901556e+16,
      "budget_used_percent": 22.680561511901555
    },
    {
      "type": "training",
      "description": "Training step 1907",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:56",
      "total_flops_so_far": 2.2692443628005748e+16,
      "budget_used_percent": 22.692443628005748
    },
    {
      "type": "training",
      "description": "Training step 1908",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:57",
      "total_flops_so_far": 2.270432574410994e+16,
      "budget_used_percent": 22.70432574410994
    },
    {
      "type": "training",
      "description": "Training step 1909",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:57",
      "total_flops_so_far": 2.2716207860214132e+16,
      "budget_used_percent": 22.716207860214134
    },
    {
      "type": "training",
      "description": "Training step 1910",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:58",
      "total_flops_so_far": 2.2728089976318324e+16,
      "budget_used_percent": 22.728089976318326
    },
    {
      "type": "training",
      "description": "Training step 1911",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:58",
      "total_flops_so_far": 2.2739972092422516e+16,
      "budget_used_percent": 22.739972092422516
    },
    {
      "type": "training",
      "description": "Training step 1912",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:26:59",
      "total_flops_so_far": 2.2751854208526708e+16,
      "budget_used_percent": 22.75185420852671
    },
    {
      "type": "training",
      "description": "Training step 1913",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:00",
      "total_flops_so_far": 2.27637363246309e+16,
      "budget_used_percent": 22.7637363246309
    },
    {
      "type": "training",
      "description": "Training step 1914",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:00",
      "total_flops_so_far": 2.2775618440735092e+16,
      "budget_used_percent": 22.775618440735094
    },
    {
      "type": "training",
      "description": "Training step 1915",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:01",
      "total_flops_so_far": 2.2787500556839284e+16,
      "budget_used_percent": 22.787500556839284
    },
    {
      "type": "training",
      "description": "Training step 1916",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:02",
      "total_flops_so_far": 2.2799382672943476e+16,
      "budget_used_percent": 22.799382672943477
    },
    {
      "type": "training",
      "description": "Training step 1917",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:02",
      "total_flops_so_far": 2.2811264789047668e+16,
      "budget_used_percent": 22.811264789047666
    },
    {
      "type": "training",
      "description": "Training step 1918",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:03",
      "total_flops_so_far": 2.282314690515186e+16,
      "budget_used_percent": 22.82314690515186
    },
    {
      "type": "training",
      "description": "Training step 1919",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:03",
      "total_flops_so_far": 2.2835029021256052e+16,
      "budget_used_percent": 22.83502902125605
    },
    {
      "type": "training",
      "description": "Training step 1920",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:04",
      "total_flops_so_far": 2.2846911137360244e+16,
      "budget_used_percent": 22.846911137360244
    },
    {
      "type": "training",
      "description": "Training step 1921",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:05",
      "total_flops_so_far": 2.2858793253464436e+16,
      "budget_used_percent": 22.858793253464434
    },
    {
      "type": "training",
      "description": "Training step 1922",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:05",
      "total_flops_so_far": 2.2870675369568628e+16,
      "budget_used_percent": 22.870675369568627
    },
    {
      "type": "training",
      "description": "Training step 1923",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:06",
      "total_flops_so_far": 2.288255748567282e+16,
      "budget_used_percent": 22.88255748567282
    },
    {
      "type": "training",
      "description": "Training step 1924",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:06",
      "total_flops_so_far": 2.2894439601777012e+16,
      "budget_used_percent": 22.894439601777012
    },
    {
      "type": "training",
      "description": "Training step 1925",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:07",
      "total_flops_so_far": 2.2906321717881204e+16,
      "budget_used_percent": 22.9063217178812
    },
    {
      "type": "training",
      "description": "Training step 1926",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:08",
      "total_flops_so_far": 2.2918203833985396e+16,
      "budget_used_percent": 22.918203833985395
    },
    {
      "type": "training",
      "description": "Training step 1927",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:08",
      "total_flops_so_far": 2.2930085950089588e+16,
      "budget_used_percent": 22.930085950089587
    },
    {
      "type": "training",
      "description": "Training step 1928",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:09",
      "total_flops_so_far": 2.294196806619378e+16,
      "budget_used_percent": 22.94196806619378
    },
    {
      "type": "training",
      "description": "Training step 1929",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:10",
      "total_flops_so_far": 2.2953850182297972e+16,
      "budget_used_percent": 22.953850182297973
    },
    {
      "type": "training",
      "description": "Training step 1930",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:10",
      "total_flops_so_far": 2.2965732298402164e+16,
      "budget_used_percent": 22.965732298402163
    },
    {
      "type": "training",
      "description": "Training step 1931",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:11",
      "total_flops_so_far": 2.2977614414506356e+16,
      "budget_used_percent": 22.977614414506355
    },
    {
      "type": "training",
      "description": "Training step 1932",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:11",
      "total_flops_so_far": 2.2989496530610548e+16,
      "budget_used_percent": 22.98949653061055
    },
    {
      "type": "training",
      "description": "Training step 1933",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:12",
      "total_flops_so_far": 2.300137864671474e+16,
      "budget_used_percent": 23.00137864671474
    },
    {
      "type": "training",
      "description": "Training step 1934",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:13",
      "total_flops_so_far": 2.3013260762818932e+16,
      "budget_used_percent": 23.01326076281893
    },
    {
      "type": "training",
      "description": "Training step 1935",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:13",
      "total_flops_so_far": 2.3025142878923124e+16,
      "budget_used_percent": 23.025142878923123
    },
    {
      "type": "training",
      "description": "Training step 1936",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:14",
      "total_flops_so_far": 2.3037024995027316e+16,
      "budget_used_percent": 23.037024995027316
    },
    {
      "type": "training",
      "description": "Training step 1937",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:15",
      "total_flops_so_far": 2.3048907111131508e+16,
      "budget_used_percent": 23.04890711113151
    },
    {
      "type": "training",
      "description": "Training step 1938",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:15",
      "total_flops_so_far": 2.30607892272357e+16,
      "budget_used_percent": 23.060789227235702
    },
    {
      "type": "training",
      "description": "Training step 1939",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:16",
      "total_flops_so_far": 2.3072671343339892e+16,
      "budget_used_percent": 23.07267134333989
    },
    {
      "type": "training",
      "description": "Training step 1940",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:16",
      "total_flops_so_far": 2.3084553459444084e+16,
      "budget_used_percent": 23.084553459444084
    },
    {
      "type": "training",
      "description": "Training step 1941",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:17",
      "total_flops_so_far": 2.3096435575548276e+16,
      "budget_used_percent": 23.096435575548277
    },
    {
      "type": "training",
      "description": "Training step 1942",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:18",
      "total_flops_so_far": 2.3108317691652468e+16,
      "budget_used_percent": 23.10831769165247
    },
    {
      "type": "training",
      "description": "Training step 1943",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:18",
      "total_flops_so_far": 2.312019980775666e+16,
      "budget_used_percent": 23.120199807756663
    },
    {
      "type": "training",
      "description": "Training step 1944",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:19",
      "total_flops_so_far": 2.3132081923860852e+16,
      "budget_used_percent": 23.132081923860852
    },
    {
      "type": "training",
      "description": "Training step 1945",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:20",
      "total_flops_so_far": 2.3143964039965044e+16,
      "budget_used_percent": 23.143964039965045
    },
    {
      "type": "training",
      "description": "Training step 1946",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:20",
      "total_flops_so_far": 2.3155846156069236e+16,
      "budget_used_percent": 23.155846156069234
    },
    {
      "type": "training",
      "description": "Training step 1947",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:21",
      "total_flops_so_far": 2.3167728272173428e+16,
      "budget_used_percent": 23.167728272173427
    },
    {
      "type": "training",
      "description": "Training step 1948",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:21",
      "total_flops_so_far": 2.317961038827762e+16,
      "budget_used_percent": 23.17961038827762
    },
    {
      "type": "training",
      "description": "Training step 1949",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:22",
      "total_flops_so_far": 2.3191492504381812e+16,
      "budget_used_percent": 23.19149250438181
    },
    {
      "type": "training",
      "description": "Training step 1950",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:23",
      "total_flops_so_far": 2.3203374620486004e+16,
      "budget_used_percent": 23.203374620486002
    },
    {
      "type": "training",
      "description": "Training step 1951",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:23",
      "total_flops_so_far": 2.3215256736590196e+16,
      "budget_used_percent": 23.215256736590195
    },
    {
      "type": "training",
      "description": "Training step 1952",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:24",
      "total_flops_so_far": 2.3227138852694388e+16,
      "budget_used_percent": 23.227138852694388
    },
    {
      "type": "training",
      "description": "Training step 1953",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:25",
      "total_flops_so_far": 2.323902096879858e+16,
      "budget_used_percent": 23.23902096879858
    },
    {
      "type": "training",
      "description": "Training step 1954",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:25",
      "total_flops_so_far": 2.3250903084902772e+16,
      "budget_used_percent": 23.25090308490277
    },
    {
      "type": "training",
      "description": "Training step 1955",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:26",
      "total_flops_so_far": 2.3262785201006964e+16,
      "budget_used_percent": 23.262785201006963
    },
    {
      "type": "training",
      "description": "Training step 1956",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:26",
      "total_flops_so_far": 2.3274667317111156e+16,
      "budget_used_percent": 23.274667317111156
    },
    {
      "type": "training",
      "description": "Training step 1957",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:27",
      "total_flops_so_far": 2.3286549433215348e+16,
      "budget_used_percent": 23.28654943321535
    },
    {
      "type": "training",
      "description": "Training step 1958",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:28",
      "total_flops_so_far": 2.329843154931954e+16,
      "budget_used_percent": 23.298431549319538
    },
    {
      "type": "training",
      "description": "Training step 1959",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:28",
      "total_flops_so_far": 2.3310313665423732e+16,
      "budget_used_percent": 23.31031366542373
    },
    {
      "type": "training",
      "description": "Training step 1960",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:29",
      "total_flops_so_far": 2.3322195781527924e+16,
      "budget_used_percent": 23.322195781527924
    },
    {
      "type": "training",
      "description": "Training step 1961",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:30",
      "total_flops_so_far": 2.3334077897632116e+16,
      "budget_used_percent": 23.334077897632117
    },
    {
      "type": "training",
      "description": "Training step 1962",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:30",
      "total_flops_so_far": 2.3345960013736308e+16,
      "budget_used_percent": 23.34596001373631
    },
    {
      "type": "training",
      "description": "Training step 1963",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:31",
      "total_flops_so_far": 2.33578421298405e+16,
      "budget_used_percent": 23.3578421298405
    },
    {
      "type": "training",
      "description": "Training step 1964",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:31",
      "total_flops_so_far": 2.3369724245944692e+16,
      "budget_used_percent": 23.369724245944692
    },
    {
      "type": "training",
      "description": "Training step 1965",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:32",
      "total_flops_so_far": 2.3381606362048884e+16,
      "budget_used_percent": 23.381606362048885
    },
    {
      "type": "training",
      "description": "Training step 1966",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:33",
      "total_flops_so_far": 2.3393488478153076e+16,
      "budget_used_percent": 23.393488478153078
    },
    {
      "type": "training",
      "description": "Training step 1967",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:33",
      "total_flops_so_far": 2.3405370594257268e+16,
      "budget_used_percent": 23.40537059425727
    },
    {
      "type": "training",
      "description": "Training step 1968",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:34",
      "total_flops_so_far": 2.341725271036146e+16,
      "budget_used_percent": 23.41725271036146
    },
    {
      "type": "training",
      "description": "Training step 1969",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:34",
      "total_flops_so_far": 2.3429134826465652e+16,
      "budget_used_percent": 23.429134826465653
    },
    {
      "type": "training",
      "description": "Training step 1970",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:35",
      "total_flops_so_far": 2.3441016942569844e+16,
      "budget_used_percent": 23.441016942569846
    },
    {
      "type": "training",
      "description": "Training step 1971",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:36",
      "total_flops_so_far": 2.3452899058674036e+16,
      "budget_used_percent": 23.45289905867404
    },
    {
      "type": "training",
      "description": "Training step 1972",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:36",
      "total_flops_so_far": 2.3464781174778228e+16,
      "budget_used_percent": 23.464781174778228
    },
    {
      "type": "training",
      "description": "Training step 1973",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:37",
      "total_flops_so_far": 2.347666329088242e+16,
      "budget_used_percent": 23.47666329088242
    },
    {
      "type": "training",
      "description": "Training step 1974",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:38",
      "total_flops_so_far": 2.3488545406986612e+16,
      "budget_used_percent": 23.48854540698661
    },
    {
      "type": "training",
      "description": "Training step 1975",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:38",
      "total_flops_so_far": 2.3500427523090804e+16,
      "budget_used_percent": 23.500427523090803
    },
    {
      "type": "training",
      "description": "Training step 1976",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:39",
      "total_flops_so_far": 2.3512309639194996e+16,
      "budget_used_percent": 23.512309639194996
    },
    {
      "type": "training",
      "description": "Training step 1977",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:39",
      "total_flops_so_far": 2.3524191755299188e+16,
      "budget_used_percent": 23.52419175529919
    },
    {
      "type": "training",
      "description": "Training step 1978",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:40",
      "total_flops_so_far": 2.353607387140338e+16,
      "budget_used_percent": 23.536073871403378
    },
    {
      "type": "training",
      "description": "Training step 1979",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:41",
      "total_flops_so_far": 2.3547955987507572e+16,
      "budget_used_percent": 23.54795598750757
    },
    {
      "type": "training",
      "description": "Training step 1980",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:41",
      "total_flops_so_far": 2.3559838103611764e+16,
      "budget_used_percent": 23.559838103611764
    },
    {
      "type": "training",
      "description": "Training step 1981",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:42",
      "total_flops_so_far": 2.3571720219715956e+16,
      "budget_used_percent": 23.571720219715957
    },
    {
      "type": "training",
      "description": "Training step 1982",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:43",
      "total_flops_so_far": 2.3583602335820148e+16,
      "budget_used_percent": 23.583602335820146
    },
    {
      "type": "training",
      "description": "Training step 1983",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:43",
      "total_flops_so_far": 2.359548445192434e+16,
      "budget_used_percent": 23.59548445192434
    },
    {
      "type": "training",
      "description": "Training step 1984",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:44",
      "total_flops_so_far": 2.3607366568028532e+16,
      "budget_used_percent": 23.60736656802853
    },
    {
      "type": "training",
      "description": "Training step 1985",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:44",
      "total_flops_so_far": 2.3619248684132724e+16,
      "budget_used_percent": 23.619248684132724
    },
    {
      "type": "training",
      "description": "Training step 1986",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:45",
      "total_flops_so_far": 2.3631130800236916e+16,
      "budget_used_percent": 23.631130800236917
    },
    {
      "type": "training",
      "description": "Training step 1987",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:46",
      "total_flops_so_far": 2.3643012916341108e+16,
      "budget_used_percent": 23.643012916341107
    },
    {
      "type": "training",
      "description": "Training step 1988",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:46",
      "total_flops_so_far": 2.36548950324453e+16,
      "budget_used_percent": 23.6548950324453
    },
    {
      "type": "training",
      "description": "Training step 1989",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:47",
      "total_flops_so_far": 2.3666777148549492e+16,
      "budget_used_percent": 23.666777148549492
    },
    {
      "type": "training",
      "description": "Training step 1990",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:48",
      "total_flops_so_far": 2.3678659264653684e+16,
      "budget_used_percent": 23.678659264653685
    },
    {
      "type": "training",
      "description": "Training step 1991",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:48",
      "total_flops_so_far": 2.3690541380757876e+16,
      "budget_used_percent": 23.690541380757875
    },
    {
      "type": "training",
      "description": "Training step 1992",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:49",
      "total_flops_so_far": 2.3702423496862068e+16,
      "budget_used_percent": 23.702423496862068
    },
    {
      "type": "training",
      "description": "Training step 1993",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:49",
      "total_flops_so_far": 2.371430561296626e+16,
      "budget_used_percent": 23.71430561296626
    },
    {
      "type": "training",
      "description": "Training step 1994",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:50",
      "total_flops_so_far": 2.3726187729070452e+16,
      "budget_used_percent": 23.726187729070453
    },
    {
      "type": "training",
      "description": "Training step 1995",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:51",
      "total_flops_so_far": 2.3738069845174644e+16,
      "budget_used_percent": 23.738069845174646
    },
    {
      "type": "training",
      "description": "Training step 1996",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:51",
      "total_flops_so_far": 2.3749951961278836e+16,
      "budget_used_percent": 23.749951961278835
    },
    {
      "type": "training",
      "description": "Training step 1997",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:52",
      "total_flops_so_far": 2.3761834077383028e+16,
      "budget_used_percent": 23.76183407738303
    },
    {
      "type": "training",
      "description": "Training step 1998",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:53",
      "total_flops_so_far": 2.377371619348722e+16,
      "budget_used_percent": 23.77371619348722
    },
    {
      "type": "training",
      "description": "Training step 1999",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:27:53",
      "total_flops_so_far": 2.3785598309591412e+16,
      "budget_used_percent": 23.785598309591414
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:28:01",
      "total_flops_so_far": 2.3786308938729268e+16,
      "budget_used_percent": 23.786308938729267
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 714333709232.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:28:09",
      "total_flops_so_far": 2.37870232724385e+16,
      "budget_used_percent": 23.7870232724385
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:28:16",
      "total_flops_so_far": 2.3787735753501852e+16,
      "budget_used_percent": 23.787735753501853
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:28:23",
      "total_flops_so_far": 2.3788446382639708e+16,
      "budget_used_percent": 23.78844638263971
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 713407296244.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:28:31",
      "total_flops_so_far": 2.378915978993595e+16,
      "budget_used_percent": 23.78915978993595
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 5",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:28:38",
      "total_flops_so_far": 2.378987041907381e+16,
      "budget_used_percent": 23.789870419073807
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 6",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:28:45",
      "total_flops_so_far": 2.379058290013716e+16,
      "budget_used_percent": 23.79058290013716
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 7",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:28:53",
      "total_flops_so_far": 2.379129538120051e+16,
      "budget_used_percent": 23.791295381200513
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 8",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:00",
      "total_flops_so_far": 2.3792007862263864e+16,
      "budget_used_percent": 23.792007862263866
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 9",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:08",
      "total_flops_so_far": 2.3792720343327216e+16,
      "budget_used_percent": 23.79272034332722
    },
    {
      "type": "training",
      "description": "Training step 2000",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:08",
      "total_flops_so_far": 2.380460245943141e+16,
      "budget_used_percent": 23.804602459431408
    },
    {
      "type": "training",
      "description": "Training step 2001",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:09",
      "total_flops_so_far": 2.38164845755356e+16,
      "budget_used_percent": 23.8164845755356
    },
    {
      "type": "training",
      "description": "Training step 2002",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:09",
      "total_flops_so_far": 2.382836669163979e+16,
      "budget_used_percent": 23.82836669163979
    },
    {
      "type": "training",
      "description": "Training step 2003",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:10",
      "total_flops_so_far": 2.3840248807743984e+16,
      "budget_used_percent": 23.840248807743983
    },
    {
      "type": "training",
      "description": "Training step 2004",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:10",
      "total_flops_so_far": 2.3852130923848176e+16,
      "budget_used_percent": 23.852130923848176
    },
    {
      "type": "training",
      "description": "Training step 2005",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:11",
      "total_flops_so_far": 2.386401303995237e+16,
      "budget_used_percent": 23.86401303995237
    },
    {
      "type": "training",
      "description": "Training step 2006",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:12",
      "total_flops_so_far": 2.387589515605656e+16,
      "budget_used_percent": 23.875895156056558
    },
    {
      "type": "training",
      "description": "Training step 2007",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:12",
      "total_flops_so_far": 2.388777727216075e+16,
      "budget_used_percent": 23.88777727216075
    },
    {
      "type": "training",
      "description": "Training step 2008",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:13",
      "total_flops_so_far": 2.3899659388264944e+16,
      "budget_used_percent": 23.899659388264944
    },
    {
      "type": "training",
      "description": "Training step 2009",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:14",
      "total_flops_so_far": 2.3911541504369136e+16,
      "budget_used_percent": 23.911541504369136
    },
    {
      "type": "training",
      "description": "Training step 2010",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:14",
      "total_flops_so_far": 2.392342362047333e+16,
      "budget_used_percent": 23.92342362047333
    },
    {
      "type": "training",
      "description": "Training step 2011",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:15",
      "total_flops_so_far": 2.393530573657752e+16,
      "budget_used_percent": 23.93530573657752
    },
    {
      "type": "training",
      "description": "Training step 2012",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:15",
      "total_flops_so_far": 2.394718785268171e+16,
      "budget_used_percent": 23.94718785268171
    },
    {
      "type": "training",
      "description": "Training step 2013",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:16",
      "total_flops_so_far": 2.3959069968785904e+16,
      "budget_used_percent": 23.959069968785904
    },
    {
      "type": "training",
      "description": "Training step 2014",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:17",
      "total_flops_so_far": 2.3970952084890096e+16,
      "budget_used_percent": 23.970952084890097
    },
    {
      "type": "training",
      "description": "Training step 2015",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:17",
      "total_flops_so_far": 2.398283420099429e+16,
      "budget_used_percent": 23.982834200994287
    },
    {
      "type": "training",
      "description": "Training step 2016",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:18",
      "total_flops_so_far": 2.399471631709848e+16,
      "budget_used_percent": 23.99471631709848
    },
    {
      "type": "training",
      "description": "Training step 2017",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:19",
      "total_flops_so_far": 2.400659843320267e+16,
      "budget_used_percent": 24.006598433202672
    },
    {
      "type": "training",
      "description": "Training step 2018",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:19",
      "total_flops_so_far": 2.4018480549306864e+16,
      "budget_used_percent": 24.018480549306865
    },
    {
      "type": "training",
      "description": "Training step 2019",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:20",
      "total_flops_so_far": 2.4030362665411056e+16,
      "budget_used_percent": 24.030362665411058
    },
    {
      "type": "training",
      "description": "Training step 2020",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:20",
      "total_flops_so_far": 2.404224478151525e+16,
      "budget_used_percent": 24.042244781515247
    },
    {
      "type": "training",
      "description": "Training step 2021",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:21",
      "total_flops_so_far": 2.405412689761944e+16,
      "budget_used_percent": 24.05412689761944
    },
    {
      "type": "training",
      "description": "Training step 2022",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:22",
      "total_flops_so_far": 2.406600901372363e+16,
      "budget_used_percent": 24.066009013723633
    },
    {
      "type": "training",
      "description": "Training step 2023",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:22",
      "total_flops_so_far": 2.4077891129827824e+16,
      "budget_used_percent": 24.077891129827826
    },
    {
      "type": "training",
      "description": "Training step 2024",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:23",
      "total_flops_so_far": 2.4089773245932016e+16,
      "budget_used_percent": 24.089773245932015
    },
    {
      "type": "training",
      "description": "Training step 2025",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:24",
      "total_flops_so_far": 2.410165536203621e+16,
      "budget_used_percent": 24.101655362036208
    },
    {
      "type": "training",
      "description": "Training step 2026",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:24",
      "total_flops_so_far": 2.41135374781404e+16,
      "budget_used_percent": 24.1135374781404
    },
    {
      "type": "training",
      "description": "Training step 2027",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:25",
      "total_flops_so_far": 2.412541959424459e+16,
      "budget_used_percent": 24.125419594244594
    },
    {
      "type": "training",
      "description": "Training step 2028",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:25",
      "total_flops_so_far": 2.4137301710348784e+16,
      "budget_used_percent": 24.137301710348787
    },
    {
      "type": "training",
      "description": "Training step 2029",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:26",
      "total_flops_so_far": 2.4149183826452976e+16,
      "budget_used_percent": 24.149183826452976
    },
    {
      "type": "training",
      "description": "Training step 2030",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:27",
      "total_flops_so_far": 2.416106594255717e+16,
      "budget_used_percent": 24.161065942557165
    },
    {
      "type": "training",
      "description": "Training step 2031",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:27",
      "total_flops_so_far": 2.417294805866136e+16,
      "budget_used_percent": 24.17294805866136
    },
    {
      "type": "training",
      "description": "Training step 2032",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:28",
      "total_flops_so_far": 2.418483017476555e+16,
      "budget_used_percent": 24.18483017476555
    },
    {
      "type": "training",
      "description": "Training step 2033",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:29",
      "total_flops_so_far": 2.4196712290869744e+16,
      "budget_used_percent": 24.196712290869744
    },
    {
      "type": "training",
      "description": "Training step 2034",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:29",
      "total_flops_so_far": 2.4208594406973936e+16,
      "budget_used_percent": 24.208594406973933
    },
    {
      "type": "training",
      "description": "Training step 2035",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:30",
      "total_flops_so_far": 2.422047652307813e+16,
      "budget_used_percent": 24.220476523078126
    },
    {
      "type": "training",
      "description": "Training step 2036",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:30",
      "total_flops_so_far": 2.423235863918232e+16,
      "budget_used_percent": 24.23235863918232
    },
    {
      "type": "training",
      "description": "Training step 2037",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:31",
      "total_flops_so_far": 2.424424075528651e+16,
      "budget_used_percent": 24.244240755286512
    },
    {
      "type": "training",
      "description": "Training step 2038",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:32",
      "total_flops_so_far": 2.4256122871390704e+16,
      "budget_used_percent": 24.256122871390705
    },
    {
      "type": "training",
      "description": "Training step 2039",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:32",
      "total_flops_so_far": 2.4268004987494896e+16,
      "budget_used_percent": 24.268004987494894
    },
    {
      "type": "training",
      "description": "Training step 2040",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:33",
      "total_flops_so_far": 2.427988710359909e+16,
      "budget_used_percent": 24.279887103599087
    },
    {
      "type": "training",
      "description": "Training step 2041",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:33",
      "total_flops_so_far": 2.429176921970328e+16,
      "budget_used_percent": 24.29176921970328
    },
    {
      "type": "training",
      "description": "Training step 2042",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:34",
      "total_flops_so_far": 2.430365133580747e+16,
      "budget_used_percent": 24.303651335807473
    },
    {
      "type": "training",
      "description": "Training step 2043",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:35",
      "total_flops_so_far": 2.4315533451911664e+16,
      "budget_used_percent": 24.315533451911662
    },
    {
      "type": "training",
      "description": "Training step 2044",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:35",
      "total_flops_so_far": 2.4327415568015856e+16,
      "budget_used_percent": 24.327415568015855
    },
    {
      "type": "training",
      "description": "Training step 2045",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:36",
      "total_flops_so_far": 2.433929768412005e+16,
      "budget_used_percent": 24.339297684120048
    },
    {
      "type": "training",
      "description": "Training step 2046",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:37",
      "total_flops_so_far": 2.435117980022424e+16,
      "budget_used_percent": 24.35117980022424
    },
    {
      "type": "training",
      "description": "Training step 2047",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:37",
      "total_flops_so_far": 2.436306191632843e+16,
      "budget_used_percent": 24.363061916328434
    },
    {
      "type": "training",
      "description": "Training step 2048",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:38",
      "total_flops_so_far": 2.4374944032432624e+16,
      "budget_used_percent": 24.374944032432623
    },
    {
      "type": "training",
      "description": "Training step 2049",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:38",
      "total_flops_so_far": 2.4386826148536816e+16,
      "budget_used_percent": 24.386826148536816
    },
    {
      "type": "training",
      "description": "Training step 2050",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:39",
      "total_flops_so_far": 2.439870826464101e+16,
      "budget_used_percent": 24.39870826464101
    },
    {
      "type": "training",
      "description": "Training step 2051",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:40",
      "total_flops_so_far": 2.44105903807452e+16,
      "budget_used_percent": 24.4105903807452
    },
    {
      "type": "training",
      "description": "Training step 2052",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:40",
      "total_flops_so_far": 2.442247249684939e+16,
      "budget_used_percent": 24.422472496849394
    },
    {
      "type": "training",
      "description": "Training step 2053",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:41",
      "total_flops_so_far": 2.4434354612953584e+16,
      "budget_used_percent": 24.434354612953584
    },
    {
      "type": "training",
      "description": "Training step 2054",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:42",
      "total_flops_so_far": 2.4446236729057776e+16,
      "budget_used_percent": 24.446236729057777
    },
    {
      "type": "training",
      "description": "Training step 2055",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:42",
      "total_flops_so_far": 2.445811884516197e+16,
      "budget_used_percent": 24.45811884516197
    },
    {
      "type": "training",
      "description": "Training step 2056",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:43",
      "total_flops_so_far": 2.447000096126616e+16,
      "budget_used_percent": 24.470000961266162
    },
    {
      "type": "training",
      "description": "Training step 2057",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:43",
      "total_flops_so_far": 2.448188307737035e+16,
      "budget_used_percent": 24.48188307737035
    },
    {
      "type": "training",
      "description": "Training step 2058",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:44",
      "total_flops_so_far": 2.4493765193474544e+16,
      "budget_used_percent": 24.49376519347454
    },
    {
      "type": "training",
      "description": "Training step 2059",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:45",
      "total_flops_so_far": 2.4505647309578736e+16,
      "budget_used_percent": 24.505647309578734
    },
    {
      "type": "training",
      "description": "Training step 2060",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:45",
      "total_flops_so_far": 2.451752942568293e+16,
      "budget_used_percent": 24.517529425682927
    },
    {
      "type": "training",
      "description": "Training step 2061",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:46",
      "total_flops_so_far": 2.452941154178712e+16,
      "budget_used_percent": 24.52941154178712
    },
    {
      "type": "training",
      "description": "Training step 2062",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:47",
      "total_flops_so_far": 2.454129365789131e+16,
      "budget_used_percent": 24.541293657891313
    },
    {
      "type": "training",
      "description": "Training step 2063",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:47",
      "total_flops_so_far": 2.4553175773995504e+16,
      "budget_used_percent": 24.553175773995502
    },
    {
      "type": "training",
      "description": "Training step 2064",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:48",
      "total_flops_so_far": 2.4565057890099696e+16,
      "budget_used_percent": 24.565057890099695
    },
    {
      "type": "training",
      "description": "Training step 2065",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:48",
      "total_flops_so_far": 2.457694000620389e+16,
      "budget_used_percent": 24.576940006203888
    },
    {
      "type": "training",
      "description": "Training step 2066",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:49",
      "total_flops_so_far": 2.458882212230808e+16,
      "budget_used_percent": 24.58882212230808
    },
    {
      "type": "training",
      "description": "Training step 2067",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:50",
      "total_flops_so_far": 2.460070423841227e+16,
      "budget_used_percent": 24.60070423841227
    },
    {
      "type": "training",
      "description": "Training step 2068",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:50",
      "total_flops_so_far": 2.4612586354516464e+16,
      "budget_used_percent": 24.612586354516463
    },
    {
      "type": "training",
      "description": "Training step 2069",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:51",
      "total_flops_so_far": 2.4624468470620656e+16,
      "budget_used_percent": 24.624468470620656
    },
    {
      "type": "training",
      "description": "Training step 2070",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:52",
      "total_flops_so_far": 2.463635058672485e+16,
      "budget_used_percent": 24.63635058672485
    },
    {
      "type": "training",
      "description": "Training step 2071",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:52",
      "total_flops_so_far": 2.464823270282904e+16,
      "budget_used_percent": 24.64823270282904
    },
    {
      "type": "training",
      "description": "Training step 2072",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:53",
      "total_flops_so_far": 2.466011481893323e+16,
      "budget_used_percent": 24.66011481893323
    },
    {
      "type": "training",
      "description": "Training step 2073",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:53",
      "total_flops_so_far": 2.4671996935037424e+16,
      "budget_used_percent": 24.671996935037424
    },
    {
      "type": "training",
      "description": "Training step 2074",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:54",
      "total_flops_so_far": 2.4683879051141616e+16,
      "budget_used_percent": 24.683879051141616
    },
    {
      "type": "training",
      "description": "Training step 2075",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:55",
      "total_flops_so_far": 2.469576116724581e+16,
      "budget_used_percent": 24.69576116724581
    },
    {
      "type": "training",
      "description": "Training step 2076",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:55",
      "total_flops_so_far": 2.470764328335e+16,
      "budget_used_percent": 24.707643283350002
    },
    {
      "type": "training",
      "description": "Training step 2077",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:56",
      "total_flops_so_far": 2.471952539945419e+16,
      "budget_used_percent": 24.71952539945419
    },
    {
      "type": "training",
      "description": "Training step 2078",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:57",
      "total_flops_so_far": 2.4731407515558384e+16,
      "budget_used_percent": 24.731407515558384
    },
    {
      "type": "training",
      "description": "Training step 2079",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:57",
      "total_flops_so_far": 2.4743289631662576e+16,
      "budget_used_percent": 24.743289631662577
    },
    {
      "type": "training",
      "description": "Training step 2080",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:58",
      "total_flops_so_far": 2.475517174776677e+16,
      "budget_used_percent": 24.75517174776677
    },
    {
      "type": "training",
      "description": "Training step 2081",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:58",
      "total_flops_so_far": 2.476705386387096e+16,
      "budget_used_percent": 24.76705386387096
    },
    {
      "type": "training",
      "description": "Training step 2082",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:29:59",
      "total_flops_so_far": 2.477893597997515e+16,
      "budget_used_percent": 24.778935979975152
    },
    {
      "type": "training",
      "description": "Training step 2083",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:00",
      "total_flops_so_far": 2.4790818096079344e+16,
      "budget_used_percent": 24.790818096079345
    },
    {
      "type": "training",
      "description": "Training step 2084",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:00",
      "total_flops_so_far": 2.4802700212183536e+16,
      "budget_used_percent": 24.802700212183538
    },
    {
      "type": "training",
      "description": "Training step 2085",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:01",
      "total_flops_so_far": 2.481458232828773e+16,
      "budget_used_percent": 24.81458232828773
    },
    {
      "type": "training",
      "description": "Training step 2086",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:02",
      "total_flops_so_far": 2.482646444439192e+16,
      "budget_used_percent": 24.82646444439192
    },
    {
      "type": "training",
      "description": "Training step 2087",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:02",
      "total_flops_so_far": 2.483834656049611e+16,
      "budget_used_percent": 24.83834656049611
    },
    {
      "type": "training",
      "description": "Training step 2088",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:03",
      "total_flops_so_far": 2.4850228676600304e+16,
      "budget_used_percent": 24.850228676600302
    },
    {
      "type": "training",
      "description": "Training step 2089",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:03",
      "total_flops_so_far": 2.4862110792704496e+16,
      "budget_used_percent": 24.862110792704495
    },
    {
      "type": "training",
      "description": "Training step 2090",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:04",
      "total_flops_so_far": 2.487399290880869e+16,
      "budget_used_percent": 24.873992908808688
    },
    {
      "type": "training",
      "description": "Training step 2091",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:05",
      "total_flops_so_far": 2.488587502491288e+16,
      "budget_used_percent": 24.885875024912878
    },
    {
      "type": "training",
      "description": "Training step 2092",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:05",
      "total_flops_so_far": 2.489775714101707e+16,
      "budget_used_percent": 24.89775714101707
    },
    {
      "type": "training",
      "description": "Training step 2093",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:06",
      "total_flops_so_far": 2.4909639257121264e+16,
      "budget_used_percent": 24.909639257121263
    },
    {
      "type": "training",
      "description": "Training step 2094",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:07",
      "total_flops_so_far": 2.4921521373225456e+16,
      "budget_used_percent": 24.921521373225456
    },
    {
      "type": "training",
      "description": "Training step 2095",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:07",
      "total_flops_so_far": 2.493340348932965e+16,
      "budget_used_percent": 24.93340348932965
    },
    {
      "type": "training",
      "description": "Training step 2096",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:08",
      "total_flops_so_far": 2.494528560543384e+16,
      "budget_used_percent": 24.94528560543384
    },
    {
      "type": "training",
      "description": "Training step 2097",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:08",
      "total_flops_so_far": 2.495716772153803e+16,
      "budget_used_percent": 24.95716772153803
    },
    {
      "type": "training",
      "description": "Training step 2098",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:09",
      "total_flops_so_far": 2.4969049837642224e+16,
      "budget_used_percent": 24.969049837642224
    },
    {
      "type": "training",
      "description": "Training step 2099",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:10",
      "total_flops_so_far": 2.4980931953746416e+16,
      "budget_used_percent": 24.980931953746417
    },
    {
      "type": "training",
      "description": "Training step 2100",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:10",
      "total_flops_so_far": 2.499281406985061e+16,
      "budget_used_percent": 24.992814069850606
    },
    {
      "type": "training",
      "description": "Training step 2101",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:11",
      "total_flops_so_far": 2.50046961859548e+16,
      "budget_used_percent": 25.0046961859548
    },
    {
      "type": "training",
      "description": "Training step 2102",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:12",
      "total_flops_so_far": 2.501657830205899e+16,
      "budget_used_percent": 25.01657830205899
    },
    {
      "type": "training",
      "description": "Training step 2103",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:12",
      "total_flops_so_far": 2.5028460418163184e+16,
      "budget_used_percent": 25.028460418163185
    },
    {
      "type": "training",
      "description": "Training step 2104",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:13",
      "total_flops_so_far": 2.5040342534267376e+16,
      "budget_used_percent": 25.040342534267374
    },
    {
      "type": "training",
      "description": "Training step 2105",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:13",
      "total_flops_so_far": 2.505222465037157e+16,
      "budget_used_percent": 25.052224650371567
    },
    {
      "type": "training",
      "description": "Training step 2106",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:14",
      "total_flops_so_far": 2.506410676647576e+16,
      "budget_used_percent": 25.064106766475756
    },
    {
      "type": "training",
      "description": "Training step 2107",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:14",
      "total_flops_so_far": 2.507598888257995e+16,
      "budget_used_percent": 25.075988882579953
    },
    {
      "type": "training",
      "description": "Training step 2108",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:15",
      "total_flops_so_far": 2.5087870998684144e+16,
      "budget_used_percent": 25.087870998684142
    },
    {
      "type": "training",
      "description": "Training step 2109",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:16",
      "total_flops_so_far": 2.5099753114788336e+16,
      "budget_used_percent": 25.099753114788335
    },
    {
      "type": "training",
      "description": "Training step 2110",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:16",
      "total_flops_so_far": 2.511163523089253e+16,
      "budget_used_percent": 25.111635230892524
    },
    {
      "type": "training",
      "description": "Training step 2111",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:17",
      "total_flops_so_far": 2.512351734699672e+16,
      "budget_used_percent": 25.12351734699672
    },
    {
      "type": "training",
      "description": "Training step 2112",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:18",
      "total_flops_so_far": 2.513539946310091e+16,
      "budget_used_percent": 25.13539946310091
    },
    {
      "type": "training",
      "description": "Training step 2113",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:18",
      "total_flops_so_far": 2.5147281579205104e+16,
      "budget_used_percent": 25.147281579205107
    },
    {
      "type": "training",
      "description": "Training step 2114",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:19",
      "total_flops_so_far": 2.5159163695309296e+16,
      "budget_used_percent": 25.159163695309296
    },
    {
      "type": "training",
      "description": "Training step 2115",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:19",
      "total_flops_so_far": 2.517104581141349e+16,
      "budget_used_percent": 25.17104581141349
    },
    {
      "type": "training",
      "description": "Training step 2116",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:20",
      "total_flops_so_far": 2.518292792751768e+16,
      "budget_used_percent": 25.182927927517678
    },
    {
      "type": "training",
      "description": "Training step 2117",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:21",
      "total_flops_so_far": 2.519481004362187e+16,
      "budget_used_percent": 25.194810043621874
    },
    {
      "type": "training",
      "description": "Training step 2118",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:21",
      "total_flops_so_far": 2.5206692159726064e+16,
      "budget_used_percent": 25.206692159726064
    },
    {
      "type": "training",
      "description": "Training step 2119",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:22",
      "total_flops_so_far": 2.5218574275830256e+16,
      "budget_used_percent": 25.218574275830257
    },
    {
      "type": "training",
      "description": "Training step 2120",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:23",
      "total_flops_so_far": 2.523045639193445e+16,
      "budget_used_percent": 25.230456391934446
    },
    {
      "type": "training",
      "description": "Training step 2121",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:23",
      "total_flops_so_far": 2.524233850803864e+16,
      "budget_used_percent": 25.242338508038642
    },
    {
      "type": "training",
      "description": "Training step 2122",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:24",
      "total_flops_so_far": 2.525422062414283e+16,
      "budget_used_percent": 25.25422062414283
    },
    {
      "type": "training",
      "description": "Training step 2123",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:24",
      "total_flops_so_far": 2.5266102740247024e+16,
      "budget_used_percent": 25.266102740247025
    },
    {
      "type": "training",
      "description": "Training step 2124",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:25",
      "total_flops_so_far": 2.5277984856351216e+16,
      "budget_used_percent": 25.277984856351214
    },
    {
      "type": "training",
      "description": "Training step 2125",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:26",
      "total_flops_so_far": 2.528986697245541e+16,
      "budget_used_percent": 25.28986697245541
    },
    {
      "type": "training",
      "description": "Training step 2126",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:26",
      "total_flops_so_far": 2.53017490885596e+16,
      "budget_used_percent": 25.3017490885596
    },
    {
      "type": "training",
      "description": "Training step 2127",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:27",
      "total_flops_so_far": 2.531363120466379e+16,
      "budget_used_percent": 25.313631204663796
    },
    {
      "type": "training",
      "description": "Training step 2128",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:28",
      "total_flops_so_far": 2.5325513320767984e+16,
      "budget_used_percent": 25.325513320767985
    },
    {
      "type": "training",
      "description": "Training step 2129",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:28",
      "total_flops_so_far": 2.5337395436872176e+16,
      "budget_used_percent": 25.33739543687218
    },
    {
      "type": "training",
      "description": "Training step 2130",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:29",
      "total_flops_so_far": 2.534927755297637e+16,
      "budget_used_percent": 25.349277552976368
    },
    {
      "type": "training",
      "description": "Training step 2131",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:29",
      "total_flops_so_far": 2.536115966908056e+16,
      "budget_used_percent": 25.361159669080557
    },
    {
      "type": "training",
      "description": "Training step 2132",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:30",
      "total_flops_so_far": 2.537304178518475e+16,
      "budget_used_percent": 25.373041785184753
    },
    {
      "type": "training",
      "description": "Training step 2133",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:31",
      "total_flops_so_far": 2.5384923901288944e+16,
      "budget_used_percent": 25.384923901288943
    },
    {
      "type": "training",
      "description": "Training step 2134",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:31",
      "total_flops_so_far": 2.5396806017393136e+16,
      "budget_used_percent": 25.396806017393136
    },
    {
      "type": "training",
      "description": "Training step 2135",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:32",
      "total_flops_so_far": 2.540868813349733e+16,
      "budget_used_percent": 25.408688133497325
    },
    {
      "type": "training",
      "description": "Training step 2136",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:33",
      "total_flops_so_far": 2.542057024960152e+16,
      "budget_used_percent": 25.42057024960152
    },
    {
      "type": "training",
      "description": "Training step 2137",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:33",
      "total_flops_so_far": 2.543245236570571e+16,
      "budget_used_percent": 25.43245236570571
    },
    {
      "type": "training",
      "description": "Training step 2138",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:34",
      "total_flops_so_far": 2.5444334481809904e+16,
      "budget_used_percent": 25.444334481809904
    },
    {
      "type": "training",
      "description": "Training step 2139",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:34",
      "total_flops_so_far": 2.5456216597914096e+16,
      "budget_used_percent": 25.456216597914093
    },
    {
      "type": "training",
      "description": "Training step 2140",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:35",
      "total_flops_so_far": 2.546809871401829e+16,
      "budget_used_percent": 25.46809871401829
    },
    {
      "type": "training",
      "description": "Training step 2141",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:36",
      "total_flops_so_far": 2.547998083012248e+16,
      "budget_used_percent": 25.47998083012248
    },
    {
      "type": "training",
      "description": "Training step 2142",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:36",
      "total_flops_so_far": 2.549186294622667e+16,
      "budget_used_percent": 25.491862946226675
    },
    {
      "type": "training",
      "description": "Training step 2143",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:37",
      "total_flops_so_far": 2.5503745062330864e+16,
      "budget_used_percent": 25.503745062330864
    },
    {
      "type": "training",
      "description": "Training step 2144",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:38",
      "total_flops_so_far": 2.5515627178435056e+16,
      "budget_used_percent": 25.515627178435057
    },
    {
      "type": "training",
      "description": "Training step 2145",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:38",
      "total_flops_so_far": 2.552750929453925e+16,
      "budget_used_percent": 25.527509294539247
    },
    {
      "type": "training",
      "description": "Training step 2146",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:39",
      "total_flops_so_far": 2.553939141064344e+16,
      "budget_used_percent": 25.539391410643443
    },
    {
      "type": "training",
      "description": "Training step 2147",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:39",
      "total_flops_so_far": 2.555127352674763e+16,
      "budget_used_percent": 25.551273526747632
    },
    {
      "type": "training",
      "description": "Training step 2148",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:40",
      "total_flops_so_far": 2.5563155642851824e+16,
      "budget_used_percent": 25.563155642851825
    },
    {
      "type": "training",
      "description": "Training step 2149",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:41",
      "total_flops_so_far": 2.5575037758956016e+16,
      "budget_used_percent": 25.575037758956014
    },
    {
      "type": "training",
      "description": "Training step 2150",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:41",
      "total_flops_so_far": 2.558691987506021e+16,
      "budget_used_percent": 25.58691987506021
    },
    {
      "type": "training",
      "description": "Training step 2151",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:42",
      "total_flops_so_far": 2.55988019911644e+16,
      "budget_used_percent": 25.5988019911644
    },
    {
      "type": "training",
      "description": "Training step 2152",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:43",
      "total_flops_so_far": 2.561068410726859e+16,
      "budget_used_percent": 25.610684107268593
    },
    {
      "type": "training",
      "description": "Training step 2153",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:43",
      "total_flops_so_far": 2.5622566223372784e+16,
      "budget_used_percent": 25.622566223372782
    },
    {
      "type": "training",
      "description": "Training step 2154",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:44",
      "total_flops_so_far": 2.5634448339476976e+16,
      "budget_used_percent": 25.63444833947698
    },
    {
      "type": "training",
      "description": "Training step 2155",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:44",
      "total_flops_so_far": 2.564633045558117e+16,
      "budget_used_percent": 25.646330455581168
    },
    {
      "type": "training",
      "description": "Training step 2156",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:45",
      "total_flops_so_far": 2.565821257168536e+16,
      "budget_used_percent": 25.65821257168536
    },
    {
      "type": "training",
      "description": "Training step 2157",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:46",
      "total_flops_so_far": 2.567009468778955e+16,
      "budget_used_percent": 25.67009468778955
    },
    {
      "type": "training",
      "description": "Training step 2158",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:46",
      "total_flops_so_far": 2.5681976803893744e+16,
      "budget_used_percent": 25.68197680389374
    },
    {
      "type": "training",
      "description": "Training step 2159",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:47",
      "total_flops_so_far": 2.5693858919997936e+16,
      "budget_used_percent": 25.693858919997936
    },
    {
      "type": "training",
      "description": "Training step 2160",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:48",
      "total_flops_so_far": 2.570574103610213e+16,
      "budget_used_percent": 25.705741036102125
    },
    {
      "type": "training",
      "description": "Training step 2161",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:48",
      "total_flops_so_far": 2.571762315220632e+16,
      "budget_used_percent": 25.717623152206322
    },
    {
      "type": "training",
      "description": "Training step 2162",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:49",
      "total_flops_so_far": 2.572950526831051e+16,
      "budget_used_percent": 25.72950526831051
    },
    {
      "type": "training",
      "description": "Training step 2163",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:50",
      "total_flops_so_far": 2.5741387384414704e+16,
      "budget_used_percent": 25.741387384414704
    },
    {
      "type": "training",
      "description": "Training step 2164",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:50",
      "total_flops_so_far": 2.5753269500518896e+16,
      "budget_used_percent": 25.753269500518893
    },
    {
      "type": "training",
      "description": "Training step 2165",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:51",
      "total_flops_so_far": 2.576515161662309e+16,
      "budget_used_percent": 25.76515161662309
    },
    {
      "type": "training",
      "description": "Training step 2166",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:51",
      "total_flops_so_far": 2.577703373272728e+16,
      "budget_used_percent": 25.77703373272728
    },
    {
      "type": "training",
      "description": "Training step 2167",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:52",
      "total_flops_so_far": 2.578891584883147e+16,
      "budget_used_percent": 25.788915848831472
    },
    {
      "type": "training",
      "description": "Training step 2168",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:53",
      "total_flops_so_far": 2.5800797964935664e+16,
      "budget_used_percent": 25.80079796493566
    },
    {
      "type": "training",
      "description": "Training step 2169",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:53",
      "total_flops_so_far": 2.5812680081039856e+16,
      "budget_used_percent": 25.812680081039858
    },
    {
      "type": "training",
      "description": "Training step 2170",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:54",
      "total_flops_so_far": 2.582456219714405e+16,
      "budget_used_percent": 25.824562197144047
    },
    {
      "type": "training",
      "description": "Training step 2171",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:55",
      "total_flops_so_far": 2.583644431324824e+16,
      "budget_used_percent": 25.83644431324824
    },
    {
      "type": "training",
      "description": "Training step 2172",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:55",
      "total_flops_so_far": 2.584832642935243e+16,
      "budget_used_percent": 25.84832642935243
    },
    {
      "type": "training",
      "description": "Training step 2173",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:56",
      "total_flops_so_far": 2.5860208545456624e+16,
      "budget_used_percent": 25.860208545456626
    },
    {
      "type": "training",
      "description": "Training step 2174",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:56",
      "total_flops_so_far": 2.5872090661560816e+16,
      "budget_used_percent": 25.872090661560815
    },
    {
      "type": "training",
      "description": "Training step 2175",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:57",
      "total_flops_so_far": 2.588397277766501e+16,
      "budget_used_percent": 25.883972777665008
    },
    {
      "type": "training",
      "description": "Training step 2176",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:58",
      "total_flops_so_far": 2.58958548937692e+16,
      "budget_used_percent": 25.895854893769197
    },
    {
      "type": "training",
      "description": "Training step 2177",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:58",
      "total_flops_so_far": 2.590773700987339e+16,
      "budget_used_percent": 25.907737009873394
    },
    {
      "type": "training",
      "description": "Training step 2178",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:30:59",
      "total_flops_so_far": 2.5919619125977584e+16,
      "budget_used_percent": 25.919619125977583
    },
    {
      "type": "training",
      "description": "Training step 2179",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:00",
      "total_flops_so_far": 2.5931501242081776e+16,
      "budget_used_percent": 25.93150124208178
    },
    {
      "type": "training",
      "description": "Training step 2180",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:00",
      "total_flops_so_far": 2.594338335818597e+16,
      "budget_used_percent": 25.94338335818597
    },
    {
      "type": "training",
      "description": "Training step 2181",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:01",
      "total_flops_so_far": 2.595526547429016e+16,
      "budget_used_percent": 25.95526547429016
    },
    {
      "type": "training",
      "description": "Training step 2182",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:01",
      "total_flops_so_far": 2.596714759039435e+16,
      "budget_used_percent": 25.96714759039435
    },
    {
      "type": "training",
      "description": "Training step 2183",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:02",
      "total_flops_so_far": 2.5979029706498544e+16,
      "budget_used_percent": 25.979029706498547
    },
    {
      "type": "training",
      "description": "Training step 2184",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:03",
      "total_flops_so_far": 2.5990911822602736e+16,
      "budget_used_percent": 25.990911822602737
    },
    {
      "type": "training",
      "description": "Training step 2185",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:03",
      "total_flops_so_far": 2.600279393870693e+16,
      "budget_used_percent": 26.00279393870693
    },
    {
      "type": "training",
      "description": "Training step 2186",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:04",
      "total_flops_so_far": 2.601467605481112e+16,
      "budget_used_percent": 26.01467605481112
    },
    {
      "type": "training",
      "description": "Training step 2187",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:05",
      "total_flops_so_far": 2.602655817091531e+16,
      "budget_used_percent": 26.026558170915308
    },
    {
      "type": "training",
      "description": "Training step 2188",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:05",
      "total_flops_so_far": 2.6038440287019504e+16,
      "budget_used_percent": 26.038440287019505
    },
    {
      "type": "training",
      "description": "Training step 2189",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:06",
      "total_flops_so_far": 2.6050322403123696e+16,
      "budget_used_percent": 26.050322403123694
    },
    {
      "type": "training",
      "description": "Training step 2190",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:06",
      "total_flops_so_far": 2.606220451922789e+16,
      "budget_used_percent": 26.062204519227887
    },
    {
      "type": "training",
      "description": "Training step 2191",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:07",
      "total_flops_so_far": 2.607408663533208e+16,
      "budget_used_percent": 26.074086635332076
    },
    {
      "type": "training",
      "description": "Training step 2192",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:08",
      "total_flops_so_far": 2.608596875143627e+16,
      "budget_used_percent": 26.085968751436273
    },
    {
      "type": "training",
      "description": "Training step 2193",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:08",
      "total_flops_so_far": 2.6097850867540464e+16,
      "budget_used_percent": 26.097850867540462
    },
    {
      "type": "training",
      "description": "Training step 2194",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:09",
      "total_flops_so_far": 2.6109732983644656e+16,
      "budget_used_percent": 26.10973298364466
    },
    {
      "type": "training",
      "description": "Training step 2195",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:10",
      "total_flops_so_far": 2.612161509974885e+16,
      "budget_used_percent": 26.121615099748848
    },
    {
      "type": "training",
      "description": "Training step 2196",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:10",
      "total_flops_so_far": 2.613349721585304e+16,
      "budget_used_percent": 26.13349721585304
    },
    {
      "type": "training",
      "description": "Training step 2197",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:11",
      "total_flops_so_far": 2.614537933195723e+16,
      "budget_used_percent": 26.14537933195723
    },
    {
      "type": "training",
      "description": "Training step 2198",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:11",
      "total_flops_so_far": 2.6157261448061424e+16,
      "budget_used_percent": 26.157261448061426
    },
    {
      "type": "training",
      "description": "Training step 2199",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:12",
      "total_flops_so_far": 2.6169143564165616e+16,
      "budget_used_percent": 26.169143564165616
    },
    {
      "type": "training",
      "description": "Training step 2200",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:13",
      "total_flops_so_far": 2.618102568026981e+16,
      "budget_used_percent": 26.18102568026981
    },
    {
      "type": "training",
      "description": "Training step 2201",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:13",
      "total_flops_so_far": 2.6192907796374e+16,
      "budget_used_percent": 26.192907796373998
    },
    {
      "type": "training",
      "description": "Training step 2202",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:14",
      "total_flops_so_far": 2.620478991247819e+16,
      "budget_used_percent": 26.204789912478194
    },
    {
      "type": "training",
      "description": "Training step 2203",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:15",
      "total_flops_so_far": 2.6216672028582384e+16,
      "budget_used_percent": 26.216672028582384
    },
    {
      "type": "training",
      "description": "Training step 2204",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:15",
      "total_flops_so_far": 2.6228554144686576e+16,
      "budget_used_percent": 26.228554144686576
    },
    {
      "type": "training",
      "description": "Training step 2205",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:16",
      "total_flops_so_far": 2.624043626079077e+16,
      "budget_used_percent": 26.240436260790766
    },
    {
      "type": "training",
      "description": "Training step 2206",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:16",
      "total_flops_so_far": 2.625231837689496e+16,
      "budget_used_percent": 26.252318376894962
    },
    {
      "type": "training",
      "description": "Training step 2207",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:17",
      "total_flops_so_far": 2.626420049299915e+16,
      "budget_used_percent": 26.26420049299915
    },
    {
      "type": "training",
      "description": "Training step 2208",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:18",
      "total_flops_so_far": 2.6276082609103344e+16,
      "budget_used_percent": 26.276082609103348
    },
    {
      "type": "training",
      "description": "Training step 2209",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:18",
      "total_flops_so_far": 2.6287964725207536e+16,
      "budget_used_percent": 26.287964725207537
    },
    {
      "type": "training",
      "description": "Training step 2210",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:19",
      "total_flops_so_far": 2.629984684131173e+16,
      "budget_used_percent": 26.29984684131173
    },
    {
      "type": "training",
      "description": "Training step 2211",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:20",
      "total_flops_so_far": 2.631172895741592e+16,
      "budget_used_percent": 26.31172895741592
    },
    {
      "type": "training",
      "description": "Training step 2212",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:20",
      "total_flops_so_far": 2.632361107352011e+16,
      "budget_used_percent": 26.323611073520116
    },
    {
      "type": "training",
      "description": "Training step 2213",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:21",
      "total_flops_so_far": 2.6335493189624304e+16,
      "budget_used_percent": 26.335493189624305
    },
    {
      "type": "training",
      "description": "Training step 2214",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:21",
      "total_flops_so_far": 2.6347375305728496e+16,
      "budget_used_percent": 26.347375305728498
    },
    {
      "type": "training",
      "description": "Training step 2215",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:22",
      "total_flops_so_far": 2.635925742183269e+16,
      "budget_used_percent": 26.359257421832687
    },
    {
      "type": "training",
      "description": "Training step 2216",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:23",
      "total_flops_so_far": 2.637113953793688e+16,
      "budget_used_percent": 26.371139537936877
    },
    {
      "type": "training",
      "description": "Training step 2217",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:23",
      "total_flops_so_far": 2.638302165404107e+16,
      "budget_used_percent": 26.383021654041073
    },
    {
      "type": "training",
      "description": "Training step 2218",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:24",
      "total_flops_so_far": 2.6394903770145264e+16,
      "budget_used_percent": 26.394903770145262
    },
    {
      "type": "training",
      "description": "Training step 2219",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:25",
      "total_flops_so_far": 2.6406785886249456e+16,
      "budget_used_percent": 26.406785886249455
    },
    {
      "type": "training",
      "description": "Training step 2220",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:25",
      "total_flops_so_far": 2.641866800235365e+16,
      "budget_used_percent": 26.418668002353645
    },
    {
      "type": "training",
      "description": "Training step 2221",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:26",
      "total_flops_so_far": 2.643055011845784e+16,
      "budget_used_percent": 26.43055011845784
    },
    {
      "type": "training",
      "description": "Training step 2222",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:27",
      "total_flops_so_far": 2.644243223456203e+16,
      "budget_used_percent": 26.44243223456203
    },
    {
      "type": "training",
      "description": "Training step 2223",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:27",
      "total_flops_so_far": 2.6454314350666224e+16,
      "budget_used_percent": 26.454314350666223
    },
    {
      "type": "training",
      "description": "Training step 2224",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:28",
      "total_flops_so_far": 2.6466196466770416e+16,
      "budget_used_percent": 26.466196466770413
    },
    {
      "type": "training",
      "description": "Training step 2225",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:28",
      "total_flops_so_far": 2.647807858287461e+16,
      "budget_used_percent": 26.47807858287461
    },
    {
      "type": "training",
      "description": "Training step 2226",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:29",
      "total_flops_so_far": 2.64899606989788e+16,
      "budget_used_percent": 26.4899606989788
    },
    {
      "type": "training",
      "description": "Training step 2227",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:30",
      "total_flops_so_far": 2.650184281508299e+16,
      "budget_used_percent": 26.501842815082995
    },
    {
      "type": "training",
      "description": "Training step 2228",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:30",
      "total_flops_so_far": 2.6513724931187184e+16,
      "budget_used_percent": 26.513724931187184
    },
    {
      "type": "training",
      "description": "Training step 2229",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:31",
      "total_flops_so_far": 2.6525607047291376e+16,
      "budget_used_percent": 26.525607047291377
    },
    {
      "type": "training",
      "description": "Training step 2230",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:32",
      "total_flops_so_far": 2.653748916339557e+16,
      "budget_used_percent": 26.537489163395566
    },
    {
      "type": "training",
      "description": "Training step 2231",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:32",
      "total_flops_so_far": 2.654937127949976e+16,
      "budget_used_percent": 26.549371279499763
    },
    {
      "type": "training",
      "description": "Training step 2232",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:33",
      "total_flops_so_far": 2.656125339560395e+16,
      "budget_used_percent": 26.561253395603952
    },
    {
      "type": "training",
      "description": "Training step 2233",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:34",
      "total_flops_so_far": 2.6573135511708144e+16,
      "budget_used_percent": 26.573135511708145
    },
    {
      "type": "training",
      "description": "Training step 2234",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:34",
      "total_flops_so_far": 2.6585017627812336e+16,
      "budget_used_percent": 26.585017627812334
    },
    {
      "type": "training",
      "description": "Training step 2235",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:35",
      "total_flops_so_far": 2.659689974391653e+16,
      "budget_used_percent": 26.59689974391653
    },
    {
      "type": "training",
      "description": "Training step 2236",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:36",
      "total_flops_so_far": 2.660878186002072e+16,
      "budget_used_percent": 26.60878186002072
    },
    {
      "type": "training",
      "description": "Training step 2237",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:36",
      "total_flops_so_far": 2.662066397612491e+16,
      "budget_used_percent": 26.620663976124913
    },
    {
      "type": "training",
      "description": "Training step 2238",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:37",
      "total_flops_so_far": 2.6632546092229104e+16,
      "budget_used_percent": 26.632546092229102
    },
    {
      "type": "training",
      "description": "Training step 2239",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:37",
      "total_flops_so_far": 2.6644428208333296e+16,
      "budget_used_percent": 26.6444282083333
    },
    {
      "type": "training",
      "description": "Training step 2240",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:38",
      "total_flops_so_far": 2.665631032443749e+16,
      "budget_used_percent": 26.656310324437488
    },
    {
      "type": "training",
      "description": "Training step 2241",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:39",
      "total_flops_so_far": 2.666819244054168e+16,
      "budget_used_percent": 26.66819244054168
    },
    {
      "type": "training",
      "description": "Training step 2242",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:39",
      "total_flops_so_far": 2.668007455664587e+16,
      "budget_used_percent": 26.68007455664587
    },
    {
      "type": "training",
      "description": "Training step 2243",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:40",
      "total_flops_so_far": 2.6691956672750064e+16,
      "budget_used_percent": 26.69195667275006
    },
    {
      "type": "training",
      "description": "Training step 2244",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:41",
      "total_flops_so_far": 2.6703838788854256e+16,
      "budget_used_percent": 26.703838788854256
    },
    {
      "type": "training",
      "description": "Training step 2245",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:41",
      "total_flops_so_far": 2.671572090495845e+16,
      "budget_used_percent": 26.715720904958445
    },
    {
      "type": "training",
      "description": "Training step 2246",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:42",
      "total_flops_so_far": 2.672760302106264e+16,
      "budget_used_percent": 26.72760302106264
    },
    {
      "type": "training",
      "description": "Training step 2247",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:43",
      "total_flops_so_far": 2.673948513716683e+16,
      "budget_used_percent": 26.73948513716683
    },
    {
      "type": "training",
      "description": "Training step 2248",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:43",
      "total_flops_so_far": 2.6751367253271024e+16,
      "budget_used_percent": 26.751367253271024
    },
    {
      "type": "training",
      "description": "Training step 2249",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:44",
      "total_flops_so_far": 2.6763249369375216e+16,
      "budget_used_percent": 26.763249369375213
    },
    {
      "type": "training",
      "description": "Training step 2250",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:44",
      "total_flops_so_far": 2.677513148547941e+16,
      "budget_used_percent": 26.77513148547941
    },
    {
      "type": "training",
      "description": "Training step 2251",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:45",
      "total_flops_so_far": 2.67870136015836e+16,
      "budget_used_percent": 26.7870136015836
    },
    {
      "type": "training",
      "description": "Training step 2252",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:46",
      "total_flops_so_far": 2.679889571768779e+16,
      "budget_used_percent": 26.79889571768779
    },
    {
      "type": "training",
      "description": "Training step 2253",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:46",
      "total_flops_so_far": 2.6810777833791984e+16,
      "budget_used_percent": 26.81077783379198
    },
    {
      "type": "training",
      "description": "Training step 2254",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:47",
      "total_flops_so_far": 2.6822659949896176e+16,
      "budget_used_percent": 26.822659949896178
    },
    {
      "type": "training",
      "description": "Training step 2255",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:48",
      "total_flops_so_far": 2.683454206600037e+16,
      "budget_used_percent": 26.834542066000367
    },
    {
      "type": "training",
      "description": "Training step 2256",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:48",
      "total_flops_so_far": 2.684642418210456e+16,
      "budget_used_percent": 26.84642418210456
    },
    {
      "type": "training",
      "description": "Training step 2257",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:49",
      "total_flops_so_far": 2.685830629820875e+16,
      "budget_used_percent": 26.85830629820875
    },
    {
      "type": "training",
      "description": "Training step 2258",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:49",
      "total_flops_so_far": 2.6870188414312944e+16,
      "budget_used_percent": 26.870188414312945
    },
    {
      "type": "training",
      "description": "Training step 2259",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:50",
      "total_flops_so_far": 2.6882070530417136e+16,
      "budget_used_percent": 26.882070530417135
    },
    {
      "type": "training",
      "description": "Training step 2260",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:51",
      "total_flops_so_far": 2.689395264652133e+16,
      "budget_used_percent": 26.89395264652133
    },
    {
      "type": "training",
      "description": "Training step 2261",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:51",
      "total_flops_so_far": 2.690583476262552e+16,
      "budget_used_percent": 26.90583476262552
    },
    {
      "type": "training",
      "description": "Training step 2262",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:52",
      "total_flops_so_far": 2.691771687872971e+16,
      "budget_used_percent": 26.917716878729713
    },
    {
      "type": "training",
      "description": "Training step 2263",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:53",
      "total_flops_so_far": 2.6929598994833904e+16,
      "budget_used_percent": 26.929598994833903
    },
    {
      "type": "training",
      "description": "Training step 2264",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:53",
      "total_flops_so_far": 2.6941481110938096e+16,
      "budget_used_percent": 26.9414811109381
    },
    {
      "type": "training",
      "description": "Training step 2265",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:54",
      "total_flops_so_far": 2.695336322704229e+16,
      "budget_used_percent": 26.95336322704229
    },
    {
      "type": "training",
      "description": "Training step 2266",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:54",
      "total_flops_so_far": 2.696524534314648e+16,
      "budget_used_percent": 26.96524534314648
    },
    {
      "type": "training",
      "description": "Training step 2267",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:55",
      "total_flops_so_far": 2.697712745925067e+16,
      "budget_used_percent": 26.97712745925067
    },
    {
      "type": "training",
      "description": "Training step 2268",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:56",
      "total_flops_so_far": 2.6989009575354864e+16,
      "budget_used_percent": 26.989009575354867
    },
    {
      "type": "training",
      "description": "Training step 2269",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:56",
      "total_flops_so_far": 2.7000891691459056e+16,
      "budget_used_percent": 27.000891691459056
    },
    {
      "type": "training",
      "description": "Training step 2270",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:57",
      "total_flops_so_far": 2.701277380756325e+16,
      "budget_used_percent": 27.01277380756325
    },
    {
      "type": "training",
      "description": "Training step 2271",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:58",
      "total_flops_so_far": 2.702465592366744e+16,
      "budget_used_percent": 27.02465592366744
    },
    {
      "type": "training",
      "description": "Training step 2272",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:58",
      "total_flops_so_far": 2.703653803977163e+16,
      "budget_used_percent": 27.036538039771628
    },
    {
      "type": "training",
      "description": "Training step 2273",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:31:59",
      "total_flops_so_far": 2.7048420155875824e+16,
      "budget_used_percent": 27.048420155875824
    },
    {
      "type": "training",
      "description": "Training step 2274",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:00",
      "total_flops_so_far": 2.7060302271980016e+16,
      "budget_used_percent": 27.060302271980014
    },
    {
      "type": "training",
      "description": "Training step 2275",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:00",
      "total_flops_so_far": 2.707218438808421e+16,
      "budget_used_percent": 27.07218438808421
    },
    {
      "type": "training",
      "description": "Training step 2276",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:01",
      "total_flops_so_far": 2.70840665041884e+16,
      "budget_used_percent": 27.0840665041884
    },
    {
      "type": "training",
      "description": "Training step 2277",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:01",
      "total_flops_so_far": 2.709594862029259e+16,
      "budget_used_percent": 27.095948620292592
    },
    {
      "type": "training",
      "description": "Training step 2278",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:02",
      "total_flops_so_far": 2.7107830736396784e+16,
      "budget_used_percent": 27.10783073639678
    },
    {
      "type": "training",
      "description": "Training step 2279",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:03",
      "total_flops_so_far": 2.7119712852500976e+16,
      "budget_used_percent": 27.119712852500978
    },
    {
      "type": "training",
      "description": "Training step 2280",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:03",
      "total_flops_so_far": 2.713159496860517e+16,
      "budget_used_percent": 27.131594968605167
    },
    {
      "type": "training",
      "description": "Training step 2281",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:04",
      "total_flops_so_far": 2.714347708470936e+16,
      "budget_used_percent": 27.14347708470936
    },
    {
      "type": "training",
      "description": "Training step 2282",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:05",
      "total_flops_so_far": 2.715535920081355e+16,
      "budget_used_percent": 27.15535920081355
    },
    {
      "type": "training",
      "description": "Training step 2283",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:05",
      "total_flops_so_far": 2.7167241316917744e+16,
      "budget_used_percent": 27.167241316917746
    },
    {
      "type": "training",
      "description": "Training step 2284",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:06",
      "total_flops_so_far": 2.7179123433021936e+16,
      "budget_used_percent": 27.179123433021935
    },
    {
      "type": "training",
      "description": "Training step 2285",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:06",
      "total_flops_so_far": 2.719100554912613e+16,
      "budget_used_percent": 27.191005549126128
    },
    {
      "type": "training",
      "description": "Training step 2286",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:07",
      "total_flops_so_far": 2.720288766523032e+16,
      "budget_used_percent": 27.202887665230318
    },
    {
      "type": "training",
      "description": "Training step 2287",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:08",
      "total_flops_so_far": 2.721476978133451e+16,
      "budget_used_percent": 27.214769781334514
    },
    {
      "type": "training",
      "description": "Training step 2288",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:08",
      "total_flops_so_far": 2.7226651897438704e+16,
      "budget_used_percent": 27.226651897438703
    },
    {
      "type": "training",
      "description": "Training step 2289",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:09",
      "total_flops_so_far": 2.7238534013542896e+16,
      "budget_used_percent": 27.238534013542896
    },
    {
      "type": "training",
      "description": "Training step 2290",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:10",
      "total_flops_so_far": 2.725041612964709e+16,
      "budget_used_percent": 27.250416129647085
    },
    {
      "type": "training",
      "description": "Training step 2291",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:10",
      "total_flops_so_far": 2.726229824575128e+16,
      "budget_used_percent": 27.262298245751282
    },
    {
      "type": "training",
      "description": "Training step 2292",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:11",
      "total_flops_so_far": 2.727418036185547e+16,
      "budget_used_percent": 27.27418036185547
    },
    {
      "type": "training",
      "description": "Training step 2293",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:11",
      "total_flops_so_far": 2.7286062477959664e+16,
      "budget_used_percent": 27.286062477959668
    },
    {
      "type": "training",
      "description": "Training step 2294",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:12",
      "total_flops_so_far": 2.7297944594063856e+16,
      "budget_used_percent": 27.297944594063857
    },
    {
      "type": "training",
      "description": "Training step 2295",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:13",
      "total_flops_so_far": 2.730982671016805e+16,
      "budget_used_percent": 27.30982671016805
    },
    {
      "type": "training",
      "description": "Training step 2296",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:13",
      "total_flops_so_far": 2.732170882627224e+16,
      "budget_used_percent": 27.32170882627224
    },
    {
      "type": "training",
      "description": "Training step 2297",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:14",
      "total_flops_so_far": 2.733359094237643e+16,
      "budget_used_percent": 27.333590942376436
    },
    {
      "type": "training",
      "description": "Training step 2298",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:15",
      "total_flops_so_far": 2.7345473058480624e+16,
      "budget_used_percent": 27.345473058480625
    },
    {
      "type": "training",
      "description": "Training step 2299",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:15",
      "total_flops_so_far": 2.7357355174584816e+16,
      "budget_used_percent": 27.357355174584818
    },
    {
      "type": "training",
      "description": "Training step 2300",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:16",
      "total_flops_so_far": 2.736923729068901e+16,
      "budget_used_percent": 27.369237290689007
    },
    {
      "type": "training",
      "description": "Training step 2301",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:17",
      "total_flops_so_far": 2.73811194067932e+16,
      "budget_used_percent": 27.381119406793196
    },
    {
      "type": "training",
      "description": "Training step 2302",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:17",
      "total_flops_so_far": 2.739300152289739e+16,
      "budget_used_percent": 27.393001522897393
    },
    {
      "type": "training",
      "description": "Training step 2303",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:18",
      "total_flops_so_far": 2.7404883639001584e+16,
      "budget_used_percent": 27.404883639001582
    },
    {
      "type": "training",
      "description": "Training step 2304",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:18",
      "total_flops_so_far": 2.7416765755105776e+16,
      "budget_used_percent": 27.416765755105775
    },
    {
      "type": "training",
      "description": "Training step 2305",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:19",
      "total_flops_so_far": 2.742864787120997e+16,
      "budget_used_percent": 27.428647871209964
    },
    {
      "type": "training",
      "description": "Training step 2306",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:20",
      "total_flops_so_far": 2.744052998731416e+16,
      "budget_used_percent": 27.44052998731416
    },
    {
      "type": "training",
      "description": "Training step 2307",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:20",
      "total_flops_so_far": 2.745241210341835e+16,
      "budget_used_percent": 27.45241210341835
    },
    {
      "type": "training",
      "description": "Training step 2308",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:21",
      "total_flops_so_far": 2.7464294219522544e+16,
      "budget_used_percent": 27.464294219522543
    },
    {
      "type": "training",
      "description": "Training step 2309",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:22",
      "total_flops_so_far": 2.7476176335626736e+16,
      "budget_used_percent": 27.476176335626732
    },
    {
      "type": "training",
      "description": "Training step 2310",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:22",
      "total_flops_so_far": 2.748805845173093e+16,
      "budget_used_percent": 27.48805845173093
    },
    {
      "type": "training",
      "description": "Training step 2311",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:23",
      "total_flops_so_far": 2.749994056783512e+16,
      "budget_used_percent": 27.499940567835118
    },
    {
      "type": "training",
      "description": "Training step 2312",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:23",
      "total_flops_so_far": 2.751182268393931e+16,
      "budget_used_percent": 27.511822683939315
    },
    {
      "type": "training",
      "description": "Training step 2313",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:24",
      "total_flops_so_far": 2.7523704800043504e+16,
      "budget_used_percent": 27.523704800043504
    },
    {
      "type": "training",
      "description": "Training step 2314",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:25",
      "total_flops_so_far": 2.7535586916147696e+16,
      "budget_used_percent": 27.535586916147697
    },
    {
      "type": "training",
      "description": "Training step 2315",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:25",
      "total_flops_so_far": 2.754746903225189e+16,
      "budget_used_percent": 27.547469032251886
    },
    {
      "type": "training",
      "description": "Training step 2316",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:26",
      "total_flops_so_far": 2.755935114835608e+16,
      "budget_used_percent": 27.559351148356082
    },
    {
      "type": "training",
      "description": "Training step 2317",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:27",
      "total_flops_so_far": 2.757123326446027e+16,
      "budget_used_percent": 27.571233264460272
    },
    {
      "type": "training",
      "description": "Training step 2318",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:27",
      "total_flops_so_far": 2.7583115380564464e+16,
      "budget_used_percent": 27.583115380564465
    },
    {
      "type": "training",
      "description": "Training step 2319",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:28",
      "total_flops_so_far": 2.7594997496668656e+16,
      "budget_used_percent": 27.594997496668654
    },
    {
      "type": "training",
      "description": "Training step 2320",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:28",
      "total_flops_so_far": 2.760687961277285e+16,
      "budget_used_percent": 27.60687961277285
    },
    {
      "type": "training",
      "description": "Training step 2321",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:29",
      "total_flops_so_far": 2.761876172887704e+16,
      "budget_used_percent": 27.61876172887704
    },
    {
      "type": "training",
      "description": "Training step 2322",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:30",
      "total_flops_so_far": 2.763064384498123e+16,
      "budget_used_percent": 27.630643844981233
    },
    {
      "type": "training",
      "description": "Training step 2323",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:30",
      "total_flops_so_far": 2.7642525961085424e+16,
      "budget_used_percent": 27.642525961085422
    },
    {
      "type": "training",
      "description": "Training step 2324",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:31",
      "total_flops_so_far": 2.7654408077189616e+16,
      "budget_used_percent": 27.65440807718962
    },
    {
      "type": "training",
      "description": "Training step 2325",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:32",
      "total_flops_so_far": 2.766629019329381e+16,
      "budget_used_percent": 27.666290193293808
    },
    {
      "type": "training",
      "description": "Training step 2326",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:32",
      "total_flops_so_far": 2.7678172309398e+16,
      "budget_used_percent": 27.678172309398004
    },
    {
      "type": "training",
      "description": "Training step 2327",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:33",
      "total_flops_so_far": 2.769005442550219e+16,
      "budget_used_percent": 27.690054425502193
    },
    {
      "type": "training",
      "description": "Training step 2328",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:34",
      "total_flops_so_far": 2.7701936541606384e+16,
      "budget_used_percent": 27.701936541606386
    },
    {
      "type": "training",
      "description": "Training step 2329",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:34",
      "total_flops_so_far": 2.7713818657710576e+16,
      "budget_used_percent": 27.713818657710576
    },
    {
      "type": "training",
      "description": "Training step 2330",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:35",
      "total_flops_so_far": 2.772570077381477e+16,
      "budget_used_percent": 27.725700773814765
    },
    {
      "type": "training",
      "description": "Training step 2331",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:35",
      "total_flops_so_far": 2.773758288991896e+16,
      "budget_used_percent": 27.73758288991896
    },
    {
      "type": "training",
      "description": "Training step 2332",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:36",
      "total_flops_so_far": 2.774946500602315e+16,
      "budget_used_percent": 27.74946500602315
    },
    {
      "type": "training",
      "description": "Training step 2333",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:37",
      "total_flops_so_far": 2.7761347122127344e+16,
      "budget_used_percent": 27.761347122127344
    },
    {
      "type": "training",
      "description": "Training step 2334",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:37",
      "total_flops_so_far": 2.7773229238231536e+16,
      "budget_used_percent": 27.773229238231533
    },
    {
      "type": "training",
      "description": "Training step 2335",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:38",
      "total_flops_so_far": 2.778511135433573e+16,
      "budget_used_percent": 27.78511135433573
    },
    {
      "type": "training",
      "description": "Training step 2336",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:39",
      "total_flops_so_far": 2.779699347043992e+16,
      "budget_used_percent": 27.79699347043992
    },
    {
      "type": "training",
      "description": "Training step 2337",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:39",
      "total_flops_so_far": 2.780887558654411e+16,
      "budget_used_percent": 27.80887558654411
    },
    {
      "type": "training",
      "description": "Training step 2338",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:40",
      "total_flops_so_far": 2.7820757702648304e+16,
      "budget_used_percent": 27.8207577026483
    },
    {
      "type": "training",
      "description": "Training step 2339",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:40",
      "total_flops_so_far": 2.7832639818752496e+16,
      "budget_used_percent": 27.832639818752497
    },
    {
      "type": "training",
      "description": "Training step 2340",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:41",
      "total_flops_so_far": 2.784452193485669e+16,
      "budget_used_percent": 27.844521934856687
    },
    {
      "type": "training",
      "description": "Training step 2341",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:42",
      "total_flops_so_far": 2.785640405096088e+16,
      "budget_used_percent": 27.856404050960883
    },
    {
      "type": "training",
      "description": "Training step 2342",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:42",
      "total_flops_so_far": 2.786828616706507e+16,
      "budget_used_percent": 27.868286167065072
    },
    {
      "type": "training",
      "description": "Training step 2343",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:43",
      "total_flops_so_far": 2.7880168283169264e+16,
      "budget_used_percent": 27.880168283169265
    },
    {
      "type": "training",
      "description": "Training step 2344",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:44",
      "total_flops_so_far": 2.7892050399273456e+16,
      "budget_used_percent": 27.892050399273455
    },
    {
      "type": "training",
      "description": "Training step 2345",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:44",
      "total_flops_so_far": 2.790393251537765e+16,
      "budget_used_percent": 27.90393251537765
    },
    {
      "type": "training",
      "description": "Training step 2346",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:45",
      "total_flops_so_far": 2.791581463148184e+16,
      "budget_used_percent": 27.91581463148184
    },
    {
      "type": "training",
      "description": "Training step 2347",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:46",
      "total_flops_so_far": 2.792769674758603e+16,
      "budget_used_percent": 27.927696747586033
    },
    {
      "type": "training",
      "description": "Training step 2348",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:46",
      "total_flops_so_far": 2.7939578863690224e+16,
      "budget_used_percent": 27.939578863690222
    },
    {
      "type": "training",
      "description": "Training step 2349",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:47",
      "total_flops_so_far": 2.7951460979794416e+16,
      "budget_used_percent": 27.95146097979442
    },
    {
      "type": "training",
      "description": "Training step 2350",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:47",
      "total_flops_so_far": 2.796334309589861e+16,
      "budget_used_percent": 27.96334309589861
    },
    {
      "type": "training",
      "description": "Training step 2351",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:48",
      "total_flops_so_far": 2.79752252120028e+16,
      "budget_used_percent": 27.9752252120028
    },
    {
      "type": "training",
      "description": "Training step 2352",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:49",
      "total_flops_so_far": 2.798710732810699e+16,
      "budget_used_percent": 27.98710732810699
    },
    {
      "type": "training",
      "description": "Training step 2353",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:49",
      "total_flops_so_far": 2.7998989444211184e+16,
      "budget_used_percent": 27.998989444211187
    },
    {
      "type": "training",
      "description": "Training step 2354",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:50",
      "total_flops_so_far": 2.8010871560315376e+16,
      "budget_used_percent": 28.010871560315376
    },
    {
      "type": "training",
      "description": "Training step 2355",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:51",
      "total_flops_so_far": 2.802275367641957e+16,
      "budget_used_percent": 28.02275367641957
    },
    {
      "type": "training",
      "description": "Training step 2356",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:51",
      "total_flops_so_far": 2.803463579252376e+16,
      "budget_used_percent": 28.03463579252376
    },
    {
      "type": "training",
      "description": "Training step 2357",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:52",
      "total_flops_so_far": 2.804651790862795e+16,
      "budget_used_percent": 28.046517908627948
    },
    {
      "type": "training",
      "description": "Training step 2358",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:52",
      "total_flops_so_far": 2.8058400024732144e+16,
      "budget_used_percent": 28.058400024732144
    },
    {
      "type": "training",
      "description": "Training step 2359",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:53",
      "total_flops_so_far": 2.8070282140836336e+16,
      "budget_used_percent": 28.070282140836333
    },
    {
      "type": "training",
      "description": "Training step 2360",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:54",
      "total_flops_so_far": 2.808216425694053e+16,
      "budget_used_percent": 28.08216425694053
    },
    {
      "type": "training",
      "description": "Training step 2361",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:54",
      "total_flops_so_far": 2.809404637304472e+16,
      "budget_used_percent": 28.09404637304472
    },
    {
      "type": "training",
      "description": "Training step 2362",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:55",
      "total_flops_so_far": 2.810592848914891e+16,
      "budget_used_percent": 28.105928489148912
    },
    {
      "type": "training",
      "description": "Training step 2363",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:56",
      "total_flops_so_far": 2.8117810605253104e+16,
      "budget_used_percent": 28.1178106052531
    },
    {
      "type": "training",
      "description": "Training step 2364",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:56",
      "total_flops_so_far": 2.8129692721357296e+16,
      "budget_used_percent": 28.129692721357298
    },
    {
      "type": "training",
      "description": "Training step 2365",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:57",
      "total_flops_so_far": 2.814157483746149e+16,
      "budget_used_percent": 28.141574837461487
    },
    {
      "type": "training",
      "description": "Training step 2366",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:58",
      "total_flops_so_far": 2.815345695356568e+16,
      "budget_used_percent": 28.15345695356568
    },
    {
      "type": "training",
      "description": "Training step 2367",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:58",
      "total_flops_so_far": 2.816533906966987e+16,
      "budget_used_percent": 28.16533906966987
    },
    {
      "type": "training",
      "description": "Training step 2368",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:59",
      "total_flops_so_far": 2.8177221185774064e+16,
      "budget_used_percent": 28.177221185774066
    },
    {
      "type": "training",
      "description": "Training step 2369",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:32:59",
      "total_flops_so_far": 2.8189103301878256e+16,
      "budget_used_percent": 28.189103301878255
    },
    {
      "type": "training",
      "description": "Training step 2370",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:00",
      "total_flops_so_far": 2.820098541798245e+16,
      "budget_used_percent": 28.200985417982448
    },
    {
      "type": "training",
      "description": "Training step 2371",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:00",
      "total_flops_so_far": 2.821286753408664e+16,
      "budget_used_percent": 28.212867534086637
    },
    {
      "type": "training",
      "description": "Training step 2372",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:01",
      "total_flops_so_far": 2.822474965019083e+16,
      "budget_used_percent": 28.224749650190834
    },
    {
      "type": "training",
      "description": "Training step 2373",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:02",
      "total_flops_so_far": 2.8236631766295024e+16,
      "budget_used_percent": 28.236631766295023
    },
    {
      "type": "training",
      "description": "Training step 2374",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:02",
      "total_flops_so_far": 2.8248513882399216e+16,
      "budget_used_percent": 28.248513882399216
    },
    {
      "type": "training",
      "description": "Training step 2375",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:03",
      "total_flops_so_far": 2.826039599850341e+16,
      "budget_used_percent": 28.260395998503405
    },
    {
      "type": "training",
      "description": "Training step 2376",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:04",
      "total_flops_so_far": 2.82722781146076e+16,
      "budget_used_percent": 28.2722781146076
    },
    {
      "type": "training",
      "description": "Training step 2377",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:04",
      "total_flops_so_far": 2.828416023071179e+16,
      "budget_used_percent": 28.28416023071179
    },
    {
      "type": "training",
      "description": "Training step 2378",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:05",
      "total_flops_so_far": 2.8296042346815984e+16,
      "budget_used_percent": 28.296042346815987
    },
    {
      "type": "training",
      "description": "Training step 2379",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:05",
      "total_flops_so_far": 2.8307924462920176e+16,
      "budget_used_percent": 28.307924462920177
    },
    {
      "type": "training",
      "description": "Training step 2380",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:06",
      "total_flops_so_far": 2.831980657902437e+16,
      "budget_used_percent": 28.31980657902437
    },
    {
      "type": "training",
      "description": "Training step 2381",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:07",
      "total_flops_so_far": 2.833168869512856e+16,
      "budget_used_percent": 28.33168869512856
    },
    {
      "type": "training",
      "description": "Training step 2382",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:07",
      "total_flops_so_far": 2.834357081123275e+16,
      "budget_used_percent": 28.343570811232755
    },
    {
      "type": "training",
      "description": "Training step 2383",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:08",
      "total_flops_so_far": 2.8355452927336944e+16,
      "budget_used_percent": 28.355452927336945
    },
    {
      "type": "training",
      "description": "Training step 2384",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:09",
      "total_flops_so_far": 2.8367335043441136e+16,
      "budget_used_percent": 28.367335043441138
    },
    {
      "type": "training",
      "description": "Training step 2385",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:09",
      "total_flops_so_far": 2.837921715954533e+16,
      "budget_used_percent": 28.379217159545327
    },
    {
      "type": "training",
      "description": "Training step 2386",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:10",
      "total_flops_so_far": 2.839109927564952e+16,
      "budget_used_percent": 28.391099275649516
    },
    {
      "type": "training",
      "description": "Training step 2387",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:11",
      "total_flops_so_far": 2.840298139175371e+16,
      "budget_used_percent": 28.402981391753713
    },
    {
      "type": "training",
      "description": "Training step 2388",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:11",
      "total_flops_so_far": 2.8414863507857904e+16,
      "budget_used_percent": 28.414863507857902
    },
    {
      "type": "training",
      "description": "Training step 2389",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:12",
      "total_flops_so_far": 2.8426745623962096e+16,
      "budget_used_percent": 28.426745623962095
    },
    {
      "type": "training",
      "description": "Training step 2390",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:12",
      "total_flops_so_far": 2.843862774006629e+16,
      "budget_used_percent": 28.438627740066284
    },
    {
      "type": "training",
      "description": "Training step 2391",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:13",
      "total_flops_so_far": 2.845050985617048e+16,
      "budget_used_percent": 28.45050985617048
    },
    {
      "type": "training",
      "description": "Training step 2392",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:14",
      "total_flops_so_far": 2.846239197227467e+16,
      "budget_used_percent": 28.46239197227467
    },
    {
      "type": "training",
      "description": "Training step 2393",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:14",
      "total_flops_so_far": 2.8474274088378864e+16,
      "budget_used_percent": 28.474274088378866
    },
    {
      "type": "training",
      "description": "Training step 2394",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:15",
      "total_flops_so_far": 2.8486156204483056e+16,
      "budget_used_percent": 28.486156204483056
    },
    {
      "type": "training",
      "description": "Training step 2395",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:16",
      "total_flops_so_far": 2.849803832058725e+16,
      "budget_used_percent": 28.49803832058725
    },
    {
      "type": "training",
      "description": "Training step 2396",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:16",
      "total_flops_so_far": 2.850992043669144e+16,
      "budget_used_percent": 28.509920436691438
    },
    {
      "type": "training",
      "description": "Training step 2397",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:17",
      "total_flops_so_far": 2.852180255279563e+16,
      "budget_used_percent": 28.521802552795634
    },
    {
      "type": "training",
      "description": "Training step 2398",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:18",
      "total_flops_so_far": 2.8533684668899824e+16,
      "budget_used_percent": 28.533684668899824
    },
    {
      "type": "training",
      "description": "Training step 2399",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:18",
      "total_flops_so_far": 2.8545566785004016e+16,
      "budget_used_percent": 28.545566785004016
    },
    {
      "type": "training",
      "description": "Training step 2400",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:19",
      "total_flops_so_far": 2.855744890110821e+16,
      "budget_used_percent": 28.557448901108206
    },
    {
      "type": "training",
      "description": "Training step 2401",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:20",
      "total_flops_so_far": 2.85693310172124e+16,
      "budget_used_percent": 28.569331017212402
    },
    {
      "type": "training",
      "description": "Training step 2402",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:20",
      "total_flops_so_far": 2.858121313331659e+16,
      "budget_used_percent": 28.58121313331659
    },
    {
      "type": "training",
      "description": "Training step 2403",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:21",
      "total_flops_so_far": 2.8593095249420784e+16,
      "budget_used_percent": 28.593095249420784
    },
    {
      "type": "training",
      "description": "Training step 2404",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:22",
      "total_flops_so_far": 2.8604977365524976e+16,
      "budget_used_percent": 28.604977365524974
    },
    {
      "type": "training",
      "description": "Training step 2405",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:22",
      "total_flops_so_far": 2.861685948162917e+16,
      "budget_used_percent": 28.61685948162917
    },
    {
      "type": "training",
      "description": "Training step 2406",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:23",
      "total_flops_so_far": 2.862874159773336e+16,
      "budget_used_percent": 28.62874159773336
    },
    {
      "type": "training",
      "description": "Training step 2407",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:24",
      "total_flops_so_far": 2.864062371383755e+16,
      "budget_used_percent": 28.640623713837556
    },
    {
      "type": "training",
      "description": "Training step 2408",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:24",
      "total_flops_so_far": 2.8652505829941744e+16,
      "budget_used_percent": 28.652505829941745
    },
    {
      "type": "training",
      "description": "Training step 2409",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:25",
      "total_flops_so_far": 2.8664387946045936e+16,
      "budget_used_percent": 28.664387946045938
    },
    {
      "type": "training",
      "description": "Training step 2410",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:25",
      "total_flops_so_far": 2.867627006215013e+16,
      "budget_used_percent": 28.676270062150127
    },
    {
      "type": "training",
      "description": "Training step 2411",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:26",
      "total_flops_so_far": 2.868815217825432e+16,
      "budget_used_percent": 28.688152178254324
    },
    {
      "type": "training",
      "description": "Training step 2412",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:27",
      "total_flops_so_far": 2.870003429435851e+16,
      "budget_used_percent": 28.700034294358513
    },
    {
      "type": "training",
      "description": "Training step 2413",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:27",
      "total_flops_so_far": 2.8711916410462704e+16,
      "budget_used_percent": 28.711916410462706
    },
    {
      "type": "training",
      "description": "Training step 2414",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:28",
      "total_flops_so_far": 2.8723798526566896e+16,
      "budget_used_percent": 28.723798526566895
    },
    {
      "type": "training",
      "description": "Training step 2415",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:29",
      "total_flops_so_far": 2.873568064267109e+16,
      "budget_used_percent": 28.735680642671085
    },
    {
      "type": "training",
      "description": "Training step 2416",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:29",
      "total_flops_so_far": 2.874756275877528e+16,
      "budget_used_percent": 28.74756275877528
    },
    {
      "type": "training",
      "description": "Training step 2417",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:30",
      "total_flops_so_far": 2.875944487487947e+16,
      "budget_used_percent": 28.75944487487947
    },
    {
      "type": "training",
      "description": "Training step 2418",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:30",
      "total_flops_so_far": 2.8771326990983664e+16,
      "budget_used_percent": 28.771326990983663
    },
    {
      "type": "training",
      "description": "Training step 2419",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:31",
      "total_flops_so_far": 2.8783209107087856e+16,
      "budget_used_percent": 28.783209107087853
    },
    {
      "type": "training",
      "description": "Training step 2420",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:32",
      "total_flops_so_far": 2.879509122319205e+16,
      "budget_used_percent": 28.79509122319205
    },
    {
      "type": "training",
      "description": "Training step 2421",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:32",
      "total_flops_so_far": 2.880697333929624e+16,
      "budget_used_percent": 28.80697333929624
    },
    {
      "type": "training",
      "description": "Training step 2422",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:33",
      "total_flops_so_far": 2.881885545540043e+16,
      "budget_used_percent": 28.81885545540043
    },
    {
      "type": "training",
      "description": "Training step 2423",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:34",
      "total_flops_so_far": 2.8830737571504624e+16,
      "budget_used_percent": 28.83073757150462
    },
    {
      "type": "training",
      "description": "Training step 2424",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:34",
      "total_flops_so_far": 2.8842619687608816e+16,
      "budget_used_percent": 28.842619687608817
    },
    {
      "type": "training",
      "description": "Training step 2425",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:35",
      "total_flops_so_far": 2.885450180371301e+16,
      "budget_used_percent": 28.854501803713006
    },
    {
      "type": "training",
      "description": "Training step 2426",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:36",
      "total_flops_so_far": 2.88663839198172e+16,
      "budget_used_percent": 28.866383919817203
    },
    {
      "type": "training",
      "description": "Training step 2427",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:36",
      "total_flops_so_far": 2.887826603592139e+16,
      "budget_used_percent": 28.878266035921392
    },
    {
      "type": "training",
      "description": "Training step 2428",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:37",
      "total_flops_so_far": 2.8890148152025584e+16,
      "budget_used_percent": 28.890148152025585
    },
    {
      "type": "training",
      "description": "Training step 2429",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:37",
      "total_flops_so_far": 2.8902030268129776e+16,
      "budget_used_percent": 28.902030268129774
    },
    {
      "type": "training",
      "description": "Training step 2430",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:38",
      "total_flops_so_far": 2.891391238423397e+16,
      "budget_used_percent": 28.91391238423397
    },
    {
      "type": "training",
      "description": "Training step 2431",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:39",
      "total_flops_so_far": 2.892579450033816e+16,
      "budget_used_percent": 28.92579450033816
    },
    {
      "type": "training",
      "description": "Training step 2432",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:39",
      "total_flops_so_far": 2.893767661644235e+16,
      "budget_used_percent": 28.937676616442353
    },
    {
      "type": "training",
      "description": "Training step 2433",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:40",
      "total_flops_so_far": 2.8949558732546544e+16,
      "budget_used_percent": 28.949558732546542
    },
    {
      "type": "training",
      "description": "Training step 2434",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:41",
      "total_flops_so_far": 2.8961440848650736e+16,
      "budget_used_percent": 28.96144084865074
    },
    {
      "type": "training",
      "description": "Training step 2435",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:41",
      "total_flops_so_far": 2.897332296475493e+16,
      "budget_used_percent": 28.973322964754928
    },
    {
      "type": "training",
      "description": "Training step 2436",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:42",
      "total_flops_so_far": 2.898520508085912e+16,
      "budget_used_percent": 28.98520508085912
    },
    {
      "type": "training",
      "description": "Training step 2437",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:43",
      "total_flops_so_far": 2.899708719696331e+16,
      "budget_used_percent": 28.99708719696331
    },
    {
      "type": "training",
      "description": "Training step 2438",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:43",
      "total_flops_so_far": 2.9008969313067504e+16,
      "budget_used_percent": 29.008969313067507
    },
    {
      "type": "training",
      "description": "Training step 2439",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:44",
      "total_flops_so_far": 2.9020851429171696e+16,
      "budget_used_percent": 29.020851429171696
    },
    {
      "type": "training",
      "description": "Training step 2440",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:44",
      "total_flops_so_far": 2.903273354527589e+16,
      "budget_used_percent": 29.03273354527589
    },
    {
      "type": "training",
      "description": "Training step 2441",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:45",
      "total_flops_so_far": 2.904461566138008e+16,
      "budget_used_percent": 29.044615661380078
    },
    {
      "type": "training",
      "description": "Training step 2442",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:46",
      "total_flops_so_far": 2.905649777748427e+16,
      "budget_used_percent": 29.056497777484275
    },
    {
      "type": "training",
      "description": "Training step 2443",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:46",
      "total_flops_so_far": 2.9068379893588464e+16,
      "budget_used_percent": 29.068379893588464
    },
    {
      "type": "training",
      "description": "Training step 2444",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:47",
      "total_flops_so_far": 2.9080262009692656e+16,
      "budget_used_percent": 29.080262009692653
    },
    {
      "type": "training",
      "description": "Training step 2445",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:48",
      "total_flops_so_far": 2.909214412579685e+16,
      "budget_used_percent": 29.09214412579685
    },
    {
      "type": "training",
      "description": "Training step 2446",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:48",
      "total_flops_so_far": 2.910402624190104e+16,
      "budget_used_percent": 29.10402624190104
    },
    {
      "type": "training",
      "description": "Training step 2447",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:49",
      "total_flops_so_far": 2.911590835800523e+16,
      "budget_used_percent": 29.115908358005232
    },
    {
      "type": "training",
      "description": "Training step 2448",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:49",
      "total_flops_so_far": 2.9127790474109424e+16,
      "budget_used_percent": 29.12779047410942
    },
    {
      "type": "training",
      "description": "Training step 2449",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:50",
      "total_flops_so_far": 2.9139672590213616e+16,
      "budget_used_percent": 29.139672590213618
    },
    {
      "type": "training",
      "description": "Training step 2450",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:51",
      "total_flops_so_far": 2.915155470631781e+16,
      "budget_used_percent": 29.151554706317807
    },
    {
      "type": "training",
      "description": "Training step 2451",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:51",
      "total_flops_so_far": 2.9163436822422e+16,
      "budget_used_percent": 29.163436822422
    },
    {
      "type": "training",
      "description": "Training step 2452",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:52",
      "total_flops_so_far": 2.917531893852619e+16,
      "budget_used_percent": 29.17531893852619
    },
    {
      "type": "training",
      "description": "Training step 2453",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:53",
      "total_flops_so_far": 2.9187201054630384e+16,
      "budget_used_percent": 29.187201054630385
    },
    {
      "type": "training",
      "description": "Training step 2454",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:53",
      "total_flops_so_far": 2.9199083170734576e+16,
      "budget_used_percent": 29.199083170734575
    },
    {
      "type": "training",
      "description": "Training step 2455",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:54",
      "total_flops_so_far": 2.921096528683877e+16,
      "budget_used_percent": 29.210965286838768
    },
    {
      "type": "training",
      "description": "Training step 2456",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:55",
      "total_flops_so_far": 2.922284740294296e+16,
      "budget_used_percent": 29.222847402942957
    },
    {
      "type": "training",
      "description": "Training step 2457",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:55",
      "total_flops_so_far": 2.923472951904715e+16,
      "budget_used_percent": 29.234729519047153
    },
    {
      "type": "training",
      "description": "Training step 2458",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:56",
      "total_flops_so_far": 2.9246611635151344e+16,
      "budget_used_percent": 29.246611635151343
    },
    {
      "type": "training",
      "description": "Training step 2459",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:56",
      "total_flops_so_far": 2.9258493751255536e+16,
      "budget_used_percent": 29.25849375125554
    },
    {
      "type": "training",
      "description": "Training step 2460",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:57",
      "total_flops_so_far": 2.927037586735973e+16,
      "budget_used_percent": 29.27037586735973
    },
    {
      "type": "training",
      "description": "Training step 2461",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:58",
      "total_flops_so_far": 2.928225798346392e+16,
      "budget_used_percent": 29.28225798346392
    },
    {
      "type": "training",
      "description": "Training step 2462",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:58",
      "total_flops_so_far": 2.929414009956811e+16,
      "budget_used_percent": 29.29414009956811
    },
    {
      "type": "training",
      "description": "Training step 2463",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:33:59",
      "total_flops_so_far": 2.9306022215672304e+16,
      "budget_used_percent": 29.306022215672307
    },
    {
      "type": "training",
      "description": "Training step 2464",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:00",
      "total_flops_so_far": 2.9317904331776496e+16,
      "budget_used_percent": 29.317904331776496
    },
    {
      "type": "training",
      "description": "Training step 2465",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:00",
      "total_flops_so_far": 2.932978644788069e+16,
      "budget_used_percent": 29.32978644788069
    },
    {
      "type": "training",
      "description": "Training step 2466",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:01",
      "total_flops_so_far": 2.934166856398488e+16,
      "budget_used_percent": 29.34166856398488
    },
    {
      "type": "training",
      "description": "Training step 2467",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:02",
      "total_flops_so_far": 2.935355068008907e+16,
      "budget_used_percent": 29.353550680089075
    },
    {
      "type": "training",
      "description": "Training step 2468",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:02",
      "total_flops_so_far": 2.9365432796193264e+16,
      "budget_used_percent": 29.365432796193264
    },
    {
      "type": "training",
      "description": "Training step 2469",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:03",
      "total_flops_so_far": 2.9377314912297456e+16,
      "budget_used_percent": 29.377314912297457
    },
    {
      "type": "training",
      "description": "Training step 2470",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:03",
      "total_flops_so_far": 2.938919702840165e+16,
      "budget_used_percent": 29.389197028401647
    },
    {
      "type": "training",
      "description": "Training step 2471",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:04",
      "total_flops_so_far": 2.940107914450584e+16,
      "budget_used_percent": 29.401079144505836
    },
    {
      "type": "training",
      "description": "Training step 2472",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:05",
      "total_flops_so_far": 2.941296126061003e+16,
      "budget_used_percent": 29.412961260610032
    },
    {
      "type": "training",
      "description": "Training step 2473",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:05",
      "total_flops_so_far": 2.9424843376714224e+16,
      "budget_used_percent": 29.42484337671422
    },
    {
      "type": "training",
      "description": "Training step 2474",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:06",
      "total_flops_so_far": 2.9436725492818416e+16,
      "budget_used_percent": 29.436725492818418
    },
    {
      "type": "training",
      "description": "Training step 2475",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:07",
      "total_flops_so_far": 2.944860760892261e+16,
      "budget_used_percent": 29.448607608922607
    },
    {
      "type": "training",
      "description": "Training step 2476",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:07",
      "total_flops_so_far": 2.94604897250268e+16,
      "budget_used_percent": 29.4604897250268
    },
    {
      "type": "training",
      "description": "Training step 2477",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:08",
      "total_flops_so_far": 2.947237184113099e+16,
      "budget_used_percent": 29.47237184113099
    },
    {
      "type": "training",
      "description": "Training step 2478",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:09",
      "total_flops_so_far": 2.9484253957235184e+16,
      "budget_used_percent": 29.484253957235186
    },
    {
      "type": "training",
      "description": "Training step 2479",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:09",
      "total_flops_so_far": 2.9496136073339376e+16,
      "budget_used_percent": 29.496136073339375
    },
    {
      "type": "training",
      "description": "Training step 2480",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:10",
      "total_flops_so_far": 2.950801818944357e+16,
      "budget_used_percent": 29.50801818944357
    },
    {
      "type": "training",
      "description": "Training step 2481",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:10",
      "total_flops_so_far": 2.951990030554776e+16,
      "budget_used_percent": 29.519900305547758
    },
    {
      "type": "training",
      "description": "Training step 2482",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:11",
      "total_flops_so_far": 2.953178242165195e+16,
      "budget_used_percent": 29.531782421651954
    },
    {
      "type": "training",
      "description": "Training step 2483",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:12",
      "total_flops_so_far": 2.9543664537756144e+16,
      "budget_used_percent": 29.543664537756143
    },
    {
      "type": "training",
      "description": "Training step 2484",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:12",
      "total_flops_so_far": 2.9555546653860336e+16,
      "budget_used_percent": 29.555546653860336
    },
    {
      "type": "training",
      "description": "Training step 2485",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:13",
      "total_flops_so_far": 2.956742876996453e+16,
      "budget_used_percent": 29.567428769964526
    },
    {
      "type": "training",
      "description": "Training step 2486",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:14",
      "total_flops_so_far": 2.957931088606872e+16,
      "budget_used_percent": 29.579310886068722
    },
    {
      "type": "training",
      "description": "Training step 2487",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:14",
      "total_flops_so_far": 2.959119300217291e+16,
      "budget_used_percent": 29.59119300217291
    },
    {
      "type": "training",
      "description": "Training step 2488",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:15",
      "total_flops_so_far": 2.9603075118277104e+16,
      "budget_used_percent": 29.603075118277104
    },
    {
      "type": "training",
      "description": "Training step 2489",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:16",
      "total_flops_so_far": 2.9614957234381296e+16,
      "budget_used_percent": 29.614957234381293
    },
    {
      "type": "training",
      "description": "Training step 2490",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:16",
      "total_flops_so_far": 2.962683935048549e+16,
      "budget_used_percent": 29.62683935048549
    },
    {
      "type": "training",
      "description": "Training step 2491",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:17",
      "total_flops_so_far": 2.963872146658968e+16,
      "budget_used_percent": 29.63872146658968
    },
    {
      "type": "training",
      "description": "Training step 2492",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:17",
      "total_flops_so_far": 2.965060358269387e+16,
      "budget_used_percent": 29.650603582693876
    },
    {
      "type": "training",
      "description": "Training step 2493",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:18",
      "total_flops_so_far": 2.9662485698798064e+16,
      "budget_used_percent": 29.662485698798065
    },
    {
      "type": "training",
      "description": "Training step 2494",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:19",
      "total_flops_so_far": 2.9674367814902256e+16,
      "budget_used_percent": 29.674367814902258
    },
    {
      "type": "training",
      "description": "Training step 2495",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:19",
      "total_flops_so_far": 2.968624993100645e+16,
      "budget_used_percent": 29.686249931006447
    },
    {
      "type": "training",
      "description": "Training step 2496",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:20",
      "total_flops_so_far": 2.969813204711064e+16,
      "budget_used_percent": 29.698132047110644
    },
    {
      "type": "training",
      "description": "Training step 2497",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:21",
      "total_flops_so_far": 2.971001416321483e+16,
      "budget_used_percent": 29.710014163214833
    },
    {
      "type": "training",
      "description": "Training step 2498",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:21",
      "total_flops_so_far": 2.9721896279319024e+16,
      "budget_used_percent": 29.721896279319026
    },
    {
      "type": "training",
      "description": "Training step 2499",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:22",
      "total_flops_so_far": 2.9733778395423216e+16,
      "budget_used_percent": 29.733778395423215
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:30",
      "total_flops_so_far": 2.973448902456107e+16,
      "budget_used_percent": 29.73448902456107
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 714333709232.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:37",
      "total_flops_so_far": 2.9735203358270304e+16,
      "budget_used_percent": 29.7352033582703
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:44",
      "total_flops_so_far": 2.9735915839333656e+16,
      "budget_used_percent": 29.735915839333654
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:52",
      "total_flops_so_far": 2.973662646847151e+16,
      "budget_used_percent": 29.73662646847151
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 713407296244.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:34:59",
      "total_flops_so_far": 2.9737339875767756e+16,
      "budget_used_percent": 29.737339875767756
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 5",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:06",
      "total_flops_so_far": 2.9738050504905612e+16,
      "budget_used_percent": 29.738050504905612
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 6",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:14",
      "total_flops_so_far": 2.9738762985968964e+16,
      "budget_used_percent": 29.738762985968965
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 7",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:21",
      "total_flops_so_far": 2.9739475467032316e+16,
      "budget_used_percent": 29.739475467032317
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 8",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:28",
      "total_flops_so_far": 2.9740187948095668e+16,
      "budget_used_percent": 29.74018794809567
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 9",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:36",
      "total_flops_so_far": 2.974090042915902e+16,
      "budget_used_percent": 29.740900429159016
    },
    {
      "type": "training",
      "description": "Training step 2500",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:36",
      "total_flops_so_far": 2.9752782545263212e+16,
      "budget_used_percent": 29.752782545263212
    },
    {
      "type": "training",
      "description": "Training step 2501",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:37",
      "total_flops_so_far": 2.9764664661367404e+16,
      "budget_used_percent": 29.7646646613674
    },
    {
      "type": "training",
      "description": "Training step 2502",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:37",
      "total_flops_so_far": 2.9776546777471596e+16,
      "budget_used_percent": 29.776546777471598
    },
    {
      "type": "training",
      "description": "Training step 2503",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:38",
      "total_flops_so_far": 2.9788428893575788e+16,
      "budget_used_percent": 29.788428893575787
    },
    {
      "type": "training",
      "description": "Training step 2504",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:39",
      "total_flops_so_far": 2.980031100967998e+16,
      "budget_used_percent": 29.80031100967998
    },
    {
      "type": "training",
      "description": "Training step 2505",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:39",
      "total_flops_so_far": 2.9812193125784172e+16,
      "budget_used_percent": 29.81219312578417
    },
    {
      "type": "training",
      "description": "Training step 2506",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:40",
      "total_flops_so_far": 2.9824075241888364e+16,
      "budget_used_percent": 29.824075241888366
    },
    {
      "type": "training",
      "description": "Training step 2507",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:41",
      "total_flops_so_far": 2.9835957357992556e+16,
      "budget_used_percent": 29.835957357992555
    },
    {
      "type": "training",
      "description": "Training step 2508",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:41",
      "total_flops_so_far": 2.9847839474096748e+16,
      "budget_used_percent": 29.847839474096748
    },
    {
      "type": "training",
      "description": "Training step 2509",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:42",
      "total_flops_so_far": 2.985972159020094e+16,
      "budget_used_percent": 29.859721590200937
    },
    {
      "type": "training",
      "description": "Training step 2510",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:43",
      "total_flops_so_far": 2.9871603706305132e+16,
      "budget_used_percent": 29.871603706305134
    },
    {
      "type": "training",
      "description": "Training step 2511",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:43",
      "total_flops_so_far": 2.9883485822409324e+16,
      "budget_used_percent": 29.883485822409323
    },
    {
      "type": "training",
      "description": "Training step 2512",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:44",
      "total_flops_so_far": 2.9895367938513516e+16,
      "budget_used_percent": 29.895367938513516
    },
    {
      "type": "training",
      "description": "Training step 2513",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:45",
      "total_flops_so_far": 2.9907250054617708e+16,
      "budget_used_percent": 29.907250054617705
    },
    {
      "type": "training",
      "description": "Training step 2514",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:45",
      "total_flops_so_far": 2.99191321707219e+16,
      "budget_used_percent": 29.919132170721902
    },
    {
      "type": "training",
      "description": "Training step 2515",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:46",
      "total_flops_so_far": 2.9931014286826092e+16,
      "budget_used_percent": 29.93101428682609
    },
    {
      "type": "training",
      "description": "Training step 2516",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:46",
      "total_flops_so_far": 2.9942896402930284e+16,
      "budget_used_percent": 29.942896402930288
    },
    {
      "type": "training",
      "description": "Training step 2517",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:47",
      "total_flops_so_far": 2.9954778519034476e+16,
      "budget_used_percent": 29.954778519034477
    },
    {
      "type": "training",
      "description": "Training step 2518",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:48",
      "total_flops_so_far": 2.9966660635138668e+16,
      "budget_used_percent": 29.96666063513867
    },
    {
      "type": "training",
      "description": "Training step 2519",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:48",
      "total_flops_so_far": 2.997854275124286e+16,
      "budget_used_percent": 29.97854275124286
    },
    {
      "type": "training",
      "description": "Training step 2520",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:49",
      "total_flops_so_far": 2.9990424867347052e+16,
      "budget_used_percent": 29.990424867347055
    },
    {
      "type": "training",
      "description": "Training step 2521",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:50",
      "total_flops_so_far": 3.0002306983451244e+16,
      "budget_used_percent": 30.002306983451245
    },
    {
      "type": "training",
      "description": "Training step 2522",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:50",
      "total_flops_so_far": 3.0014189099555436e+16,
      "budget_used_percent": 30.014189099555438
    },
    {
      "type": "training",
      "description": "Training step 2523",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:51",
      "total_flops_so_far": 3.0026071215659628e+16,
      "budget_used_percent": 30.026071215659627
    },
    {
      "type": "training",
      "description": "Training step 2524",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:51",
      "total_flops_so_far": 3.003795333176382e+16,
      "budget_used_percent": 30.037953331763823
    },
    {
      "type": "training",
      "description": "Training step 2525",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:52",
      "total_flops_so_far": 3.0049835447868012e+16,
      "budget_used_percent": 30.049835447868013
    },
    {
      "type": "training",
      "description": "Training step 2526",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:53",
      "total_flops_so_far": 3.0061717563972204e+16,
      "budget_used_percent": 30.061717563972206
    },
    {
      "type": "training",
      "description": "Training step 2527",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:53",
      "total_flops_so_far": 3.0073599680076396e+16,
      "budget_used_percent": 30.073599680076395
    },
    {
      "type": "training",
      "description": "Training step 2528",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:54",
      "total_flops_so_far": 3.0085481796180588e+16,
      "budget_used_percent": 30.085481796180584
    },
    {
      "type": "training",
      "description": "Training step 2529",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:55",
      "total_flops_so_far": 3.009736391228478e+16,
      "budget_used_percent": 30.09736391228478
    },
    {
      "type": "training",
      "description": "Training step 2530",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:55",
      "total_flops_so_far": 3.0109246028388972e+16,
      "budget_used_percent": 30.10924602838897
    },
    {
      "type": "training",
      "description": "Training step 2531",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:56",
      "total_flops_so_far": 3.0121128144493164e+16,
      "budget_used_percent": 30.121128144493163
    },
    {
      "type": "training",
      "description": "Training step 2532",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:57",
      "total_flops_so_far": 3.0133010260597356e+16,
      "budget_used_percent": 30.133010260597352
    },
    {
      "type": "training",
      "description": "Training step 2533",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:57",
      "total_flops_so_far": 3.0144892376701548e+16,
      "budget_used_percent": 30.14489237670155
    },
    {
      "type": "training",
      "description": "Training step 2534",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:58",
      "total_flops_so_far": 3.015677449280574e+16,
      "budget_used_percent": 30.156774492805738
    },
    {
      "type": "training",
      "description": "Training step 2535",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:58",
      "total_flops_so_far": 3.0168656608909932e+16,
      "budget_used_percent": 30.168656608909934
    },
    {
      "type": "training",
      "description": "Training step 2536",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:35:59",
      "total_flops_so_far": 3.0180538725014124e+16,
      "budget_used_percent": 30.180538725014124
    },
    {
      "type": "training",
      "description": "Training step 2537",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:00",
      "total_flops_so_far": 3.0192420841118316e+16,
      "budget_used_percent": 30.192420841118317
    },
    {
      "type": "training",
      "description": "Training step 2538",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:00",
      "total_flops_so_far": 3.0204302957222508e+16,
      "budget_used_percent": 30.204302957222506
    },
    {
      "type": "training",
      "description": "Training step 2539",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:01",
      "total_flops_so_far": 3.02161850733267e+16,
      "budget_used_percent": 30.216185073326702
    },
    {
      "type": "training",
      "description": "Training step 2540",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:02",
      "total_flops_so_far": 3.0228067189430892e+16,
      "budget_used_percent": 30.22806718943089
    },
    {
      "type": "training",
      "description": "Training step 2541",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:02",
      "total_flops_so_far": 3.0239949305535084e+16,
      "budget_used_percent": 30.239949305535085
    },
    {
      "type": "training",
      "description": "Training step 2542",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:03",
      "total_flops_so_far": 3.0251831421639276e+16,
      "budget_used_percent": 30.251831421639274
    },
    {
      "type": "training",
      "description": "Training step 2543",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:04",
      "total_flops_so_far": 3.0263713537743468e+16,
      "budget_used_percent": 30.26371353774347
    },
    {
      "type": "training",
      "description": "Training step 2544",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:04",
      "total_flops_so_far": 3.027559565384766e+16,
      "budget_used_percent": 30.27559565384766
    },
    {
      "type": "training",
      "description": "Training step 2545",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:05",
      "total_flops_so_far": 3.0287477769951852e+16,
      "budget_used_percent": 30.287477769951852
    },
    {
      "type": "training",
      "description": "Training step 2546",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:05",
      "total_flops_so_far": 3.0299359886056044e+16,
      "budget_used_percent": 30.299359886056042
    },
    {
      "type": "training",
      "description": "Training step 2547",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:06",
      "total_flops_so_far": 3.0311242002160236e+16,
      "budget_used_percent": 30.31124200216024
    },
    {
      "type": "training",
      "description": "Training step 2548",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:07",
      "total_flops_so_far": 3.0323124118264428e+16,
      "budget_used_percent": 30.323124118264428
    },
    {
      "type": "training",
      "description": "Training step 2549",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:07",
      "total_flops_so_far": 3.033500623436862e+16,
      "budget_used_percent": 30.33500623436862
    },
    {
      "type": "training",
      "description": "Training step 2550",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:08",
      "total_flops_so_far": 3.0346888350472812e+16,
      "budget_used_percent": 30.34688835047281
    },
    {
      "type": "training",
      "description": "Training step 2551",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:09",
      "total_flops_so_far": 3.0358770466577004e+16,
      "budget_used_percent": 30.358770466577006
    },
    {
      "type": "training",
      "description": "Training step 2552",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:09",
      "total_flops_so_far": 3.0370652582681196e+16,
      "budget_used_percent": 30.370652582681195
    },
    {
      "type": "training",
      "description": "Training step 2553",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:10",
      "total_flops_so_far": 3.0382534698785388e+16,
      "budget_used_percent": 30.382534698785392
    },
    {
      "type": "training",
      "description": "Training step 2554",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:11",
      "total_flops_so_far": 3.039441681488958e+16,
      "budget_used_percent": 30.39441681488958
    },
    {
      "type": "training",
      "description": "Training step 2555",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:11",
      "total_flops_so_far": 3.0406298930993772e+16,
      "budget_used_percent": 30.40629893099377
    },
    {
      "type": "training",
      "description": "Training step 2556",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:12",
      "total_flops_so_far": 3.0418181047097964e+16,
      "budget_used_percent": 30.418181047097963
    },
    {
      "type": "training",
      "description": "Training step 2557",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:12",
      "total_flops_so_far": 3.0430063163202156e+16,
      "budget_used_percent": 30.430063163202153
    },
    {
      "type": "training",
      "description": "Training step 2558",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:13",
      "total_flops_so_far": 3.0441945279306348e+16,
      "budget_used_percent": 30.44194527930635
    },
    {
      "type": "training",
      "description": "Training step 2559",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:14",
      "total_flops_so_far": 3.045382739541054e+16,
      "budget_used_percent": 30.45382739541054
    },
    {
      "type": "training",
      "description": "Training step 2560",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:14",
      "total_flops_so_far": 3.0465709511514732e+16,
      "budget_used_percent": 30.46570951151473
    },
    {
      "type": "training",
      "description": "Training step 2561",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:15",
      "total_flops_so_far": 3.0477591627618924e+16,
      "budget_used_percent": 30.47759162761892
    },
    {
      "type": "training",
      "description": "Training step 2562",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:16",
      "total_flops_so_far": 3.0489473743723116e+16,
      "budget_used_percent": 30.489473743723117
    },
    {
      "type": "training",
      "description": "Training step 2563",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:16",
      "total_flops_so_far": 3.0501355859827308e+16,
      "budget_used_percent": 30.501355859827306
    },
    {
      "type": "training",
      "description": "Training step 2564",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:17",
      "total_flops_so_far": 3.05132379759315e+16,
      "budget_used_percent": 30.5132379759315
    },
    {
      "type": "training",
      "description": "Training step 2565",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:18",
      "total_flops_so_far": 3.0525120092035692e+16,
      "budget_used_percent": 30.52512009203569
    },
    {
      "type": "training",
      "description": "Training step 2566",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:18",
      "total_flops_so_far": 3.0537002208139884e+16,
      "budget_used_percent": 30.537002208139885
    },
    {
      "type": "training",
      "description": "Training step 2567",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:19",
      "total_flops_so_far": 3.0548884324244076e+16,
      "budget_used_percent": 30.548884324244074
    },
    {
      "type": "training",
      "description": "Training step 2568",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:19",
      "total_flops_so_far": 3.0560766440348268e+16,
      "budget_used_percent": 30.56076644034827
    },
    {
      "type": "training",
      "description": "Training step 2569",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:20",
      "total_flops_so_far": 3.057264855645246e+16,
      "budget_used_percent": 30.57264855645246
    },
    {
      "type": "training",
      "description": "Training step 2570",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:21",
      "total_flops_so_far": 3.0584530672556652e+16,
      "budget_used_percent": 30.584530672556653
    },
    {
      "type": "training",
      "description": "Training step 2571",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:21",
      "total_flops_so_far": 3.0596412788660844e+16,
      "budget_used_percent": 30.596412788660842
    },
    {
      "type": "training",
      "description": "Training step 2572",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:22",
      "total_flops_so_far": 3.0608294904765036e+16,
      "budget_used_percent": 30.60829490476504
    },
    {
      "type": "training",
      "description": "Training step 2573",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:23",
      "total_flops_so_far": 3.0620177020869228e+16,
      "budget_used_percent": 30.620177020869228
    },
    {
      "type": "training",
      "description": "Training step 2574",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:23",
      "total_flops_so_far": 3.063205913697342e+16,
      "budget_used_percent": 30.63205913697342
    },
    {
      "type": "training",
      "description": "Training step 2575",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:24",
      "total_flops_so_far": 3.0643941253077612e+16,
      "budget_used_percent": 30.64394125307761
    },
    {
      "type": "training",
      "description": "Training step 2576",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:25",
      "total_flops_so_far": 3.0655823369181804e+16,
      "budget_used_percent": 30.655823369181807
    },
    {
      "type": "training",
      "description": "Training step 2577",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:25",
      "total_flops_so_far": 3.0667705485285996e+16,
      "budget_used_percent": 30.667705485285996
    },
    {
      "type": "training",
      "description": "Training step 2578",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:26",
      "total_flops_so_far": 3.0679587601390188e+16,
      "budget_used_percent": 30.67958760139019
    },
    {
      "type": "training",
      "description": "Training step 2579",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:26",
      "total_flops_so_far": 3.069146971749438e+16,
      "budget_used_percent": 30.69146971749438
    },
    {
      "type": "training",
      "description": "Training step 2580",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:27",
      "total_flops_so_far": 3.0703351833598572e+16,
      "budget_used_percent": 30.703351833598575
    },
    {
      "type": "training",
      "description": "Training step 2581",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:28",
      "total_flops_so_far": 3.0715233949702764e+16,
      "budget_used_percent": 30.715233949702764
    },
    {
      "type": "training",
      "description": "Training step 2582",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:28",
      "total_flops_so_far": 3.0727116065806956e+16,
      "budget_used_percent": 30.72711606580696
    },
    {
      "type": "training",
      "description": "Training step 2583",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:29",
      "total_flops_so_far": 3.0738998181911148e+16,
      "budget_used_percent": 30.73899818191115
    },
    {
      "type": "training",
      "description": "Training step 2584",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:30",
      "total_flops_so_far": 3.075088029801534e+16,
      "budget_used_percent": 30.75088029801534
    },
    {
      "type": "training",
      "description": "Training step 2585",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:30",
      "total_flops_so_far": 3.0762762414119532e+16,
      "budget_used_percent": 30.762762414119532
    },
    {
      "type": "training",
      "description": "Training step 2586",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:31",
      "total_flops_so_far": 3.0774644530223724e+16,
      "budget_used_percent": 30.77464453022372
    },
    {
      "type": "training",
      "description": "Training step 2587",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:32",
      "total_flops_so_far": 3.0786526646327916e+16,
      "budget_used_percent": 30.786526646327918
    },
    {
      "type": "training",
      "description": "Training step 2588",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:32",
      "total_flops_so_far": 3.0798408762432108e+16,
      "budget_used_percent": 30.798408762432107
    },
    {
      "type": "training",
      "description": "Training step 2589",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:33",
      "total_flops_so_far": 3.08102908785363e+16,
      "budget_used_percent": 30.8102908785363
    },
    {
      "type": "training",
      "description": "Training step 2590",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:33",
      "total_flops_so_far": 3.0822172994640492e+16,
      "budget_used_percent": 30.82217299464049
    },
    {
      "type": "training",
      "description": "Training step 2591",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:34",
      "total_flops_so_far": 3.0834055110744684e+16,
      "budget_used_percent": 30.834055110744686
    },
    {
      "type": "training",
      "description": "Training step 2592",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:35",
      "total_flops_so_far": 3.0845937226848876e+16,
      "budget_used_percent": 30.845937226848875
    },
    {
      "type": "training",
      "description": "Training step 2593",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:35",
      "total_flops_so_far": 3.0857819342953068e+16,
      "budget_used_percent": 30.857819342953068
    },
    {
      "type": "training",
      "description": "Training step 2594",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:36",
      "total_flops_so_far": 3.086970145905726e+16,
      "budget_used_percent": 30.869701459057257
    },
    {
      "type": "training",
      "description": "Training step 2595",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:37",
      "total_flops_so_far": 3.0881583575161452e+16,
      "budget_used_percent": 30.881583575161454
    },
    {
      "type": "training",
      "description": "Training step 2596",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:37",
      "total_flops_so_far": 3.0893465691265644e+16,
      "budget_used_percent": 30.893465691265643
    },
    {
      "type": "training",
      "description": "Training step 2597",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:38",
      "total_flops_so_far": 3.0905347807369836e+16,
      "budget_used_percent": 30.905347807369836
    },
    {
      "type": "training",
      "description": "Training step 2598",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:39",
      "total_flops_so_far": 3.0917229923474028e+16,
      "budget_used_percent": 30.917229923474025
    },
    {
      "type": "training",
      "description": "Training step 2599",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:39",
      "total_flops_so_far": 3.092911203957822e+16,
      "budget_used_percent": 30.92911203957822
    },
    {
      "type": "training",
      "description": "Training step 2600",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:40",
      "total_flops_so_far": 3.0940994155682412e+16,
      "budget_used_percent": 30.94099415568241
    },
    {
      "type": "training",
      "description": "Training step 2601",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:40",
      "total_flops_so_far": 3.0952876271786604e+16,
      "budget_used_percent": 30.952876271786607
    },
    {
      "type": "training",
      "description": "Training step 2602",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:41",
      "total_flops_so_far": 3.0964758387890796e+16,
      "budget_used_percent": 30.964758387890797
    },
    {
      "type": "training",
      "description": "Training step 2603",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:42",
      "total_flops_so_far": 3.0976640503994988e+16,
      "budget_used_percent": 30.97664050399499
    },
    {
      "type": "training",
      "description": "Training step 2604",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:42",
      "total_flops_so_far": 3.098852262009918e+16,
      "budget_used_percent": 30.98852262009918
    },
    {
      "type": "training",
      "description": "Training step 2605",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:43",
      "total_flops_so_far": 3.1000404736203372e+16,
      "budget_used_percent": 31.000404736203375
    },
    {
      "type": "training",
      "description": "Training step 2606",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:44",
      "total_flops_so_far": 3.1012286852307564e+16,
      "budget_used_percent": 31.012286852307565
    },
    {
      "type": "training",
      "description": "Training step 2607",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:44",
      "total_flops_so_far": 3.1024168968411756e+16,
      "budget_used_percent": 31.024168968411757
    },
    {
      "type": "training",
      "description": "Training step 2608",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:45",
      "total_flops_so_far": 3.1036051084515948e+16,
      "budget_used_percent": 31.036051084515947
    },
    {
      "type": "training",
      "description": "Training step 2609",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:46",
      "total_flops_so_far": 3.104793320062014e+16,
      "budget_used_percent": 31.047933200620143
    },
    {
      "type": "training",
      "description": "Training step 2610",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:46",
      "total_flops_so_far": 3.1059815316724332e+16,
      "budget_used_percent": 31.059815316724332
    },
    {
      "type": "training",
      "description": "Training step 2611",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:47",
      "total_flops_so_far": 3.1071697432828524e+16,
      "budget_used_percent": 31.071697432828525
    },
    {
      "type": "training",
      "description": "Training step 2612",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:47",
      "total_flops_so_far": 3.1083579548932716e+16,
      "budget_used_percent": 31.083579548932715
    },
    {
      "type": "training",
      "description": "Training step 2613",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:48",
      "total_flops_so_far": 3.1095461665036908e+16,
      "budget_used_percent": 31.095461665036904
    },
    {
      "type": "training",
      "description": "Training step 2614",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:49",
      "total_flops_so_far": 3.11073437811411e+16,
      "budget_used_percent": 31.1073437811411
    },
    {
      "type": "training",
      "description": "Training step 2615",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:50",
      "total_flops_so_far": 3.1119225897245292e+16,
      "budget_used_percent": 31.11922589724529
    },
    {
      "type": "training",
      "description": "Training step 2616",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:50",
      "total_flops_so_far": 3.1131108013349484e+16,
      "budget_used_percent": 31.131108013349483
    },
    {
      "type": "training",
      "description": "Training step 2617",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:51",
      "total_flops_so_far": 3.1142990129453676e+16,
      "budget_used_percent": 31.142990129453672
    },
    {
      "type": "training",
      "description": "Training step 2618",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:52",
      "total_flops_so_far": 3.1154872245557868e+16,
      "budget_used_percent": 31.15487224555787
    },
    {
      "type": "training",
      "description": "Training step 2619",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:52",
      "total_flops_so_far": 3.116675436166206e+16,
      "budget_used_percent": 31.166754361662058
    },
    {
      "type": "training",
      "description": "Training step 2620",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:53",
      "total_flops_so_far": 3.1178636477766252e+16,
      "budget_used_percent": 31.178636477766254
    },
    {
      "type": "training",
      "description": "Training step 2621",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:54",
      "total_flops_so_far": 3.1190518593870444e+16,
      "budget_used_percent": 31.190518593870443
    },
    {
      "type": "training",
      "description": "Training step 2622",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:54",
      "total_flops_so_far": 3.1202400709974636e+16,
      "budget_used_percent": 31.202400709974636
    },
    {
      "type": "training",
      "description": "Training step 2623",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:55",
      "total_flops_so_far": 3.1214282826078828e+16,
      "budget_used_percent": 31.214282826078826
    },
    {
      "type": "training",
      "description": "Training step 2624",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:55",
      "total_flops_so_far": 3.122616494218302e+16,
      "budget_used_percent": 31.226164942183022
    },
    {
      "type": "training",
      "description": "Training step 2625",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:56",
      "total_flops_so_far": 3.1238047058287212e+16,
      "budget_used_percent": 31.23804705828721
    },
    {
      "type": "training",
      "description": "Training step 2626",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:57",
      "total_flops_so_far": 3.1249929174391404e+16,
      "budget_used_percent": 31.249929174391404
    },
    {
      "type": "training",
      "description": "Training step 2627",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:57",
      "total_flops_so_far": 3.1261811290495596e+16,
      "budget_used_percent": 31.261811290495594
    },
    {
      "type": "training",
      "description": "Training step 2628",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:58",
      "total_flops_so_far": 3.1273693406599788e+16,
      "budget_used_percent": 31.27369340659979
    },
    {
      "type": "training",
      "description": "Training step 2629",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:59",
      "total_flops_so_far": 3.128557552270398e+16,
      "budget_used_percent": 31.28557552270398
    },
    {
      "type": "training",
      "description": "Training step 2630",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:36:59",
      "total_flops_so_far": 3.1297457638808172e+16,
      "budget_used_percent": 31.297457638808172
    },
    {
      "type": "training",
      "description": "Training step 2631",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:00",
      "total_flops_so_far": 3.1309339754912364e+16,
      "budget_used_percent": 31.30933975491236
    },
    {
      "type": "training",
      "description": "Training step 2632",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:00",
      "total_flops_so_far": 3.1321221871016556e+16,
      "budget_used_percent": 31.321221871016558
    },
    {
      "type": "training",
      "description": "Training step 2633",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:01",
      "total_flops_so_far": 3.1333103987120748e+16,
      "budget_used_percent": 31.333103987120747
    },
    {
      "type": "training",
      "description": "Training step 2634",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:02",
      "total_flops_so_far": 3.134498610322494e+16,
      "budget_used_percent": 31.344986103224944
    },
    {
      "type": "training",
      "description": "Training step 2635",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:02",
      "total_flops_so_far": 3.1356868219329132e+16,
      "budget_used_percent": 31.356868219329133
    },
    {
      "type": "training",
      "description": "Training step 2636",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:03",
      "total_flops_so_far": 3.1368750335433324e+16,
      "budget_used_percent": 31.368750335433326
    },
    {
      "type": "training",
      "description": "Training step 2637",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:03",
      "total_flops_so_far": 3.1380632451537516e+16,
      "budget_used_percent": 31.380632451537515
    },
    {
      "type": "training",
      "description": "Training step 2638",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:04",
      "total_flops_so_far": 3.1392514567641708e+16,
      "budget_used_percent": 31.39251456764171
    },
    {
      "type": "training",
      "description": "Training step 2639",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:05",
      "total_flops_so_far": 3.14043966837459e+16,
      "budget_used_percent": 31.4043966837459
    },
    {
      "type": "training",
      "description": "Training step 2640",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:05",
      "total_flops_so_far": 3.1416278799850092e+16,
      "budget_used_percent": 31.416278799850094
    },
    {
      "type": "training",
      "description": "Training step 2641",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:06",
      "total_flops_so_far": 3.1428160915954284e+16,
      "budget_used_percent": 31.428160915954283
    },
    {
      "type": "training",
      "description": "Training step 2642",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:07",
      "total_flops_so_far": 3.1440043032058476e+16,
      "budget_used_percent": 31.440043032058473
    },
    {
      "type": "training",
      "description": "Training step 2643",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:07",
      "total_flops_so_far": 3.1451925148162668e+16,
      "budget_used_percent": 31.45192514816267
    },
    {
      "type": "training",
      "description": "Training step 2644",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:08",
      "total_flops_so_far": 3.146380726426686e+16,
      "budget_used_percent": 31.46380726426686
    },
    {
      "type": "training",
      "description": "Training step 2645",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:09",
      "total_flops_so_far": 3.1475689380371052e+16,
      "budget_used_percent": 31.47568938037105
    },
    {
      "type": "training",
      "description": "Training step 2646",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:09",
      "total_flops_so_far": 3.1487571496475244e+16,
      "budget_used_percent": 31.48757149647524
    },
    {
      "type": "training",
      "description": "Training step 2647",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:10",
      "total_flops_so_far": 3.1499453612579436e+16,
      "budget_used_percent": 31.499453612579437
    },
    {
      "type": "training",
      "description": "Training step 2648",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:11",
      "total_flops_so_far": 3.1511335728683628e+16,
      "budget_used_percent": 31.511335728683626
    },
    {
      "type": "training",
      "description": "Training step 2649",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:11",
      "total_flops_so_far": 3.152321784478782e+16,
      "budget_used_percent": 31.523217844787823
    },
    {
      "type": "training",
      "description": "Training step 2650",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:12",
      "total_flops_so_far": 3.1535099960892012e+16,
      "budget_used_percent": 31.535099960892012
    },
    {
      "type": "training",
      "description": "Training step 2651",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:12",
      "total_flops_so_far": 3.1546982076996204e+16,
      "budget_used_percent": 31.546982076996205
    },
    {
      "type": "training",
      "description": "Training step 2652",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:13",
      "total_flops_so_far": 3.1558864193100396e+16,
      "budget_used_percent": 31.558864193100394
    },
    {
      "type": "training",
      "description": "Training step 2653",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:14",
      "total_flops_so_far": 3.1570746309204588e+16,
      "budget_used_percent": 31.57074630920459
    },
    {
      "type": "training",
      "description": "Training step 2654",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:14",
      "total_flops_so_far": 3.158262842530878e+16,
      "budget_used_percent": 31.58262842530878
    },
    {
      "type": "training",
      "description": "Training step 2655",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:15",
      "total_flops_so_far": 3.1594510541412972e+16,
      "budget_used_percent": 31.594510541412973
    },
    {
      "type": "training",
      "description": "Training step 2656",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:16",
      "total_flops_so_far": 3.1606392657517164e+16,
      "budget_used_percent": 31.606392657517162
    },
    {
      "type": "training",
      "description": "Training step 2657",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:16",
      "total_flops_so_far": 3.1618274773621356e+16,
      "budget_used_percent": 31.61827477362136
    },
    {
      "type": "training",
      "description": "Training step 2658",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:17",
      "total_flops_so_far": 3.1630156889725548e+16,
      "budget_used_percent": 31.630156889725548
    },
    {
      "type": "training",
      "description": "Training step 2659",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:18",
      "total_flops_so_far": 3.164203900582974e+16,
      "budget_used_percent": 31.64203900582974
    },
    {
      "type": "training",
      "description": "Training step 2660",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:18",
      "total_flops_so_far": 3.1653921121933932e+16,
      "budget_used_percent": 31.65392112193393
    },
    {
      "type": "training",
      "description": "Training step 2661",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:19",
      "total_flops_so_far": 3.1665803238038124e+16,
      "budget_used_percent": 31.665803238038126
    },
    {
      "type": "training",
      "description": "Training step 2662",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:19",
      "total_flops_so_far": 3.1677685354142316e+16,
      "budget_used_percent": 31.677685354142316
    },
    {
      "type": "training",
      "description": "Training step 2663",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:20",
      "total_flops_so_far": 3.1689567470246508e+16,
      "budget_used_percent": 31.68956747024651
    },
    {
      "type": "training",
      "description": "Training step 2664",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:21",
      "total_flops_so_far": 3.17014495863507e+16,
      "budget_used_percent": 31.701449586350698
    },
    {
      "type": "training",
      "description": "Training step 2665",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:21",
      "total_flops_so_far": 3.1713331702454892e+16,
      "budget_used_percent": 31.713331702454894
    },
    {
      "type": "training",
      "description": "Training step 2666",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:22",
      "total_flops_so_far": 3.1725213818559084e+16,
      "budget_used_percent": 31.725213818559084
    },
    {
      "type": "training",
      "description": "Training step 2667",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:23",
      "total_flops_so_far": 3.1737095934663276e+16,
      "budget_used_percent": 31.73709593466328
    },
    {
      "type": "training",
      "description": "Training step 2668",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:23",
      "total_flops_so_far": 3.1748978050767468e+16,
      "budget_used_percent": 31.74897805076747
    },
    {
      "type": "training",
      "description": "Training step 2669",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:24",
      "total_flops_so_far": 3.176086016687166e+16,
      "budget_used_percent": 31.76086016687166
    },
    {
      "type": "training",
      "description": "Training step 2670",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:25",
      "total_flops_so_far": 3.1772742282975852e+16,
      "budget_used_percent": 31.77274228297585
    },
    {
      "type": "training",
      "description": "Training step 2671",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:25",
      "total_flops_so_far": 3.1784624399080044e+16,
      "budget_used_percent": 31.78462439908004
    },
    {
      "type": "training",
      "description": "Training step 2672",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:26",
      "total_flops_so_far": 3.1796506515184236e+16,
      "budget_used_percent": 31.796506515184237
    },
    {
      "type": "training",
      "description": "Training step 2673",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:27",
      "total_flops_so_far": 3.1808388631288428e+16,
      "budget_used_percent": 31.808388631288427
    },
    {
      "type": "training",
      "description": "Training step 2674",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:27",
      "total_flops_so_far": 3.182027074739262e+16,
      "budget_used_percent": 31.82027074739262
    },
    {
      "type": "training",
      "description": "Training step 2675",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:28",
      "total_flops_so_far": 3.1832152863496812e+16,
      "budget_used_percent": 31.83215286349681
    },
    {
      "type": "training",
      "description": "Training step 2676",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:28",
      "total_flops_so_far": 3.1844034979601004e+16,
      "budget_used_percent": 31.844034979601005
    },
    {
      "type": "training",
      "description": "Training step 2677",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:29",
      "total_flops_so_far": 3.1855917095705196e+16,
      "budget_used_percent": 31.855917095705195
    },
    {
      "type": "training",
      "description": "Training step 2678",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:30",
      "total_flops_so_far": 3.1867799211809388e+16,
      "budget_used_percent": 31.867799211809388
    },
    {
      "type": "training",
      "description": "Training step 2679",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:30",
      "total_flops_so_far": 3.187968132791358e+16,
      "budget_used_percent": 31.879681327913577
    },
    {
      "type": "training",
      "description": "Training step 2680",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:31",
      "total_flops_so_far": 3.1891563444017772e+16,
      "budget_used_percent": 31.891563444017773
    },
    {
      "type": "training",
      "description": "Training step 2681",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:32",
      "total_flops_so_far": 3.1903445560121964e+16,
      "budget_used_percent": 31.903445560121963
    },
    {
      "type": "training",
      "description": "Training step 2682",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:32",
      "total_flops_so_far": 3.1915327676226156e+16,
      "budget_used_percent": 31.915327676226156
    },
    {
      "type": "training",
      "description": "Training step 2683",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:33",
      "total_flops_so_far": 3.1927209792330348e+16,
      "budget_used_percent": 31.927209792330345
    },
    {
      "type": "training",
      "description": "Training step 2684",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:34",
      "total_flops_so_far": 3.193909190843454e+16,
      "budget_used_percent": 31.93909190843454
    },
    {
      "type": "training",
      "description": "Training step 2685",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:34",
      "total_flops_so_far": 3.1950974024538732e+16,
      "budget_used_percent": 31.95097402453873
    },
    {
      "type": "training",
      "description": "Training step 2686",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:35",
      "total_flops_so_far": 3.1962856140642924e+16,
      "budget_used_percent": 31.962856140642927
    },
    {
      "type": "training",
      "description": "Training step 2687",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:35",
      "total_flops_so_far": 3.1974738256747116e+16,
      "budget_used_percent": 31.974738256747116
    },
    {
      "type": "training",
      "description": "Training step 2688",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:36",
      "total_flops_so_far": 3.1986620372851308e+16,
      "budget_used_percent": 31.98662037285131
    },
    {
      "type": "training",
      "description": "Training step 2689",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:37",
      "total_flops_so_far": 3.19985024889555e+16,
      "budget_used_percent": 31.9985024889555
    },
    {
      "type": "training",
      "description": "Training step 2690",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:37",
      "total_flops_so_far": 3.2010384605059692e+16,
      "budget_used_percent": 32.01038460505969
    },
    {
      "type": "training",
      "description": "Training step 2691",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:38",
      "total_flops_so_far": 3.2022266721163884e+16,
      "budget_used_percent": 32.02226672116388
    },
    {
      "type": "training",
      "description": "Training step 2692",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:39",
      "total_flops_so_far": 3.2034148837268076e+16,
      "budget_used_percent": 32.03414883726808
    },
    {
      "type": "training",
      "description": "Training step 2693",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:39",
      "total_flops_so_far": 3.2046030953372268e+16,
      "budget_used_percent": 32.04603095337227
    },
    {
      "type": "training",
      "description": "Training step 2694",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:40",
      "total_flops_so_far": 3.205791306947646e+16,
      "budget_used_percent": 32.05791306947646
    },
    {
      "type": "training",
      "description": "Training step 2695",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:41",
      "total_flops_so_far": 3.2069795185580652e+16,
      "budget_used_percent": 32.06979518558065
    },
    {
      "type": "training",
      "description": "Training step 2696",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:41",
      "total_flops_so_far": 3.2081677301684844e+16,
      "budget_used_percent": 32.08167730168485
    },
    {
      "type": "training",
      "description": "Training step 2697",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:42",
      "total_flops_so_far": 3.2093559417789036e+16,
      "budget_used_percent": 32.09355941778904
    },
    {
      "type": "training",
      "description": "Training step 2698",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:43",
      "total_flops_so_far": 3.2105441533893228e+16,
      "budget_used_percent": 32.10544153389323
    },
    {
      "type": "training",
      "description": "Training step 2699",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:43",
      "total_flops_so_far": 3.211732364999742e+16,
      "budget_used_percent": 32.11732364999742
    },
    {
      "type": "training",
      "description": "Training step 2700",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:44",
      "total_flops_so_far": 3.2129205766101612e+16,
      "budget_used_percent": 32.129205766101606
    },
    {
      "type": "training",
      "description": "Training step 2701",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:44",
      "total_flops_so_far": 3.2141087882205804e+16,
      "budget_used_percent": 32.1410878822058
    },
    {
      "type": "training",
      "description": "Training step 2702",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:45",
      "total_flops_so_far": 3.2152969998309996e+16,
      "budget_used_percent": 32.15296999830999
    },
    {
      "type": "training",
      "description": "Training step 2703",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:46",
      "total_flops_so_far": 3.2164852114414188e+16,
      "budget_used_percent": 32.16485211441419
    },
    {
      "type": "training",
      "description": "Training step 2704",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:46",
      "total_flops_so_far": 3.217673423051838e+16,
      "budget_used_percent": 32.17673423051838
    },
    {
      "type": "training",
      "description": "Training step 2705",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:47",
      "total_flops_so_far": 3.2188616346622572e+16,
      "budget_used_percent": 32.188616346622574
    },
    {
      "type": "training",
      "description": "Training step 2706",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:48",
      "total_flops_so_far": 3.2200498462726764e+16,
      "budget_used_percent": 32.20049846272676
    },
    {
      "type": "training",
      "description": "Training step 2707",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:48",
      "total_flops_so_far": 3.2212380578830956e+16,
      "budget_used_percent": 32.21238057883096
    },
    {
      "type": "training",
      "description": "Training step 2708",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:49",
      "total_flops_so_far": 3.2224262694935148e+16,
      "budget_used_percent": 32.22426269493515
    },
    {
      "type": "training",
      "description": "Training step 2709",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:50",
      "total_flops_so_far": 3.223614481103934e+16,
      "budget_used_percent": 32.23614481103934
    },
    {
      "type": "training",
      "description": "Training step 2710",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:50",
      "total_flops_so_far": 3.2248026927143532e+16,
      "budget_used_percent": 32.24802692714353
    },
    {
      "type": "training",
      "description": "Training step 2711",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:51",
      "total_flops_so_far": 3.2259909043247724e+16,
      "budget_used_percent": 32.259909043247724
    },
    {
      "type": "training",
      "description": "Training step 2712",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:51",
      "total_flops_so_far": 3.2271791159351916e+16,
      "budget_used_percent": 32.27179115935191
    },
    {
      "type": "training",
      "description": "Training step 2713",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:52",
      "total_flops_so_far": 3.2283673275456108e+16,
      "budget_used_percent": 32.28367327545611
    },
    {
      "type": "training",
      "description": "Training step 2714",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:53",
      "total_flops_so_far": 3.22955553915603e+16,
      "budget_used_percent": 32.2955553915603
    },
    {
      "type": "training",
      "description": "Training step 2715",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:53",
      "total_flops_so_far": 3.2307437507664492e+16,
      "budget_used_percent": 32.307437507664496
    },
    {
      "type": "training",
      "description": "Training step 2716",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:54",
      "total_flops_so_far": 3.2319319623768684e+16,
      "budget_used_percent": 32.319319623768685
    },
    {
      "type": "training",
      "description": "Training step 2717",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:55",
      "total_flops_so_far": 3.2331201739872876e+16,
      "budget_used_percent": 32.33120173987288
    },
    {
      "type": "training",
      "description": "Training step 2718",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:55",
      "total_flops_so_far": 3.2343083855977068e+16,
      "budget_used_percent": 32.34308385597707
    },
    {
      "type": "training",
      "description": "Training step 2719",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:56",
      "total_flops_so_far": 3.235496597208126e+16,
      "budget_used_percent": 32.35496597208126
    },
    {
      "type": "training",
      "description": "Training step 2720",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:57",
      "total_flops_so_far": 3.2366848088185452e+16,
      "budget_used_percent": 32.36684808818545
    },
    {
      "type": "training",
      "description": "Training step 2721",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:57",
      "total_flops_so_far": 3.2378730204289644e+16,
      "budget_used_percent": 32.378730204289646
    },
    {
      "type": "training",
      "description": "Training step 2722",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:58",
      "total_flops_so_far": 3.2390612320393836e+16,
      "budget_used_percent": 32.390612320393835
    },
    {
      "type": "training",
      "description": "Training step 2723",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:59",
      "total_flops_so_far": 3.2402494436498028e+16,
      "budget_used_percent": 32.40249443649803
    },
    {
      "type": "training",
      "description": "Training step 2724",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:37:59",
      "total_flops_so_far": 3.241437655260222e+16,
      "budget_used_percent": 32.41437655260222
    },
    {
      "type": "training",
      "description": "Training step 2725",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:00",
      "total_flops_so_far": 3.2426258668706412e+16,
      "budget_used_percent": 32.42625866870642
    },
    {
      "type": "training",
      "description": "Training step 2726",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:00",
      "total_flops_so_far": 3.2438140784810604e+16,
      "budget_used_percent": 32.43814078481061
    },
    {
      "type": "training",
      "description": "Training step 2727",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:01",
      "total_flops_so_far": 3.2450022900914796e+16,
      "budget_used_percent": 32.450022900914796
    },
    {
      "type": "training",
      "description": "Training step 2728",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:02",
      "total_flops_so_far": 3.2461905017018988e+16,
      "budget_used_percent": 32.461905017018985
    },
    {
      "type": "training",
      "description": "Training step 2729",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:03",
      "total_flops_so_far": 3.247378713312318e+16,
      "budget_used_percent": 32.473787133123174
    },
    {
      "type": "training",
      "description": "Training step 2730",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:03",
      "total_flops_so_far": 3.2485669249227372e+16,
      "budget_used_percent": 32.48566924922737
    },
    {
      "type": "training",
      "description": "Training step 2731",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:04",
      "total_flops_so_far": 3.2497551365331564e+16,
      "budget_used_percent": 32.49755136533156
    },
    {
      "type": "training",
      "description": "Training step 2732",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:05",
      "total_flops_so_far": 3.2509433481435756e+16,
      "budget_used_percent": 32.50943348143576
    },
    {
      "type": "training",
      "description": "Training step 2733",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:05",
      "total_flops_so_far": 3.2521315597539948e+16,
      "budget_used_percent": 32.521315597539946
    },
    {
      "type": "training",
      "description": "Training step 2734",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:06",
      "total_flops_so_far": 3.253319771364414e+16,
      "budget_used_percent": 32.53319771364414
    },
    {
      "type": "training",
      "description": "Training step 2735",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:07",
      "total_flops_so_far": 3.2545079829748332e+16,
      "budget_used_percent": 32.54507982974833
    },
    {
      "type": "training",
      "description": "Training step 2736",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:07",
      "total_flops_so_far": 3.2556961945852524e+16,
      "budget_used_percent": 32.55696194585253
    },
    {
      "type": "training",
      "description": "Training step 2737",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:08",
      "total_flops_so_far": 3.2568844061956716e+16,
      "budget_used_percent": 32.56884406195672
    },
    {
      "type": "training",
      "description": "Training step 2738",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:08",
      "total_flops_so_far": 3.2580726178060908e+16,
      "budget_used_percent": 32.58072617806091
    },
    {
      "type": "training",
      "description": "Training step 2739",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:09",
      "total_flops_so_far": 3.25926082941651e+16,
      "budget_used_percent": 32.592608294165096
    },
    {
      "type": "training",
      "description": "Training step 2740",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:10",
      "total_flops_so_far": 3.2604490410269292e+16,
      "budget_used_percent": 32.60449041026929
    },
    {
      "type": "training",
      "description": "Training step 2741",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:10",
      "total_flops_so_far": 3.2616372526373484e+16,
      "budget_used_percent": 32.61637252637348
    },
    {
      "type": "training",
      "description": "Training step 2742",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:11",
      "total_flops_so_far": 3.2628254642477676e+16,
      "budget_used_percent": 32.62825464247768
    },
    {
      "type": "training",
      "description": "Training step 2743",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:12",
      "total_flops_so_far": 3.2640136758581868e+16,
      "budget_used_percent": 32.64013675858187
    },
    {
      "type": "training",
      "description": "Training step 2744",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:12",
      "total_flops_so_far": 3.265201887468606e+16,
      "budget_used_percent": 32.652018874686064
    },
    {
      "type": "training",
      "description": "Training step 2745",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:13",
      "total_flops_so_far": 3.2663900990790252e+16,
      "budget_used_percent": 32.66390099079025
    },
    {
      "type": "training",
      "description": "Training step 2746",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:14",
      "total_flops_so_far": 3.2675783106894444e+16,
      "budget_used_percent": 32.67578310689444
    },
    {
      "type": "training",
      "description": "Training step 2747",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:14",
      "total_flops_so_far": 3.2687665222998636e+16,
      "budget_used_percent": 32.68766522299863
    },
    {
      "type": "training",
      "description": "Training step 2748",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:15",
      "total_flops_so_far": 3.2699547339102828e+16,
      "budget_used_percent": 32.69954733910283
    },
    {
      "type": "training",
      "description": "Training step 2749",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:15",
      "total_flops_so_far": 3.271142945520702e+16,
      "budget_used_percent": 32.71142945520702
    },
    {
      "type": "training",
      "description": "Training step 2750",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:16",
      "total_flops_so_far": 3.2723311571311212e+16,
      "budget_used_percent": 32.723311571311214
    },
    {
      "type": "training",
      "description": "Training step 2751",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:17",
      "total_flops_so_far": 3.2735193687415404e+16,
      "budget_used_percent": 32.7351936874154
    },
    {
      "type": "training",
      "description": "Training step 2752",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:17",
      "total_flops_so_far": 3.2747075803519596e+16,
      "budget_used_percent": 32.7470758035196
    },
    {
      "type": "training",
      "description": "Training step 2753",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:18",
      "total_flops_so_far": 3.2758957919623788e+16,
      "budget_used_percent": 32.75895791962379
    },
    {
      "type": "training",
      "description": "Training step 2754",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:19",
      "total_flops_so_far": 3.277084003572798e+16,
      "budget_used_percent": 32.770840035727986
    },
    {
      "type": "training",
      "description": "Training step 2755",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:19",
      "total_flops_so_far": 3.2782722151832172e+16,
      "budget_used_percent": 32.782722151832175
    },
    {
      "type": "training",
      "description": "Training step 2756",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:20",
      "total_flops_so_far": 3.2794604267936364e+16,
      "budget_used_percent": 32.794604267936364
    },
    {
      "type": "training",
      "description": "Training step 2757",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:21",
      "total_flops_so_far": 3.2806486384040556e+16,
      "budget_used_percent": 32.806486384040554
    },
    {
      "type": "training",
      "description": "Training step 2758",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:21",
      "total_flops_so_far": 3.2818368500144748e+16,
      "budget_used_percent": 32.81836850014474
    },
    {
      "type": "training",
      "description": "Training step 2759",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:22",
      "total_flops_so_far": 3.283025061624894e+16,
      "budget_used_percent": 32.83025061624894
    },
    {
      "type": "training",
      "description": "Training step 2760",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:23",
      "total_flops_so_far": 3.2842132732353132e+16,
      "budget_used_percent": 32.84213273235313
    },
    {
      "type": "training",
      "description": "Training step 2761",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:23",
      "total_flops_so_far": 3.2854014848457324e+16,
      "budget_used_percent": 32.854014848457325
    },
    {
      "type": "training",
      "description": "Training step 2762",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:24",
      "total_flops_so_far": 3.2865896964561516e+16,
      "budget_used_percent": 32.865896964561514
    },
    {
      "type": "training",
      "description": "Training step 2763",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:24",
      "total_flops_so_far": 3.2877779080665708e+16,
      "budget_used_percent": 32.87777908066571
    },
    {
      "type": "training",
      "description": "Training step 2764",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:25",
      "total_flops_so_far": 3.28896611967699e+16,
      "budget_used_percent": 32.8896611967699
    },
    {
      "type": "training",
      "description": "Training step 2765",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:26",
      "total_flops_so_far": 3.2901543312874092e+16,
      "budget_used_percent": 32.9015433128741
    },
    {
      "type": "training",
      "description": "Training step 2766",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:26",
      "total_flops_so_far": 3.2913425428978284e+16,
      "budget_used_percent": 32.913425428978286
    },
    {
      "type": "training",
      "description": "Training step 2767",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:27",
      "total_flops_so_far": 3.2925307545082476e+16,
      "budget_used_percent": 32.925307545082475
    },
    {
      "type": "training",
      "description": "Training step 2768",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:28",
      "total_flops_so_far": 3.2937189661186668e+16,
      "budget_used_percent": 32.937189661186665
    },
    {
      "type": "training",
      "description": "Training step 2769",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:28",
      "total_flops_so_far": 3.294907177729086e+16,
      "budget_used_percent": 32.94907177729086
    },
    {
      "type": "training",
      "description": "Training step 2770",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:29",
      "total_flops_so_far": 3.2960953893395052e+16,
      "budget_used_percent": 32.96095389339505
    },
    {
      "type": "training",
      "description": "Training step 2771",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:30",
      "total_flops_so_far": 3.2972836009499244e+16,
      "budget_used_percent": 32.97283600949925
    },
    {
      "type": "training",
      "description": "Training step 2772",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:30",
      "total_flops_so_far": 3.2984718125603436e+16,
      "budget_used_percent": 32.984718125603436
    },
    {
      "type": "training",
      "description": "Training step 2773",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:31",
      "total_flops_so_far": 3.2996600241707628e+16,
      "budget_used_percent": 32.99660024170763
    },
    {
      "type": "training",
      "description": "Training step 2774",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:32",
      "total_flops_so_far": 3.300848235781182e+16,
      "budget_used_percent": 33.00848235781182
    },
    {
      "type": "training",
      "description": "Training step 2775",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:32",
      "total_flops_so_far": 3.3020364473916012e+16,
      "budget_used_percent": 33.02036447391601
    },
    {
      "type": "training",
      "description": "Training step 2776",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:33",
      "total_flops_so_far": 3.3032246590020204e+16,
      "budget_used_percent": 33.0322465900202
    },
    {
      "type": "training",
      "description": "Training step 2777",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:33",
      "total_flops_so_far": 3.3044128706124396e+16,
      "budget_used_percent": 33.0441287061244
    },
    {
      "type": "training",
      "description": "Training step 2778",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:34",
      "total_flops_so_far": 3.3056010822228588e+16,
      "budget_used_percent": 33.056010822228586
    },
    {
      "type": "training",
      "description": "Training step 2779",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:35",
      "total_flops_so_far": 3.306789293833278e+16,
      "budget_used_percent": 33.06789293833278
    },
    {
      "type": "training",
      "description": "Training step 2780",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:35",
      "total_flops_so_far": 3.3079775054436972e+16,
      "budget_used_percent": 33.07977505443697
    },
    {
      "type": "training",
      "description": "Training step 2781",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:36",
      "total_flops_so_far": 3.3091657170541164e+16,
      "budget_used_percent": 33.09165717054117
    },
    {
      "type": "training",
      "description": "Training step 2782",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:37",
      "total_flops_so_far": 3.3103539286645356e+16,
      "budget_used_percent": 33.10353928664536
    },
    {
      "type": "training",
      "description": "Training step 2783",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:37",
      "total_flops_so_far": 3.3115421402749548e+16,
      "budget_used_percent": 33.11542140274955
    },
    {
      "type": "training",
      "description": "Training step 2784",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:38",
      "total_flops_so_far": 3.312730351885374e+16,
      "budget_used_percent": 33.12730351885374
    },
    {
      "type": "training",
      "description": "Training step 2785",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:39",
      "total_flops_so_far": 3.3139185634957932e+16,
      "budget_used_percent": 33.13918563495793
    },
    {
      "type": "training",
      "description": "Training step 2786",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:39",
      "total_flops_so_far": 3.3151067751062124e+16,
      "budget_used_percent": 33.15106775106212
    },
    {
      "type": "training",
      "description": "Training step 2787",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:40",
      "total_flops_so_far": 3.3162949867166316e+16,
      "budget_used_percent": 33.16294986716631
    },
    {
      "type": "training",
      "description": "Training step 2788",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:41",
      "total_flops_so_far": 3.3174831983270508e+16,
      "budget_used_percent": 33.17483198327051
    },
    {
      "type": "training",
      "description": "Training step 2789",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:41",
      "total_flops_so_far": 3.31867140993747e+16,
      "budget_used_percent": 33.1867140993747
    },
    {
      "type": "training",
      "description": "Training step 2790",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:42",
      "total_flops_so_far": 3.3198596215478892e+16,
      "budget_used_percent": 33.198596215478894
    },
    {
      "type": "training",
      "description": "Training step 2791",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:42",
      "total_flops_so_far": 3.3210478331583084e+16,
      "budget_used_percent": 33.21047833158308
    },
    {
      "type": "training",
      "description": "Training step 2792",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:43",
      "total_flops_so_far": 3.3222360447687276e+16,
      "budget_used_percent": 33.22236044768728
    },
    {
      "type": "training",
      "description": "Training step 2793",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:44",
      "total_flops_so_far": 3.3234242563791468e+16,
      "budget_used_percent": 33.23424256379147
    },
    {
      "type": "training",
      "description": "Training step 2794",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:44",
      "total_flops_so_far": 3.324612467989566e+16,
      "budget_used_percent": 33.24612467989566
    },
    {
      "type": "training",
      "description": "Training step 2795",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:45",
      "total_flops_so_far": 3.3258006795999852e+16,
      "budget_used_percent": 33.25800679599985
    },
    {
      "type": "training",
      "description": "Training step 2796",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:46",
      "total_flops_so_far": 3.3269888912104044e+16,
      "budget_used_percent": 33.269888912104044
    },
    {
      "type": "training",
      "description": "Training step 2797",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:46",
      "total_flops_so_far": 3.3281771028208236e+16,
      "budget_used_percent": 33.28177102820823
    },
    {
      "type": "training",
      "description": "Training step 2798",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:47",
      "total_flops_so_far": 3.3293653144312428e+16,
      "budget_used_percent": 33.29365314431243
    },
    {
      "type": "training",
      "description": "Training step 2799",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:48",
      "total_flops_so_far": 3.330553526041662e+16,
      "budget_used_percent": 33.30553526041662
    },
    {
      "type": "training",
      "description": "Training step 2800",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:48",
      "total_flops_so_far": 3.3317417376520812e+16,
      "budget_used_percent": 33.317417376520815
    },
    {
      "type": "training",
      "description": "Training step 2801",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:49",
      "total_flops_so_far": 3.3329299492625004e+16,
      "budget_used_percent": 33.329299492625005
    },
    {
      "type": "training",
      "description": "Training step 2802",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:50",
      "total_flops_so_far": 3.3341181608729196e+16,
      "budget_used_percent": 33.3411816087292
    },
    {
      "type": "training",
      "description": "Training step 2803",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:50",
      "total_flops_so_far": 3.3353063724833388e+16,
      "budget_used_percent": 33.35306372483339
    },
    {
      "type": "training",
      "description": "Training step 2804",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:51",
      "total_flops_so_far": 3.336494584093758e+16,
      "budget_used_percent": 33.36494584093758
    },
    {
      "type": "training",
      "description": "Training step 2805",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:51",
      "total_flops_so_far": 3.3376827957041772e+16,
      "budget_used_percent": 33.37682795704177
    },
    {
      "type": "training",
      "description": "Training step 2806",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:52",
      "total_flops_so_far": 3.3388710073145964e+16,
      "budget_used_percent": 33.388710073145965
    },
    {
      "type": "training",
      "description": "Training step 2807",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:53",
      "total_flops_so_far": 3.3400592189250156e+16,
      "budget_used_percent": 33.400592189250155
    },
    {
      "type": "training",
      "description": "Training step 2808",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:53",
      "total_flops_so_far": 3.3412474305354348e+16,
      "budget_used_percent": 33.41247430535435
    },
    {
      "type": "training",
      "description": "Training step 2809",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:54",
      "total_flops_so_far": 3.342435642145854e+16,
      "budget_used_percent": 33.42435642145854
    },
    {
      "type": "training",
      "description": "Training step 2810",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:55",
      "total_flops_so_far": 3.3436238537562732e+16,
      "budget_used_percent": 33.43623853756274
    },
    {
      "type": "training",
      "description": "Training step 2811",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:55",
      "total_flops_so_far": 3.3448120653666924e+16,
      "budget_used_percent": 33.448120653666926
    },
    {
      "type": "training",
      "description": "Training step 2812",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:56",
      "total_flops_so_far": 3.3460002769771116e+16,
      "budget_used_percent": 33.460002769771116
    },
    {
      "type": "training",
      "description": "Training step 2813",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:57",
      "total_flops_so_far": 3.3471884885875308e+16,
      "budget_used_percent": 33.471884885875305
    },
    {
      "type": "training",
      "description": "Training step 2814",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:57",
      "total_flops_so_far": 3.34837670019795e+16,
      "budget_used_percent": 33.483767001979494
    },
    {
      "type": "training",
      "description": "Training step 2815",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:58",
      "total_flops_so_far": 3.3495649118083692e+16,
      "budget_used_percent": 33.49564911808369
    },
    {
      "type": "training",
      "description": "Training step 2816",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:59",
      "total_flops_so_far": 3.3507531234187884e+16,
      "budget_used_percent": 33.50753123418788
    },
    {
      "type": "training",
      "description": "Training step 2817",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:38:59",
      "total_flops_so_far": 3.3519413350292076e+16,
      "budget_used_percent": 33.519413350292076
    },
    {
      "type": "training",
      "description": "Training step 2818",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:00",
      "total_flops_so_far": 3.3531295466396268e+16,
      "budget_used_percent": 33.531295466396266
    },
    {
      "type": "training",
      "description": "Training step 2819",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:01",
      "total_flops_so_far": 3.354317758250046e+16,
      "budget_used_percent": 33.54317758250046
    },
    {
      "type": "training",
      "description": "Training step 2820",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:01",
      "total_flops_so_far": 3.3555059698604652e+16,
      "budget_used_percent": 33.55505969860465
    },
    {
      "type": "training",
      "description": "Training step 2821",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:02",
      "total_flops_so_far": 3.3566941814708844e+16,
      "budget_used_percent": 33.56694181470885
    },
    {
      "type": "training",
      "description": "Training step 2822",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:03",
      "total_flops_so_far": 3.3578823930813036e+16,
      "budget_used_percent": 33.57882393081304
    },
    {
      "type": "training",
      "description": "Training step 2823",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:03",
      "total_flops_so_far": 3.3590706046917228e+16,
      "budget_used_percent": 33.59070604691723
    },
    {
      "type": "training",
      "description": "Training step 2824",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:04",
      "total_flops_so_far": 3.360258816302142e+16,
      "budget_used_percent": 33.602588163021416
    },
    {
      "type": "training",
      "description": "Training step 2825",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:05",
      "total_flops_so_far": 3.3614470279125612e+16,
      "budget_used_percent": 33.61447027912561
    },
    {
      "type": "training",
      "description": "Training step 2826",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:05",
      "total_flops_so_far": 3.3626352395229804e+16,
      "budget_used_percent": 33.6263523952298
    },
    {
      "type": "training",
      "description": "Training step 2827",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:06",
      "total_flops_so_far": 3.3638234511333996e+16,
      "budget_used_percent": 33.638234511334
    },
    {
      "type": "training",
      "description": "Training step 2828",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:07",
      "total_flops_so_far": 3.3650116627438188e+16,
      "budget_used_percent": 33.65011662743819
    },
    {
      "type": "training",
      "description": "Training step 2829",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:07",
      "total_flops_so_far": 3.366199874354238e+16,
      "budget_used_percent": 33.661998743542384
    },
    {
      "type": "training",
      "description": "Training step 2830",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:08",
      "total_flops_so_far": 3.3673880859646572e+16,
      "budget_used_percent": 33.67388085964657
    },
    {
      "type": "training",
      "description": "Training step 2831",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:09",
      "total_flops_so_far": 3.3685762975750764e+16,
      "budget_used_percent": 33.68576297575076
    },
    {
      "type": "training",
      "description": "Training step 2832",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:09",
      "total_flops_so_far": 3.3697645091854956e+16,
      "budget_used_percent": 33.69764509185495
    },
    {
      "type": "training",
      "description": "Training step 2833",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:10",
      "total_flops_so_far": 3.3709527207959148e+16,
      "budget_used_percent": 33.70952720795915
    },
    {
      "type": "training",
      "description": "Training step 2834",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:10",
      "total_flops_so_far": 3.372140932406334e+16,
      "budget_used_percent": 33.72140932406334
    },
    {
      "type": "training",
      "description": "Training step 2835",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:11",
      "total_flops_so_far": 3.3733291440167532e+16,
      "budget_used_percent": 33.733291440167534
    },
    {
      "type": "training",
      "description": "Training step 2836",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:12",
      "total_flops_so_far": 3.3745173556271724e+16,
      "budget_used_percent": 33.74517355627172
    },
    {
      "type": "training",
      "description": "Training step 2837",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:12",
      "total_flops_so_far": 3.3757055672375916e+16,
      "budget_used_percent": 33.75705567237592
    },
    {
      "type": "training",
      "description": "Training step 2838",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:13",
      "total_flops_so_far": 3.3768937788480108e+16,
      "budget_used_percent": 33.76893778848011
    },
    {
      "type": "training",
      "description": "Training step 2839",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:14",
      "total_flops_so_far": 3.37808199045843e+16,
      "budget_used_percent": 33.780819904584305
    },
    {
      "type": "training",
      "description": "Training step 2840",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:14",
      "total_flops_so_far": 3.3792702020688492e+16,
      "budget_used_percent": 33.792702020688495
    },
    {
      "type": "training",
      "description": "Training step 2841",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:15",
      "total_flops_so_far": 3.3804584136792684e+16,
      "budget_used_percent": 33.804584136792684
    },
    {
      "type": "training",
      "description": "Training step 2842",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:16",
      "total_flops_so_far": 3.3816466252896876e+16,
      "budget_used_percent": 33.81646625289687
    },
    {
      "type": "training",
      "description": "Training step 2843",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:16",
      "total_flops_so_far": 3.3828348369001068e+16,
      "budget_used_percent": 33.82834836900106
    },
    {
      "type": "training",
      "description": "Training step 2844",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:17",
      "total_flops_so_far": 3.384023048510526e+16,
      "budget_used_percent": 33.84023048510526
    },
    {
      "type": "training",
      "description": "Training step 2845",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:18",
      "total_flops_so_far": 3.3852112601209452e+16,
      "budget_used_percent": 33.85211260120945
    },
    {
      "type": "training",
      "description": "Training step 2846",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:18",
      "total_flops_so_far": 3.3863994717313644e+16,
      "budget_used_percent": 33.863994717313645
    },
    {
      "type": "training",
      "description": "Training step 2847",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:19",
      "total_flops_so_far": 3.3875876833417836e+16,
      "budget_used_percent": 33.875876833417834
    },
    {
      "type": "training",
      "description": "Training step 2848",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:19",
      "total_flops_so_far": 3.3887758949522028e+16,
      "budget_used_percent": 33.88775894952203
    },
    {
      "type": "training",
      "description": "Training step 2849",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:20",
      "total_flops_so_far": 3.389964106562622e+16,
      "budget_used_percent": 33.89964106562622
    },
    {
      "type": "training",
      "description": "Training step 2850",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:21",
      "total_flops_so_far": 3.3911523181730412e+16,
      "budget_used_percent": 33.911523181730416
    },
    {
      "type": "training",
      "description": "Training step 2851",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:21",
      "total_flops_so_far": 3.3923405297834604e+16,
      "budget_used_percent": 33.923405297834606
    },
    {
      "type": "training",
      "description": "Training step 2852",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:22",
      "total_flops_so_far": 3.3935287413938796e+16,
      "budget_used_percent": 33.935287413938795
    },
    {
      "type": "training",
      "description": "Training step 2853",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:23",
      "total_flops_so_far": 3.3947169530042988e+16,
      "budget_used_percent": 33.947169530042984
    },
    {
      "type": "training",
      "description": "Training step 2854",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:23",
      "total_flops_so_far": 3.395905164614718e+16,
      "budget_used_percent": 33.95905164614718
    },
    {
      "type": "training",
      "description": "Training step 2855",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:24",
      "total_flops_so_far": 3.3970933762251372e+16,
      "budget_used_percent": 33.97093376225137
    },
    {
      "type": "training",
      "description": "Training step 2856",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:25",
      "total_flops_so_far": 3.3982815878355564e+16,
      "budget_used_percent": 33.98281587835557
    },
    {
      "type": "training",
      "description": "Training step 2857",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:25",
      "total_flops_so_far": 3.3994697994459756e+16,
      "budget_used_percent": 33.994697994459756
    },
    {
      "type": "training",
      "description": "Training step 2858",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:26",
      "total_flops_so_far": 3.4006580110563948e+16,
      "budget_used_percent": 34.00658011056395
    },
    {
      "type": "training",
      "description": "Training step 2859",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:27",
      "total_flops_so_far": 3.401846222666814e+16,
      "budget_used_percent": 34.01846222666814
    },
    {
      "type": "training",
      "description": "Training step 2860",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:27",
      "total_flops_so_far": 3.4030344342772332e+16,
      "budget_used_percent": 34.03034434277233
    },
    {
      "type": "training",
      "description": "Training step 2861",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:28",
      "total_flops_so_far": 3.4042226458876524e+16,
      "budget_used_percent": 34.04222645887652
    },
    {
      "type": "training",
      "description": "Training step 2862",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:28",
      "total_flops_so_far": 3.4054108574980716e+16,
      "budget_used_percent": 34.05410857498072
    },
    {
      "type": "training",
      "description": "Training step 2863",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:29",
      "total_flops_so_far": 3.4065990691084908e+16,
      "budget_used_percent": 34.065990691084906
    },
    {
      "type": "training",
      "description": "Training step 2864",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:30",
      "total_flops_so_far": 3.40778728071891e+16,
      "budget_used_percent": 34.0778728071891
    },
    {
      "type": "training",
      "description": "Training step 2865",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:30",
      "total_flops_so_far": 3.4089754923293292e+16,
      "budget_used_percent": 34.08975492329329
    },
    {
      "type": "training",
      "description": "Training step 2866",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:31",
      "total_flops_so_far": 3.4101637039397484e+16,
      "budget_used_percent": 34.10163703939749
    },
    {
      "type": "training",
      "description": "Training step 2867",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:32",
      "total_flops_so_far": 3.4113519155501676e+16,
      "budget_used_percent": 34.11351915550168
    },
    {
      "type": "training",
      "description": "Training step 2868",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:32",
      "total_flops_so_far": 3.4125401271605868e+16,
      "budget_used_percent": 34.12540127160587
    },
    {
      "type": "training",
      "description": "Training step 2869",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:33",
      "total_flops_so_far": 3.413728338771006e+16,
      "budget_used_percent": 34.13728338771006
    },
    {
      "type": "training",
      "description": "Training step 2870",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:34",
      "total_flops_so_far": 3.4149165503814252e+16,
      "budget_used_percent": 34.14916550381425
    },
    {
      "type": "training",
      "description": "Training step 2871",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:34",
      "total_flops_so_far": 3.4161047619918444e+16,
      "budget_used_percent": 34.16104761991844
    },
    {
      "type": "training",
      "description": "Training step 2872",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:35",
      "total_flops_so_far": 3.4172929736022636e+16,
      "budget_used_percent": 34.17292973602263
    },
    {
      "type": "training",
      "description": "Training step 2873",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:36",
      "total_flops_so_far": 3.4184811852126828e+16,
      "budget_used_percent": 34.18481185212683
    },
    {
      "type": "training",
      "description": "Training step 2874",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:36",
      "total_flops_so_far": 3.419669396823102e+16,
      "budget_used_percent": 34.19669396823102
    },
    {
      "type": "training",
      "description": "Training step 2875",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:37",
      "total_flops_so_far": 3.4208576084335212e+16,
      "budget_used_percent": 34.20857608433521
    },
    {
      "type": "training",
      "description": "Training step 2876",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:38",
      "total_flops_so_far": 3.4220458200439404e+16,
      "budget_used_percent": 34.2204582004394
    },
    {
      "type": "training",
      "description": "Training step 2877",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:38",
      "total_flops_so_far": 3.4232340316543596e+16,
      "budget_used_percent": 34.2323403165436
    },
    {
      "type": "training",
      "description": "Training step 2878",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:39",
      "total_flops_so_far": 3.4244222432647788e+16,
      "budget_used_percent": 34.24422243264779
    },
    {
      "type": "training",
      "description": "Training step 2879",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:39",
      "total_flops_so_far": 3.425610454875198e+16,
      "budget_used_percent": 34.25610454875198
    },
    {
      "type": "training",
      "description": "Training step 2880",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:40",
      "total_flops_so_far": 3.4267986664856172e+16,
      "budget_used_percent": 34.26798666485617
    },
    {
      "type": "training",
      "description": "Training step 2881",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:41",
      "total_flops_so_far": 3.4279868780960364e+16,
      "budget_used_percent": 34.27986878096036
    },
    {
      "type": "training",
      "description": "Training step 2882",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:41",
      "total_flops_so_far": 3.4291750897064556e+16,
      "budget_used_percent": 34.29175089706455
    },
    {
      "type": "training",
      "description": "Training step 2883",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:42",
      "total_flops_so_far": 3.4303633013168748e+16,
      "budget_used_percent": 34.30363301316875
    },
    {
      "type": "training",
      "description": "Training step 2884",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:43",
      "total_flops_so_far": 3.431551512927294e+16,
      "budget_used_percent": 34.31551512927294
    },
    {
      "type": "training",
      "description": "Training step 2885",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:44",
      "total_flops_so_far": 3.4327397245377132e+16,
      "budget_used_percent": 34.327397245377135
    },
    {
      "type": "training",
      "description": "Training step 2886",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:44",
      "total_flops_so_far": 3.4339279361481324e+16,
      "budget_used_percent": 34.339279361481324
    },
    {
      "type": "training",
      "description": "Training step 2887",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:45",
      "total_flops_so_far": 3.4351161477585516e+16,
      "budget_used_percent": 34.35116147758552
    },
    {
      "type": "training",
      "description": "Training step 2888",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:46",
      "total_flops_so_far": 3.4363043593689708e+16,
      "budget_used_percent": 34.36304359368971
    },
    {
      "type": "training",
      "description": "Training step 2889",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:46",
      "total_flops_so_far": 3.43749257097939e+16,
      "budget_used_percent": 34.3749257097939
    },
    {
      "type": "training",
      "description": "Training step 2890",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:47",
      "total_flops_so_far": 3.4386807825898092e+16,
      "budget_used_percent": 34.38680782589809
    },
    {
      "type": "training",
      "description": "Training step 2891",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:47",
      "total_flops_so_far": 3.4398689942002284e+16,
      "budget_used_percent": 34.398689942002285
    },
    {
      "type": "training",
      "description": "Training step 2892",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:48",
      "total_flops_so_far": 3.4410572058106476e+16,
      "budget_used_percent": 34.410572058106474
    },
    {
      "type": "training",
      "description": "Training step 2893",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:49",
      "total_flops_so_far": 3.4422454174210668e+16,
      "budget_used_percent": 34.42245417421067
    },
    {
      "type": "training",
      "description": "Training step 2894",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:49",
      "total_flops_so_far": 3.443433629031486e+16,
      "budget_used_percent": 34.43433629031486
    },
    {
      "type": "training",
      "description": "Training step 2895",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:50",
      "total_flops_so_far": 3.4446218406419052e+16,
      "budget_used_percent": 34.44621840641906
    },
    {
      "type": "training",
      "description": "Training step 2896",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:50",
      "total_flops_so_far": 3.4458100522523244e+16,
      "budget_used_percent": 34.458100522523246
    },
    {
      "type": "training",
      "description": "Training step 2897",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:51",
      "total_flops_so_far": 3.4469982638627436e+16,
      "budget_used_percent": 34.469982638627435
    },
    {
      "type": "training",
      "description": "Training step 2898",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:52",
      "total_flops_so_far": 3.4481864754731628e+16,
      "budget_used_percent": 34.48186475473163
    },
    {
      "type": "training",
      "description": "Training step 2899",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:52",
      "total_flops_so_far": 3.449374687083582e+16,
      "budget_used_percent": 34.49374687083582
    },
    {
      "type": "training",
      "description": "Training step 2900",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:53",
      "total_flops_so_far": 3.4505628986940012e+16,
      "budget_used_percent": 34.50562898694001
    },
    {
      "type": "training",
      "description": "Training step 2901",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:54",
      "total_flops_so_far": 3.4517511103044204e+16,
      "budget_used_percent": 34.5175111030442
    },
    {
      "type": "training",
      "description": "Training step 2902",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:54",
      "total_flops_so_far": 3.4529393219148396e+16,
      "budget_used_percent": 34.529393219148396
    },
    {
      "type": "training",
      "description": "Training step 2903",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:55",
      "total_flops_so_far": 3.4541275335252588e+16,
      "budget_used_percent": 34.541275335252585
    },
    {
      "type": "training",
      "description": "Training step 2904",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:56",
      "total_flops_so_far": 3.455315745135678e+16,
      "budget_used_percent": 34.55315745135678
    },
    {
      "type": "training",
      "description": "Training step 2905",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:56",
      "total_flops_so_far": 3.4565039567460972e+16,
      "budget_used_percent": 34.56503956746097
    },
    {
      "type": "training",
      "description": "Training step 2906",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:57",
      "total_flops_so_far": 3.4576921683565164e+16,
      "budget_used_percent": 34.57692168356517
    },
    {
      "type": "training",
      "description": "Training step 2907",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:58",
      "total_flops_so_far": 3.4588803799669356e+16,
      "budget_used_percent": 34.58880379966936
    },
    {
      "type": "training",
      "description": "Training step 2908",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:58",
      "total_flops_so_far": 3.4600685915773548e+16,
      "budget_used_percent": 34.600685915773546
    },
    {
      "type": "training",
      "description": "Training step 2909",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:59",
      "total_flops_so_far": 3.461256803187774e+16,
      "budget_used_percent": 34.612568031877736
    },
    {
      "type": "training",
      "description": "Training step 2910",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:39:59",
      "total_flops_so_far": 3.4624450147981932e+16,
      "budget_used_percent": 34.62445014798193
    },
    {
      "type": "training",
      "description": "Training step 2911",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:00",
      "total_flops_so_far": 3.4636332264086124e+16,
      "budget_used_percent": 34.63633226408612
    },
    {
      "type": "training",
      "description": "Training step 2912",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:01",
      "total_flops_so_far": 3.4648214380190316e+16,
      "budget_used_percent": 34.64821438019032
    },
    {
      "type": "training",
      "description": "Training step 2913",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:01",
      "total_flops_so_far": 3.4660096496294508e+16,
      "budget_used_percent": 34.66009649629451
    },
    {
      "type": "training",
      "description": "Training step 2914",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:02",
      "total_flops_so_far": 3.46719786123987e+16,
      "budget_used_percent": 34.6719786123987
    },
    {
      "type": "training",
      "description": "Training step 2915",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:03",
      "total_flops_so_far": 3.4683860728502892e+16,
      "budget_used_percent": 34.68386072850289
    },
    {
      "type": "training",
      "description": "Training step 2916",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:03",
      "total_flops_so_far": 3.4695742844607084e+16,
      "budget_used_percent": 34.69574284460709
    },
    {
      "type": "training",
      "description": "Training step 2917",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:04",
      "total_flops_so_far": 3.4707624960711276e+16,
      "budget_used_percent": 34.70762496071128
    },
    {
      "type": "training",
      "description": "Training step 2918",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:05",
      "total_flops_so_far": 3.4719507076815468e+16,
      "budget_used_percent": 34.71950707681547
    },
    {
      "type": "training",
      "description": "Training step 2919",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:05",
      "total_flops_so_far": 3.473138919291966e+16,
      "budget_used_percent": 34.73138919291966
    },
    {
      "type": "training",
      "description": "Training step 2920",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:06",
      "total_flops_so_far": 3.4743271309023852e+16,
      "budget_used_percent": 34.743271309023854
    },
    {
      "type": "training",
      "description": "Training step 2921",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:07",
      "total_flops_so_far": 3.4755153425128044e+16,
      "budget_used_percent": 34.75515342512804
    },
    {
      "type": "training",
      "description": "Training step 2922",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:07",
      "total_flops_so_far": 3.4767035541232236e+16,
      "budget_used_percent": 34.76703554123224
    },
    {
      "type": "training",
      "description": "Training step 2923",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:08",
      "total_flops_so_far": 3.4778917657336428e+16,
      "budget_used_percent": 34.77891765733643
    },
    {
      "type": "training",
      "description": "Training step 2924",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:08",
      "total_flops_so_far": 3.479079977344062e+16,
      "budget_used_percent": 34.790799773440625
    },
    {
      "type": "training",
      "description": "Training step 2925",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:09",
      "total_flops_so_far": 3.4802681889544812e+16,
      "budget_used_percent": 34.802681889544814
    },
    {
      "type": "training",
      "description": "Training step 2926",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:10",
      "total_flops_so_far": 3.4814564005649004e+16,
      "budget_used_percent": 34.814564005649004
    },
    {
      "type": "training",
      "description": "Training step 2927",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:10",
      "total_flops_so_far": 3.4826446121753196e+16,
      "budget_used_percent": 34.82644612175319
    },
    {
      "type": "training",
      "description": "Training step 2928",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:11",
      "total_flops_so_far": 3.4838328237857388e+16,
      "budget_used_percent": 34.83832823785738
    },
    {
      "type": "training",
      "description": "Training step 2929",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:12",
      "total_flops_so_far": 3.485021035396158e+16,
      "budget_used_percent": 34.85021035396158
    },
    {
      "type": "training",
      "description": "Training step 2930",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:12",
      "total_flops_so_far": 3.4862092470065772e+16,
      "budget_used_percent": 34.86209247006577
    },
    {
      "type": "training",
      "description": "Training step 2931",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:13",
      "total_flops_so_far": 3.4873974586169964e+16,
      "budget_used_percent": 34.873974586169965
    },
    {
      "type": "training",
      "description": "Training step 2932",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:14",
      "total_flops_so_far": 3.4885856702274156e+16,
      "budget_used_percent": 34.885856702274154
    },
    {
      "type": "training",
      "description": "Training step 2933",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:14",
      "total_flops_so_far": 3.4897738818378348e+16,
      "budget_used_percent": 34.89773881837835
    },
    {
      "type": "training",
      "description": "Training step 2934",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:15",
      "total_flops_so_far": 3.490962093448254e+16,
      "budget_used_percent": 34.90962093448254
    },
    {
      "type": "training",
      "description": "Training step 2935",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:16",
      "total_flops_so_far": 3.4921503050586732e+16,
      "budget_used_percent": 34.921503050586736
    },
    {
      "type": "training",
      "description": "Training step 2936",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:16",
      "total_flops_so_far": 3.4933385166690924e+16,
      "budget_used_percent": 34.933385166690925
    },
    {
      "type": "training",
      "description": "Training step 2937",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:17",
      "total_flops_so_far": 3.4945267282795116e+16,
      "budget_used_percent": 34.945267282795115
    },
    {
      "type": "training",
      "description": "Training step 2938",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:18",
      "total_flops_so_far": 3.4957149398899308e+16,
      "budget_used_percent": 34.957149398899304
    },
    {
      "type": "training",
      "description": "Training step 2939",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:18",
      "total_flops_so_far": 3.49690315150035e+16,
      "budget_used_percent": 34.9690315150035
    },
    {
      "type": "training",
      "description": "Training step 2940",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:19",
      "total_flops_so_far": 3.4980913631107692e+16,
      "budget_used_percent": 34.98091363110769
    },
    {
      "type": "training",
      "description": "Training step 2941",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:19",
      "total_flops_so_far": 3.4992795747211884e+16,
      "budget_used_percent": 34.992795747211886
    },
    {
      "type": "training",
      "description": "Training step 2942",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:20",
      "total_flops_so_far": 3.5004677863316076e+16,
      "budget_used_percent": 35.004677863316076
    },
    {
      "type": "training",
      "description": "Training step 2943",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:21",
      "total_flops_so_far": 3.5016559979420268e+16,
      "budget_used_percent": 35.01655997942027
    },
    {
      "type": "training",
      "description": "Training step 2944",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:21",
      "total_flops_so_far": 3.502844209552446e+16,
      "budget_used_percent": 35.02844209552446
    },
    {
      "type": "training",
      "description": "Training step 2945",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:22",
      "total_flops_so_far": 3.5040324211628652e+16,
      "budget_used_percent": 35.04032421162865
    },
    {
      "type": "training",
      "description": "Training step 2946",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:23",
      "total_flops_so_far": 3.5052206327732844e+16,
      "budget_used_percent": 35.05220632773284
    },
    {
      "type": "training",
      "description": "Training step 2947",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:23",
      "total_flops_so_far": 3.5064088443837036e+16,
      "budget_used_percent": 35.064088443837036
    },
    {
      "type": "training",
      "description": "Training step 2948",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:24",
      "total_flops_so_far": 3.5075970559941228e+16,
      "budget_used_percent": 35.075970559941226
    },
    {
      "type": "training",
      "description": "Training step 2949",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:25",
      "total_flops_so_far": 3.508785267604542e+16,
      "budget_used_percent": 35.08785267604542
    },
    {
      "type": "training",
      "description": "Training step 2950",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:25",
      "total_flops_so_far": 3.5099734792149612e+16,
      "budget_used_percent": 35.09973479214961
    },
    {
      "type": "training",
      "description": "Training step 2951",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:26",
      "total_flops_so_far": 3.5111616908253804e+16,
      "budget_used_percent": 35.11161690825381
    },
    {
      "type": "training",
      "description": "Training step 2952",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:27",
      "total_flops_so_far": 3.5123499024357996e+16,
      "budget_used_percent": 35.123499024358
    },
    {
      "type": "training",
      "description": "Training step 2953",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:27",
      "total_flops_so_far": 3.5135381140462188e+16,
      "budget_used_percent": 35.135381140462194
    },
    {
      "type": "training",
      "description": "Training step 2954",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:28",
      "total_flops_so_far": 3.514726325656638e+16,
      "budget_used_percent": 35.14726325656638
    },
    {
      "type": "training",
      "description": "Training step 2955",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:29",
      "total_flops_so_far": 3.5159145372670572e+16,
      "budget_used_percent": 35.15914537267057
    },
    {
      "type": "training",
      "description": "Training step 2956",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:29",
      "total_flops_so_far": 3.5171027488774764e+16,
      "budget_used_percent": 35.17102748877476
    },
    {
      "type": "training",
      "description": "Training step 2957",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:30",
      "total_flops_so_far": 3.5182909604878956e+16,
      "budget_used_percent": 35.18290960487895
    },
    {
      "type": "training",
      "description": "Training step 2958",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:30",
      "total_flops_so_far": 3.5194791720983148e+16,
      "budget_used_percent": 35.19479172098315
    },
    {
      "type": "training",
      "description": "Training step 2959",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:31",
      "total_flops_so_far": 3.520667383708734e+16,
      "budget_used_percent": 35.20667383708734
    },
    {
      "type": "training",
      "description": "Training step 2960",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:32",
      "total_flops_so_far": 3.5218555953191532e+16,
      "budget_used_percent": 35.21855595319153
    },
    {
      "type": "training",
      "description": "Training step 2961",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:32",
      "total_flops_so_far": 3.5230438069295724e+16,
      "budget_used_percent": 35.23043806929572
    },
    {
      "type": "training",
      "description": "Training step 2962",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:33",
      "total_flops_so_far": 3.5242320185399916e+16,
      "budget_used_percent": 35.24232018539992
    },
    {
      "type": "training",
      "description": "Training step 2963",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:34",
      "total_flops_so_far": 3.5254202301504108e+16,
      "budget_used_percent": 35.25420230150411
    },
    {
      "type": "training",
      "description": "Training step 2964",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:34",
      "total_flops_so_far": 3.52660844176083e+16,
      "budget_used_percent": 35.2660844176083
    },
    {
      "type": "training",
      "description": "Training step 2965",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:35",
      "total_flops_so_far": 3.5277966533712492e+16,
      "budget_used_percent": 35.27796653371249
    },
    {
      "type": "training",
      "description": "Training step 2966",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:36",
      "total_flops_so_far": 3.5289848649816684e+16,
      "budget_used_percent": 35.28984864981668
    },
    {
      "type": "training",
      "description": "Training step 2967",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:36",
      "total_flops_so_far": 3.5301730765920876e+16,
      "budget_used_percent": 35.30173076592087
    },
    {
      "type": "training",
      "description": "Training step 2968",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:37",
      "total_flops_so_far": 3.5313612882025068e+16,
      "budget_used_percent": 35.31361288202507
    },
    {
      "type": "training",
      "description": "Training step 2969",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:38",
      "total_flops_so_far": 3.532549499812926e+16,
      "budget_used_percent": 35.32549499812926
    },
    {
      "type": "training",
      "description": "Training step 2970",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:38",
      "total_flops_so_far": 3.5337377114233452e+16,
      "budget_used_percent": 35.337377114233455
    },
    {
      "type": "training",
      "description": "Training step 2971",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:39",
      "total_flops_so_far": 3.5349259230337644e+16,
      "budget_used_percent": 35.349259230337644
    },
    {
      "type": "training",
      "description": "Training step 2972",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:39",
      "total_flops_so_far": 3.5361141346441836e+16,
      "budget_used_percent": 35.36114134644184
    },
    {
      "type": "training",
      "description": "Training step 2973",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:40",
      "total_flops_so_far": 3.5373023462546028e+16,
      "budget_used_percent": 35.37302346254603
    },
    {
      "type": "training",
      "description": "Training step 2974",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:41",
      "total_flops_so_far": 3.538490557865022e+16,
      "budget_used_percent": 35.38490557865022
    },
    {
      "type": "training",
      "description": "Training step 2975",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:41",
      "total_flops_so_far": 3.5396787694754412e+16,
      "budget_used_percent": 35.39678769475441
    },
    {
      "type": "training",
      "description": "Training step 2976",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:42",
      "total_flops_so_far": 3.5408669810858604e+16,
      "budget_used_percent": 35.408669810858605
    },
    {
      "type": "training",
      "description": "Training step 2977",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:43",
      "total_flops_so_far": 3.5420551926962796e+16,
      "budget_used_percent": 35.420551926962794
    },
    {
      "type": "training",
      "description": "Training step 2978",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:43",
      "total_flops_so_far": 3.5432434043066988e+16,
      "budget_used_percent": 35.43243404306699
    },
    {
      "type": "training",
      "description": "Training step 2979",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:44",
      "total_flops_so_far": 3.544431615917118e+16,
      "budget_used_percent": 35.44431615917118
    },
    {
      "type": "training",
      "description": "Training step 2980",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:45",
      "total_flops_so_far": 3.5456198275275372e+16,
      "budget_used_percent": 35.456198275275376
    },
    {
      "type": "training",
      "description": "Training step 2981",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:45",
      "total_flops_so_far": 3.5468080391379564e+16,
      "budget_used_percent": 35.468080391379566
    },
    {
      "type": "training",
      "description": "Training step 2982",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:46",
      "total_flops_so_far": 3.5479962507483756e+16,
      "budget_used_percent": 35.479962507483755
    },
    {
      "type": "training",
      "description": "Training step 2983",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:47",
      "total_flops_so_far": 3.5491844623587948e+16,
      "budget_used_percent": 35.49184462358795
    },
    {
      "type": "training",
      "description": "Training step 2984",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:47",
      "total_flops_so_far": 3.550372673969214e+16,
      "budget_used_percent": 35.50372673969214
    },
    {
      "type": "training",
      "description": "Training step 2985",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:48",
      "total_flops_so_far": 3.5515608855796332e+16,
      "budget_used_percent": 35.51560885579633
    },
    {
      "type": "training",
      "description": "Training step 2986",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:49",
      "total_flops_so_far": 3.5527490971900524e+16,
      "budget_used_percent": 35.52749097190052
    },
    {
      "type": "training",
      "description": "Training step 2987",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:49",
      "total_flops_so_far": 3.5539373088004716e+16,
      "budget_used_percent": 35.539373088004716
    },
    {
      "type": "training",
      "description": "Training step 2988",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:50",
      "total_flops_so_far": 3.5551255204108908e+16,
      "budget_used_percent": 35.551255204108905
    },
    {
      "type": "training",
      "description": "Training step 2989",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:51",
      "total_flops_so_far": 3.55631373202131e+16,
      "budget_used_percent": 35.5631373202131
    },
    {
      "type": "training",
      "description": "Training step 2990",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:51",
      "total_flops_so_far": 3.5575019436317292e+16,
      "budget_used_percent": 35.57501943631729
    },
    {
      "type": "training",
      "description": "Training step 2991",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:52",
      "total_flops_so_far": 3.5586901552421484e+16,
      "budget_used_percent": 35.58690155242149
    },
    {
      "type": "training",
      "description": "Training step 2992",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:52",
      "total_flops_so_far": 3.5598783668525676e+16,
      "budget_used_percent": 35.59878366852568
    },
    {
      "type": "training",
      "description": "Training step 2993",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:53",
      "total_flops_so_far": 3.5610665784629868e+16,
      "budget_used_percent": 35.610665784629866
    },
    {
      "type": "training",
      "description": "Training step 2994",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:54",
      "total_flops_so_far": 3.562254790073406e+16,
      "budget_used_percent": 35.622547900734055
    },
    {
      "type": "training",
      "description": "Training step 2995",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:54",
      "total_flops_so_far": 3.5634430016838252e+16,
      "budget_used_percent": 35.63443001683825
    },
    {
      "type": "training",
      "description": "Training step 2996",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:55",
      "total_flops_so_far": 3.5646312132942444e+16,
      "budget_used_percent": 35.64631213294244
    },
    {
      "type": "training",
      "description": "Training step 2997",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:56",
      "total_flops_so_far": 3.5658194249046636e+16,
      "budget_used_percent": 35.65819424904664
    },
    {
      "type": "training",
      "description": "Training step 2998",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:56",
      "total_flops_so_far": 3.5670076365150828e+16,
      "budget_used_percent": 35.67007636515083
    },
    {
      "type": "training",
      "description": "Training step 2999",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:40:57",
      "total_flops_so_far": 3.568195848125502e+16,
      "budget_used_percent": 35.68195848125502
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:41:05",
      "total_flops_so_far": 3.5682669110392876e+16,
      "budget_used_percent": 35.68266911039288
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 714333709232.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:41:12",
      "total_flops_so_far": 3.5683383444102108e+16,
      "budget_used_percent": 35.683383444102105
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:41:20",
      "total_flops_so_far": 3.568409592516546e+16,
      "budget_used_percent": 35.68409592516546
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:41:27",
      "total_flops_so_far": 3.5684806554303316e+16,
      "budget_used_percent": 35.684806554303314
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 713407296244.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:41:34",
      "total_flops_so_far": 3.568551996159956e+16,
      "budget_used_percent": 35.68551996159956
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 5",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:41:42",
      "total_flops_so_far": 3.5686230590737416e+16,
      "budget_used_percent": 35.686230590737416
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 6",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:41:49",
      "total_flops_so_far": 3.568694307180077e+16,
      "budget_used_percent": 35.68694307180077
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 7",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:41:57",
      "total_flops_so_far": 3.568765555286412e+16,
      "budget_used_percent": 35.68765555286412
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 8",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:04",
      "total_flops_so_far": 3.568836803392747e+16,
      "budget_used_percent": 35.68836803392747
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 9",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:11",
      "total_flops_so_far": 3.5689080514990824e+16,
      "budget_used_percent": 35.68908051499082
    },
    {
      "type": "training",
      "description": "Training step 3000",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:11",
      "total_flops_so_far": 3.5700962631095016e+16,
      "budget_used_percent": 35.70096263109502
    },
    {
      "type": "training",
      "description": "Training step 3001",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:12",
      "total_flops_so_far": 3.571284474719921e+16,
      "budget_used_percent": 35.712844747199206
    },
    {
      "type": "training",
      "description": "Training step 3002",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:13",
      "total_flops_so_far": 3.57247268633034e+16,
      "budget_used_percent": 35.7247268633034
    },
    {
      "type": "training",
      "description": "Training step 3003",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:13",
      "total_flops_so_far": 3.573660897940759e+16,
      "budget_used_percent": 35.73660897940759
    },
    {
      "type": "training",
      "description": "Training step 3004",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:14",
      "total_flops_so_far": 3.5748491095511784e+16,
      "budget_used_percent": 35.74849109551179
    },
    {
      "type": "training",
      "description": "Training step 3005",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:15",
      "total_flops_so_far": 3.5760373211615976e+16,
      "budget_used_percent": 35.76037321161598
    },
    {
      "type": "training",
      "description": "Training step 3006",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:15",
      "total_flops_so_far": 3.577225532772017e+16,
      "budget_used_percent": 35.772255327720174
    },
    {
      "type": "training",
      "description": "Training step 3007",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:16",
      "total_flops_so_far": 3.578413744382436e+16,
      "budget_used_percent": 35.78413744382436
    },
    {
      "type": "training",
      "description": "Training step 3008",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:17",
      "total_flops_so_far": 3.579601955992855e+16,
      "budget_used_percent": 35.79601955992855
    },
    {
      "type": "training",
      "description": "Training step 3009",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:17",
      "total_flops_so_far": 3.5807901676032744e+16,
      "budget_used_percent": 35.80790167603274
    },
    {
      "type": "training",
      "description": "Training step 3010",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:18",
      "total_flops_so_far": 3.5819783792136936e+16,
      "budget_used_percent": 35.81978379213693
    },
    {
      "type": "training",
      "description": "Training step 3011",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:19",
      "total_flops_so_far": 3.583166590824113e+16,
      "budget_used_percent": 35.83166590824113
    },
    {
      "type": "training",
      "description": "Training step 3012",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:19",
      "total_flops_so_far": 3.584354802434532e+16,
      "budget_used_percent": 35.84354802434532
    },
    {
      "type": "training",
      "description": "Training step 3013",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:20",
      "total_flops_so_far": 3.585543014044951e+16,
      "budget_used_percent": 35.85543014044951
    },
    {
      "type": "training",
      "description": "Training step 3014",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:20",
      "total_flops_so_far": 3.5867312256553704e+16,
      "budget_used_percent": 35.8673122565537
    },
    {
      "type": "training",
      "description": "Training step 3015",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:21",
      "total_flops_so_far": 3.5879194372657896e+16,
      "budget_used_percent": 35.8791943726579
    },
    {
      "type": "training",
      "description": "Training step 3016",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:22",
      "total_flops_so_far": 3.589107648876209e+16,
      "budget_used_percent": 35.89107648876209
    },
    {
      "type": "training",
      "description": "Training step 3017",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:22",
      "total_flops_so_far": 3.590295860486628e+16,
      "budget_used_percent": 35.90295860486628
    },
    {
      "type": "training",
      "description": "Training step 3018",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:23",
      "total_flops_so_far": 3.591484072097047e+16,
      "budget_used_percent": 35.91484072097047
    },
    {
      "type": "training",
      "description": "Training step 3019",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:24",
      "total_flops_so_far": 3.5926722837074664e+16,
      "budget_used_percent": 35.926722837074664
    },
    {
      "type": "training",
      "description": "Training step 3020",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:24",
      "total_flops_so_far": 3.5938604953178856e+16,
      "budget_used_percent": 35.93860495317885
    },
    {
      "type": "training",
      "description": "Training step 3021",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:25",
      "total_flops_so_far": 3.595048706928305e+16,
      "budget_used_percent": 35.95048706928305
    },
    {
      "type": "training",
      "description": "Training step 3022",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:26",
      "total_flops_so_far": 3.596236918538724e+16,
      "budget_used_percent": 35.96236918538724
    },
    {
      "type": "training",
      "description": "Training step 3023",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:26",
      "total_flops_so_far": 3.597425130149143e+16,
      "budget_used_percent": 35.974251301491435
    },
    {
      "type": "training",
      "description": "Training step 3024",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:27",
      "total_flops_so_far": 3.5986133417595624e+16,
      "budget_used_percent": 35.986133417595624
    },
    {
      "type": "training",
      "description": "Training step 3025",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:28",
      "total_flops_so_far": 3.5998015533699816e+16,
      "budget_used_percent": 35.99801553369982
    },
    {
      "type": "training",
      "description": "Training step 3026",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:28",
      "total_flops_so_far": 3.600989764980401e+16,
      "budget_used_percent": 36.00989764980401
    },
    {
      "type": "training",
      "description": "Training step 3027",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:29",
      "total_flops_so_far": 3.60217797659082e+16,
      "budget_used_percent": 36.0217797659082
    },
    {
      "type": "training",
      "description": "Training step 3028",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:30",
      "total_flops_so_far": 3.603366188201239e+16,
      "budget_used_percent": 36.03366188201239
    },
    {
      "type": "training",
      "description": "Training step 3029",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:30",
      "total_flops_so_far": 3.6045543998116584e+16,
      "budget_used_percent": 36.045543998116585
    },
    {
      "type": "training",
      "description": "Training step 3030",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:31",
      "total_flops_so_far": 3.6057426114220776e+16,
      "budget_used_percent": 36.057426114220775
    },
    {
      "type": "training",
      "description": "Training step 3031",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:31",
      "total_flops_so_far": 3.606930823032497e+16,
      "budget_used_percent": 36.06930823032497
    },
    {
      "type": "training",
      "description": "Training step 3032",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:32",
      "total_flops_so_far": 3.608119034642916e+16,
      "budget_used_percent": 36.08119034642916
    },
    {
      "type": "training",
      "description": "Training step 3033",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:33",
      "total_flops_so_far": 3.609307246253335e+16,
      "budget_used_percent": 36.09307246253336
    },
    {
      "type": "training",
      "description": "Training step 3034",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:33",
      "total_flops_so_far": 3.6104954578637544e+16,
      "budget_used_percent": 36.104954578637546
    },
    {
      "type": "training",
      "description": "Training step 3035",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:34",
      "total_flops_so_far": 3.6116836694741736e+16,
      "budget_used_percent": 36.116836694741735
    },
    {
      "type": "training",
      "description": "Training step 3036",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:35",
      "total_flops_so_far": 3.612871881084593e+16,
      "budget_used_percent": 36.128718810845925
    },
    {
      "type": "training",
      "description": "Training step 3037",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:35",
      "total_flops_so_far": 3.614060092695012e+16,
      "budget_used_percent": 36.14060092695012
    },
    {
      "type": "training",
      "description": "Training step 3038",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:36",
      "total_flops_so_far": 3.615248304305431e+16,
      "budget_used_percent": 36.15248304305431
    },
    {
      "type": "training",
      "description": "Training step 3039",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:37",
      "total_flops_so_far": 3.6164365159158504e+16,
      "budget_used_percent": 36.1643651591585
    },
    {
      "type": "training",
      "description": "Training step 3040",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:37",
      "total_flops_so_far": 3.6176247275262696e+16,
      "budget_used_percent": 36.176247275262696
    },
    {
      "type": "training",
      "description": "Training step 3041",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:38",
      "total_flops_so_far": 3.618812939136689e+16,
      "budget_used_percent": 36.188129391366886
    },
    {
      "type": "training",
      "description": "Training step 3042",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:39",
      "total_flops_so_far": 3.620001150747108e+16,
      "budget_used_percent": 36.20001150747108
    },
    {
      "type": "training",
      "description": "Training step 3043",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:39",
      "total_flops_so_far": 3.621189362357527e+16,
      "budget_used_percent": 36.21189362357527
    },
    {
      "type": "training",
      "description": "Training step 3044",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:40",
      "total_flops_so_far": 3.6223775739679464e+16,
      "budget_used_percent": 36.22377573967947
    },
    {
      "type": "training",
      "description": "Training step 3045",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:41",
      "total_flops_so_far": 3.6235657855783656e+16,
      "budget_used_percent": 36.23565785578366
    },
    {
      "type": "training",
      "description": "Training step 3046",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:41",
      "total_flops_so_far": 3.624753997188785e+16,
      "budget_used_percent": 36.247539971887846
    },
    {
      "type": "training",
      "description": "Training step 3047",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:42",
      "total_flops_so_far": 3.625942208799204e+16,
      "budget_used_percent": 36.259422087992036
    },
    {
      "type": "training",
      "description": "Training step 3048",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:42",
      "total_flops_so_far": 3.627130420409623e+16,
      "budget_used_percent": 36.27130420409623
    },
    {
      "type": "training",
      "description": "Training step 3049",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:43",
      "total_flops_so_far": 3.6283186320200424e+16,
      "budget_used_percent": 36.28318632020042
    },
    {
      "type": "training",
      "description": "Training step 3050",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:44",
      "total_flops_so_far": 3.6295068436304616e+16,
      "budget_used_percent": 36.29506843630462
    },
    {
      "type": "training",
      "description": "Training step 3051",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:44",
      "total_flops_so_far": 3.630695055240881e+16,
      "budget_used_percent": 36.30695055240881
    },
    {
      "type": "training",
      "description": "Training step 3052",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:45",
      "total_flops_so_far": 3.6318832668513e+16,
      "budget_used_percent": 36.318832668513004
    },
    {
      "type": "training",
      "description": "Training step 3053",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:46",
      "total_flops_so_far": 3.633071478461719e+16,
      "budget_used_percent": 36.33071478461719
    },
    {
      "type": "training",
      "description": "Training step 3054",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:46",
      "total_flops_so_far": 3.6342596900721384e+16,
      "budget_used_percent": 36.34259690072138
    },
    {
      "type": "training",
      "description": "Training step 3055",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:47",
      "total_flops_so_far": 3.6354479016825576e+16,
      "budget_used_percent": 36.35447901682557
    },
    {
      "type": "training",
      "description": "Training step 3056",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:48",
      "total_flops_so_far": 3.636636113292977e+16,
      "budget_used_percent": 36.36636113292977
    },
    {
      "type": "training",
      "description": "Training step 3057",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:48",
      "total_flops_so_far": 3.637824324903396e+16,
      "budget_used_percent": 36.37824324903396
    },
    {
      "type": "training",
      "description": "Training step 3058",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:49",
      "total_flops_so_far": 3.639012536513815e+16,
      "budget_used_percent": 36.390125365138154
    },
    {
      "type": "training",
      "description": "Training step 3059",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:50",
      "total_flops_so_far": 3.6402007481242344e+16,
      "budget_used_percent": 36.40200748124234
    },
    {
      "type": "training",
      "description": "Training step 3060",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:50",
      "total_flops_so_far": 3.6413889597346536e+16,
      "budget_used_percent": 36.41388959734654
    },
    {
      "type": "training",
      "description": "Training step 3061",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:51",
      "total_flops_so_far": 3.642577171345073e+16,
      "budget_used_percent": 36.42577171345073
    },
    {
      "type": "training",
      "description": "Training step 3062",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:52",
      "total_flops_so_far": 3.643765382955492e+16,
      "budget_used_percent": 36.437653829554925
    },
    {
      "type": "training",
      "description": "Training step 3063",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:52",
      "total_flops_so_far": 3.644953594565911e+16,
      "budget_used_percent": 36.449535945659115
    },
    {
      "type": "training",
      "description": "Training step 3064",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:53",
      "total_flops_so_far": 3.6461418061763304e+16,
      "budget_used_percent": 36.461418061763304
    },
    {
      "type": "training",
      "description": "Training step 3065",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:54",
      "total_flops_so_far": 3.6473300177867496e+16,
      "budget_used_percent": 36.47330017786749
    },
    {
      "type": "training",
      "description": "Training step 3066",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:54",
      "total_flops_so_far": 3.648518229397169e+16,
      "budget_used_percent": 36.48518229397169
    },
    {
      "type": "training",
      "description": "Training step 3067",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:55",
      "total_flops_so_far": 3.649706441007588e+16,
      "budget_used_percent": 36.49706441007588
    },
    {
      "type": "training",
      "description": "Training step 3068",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:55",
      "total_flops_so_far": 3.650894652618007e+16,
      "budget_used_percent": 36.50894652618007
    },
    {
      "type": "training",
      "description": "Training step 3069",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:56",
      "total_flops_so_far": 3.6520828642284264e+16,
      "budget_used_percent": 36.520828642284265
    },
    {
      "type": "training",
      "description": "Training step 3070",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:57",
      "total_flops_so_far": 3.6532710758388456e+16,
      "budget_used_percent": 36.532710758388454
    },
    {
      "type": "training",
      "description": "Training step 3071",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:57",
      "total_flops_so_far": 3.654459287449265e+16,
      "budget_used_percent": 36.54459287449265
    },
    {
      "type": "training",
      "description": "Training step 3072",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:58",
      "total_flops_so_far": 3.655647499059684e+16,
      "budget_used_percent": 36.55647499059684
    },
    {
      "type": "training",
      "description": "Training step 3073",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:59",
      "total_flops_so_far": 3.656835710670103e+16,
      "budget_used_percent": 36.56835710670103
    },
    {
      "type": "training",
      "description": "Training step 3074",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:42:59",
      "total_flops_so_far": 3.6580239222805224e+16,
      "budget_used_percent": 36.58023922280522
    },
    {
      "type": "training",
      "description": "Training step 3075",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:00",
      "total_flops_so_far": 3.6592121338909416e+16,
      "budget_used_percent": 36.592121338909415
    },
    {
      "type": "training",
      "description": "Training step 3076",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:01",
      "total_flops_so_far": 3.660400345501361e+16,
      "budget_used_percent": 36.604003455013604
    },
    {
      "type": "training",
      "description": "Training step 3077",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:01",
      "total_flops_so_far": 3.66158855711178e+16,
      "budget_used_percent": 36.6158855711178
    },
    {
      "type": "training",
      "description": "Training step 3078",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:02",
      "total_flops_so_far": 3.662776768722199e+16,
      "budget_used_percent": 36.62776768722199
    },
    {
      "type": "training",
      "description": "Training step 3079",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:03",
      "total_flops_so_far": 3.6639649803326184e+16,
      "budget_used_percent": 36.639649803326186
    },
    {
      "type": "training",
      "description": "Training step 3080",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:03",
      "total_flops_so_far": 3.6651531919430376e+16,
      "budget_used_percent": 36.651531919430376
    },
    {
      "type": "training",
      "description": "Training step 3081",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:04",
      "total_flops_so_far": 3.666341403553457e+16,
      "budget_used_percent": 36.66341403553457
    },
    {
      "type": "training",
      "description": "Training step 3082",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:05",
      "total_flops_so_far": 3.667529615163876e+16,
      "budget_used_percent": 36.67529615163876
    },
    {
      "type": "training",
      "description": "Training step 3083",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:05",
      "total_flops_so_far": 3.668717826774295e+16,
      "budget_used_percent": 36.68717826774295
    },
    {
      "type": "training",
      "description": "Training step 3084",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:06",
      "total_flops_so_far": 3.6699060383847144e+16,
      "budget_used_percent": 36.69906038384714
    },
    {
      "type": "training",
      "description": "Training step 3085",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:06",
      "total_flops_so_far": 3.6710942499951336e+16,
      "budget_used_percent": 36.71094249995134
    },
    {
      "type": "training",
      "description": "Training step 3086",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:07",
      "total_flops_so_far": 3.672282461605553e+16,
      "budget_used_percent": 36.722824616055526
    },
    {
      "type": "training",
      "description": "Training step 3087",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:08",
      "total_flops_so_far": 3.673470673215972e+16,
      "budget_used_percent": 36.73470673215972
    },
    {
      "type": "training",
      "description": "Training step 3088",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:08",
      "total_flops_so_far": 3.674658884826391e+16,
      "budget_used_percent": 36.74658884826391
    },
    {
      "type": "training",
      "description": "Training step 3089",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:09",
      "total_flops_so_far": 3.6758470964368104e+16,
      "budget_used_percent": 36.75847096436811
    },
    {
      "type": "training",
      "description": "Training step 3090",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:10",
      "total_flops_so_far": 3.6770353080472296e+16,
      "budget_used_percent": 36.7703530804723
    },
    {
      "type": "training",
      "description": "Training step 3091",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:10",
      "total_flops_so_far": 3.678223519657649e+16,
      "budget_used_percent": 36.782235196576494
    },
    {
      "type": "training",
      "description": "Training step 3092",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:11",
      "total_flops_so_far": 3.679411731268068e+16,
      "budget_used_percent": 36.79411731268068
    },
    {
      "type": "training",
      "description": "Training step 3093",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:12",
      "total_flops_so_far": 3.680599942878487e+16,
      "budget_used_percent": 36.80599942878487
    },
    {
      "type": "training",
      "description": "Training step 3094",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:13",
      "total_flops_so_far": 3.6817881544889064e+16,
      "budget_used_percent": 36.81788154488906
    },
    {
      "type": "training",
      "description": "Training step 3095",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:13",
      "total_flops_so_far": 3.6829763660993256e+16,
      "budget_used_percent": 36.82976366099325
    },
    {
      "type": "training",
      "description": "Training step 3096",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:14",
      "total_flops_so_far": 3.684164577709745e+16,
      "budget_used_percent": 36.84164577709745
    },
    {
      "type": "training",
      "description": "Training step 3097",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:15",
      "total_flops_so_far": 3.685352789320164e+16,
      "budget_used_percent": 36.85352789320164
    },
    {
      "type": "training",
      "description": "Training step 3098",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:15",
      "total_flops_so_far": 3.686541000930583e+16,
      "budget_used_percent": 36.86541000930583
    },
    {
      "type": "training",
      "description": "Training step 3099",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:16",
      "total_flops_so_far": 3.6877292125410024e+16,
      "budget_used_percent": 36.87729212541002
    },
    {
      "type": "training",
      "description": "Training step 3100",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:17",
      "total_flops_so_far": 3.6889174241514216e+16,
      "budget_used_percent": 36.88917424151422
    },
    {
      "type": "training",
      "description": "Training step 3101",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:17",
      "total_flops_so_far": 3.690105635761841e+16,
      "budget_used_percent": 36.90105635761841
    },
    {
      "type": "training",
      "description": "Training step 3102",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:18",
      "total_flops_so_far": 3.69129384737226e+16,
      "budget_used_percent": 36.9129384737226
    },
    {
      "type": "training",
      "description": "Training step 3103",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:19",
      "total_flops_so_far": 3.692482058982679e+16,
      "budget_used_percent": 36.92482058982679
    },
    {
      "type": "training",
      "description": "Training step 3104",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:19",
      "total_flops_so_far": 3.6936702705930984e+16,
      "budget_used_percent": 36.93670270593098
    },
    {
      "type": "training",
      "description": "Training step 3105",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:20",
      "total_flops_so_far": 3.6948584822035176e+16,
      "budget_used_percent": 36.94858482203517
    },
    {
      "type": "training",
      "description": "Training step 3106",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:20",
      "total_flops_so_far": 3.696046693813937e+16,
      "budget_used_percent": 36.96046693813937
    },
    {
      "type": "training",
      "description": "Training step 3107",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:21",
      "total_flops_so_far": 3.697234905424356e+16,
      "budget_used_percent": 36.97234905424356
    },
    {
      "type": "training",
      "description": "Training step 3108",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:22",
      "total_flops_so_far": 3.698423117034775e+16,
      "budget_used_percent": 36.984231170347755
    },
    {
      "type": "training",
      "description": "Training step 3109",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:22",
      "total_flops_so_far": 3.6996113286451944e+16,
      "budget_used_percent": 36.996113286451944
    },
    {
      "type": "training",
      "description": "Training step 3110",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:23",
      "total_flops_so_far": 3.7007995402556136e+16,
      "budget_used_percent": 37.00799540255614
    },
    {
      "type": "training",
      "description": "Training step 3111",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:24",
      "total_flops_so_far": 3.701987751866033e+16,
      "budget_used_percent": 37.01987751866033
    },
    {
      "type": "training",
      "description": "Training step 3112",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:24",
      "total_flops_so_far": 3.703175963476452e+16,
      "budget_used_percent": 37.03175963476452
    },
    {
      "type": "training",
      "description": "Training step 3113",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:25",
      "total_flops_so_far": 3.704364175086871e+16,
      "budget_used_percent": 37.04364175086871
    },
    {
      "type": "training",
      "description": "Training step 3114",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:26",
      "total_flops_so_far": 3.7055523866972904e+16,
      "budget_used_percent": 37.055523866972905
    },
    {
      "type": "training",
      "description": "Training step 3115",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:26",
      "total_flops_so_far": 3.7067405983077096e+16,
      "budget_used_percent": 37.067405983077094
    },
    {
      "type": "training",
      "description": "Training step 3116",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:27",
      "total_flops_so_far": 3.707928809918129e+16,
      "budget_used_percent": 37.07928809918129
    },
    {
      "type": "training",
      "description": "Training step 3117",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:28",
      "total_flops_so_far": 3.709117021528548e+16,
      "budget_used_percent": 37.09117021528548
    },
    {
      "type": "training",
      "description": "Training step 3118",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:28",
      "total_flops_so_far": 3.710305233138967e+16,
      "budget_used_percent": 37.10305233138968
    },
    {
      "type": "training",
      "description": "Training step 3119",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:29",
      "total_flops_so_far": 3.7114934447493864e+16,
      "budget_used_percent": 37.114934447493866
    },
    {
      "type": "training",
      "description": "Training step 3120",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:30",
      "total_flops_so_far": 3.7126816563598056e+16,
      "budget_used_percent": 37.126816563598055
    },
    {
      "type": "training",
      "description": "Training step 3121",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:30",
      "total_flops_so_far": 3.713869867970225e+16,
      "budget_used_percent": 37.138698679702244
    },
    {
      "type": "training",
      "description": "Training step 3122",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:31",
      "total_flops_so_far": 3.715058079580644e+16,
      "budget_used_percent": 37.15058079580644
    },
    {
      "type": "training",
      "description": "Training step 3123",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:32",
      "total_flops_so_far": 3.716246291191063e+16,
      "budget_used_percent": 37.16246291191063
    },
    {
      "type": "training",
      "description": "Training step 3124",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:32",
      "total_flops_so_far": 3.7174345028014824e+16,
      "budget_used_percent": 37.17434502801482
    },
    {
      "type": "training",
      "description": "Training step 3125",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:33",
      "total_flops_so_far": 3.7186227144119016e+16,
      "budget_used_percent": 37.186227144119016
    },
    {
      "type": "training",
      "description": "Training step 3126",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:33",
      "total_flops_so_far": 3.719810926022321e+16,
      "budget_used_percent": 37.198109260223205
    },
    {
      "type": "training",
      "description": "Training step 3127",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:34",
      "total_flops_so_far": 3.72099913763274e+16,
      "budget_used_percent": 37.2099913763274
    },
    {
      "type": "training",
      "description": "Training step 3128",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:35",
      "total_flops_so_far": 3.722187349243159e+16,
      "budget_used_percent": 37.22187349243159
    },
    {
      "type": "training",
      "description": "Training step 3129",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:35",
      "total_flops_so_far": 3.7233755608535784e+16,
      "budget_used_percent": 37.23375560853579
    },
    {
      "type": "training",
      "description": "Training step 3130",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:36",
      "total_flops_so_far": 3.7245637724639976e+16,
      "budget_used_percent": 37.24563772463998
    },
    {
      "type": "training",
      "description": "Training step 3131",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:37",
      "total_flops_so_far": 3.725751984074417e+16,
      "budget_used_percent": 37.257519840744166
    },
    {
      "type": "training",
      "description": "Training step 3132",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:37",
      "total_flops_so_far": 3.726940195684836e+16,
      "budget_used_percent": 37.269401956848355
    },
    {
      "type": "training",
      "description": "Training step 3133",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:38",
      "total_flops_so_far": 3.728128407295255e+16,
      "budget_used_percent": 37.28128407295255
    },
    {
      "type": "training",
      "description": "Training step 3134",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:39",
      "total_flops_so_far": 3.7293166189056744e+16,
      "budget_used_percent": 37.29316618905674
    },
    {
      "type": "training",
      "description": "Training step 3135",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:39",
      "total_flops_so_far": 3.7305048305160936e+16,
      "budget_used_percent": 37.30504830516094
    },
    {
      "type": "training",
      "description": "Training step 3136",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:40",
      "total_flops_so_far": 3.731693042126513e+16,
      "budget_used_percent": 37.31693042126513
    },
    {
      "type": "training",
      "description": "Training step 3137",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:41",
      "total_flops_so_far": 3.732881253736932e+16,
      "budget_used_percent": 37.32881253736932
    },
    {
      "type": "training",
      "description": "Training step 3138",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:41",
      "total_flops_so_far": 3.734069465347351e+16,
      "budget_used_percent": 37.34069465347351
    },
    {
      "type": "training",
      "description": "Training step 3139",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:42",
      "total_flops_so_far": 3.7352576769577704e+16,
      "budget_used_percent": 37.35257676957771
    },
    {
      "type": "training",
      "description": "Training step 3140",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:43",
      "total_flops_so_far": 3.7364458885681896e+16,
      "budget_used_percent": 37.3644588856819
    },
    {
      "type": "training",
      "description": "Training step 3141",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:43",
      "total_flops_so_far": 3.737634100178609e+16,
      "budget_used_percent": 37.37634100178609
    },
    {
      "type": "training",
      "description": "Training step 3142",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:44",
      "total_flops_so_far": 3.738822311789028e+16,
      "budget_used_percent": 37.38822311789028
    },
    {
      "type": "training",
      "description": "Training step 3143",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:45",
      "total_flops_so_far": 3.740010523399447e+16,
      "budget_used_percent": 37.40010523399447
    },
    {
      "type": "training",
      "description": "Training step 3144",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:45",
      "total_flops_so_far": 3.7411987350098664e+16,
      "budget_used_percent": 37.41198735009866
    },
    {
      "type": "training",
      "description": "Training step 3145",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:46",
      "total_flops_so_far": 3.7423869466202856e+16,
      "budget_used_percent": 37.42386946620286
    },
    {
      "type": "training",
      "description": "Training step 3146",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:46",
      "total_flops_so_far": 3.743575158230705e+16,
      "budget_used_percent": 37.43575158230705
    },
    {
      "type": "training",
      "description": "Training step 3147",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:47",
      "total_flops_so_far": 3.744763369841124e+16,
      "budget_used_percent": 37.447633698411245
    },
    {
      "type": "training",
      "description": "Training step 3148",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:48",
      "total_flops_so_far": 3.745951581451543e+16,
      "budget_used_percent": 37.459515814515434
    },
    {
      "type": "training",
      "description": "Training step 3149",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:48",
      "total_flops_so_far": 3.7471397930619624e+16,
      "budget_used_percent": 37.471397930619624
    },
    {
      "type": "training",
      "description": "Training step 3150",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:49",
      "total_flops_so_far": 3.7483280046723816e+16,
      "budget_used_percent": 37.48328004672381
    },
    {
      "type": "training",
      "description": "Training step 3151",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:50",
      "total_flops_so_far": 3.749516216282801e+16,
      "budget_used_percent": 37.49516216282801
    },
    {
      "type": "training",
      "description": "Training step 3152",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:50",
      "total_flops_so_far": 3.75070442789322e+16,
      "budget_used_percent": 37.5070442789322
    },
    {
      "type": "training",
      "description": "Training step 3153",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:51",
      "total_flops_so_far": 3.751892639503639e+16,
      "budget_used_percent": 37.51892639503639
    },
    {
      "type": "training",
      "description": "Training step 3154",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:52",
      "total_flops_so_far": 3.7530808511140584e+16,
      "budget_used_percent": 37.530808511140584
    },
    {
      "type": "training",
      "description": "Training step 3155",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:53",
      "total_flops_so_far": 3.7542690627244776e+16,
      "budget_used_percent": 37.542690627244774
    },
    {
      "type": "training",
      "description": "Training step 3156",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:53",
      "total_flops_so_far": 3.755457274334897e+16,
      "budget_used_percent": 37.55457274334897
    },
    {
      "type": "training",
      "description": "Training step 3157",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:54",
      "total_flops_so_far": 3.756645485945316e+16,
      "budget_used_percent": 37.56645485945316
    },
    {
      "type": "training",
      "description": "Training step 3158",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:54",
      "total_flops_so_far": 3.757833697555735e+16,
      "budget_used_percent": 37.578336975557356
    },
    {
      "type": "training",
      "description": "Training step 3159",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:55",
      "total_flops_so_far": 3.7590219091661544e+16,
      "budget_used_percent": 37.590219091661545
    },
    {
      "type": "training",
      "description": "Training step 3160",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:56",
      "total_flops_so_far": 3.7602101207765736e+16,
      "budget_used_percent": 37.602101207765735
    },
    {
      "type": "training",
      "description": "Training step 3161",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:56",
      "total_flops_so_far": 3.761398332386993e+16,
      "budget_used_percent": 37.613983323869924
    },
    {
      "type": "training",
      "description": "Training step 3162",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:57",
      "total_flops_so_far": 3.762586543997412e+16,
      "budget_used_percent": 37.62586543997412
    },
    {
      "type": "training",
      "description": "Training step 3163",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:58",
      "total_flops_so_far": 3.763774755607831e+16,
      "budget_used_percent": 37.63774755607831
    },
    {
      "type": "training",
      "description": "Training step 3164",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:58",
      "total_flops_so_far": 3.7649629672182504e+16,
      "budget_used_percent": 37.649629672182506
    },
    {
      "type": "training",
      "description": "Training step 3165",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:43:59",
      "total_flops_so_far": 3.7661511788286696e+16,
      "budget_used_percent": 37.661511788286695
    },
    {
      "type": "training",
      "description": "Training step 3166",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:00",
      "total_flops_so_far": 3.767339390439089e+16,
      "budget_used_percent": 37.67339390439089
    },
    {
      "type": "training",
      "description": "Training step 3167",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:00",
      "total_flops_so_far": 3.768527602049508e+16,
      "budget_used_percent": 37.68527602049508
    },
    {
      "type": "training",
      "description": "Training step 3168",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:01",
      "total_flops_so_far": 3.769715813659927e+16,
      "budget_used_percent": 37.69715813659927
    },
    {
      "type": "training",
      "description": "Training step 3169",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:02",
      "total_flops_so_far": 3.7709040252703464e+16,
      "budget_used_percent": 37.70904025270346
    },
    {
      "type": "training",
      "description": "Training step 3170",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:02",
      "total_flops_so_far": 3.7720922368807656e+16,
      "budget_used_percent": 37.720922368807656
    },
    {
      "type": "training",
      "description": "Training step 3171",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:03",
      "total_flops_so_far": 3.773280448491185e+16,
      "budget_used_percent": 37.732804484911846
    },
    {
      "type": "training",
      "description": "Training step 3172",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:03",
      "total_flops_so_far": 3.774468660101604e+16,
      "budget_used_percent": 37.74468660101604
    },
    {
      "type": "training",
      "description": "Training step 3173",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:04",
      "total_flops_so_far": 3.775656871712023e+16,
      "budget_used_percent": 37.75656871712023
    },
    {
      "type": "training",
      "description": "Training step 3174",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:05",
      "total_flops_so_far": 3.7768450833224424e+16,
      "budget_used_percent": 37.76845083322443
    },
    {
      "type": "training",
      "description": "Training step 3175",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:05",
      "total_flops_so_far": 3.7780332949328616e+16,
      "budget_used_percent": 37.78033294932862
    },
    {
      "type": "training",
      "description": "Training step 3176",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:06",
      "total_flops_so_far": 3.779221506543281e+16,
      "budget_used_percent": 37.79221506543281
    },
    {
      "type": "training",
      "description": "Training step 3177",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:07",
      "total_flops_so_far": 3.7804097181537e+16,
      "budget_used_percent": 37.804097181537
    },
    {
      "type": "training",
      "description": "Training step 3178",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:07",
      "total_flops_so_far": 3.781597929764119e+16,
      "budget_used_percent": 37.81597929764119
    },
    {
      "type": "training",
      "description": "Training step 3179",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:08",
      "total_flops_so_far": 3.7827861413745384e+16,
      "budget_used_percent": 37.82786141374538
    },
    {
      "type": "training",
      "description": "Training step 3180",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:09",
      "total_flops_so_far": 3.7839743529849576e+16,
      "budget_used_percent": 37.83974352984957
    },
    {
      "type": "training",
      "description": "Training step 3181",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:09",
      "total_flops_so_far": 3.785162564595377e+16,
      "budget_used_percent": 37.85162564595377
    },
    {
      "type": "training",
      "description": "Training step 3182",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:10",
      "total_flops_so_far": 3.786350776205796e+16,
      "budget_used_percent": 37.86350776205796
    },
    {
      "type": "training",
      "description": "Training step 3183",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:11",
      "total_flops_so_far": 3.787538987816215e+16,
      "budget_used_percent": 37.87538987816215
    },
    {
      "type": "training",
      "description": "Training step 3184",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:11",
      "total_flops_so_far": 3.7887271994266344e+16,
      "budget_used_percent": 37.88727199426634
    },
    {
      "type": "training",
      "description": "Training step 3185",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:12",
      "total_flops_so_far": 3.7899154110370536e+16,
      "budget_used_percent": 37.89915411037054
    },
    {
      "type": "training",
      "description": "Training step 3186",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:13",
      "total_flops_so_far": 3.791103622647473e+16,
      "budget_used_percent": 37.91103622647473
    },
    {
      "type": "training",
      "description": "Training step 3187",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:13",
      "total_flops_so_far": 3.792291834257892e+16,
      "budget_used_percent": 37.92291834257892
    },
    {
      "type": "training",
      "description": "Training step 3188",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:14",
      "total_flops_so_far": 3.793480045868311e+16,
      "budget_used_percent": 37.93480045868311
    },
    {
      "type": "training",
      "description": "Training step 3189",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:15",
      "total_flops_so_far": 3.7946682574787304e+16,
      "budget_used_percent": 37.9466825747873
    },
    {
      "type": "training",
      "description": "Training step 3190",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:15",
      "total_flops_so_far": 3.7958564690891496e+16,
      "budget_used_percent": 37.95856469089149
    },
    {
      "type": "training",
      "description": "Training step 3191",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:16",
      "total_flops_so_far": 3.797044680699569e+16,
      "budget_used_percent": 37.97044680699569
    },
    {
      "type": "training",
      "description": "Training step 3192",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:17",
      "total_flops_so_far": 3.798232892309988e+16,
      "budget_used_percent": 37.98232892309988
    },
    {
      "type": "training",
      "description": "Training step 3193",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:17",
      "total_flops_so_far": 3.799421103920407e+16,
      "budget_used_percent": 37.994211039204075
    },
    {
      "type": "training",
      "description": "Training step 3194",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:18",
      "total_flops_so_far": 3.8006093155308264e+16,
      "budget_used_percent": 38.006093155308264
    },
    {
      "type": "training",
      "description": "Training step 3195",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:18",
      "total_flops_so_far": 3.8017975271412456e+16,
      "budget_used_percent": 38.01797527141246
    },
    {
      "type": "training",
      "description": "Training step 3196",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:19",
      "total_flops_so_far": 3.802985738751665e+16,
      "budget_used_percent": 38.02985738751665
    },
    {
      "type": "training",
      "description": "Training step 3197",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:20",
      "total_flops_so_far": 3.804173950362084e+16,
      "budget_used_percent": 38.04173950362084
    },
    {
      "type": "training",
      "description": "Training step 3198",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:20",
      "total_flops_so_far": 3.805362161972503e+16,
      "budget_used_percent": 38.05362161972503
    },
    {
      "type": "training",
      "description": "Training step 3199",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:21",
      "total_flops_so_far": 3.8065503735829224e+16,
      "budget_used_percent": 38.065503735829225
    },
    {
      "type": "training",
      "description": "Training step 3200",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:22",
      "total_flops_so_far": 3.8077385851933416e+16,
      "budget_used_percent": 38.077385851933414
    },
    {
      "type": "training",
      "description": "Training step 3201",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:22",
      "total_flops_so_far": 3.808926796803761e+16,
      "budget_used_percent": 38.08926796803761
    },
    {
      "type": "training",
      "description": "Training step 3202",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:23",
      "total_flops_so_far": 3.81011500841418e+16,
      "budget_used_percent": 38.1011500841418
    },
    {
      "type": "training",
      "description": "Training step 3203",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:24",
      "total_flops_so_far": 3.811303220024599e+16,
      "budget_used_percent": 38.113032200245996
    },
    {
      "type": "training",
      "description": "Training step 3204",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:24",
      "total_flops_so_far": 3.8124914316350184e+16,
      "budget_used_percent": 38.124914316350186
    },
    {
      "type": "training",
      "description": "Training step 3205",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:25",
      "total_flops_so_far": 3.8136796432454376e+16,
      "budget_used_percent": 38.136796432454375
    },
    {
      "type": "training",
      "description": "Training step 3206",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:26",
      "total_flops_so_far": 3.814867854855857e+16,
      "budget_used_percent": 38.148678548558564
    },
    {
      "type": "training",
      "description": "Training step 3207",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:26",
      "total_flops_so_far": 3.816056066466276e+16,
      "budget_used_percent": 38.16056066466276
    },
    {
      "type": "training",
      "description": "Training step 3208",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:27",
      "total_flops_so_far": 3.817244278076695e+16,
      "budget_used_percent": 38.17244278076695
    },
    {
      "type": "training",
      "description": "Training step 3209",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:28",
      "total_flops_so_far": 3.8184324896871144e+16,
      "budget_used_percent": 38.18432489687114
    },
    {
      "type": "training",
      "description": "Training step 3210",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:28",
      "total_flops_so_far": 3.8196207012975336e+16,
      "budget_used_percent": 38.196207012975336
    },
    {
      "type": "training",
      "description": "Training step 3211",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:29",
      "total_flops_so_far": 3.820808912907953e+16,
      "budget_used_percent": 38.208089129079525
    },
    {
      "type": "training",
      "description": "Training step 3212",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:30",
      "total_flops_so_far": 3.821997124518372e+16,
      "budget_used_percent": 38.21997124518372
    },
    {
      "type": "training",
      "description": "Training step 3213",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:30",
      "total_flops_so_far": 3.823185336128791e+16,
      "budget_used_percent": 38.23185336128791
    },
    {
      "type": "training",
      "description": "Training step 3214",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:31",
      "total_flops_so_far": 3.8243735477392104e+16,
      "budget_used_percent": 38.24373547739211
    },
    {
      "type": "training",
      "description": "Training step 3215",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:32",
      "total_flops_so_far": 3.8255617593496296e+16,
      "budget_used_percent": 38.2556175934963
    },
    {
      "type": "training",
      "description": "Training step 3216",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:32",
      "total_flops_so_far": 3.826749970960049e+16,
      "budget_used_percent": 38.267499709600486
    },
    {
      "type": "training",
      "description": "Training step 3217",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:33",
      "total_flops_so_far": 3.827938182570468e+16,
      "budget_used_percent": 38.279381825704675
    },
    {
      "type": "training",
      "description": "Training step 3218",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:33",
      "total_flops_so_far": 3.829126394180887e+16,
      "budget_used_percent": 38.29126394180887
    },
    {
      "type": "training",
      "description": "Training step 3219",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:34",
      "total_flops_so_far": 3.8303146057913064e+16,
      "budget_used_percent": 38.30314605791306
    },
    {
      "type": "training",
      "description": "Training step 3220",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:35",
      "total_flops_so_far": 3.8315028174017256e+16,
      "budget_used_percent": 38.31502817401726
    },
    {
      "type": "training",
      "description": "Training step 3221",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:35",
      "total_flops_so_far": 3.832691029012145e+16,
      "budget_used_percent": 38.32691029012145
    },
    {
      "type": "training",
      "description": "Training step 3222",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:36",
      "total_flops_so_far": 3.833879240622564e+16,
      "budget_used_percent": 38.33879240622564
    },
    {
      "type": "training",
      "description": "Training step 3223",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:37",
      "total_flops_so_far": 3.835067452232983e+16,
      "budget_used_percent": 38.35067452232983
    },
    {
      "type": "training",
      "description": "Training step 3224",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:37",
      "total_flops_so_far": 3.8362556638434024e+16,
      "budget_used_percent": 38.36255663843403
    },
    {
      "type": "training",
      "description": "Training step 3225",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:38",
      "total_flops_so_far": 3.8374438754538216e+16,
      "budget_used_percent": 38.37443875453822
    },
    {
      "type": "training",
      "description": "Training step 3226",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:39",
      "total_flops_so_far": 3.838632087064241e+16,
      "budget_used_percent": 38.38632087064241
    },
    {
      "type": "training",
      "description": "Training step 3227",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:39",
      "total_flops_so_far": 3.83982029867466e+16,
      "budget_used_percent": 38.3982029867466
    },
    {
      "type": "training",
      "description": "Training step 3228",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:40",
      "total_flops_so_far": 3.841008510285079e+16,
      "budget_used_percent": 38.41008510285079
    },
    {
      "type": "training",
      "description": "Training step 3229",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:41",
      "total_flops_so_far": 3.8421967218954984e+16,
      "budget_used_percent": 38.42196721895498
    },
    {
      "type": "training",
      "description": "Training step 3230",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:41",
      "total_flops_so_far": 3.8433849335059176e+16,
      "budget_used_percent": 38.43384933505918
    },
    {
      "type": "training",
      "description": "Training step 3231",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:42",
      "total_flops_so_far": 3.844573145116337e+16,
      "budget_used_percent": 38.44573145116337
    },
    {
      "type": "training",
      "description": "Training step 3232",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:43",
      "total_flops_so_far": 3.845761356726756e+16,
      "budget_used_percent": 38.457613567267565
    },
    {
      "type": "training",
      "description": "Training step 3233",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:43",
      "total_flops_so_far": 3.846949568337175e+16,
      "budget_used_percent": 38.469495683371754
    },
    {
      "type": "training",
      "description": "Training step 3234",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:44",
      "total_flops_so_far": 3.8481377799475944e+16,
      "budget_used_percent": 38.48137779947594
    },
    {
      "type": "training",
      "description": "Training step 3235",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:45",
      "total_flops_so_far": 3.8493259915580136e+16,
      "budget_used_percent": 38.49325991558013
    },
    {
      "type": "training",
      "description": "Training step 3236",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:45",
      "total_flops_so_far": 3.850514203168433e+16,
      "budget_used_percent": 38.50514203168433
    },
    {
      "type": "training",
      "description": "Training step 3237",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:46",
      "total_flops_so_far": 3.851702414778852e+16,
      "budget_used_percent": 38.51702414778852
    },
    {
      "type": "training",
      "description": "Training step 3238",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:47",
      "total_flops_so_far": 3.852890626389271e+16,
      "budget_used_percent": 38.52890626389271
    },
    {
      "type": "training",
      "description": "Training step 3239",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:47",
      "total_flops_so_far": 3.8540788379996904e+16,
      "budget_used_percent": 38.540788379996904
    },
    {
      "type": "training",
      "description": "Training step 3240",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:48",
      "total_flops_so_far": 3.8552670496101096e+16,
      "budget_used_percent": 38.55267049610109
    },
    {
      "type": "training",
      "description": "Training step 3241",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:49",
      "total_flops_so_far": 3.856455261220529e+16,
      "budget_used_percent": 38.56455261220529
    },
    {
      "type": "training",
      "description": "Training step 3242",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:50",
      "total_flops_so_far": 3.857643472830948e+16,
      "budget_used_percent": 38.57643472830948
    },
    {
      "type": "training",
      "description": "Training step 3243",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:50",
      "total_flops_so_far": 3.858831684441367e+16,
      "budget_used_percent": 38.588316844413676
    },
    {
      "type": "training",
      "description": "Training step 3244",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:51",
      "total_flops_so_far": 3.8600198960517864e+16,
      "budget_used_percent": 38.600198960517865
    },
    {
      "type": "training",
      "description": "Training step 3245",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:51",
      "total_flops_so_far": 3.8612081076622056e+16,
      "budget_used_percent": 38.612081076622054
    },
    {
      "type": "training",
      "description": "Training step 3246",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:52",
      "total_flops_so_far": 3.862396319272625e+16,
      "budget_used_percent": 38.623963192726244
    },
    {
      "type": "training",
      "description": "Training step 3247",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:53",
      "total_flops_so_far": 3.863584530883044e+16,
      "budget_used_percent": 38.63584530883044
    },
    {
      "type": "training",
      "description": "Training step 3248",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:53",
      "total_flops_so_far": 3.864772742493463e+16,
      "budget_used_percent": 38.64772742493463
    },
    {
      "type": "training",
      "description": "Training step 3249",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:54",
      "total_flops_so_far": 3.8659609541038824e+16,
      "budget_used_percent": 38.659609541038826
    },
    {
      "type": "training",
      "description": "Training step 3250",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:55",
      "total_flops_so_far": 3.8671491657143016e+16,
      "budget_used_percent": 38.671491657143015
    },
    {
      "type": "training",
      "description": "Training step 3251",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:55",
      "total_flops_so_far": 3.868337377324721e+16,
      "budget_used_percent": 38.68337377324721
    },
    {
      "type": "training",
      "description": "Training step 3252",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:56",
      "total_flops_so_far": 3.86952558893514e+16,
      "budget_used_percent": 38.6952558893514
    },
    {
      "type": "training",
      "description": "Training step 3253",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:57",
      "total_flops_so_far": 3.870713800545559e+16,
      "budget_used_percent": 38.70713800545559
    },
    {
      "type": "training",
      "description": "Training step 3254",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:57",
      "total_flops_so_far": 3.8719020121559784e+16,
      "budget_used_percent": 38.71902012155978
    },
    {
      "type": "training",
      "description": "Training step 3255",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:58",
      "total_flops_so_far": 3.8730902237663976e+16,
      "budget_used_percent": 38.730902237663976
    },
    {
      "type": "training",
      "description": "Training step 3256",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:59",
      "total_flops_so_far": 3.874278435376817e+16,
      "budget_used_percent": 38.742784353768165
    },
    {
      "type": "training",
      "description": "Training step 3257",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:44:59",
      "total_flops_so_far": 3.875466646987236e+16,
      "budget_used_percent": 38.75466646987236
    },
    {
      "type": "training",
      "description": "Training step 3258",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:00",
      "total_flops_so_far": 3.876654858597655e+16,
      "budget_used_percent": 38.76654858597655
    },
    {
      "type": "training",
      "description": "Training step 3259",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:01",
      "total_flops_so_far": 3.8778430702080744e+16,
      "budget_used_percent": 38.77843070208075
    },
    {
      "type": "training",
      "description": "Training step 3260",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:01",
      "total_flops_so_far": 3.8790312818184936e+16,
      "budget_used_percent": 38.79031281818494
    },
    {
      "type": "training",
      "description": "Training step 3261",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:02",
      "total_flops_so_far": 3.880219493428913e+16,
      "budget_used_percent": 38.80219493428913
    },
    {
      "type": "training",
      "description": "Training step 3262",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:03",
      "total_flops_so_far": 3.881407705039332e+16,
      "budget_used_percent": 38.81407705039332
    },
    {
      "type": "training",
      "description": "Training step 3263",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:03",
      "total_flops_so_far": 3.882595916649751e+16,
      "budget_used_percent": 38.82595916649751
    },
    {
      "type": "training",
      "description": "Training step 3264",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:04",
      "total_flops_so_far": 3.8837841282601704e+16,
      "budget_used_percent": 38.8378412826017
    },
    {
      "type": "training",
      "description": "Training step 3265",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:05",
      "total_flops_so_far": 3.8849723398705896e+16,
      "budget_used_percent": 38.8497233987059
    },
    {
      "type": "training",
      "description": "Training step 3266",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:05",
      "total_flops_so_far": 3.886160551481009e+16,
      "budget_used_percent": 38.86160551481009
    },
    {
      "type": "training",
      "description": "Training step 3267",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:06",
      "total_flops_so_far": 3.887348763091428e+16,
      "budget_used_percent": 38.873487630914276
    },
    {
      "type": "training",
      "description": "Training step 3268",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:07",
      "total_flops_so_far": 3.888536974701847e+16,
      "budget_used_percent": 38.88536974701847
    },
    {
      "type": "training",
      "description": "Training step 3269",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:07",
      "total_flops_so_far": 3.8897251863122664e+16,
      "budget_used_percent": 38.89725186312266
    },
    {
      "type": "training",
      "description": "Training step 3270",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:08",
      "total_flops_so_far": 3.8909133979226856e+16,
      "budget_used_percent": 38.90913397922686
    },
    {
      "type": "training",
      "description": "Training step 3271",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:08",
      "total_flops_so_far": 3.892101609533105e+16,
      "budget_used_percent": 38.92101609533105
    },
    {
      "type": "training",
      "description": "Training step 3272",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:09",
      "total_flops_so_far": 3.893289821143524e+16,
      "budget_used_percent": 38.932898211435244
    },
    {
      "type": "training",
      "description": "Training step 3273",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:10",
      "total_flops_so_far": 3.894478032753943e+16,
      "budget_used_percent": 38.94478032753943
    },
    {
      "type": "training",
      "description": "Training step 3274",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:10",
      "total_flops_so_far": 3.8956662443643624e+16,
      "budget_used_percent": 38.95666244364362
    },
    {
      "type": "training",
      "description": "Training step 3275",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:11",
      "total_flops_so_far": 3.8968544559747816e+16,
      "budget_used_percent": 38.96854455974781
    },
    {
      "type": "training",
      "description": "Training step 3276",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:12",
      "total_flops_so_far": 3.898042667585201e+16,
      "budget_used_percent": 38.98042667585201
    },
    {
      "type": "training",
      "description": "Training step 3277",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:12",
      "total_flops_so_far": 3.89923087919562e+16,
      "budget_used_percent": 38.9923087919562
    },
    {
      "type": "training",
      "description": "Training step 3278",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:13",
      "total_flops_so_far": 3.900419090806039e+16,
      "budget_used_percent": 39.004190908060394
    },
    {
      "type": "training",
      "description": "Training step 3279",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:14",
      "total_flops_so_far": 3.9016073024164584e+16,
      "budget_used_percent": 39.016073024164584
    },
    {
      "type": "training",
      "description": "Training step 3280",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:14",
      "total_flops_so_far": 3.9027955140268776e+16,
      "budget_used_percent": 39.02795514026878
    },
    {
      "type": "training",
      "description": "Training step 3281",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:15",
      "total_flops_so_far": 3.903983725637297e+16,
      "budget_used_percent": 39.03983725637297
    },
    {
      "type": "training",
      "description": "Training step 3282",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:16",
      "total_flops_so_far": 3.905171937247716e+16,
      "budget_used_percent": 39.05171937247716
    },
    {
      "type": "training",
      "description": "Training step 3283",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:16",
      "total_flops_so_far": 3.906360148858135e+16,
      "budget_used_percent": 39.06360148858135
    },
    {
      "type": "training",
      "description": "Training step 3284",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:17",
      "total_flops_so_far": 3.9075483604685544e+16,
      "budget_used_percent": 39.075483604685544
    },
    {
      "type": "training",
      "description": "Training step 3285",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:18",
      "total_flops_so_far": 3.9087365720789736e+16,
      "budget_used_percent": 39.087365720789734
    },
    {
      "type": "training",
      "description": "Training step 3286",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:18",
      "total_flops_so_far": 3.909924783689393e+16,
      "budget_used_percent": 39.09924783689393
    },
    {
      "type": "training",
      "description": "Training step 3287",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:19",
      "total_flops_so_far": 3.911112995299812e+16,
      "budget_used_percent": 39.11112995299812
    },
    {
      "type": "training",
      "description": "Training step 3288",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:20",
      "total_flops_so_far": 3.912301206910231e+16,
      "budget_used_percent": 39.123012069102316
    },
    {
      "type": "training",
      "description": "Training step 3289",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:20",
      "total_flops_so_far": 3.9134894185206504e+16,
      "budget_used_percent": 39.134894185206505
    },
    {
      "type": "training",
      "description": "Training step 3290",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:21",
      "total_flops_so_far": 3.9146776301310696e+16,
      "budget_used_percent": 39.1467763013107
    },
    {
      "type": "training",
      "description": "Training step 3291",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:22",
      "total_flops_so_far": 3.915865841741489e+16,
      "budget_used_percent": 39.15865841741489
    },
    {
      "type": "training",
      "description": "Training step 3292",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:22",
      "total_flops_so_far": 3.917054053351908e+16,
      "budget_used_percent": 39.17054053351908
    },
    {
      "type": "training",
      "description": "Training step 3293",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:23",
      "total_flops_so_far": 3.918242264962327e+16,
      "budget_used_percent": 39.18242264962327
    },
    {
      "type": "training",
      "description": "Training step 3294",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:24",
      "total_flops_so_far": 3.9194304765727464e+16,
      "budget_used_percent": 39.19430476572746
    },
    {
      "type": "training",
      "description": "Training step 3295",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:24",
      "total_flops_so_far": 3.9206186881831656e+16,
      "budget_used_percent": 39.206186881831655
    },
    {
      "type": "training",
      "description": "Training step 3296",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:25",
      "total_flops_so_far": 3.921806899793585e+16,
      "budget_used_percent": 39.218068997935845
    },
    {
      "type": "training",
      "description": "Training step 3297",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:26",
      "total_flops_so_far": 3.922995111404004e+16,
      "budget_used_percent": 39.22995111404004
    },
    {
      "type": "training",
      "description": "Training step 3298",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:26",
      "total_flops_so_far": 3.924183323014423e+16,
      "budget_used_percent": 39.24183323014423
    },
    {
      "type": "training",
      "description": "Training step 3299",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:27",
      "total_flops_so_far": 3.9253715346248424e+16,
      "budget_used_percent": 39.25371534624843
    },
    {
      "type": "training",
      "description": "Training step 3300",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:27",
      "total_flops_so_far": 3.9265597462352616e+16,
      "budget_used_percent": 39.265597462352616
    },
    {
      "type": "training",
      "description": "Training step 3301",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:28",
      "total_flops_so_far": 3.927747957845681e+16,
      "budget_used_percent": 39.277479578456806
    },
    {
      "type": "training",
      "description": "Training step 3302",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:29",
      "total_flops_so_far": 3.9289361694561e+16,
      "budget_used_percent": 39.289361694560995
    },
    {
      "type": "training",
      "description": "Training step 3303",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:29",
      "total_flops_so_far": 3.930124381066519e+16,
      "budget_used_percent": 39.30124381066519
    },
    {
      "type": "training",
      "description": "Training step 3304",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:30",
      "total_flops_so_far": 3.9313125926769384e+16,
      "budget_used_percent": 39.31312592676938
    },
    {
      "type": "training",
      "description": "Training step 3305",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:31",
      "total_flops_so_far": 3.9325008042873576e+16,
      "budget_used_percent": 39.32500804287358
    },
    {
      "type": "training",
      "description": "Training step 3306",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:31",
      "total_flops_so_far": 3.933689015897777e+16,
      "budget_used_percent": 39.336890158977766
    },
    {
      "type": "training",
      "description": "Training step 3307",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:32",
      "total_flops_so_far": 3.934877227508196e+16,
      "budget_used_percent": 39.34877227508196
    },
    {
      "type": "training",
      "description": "Training step 3308",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:33",
      "total_flops_so_far": 3.936065439118615e+16,
      "budget_used_percent": 39.36065439118615
    },
    {
      "type": "training",
      "description": "Training step 3309",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:33",
      "total_flops_so_far": 3.9372536507290344e+16,
      "budget_used_percent": 39.37253650729035
    },
    {
      "type": "training",
      "description": "Training step 3310",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:34",
      "total_flops_so_far": 3.9384418623394536e+16,
      "budget_used_percent": 39.38441862339454
    },
    {
      "type": "training",
      "description": "Training step 3311",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:35",
      "total_flops_so_far": 3.939630073949873e+16,
      "budget_used_percent": 39.39630073949873
    },
    {
      "type": "training",
      "description": "Training step 3312",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:35",
      "total_flops_so_far": 3.940818285560292e+16,
      "budget_used_percent": 39.40818285560292
    },
    {
      "type": "training",
      "description": "Training step 3313",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:36",
      "total_flops_so_far": 3.942006497170711e+16,
      "budget_used_percent": 39.42006497170711
    },
    {
      "type": "training",
      "description": "Training step 3314",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:37",
      "total_flops_so_far": 3.9431947087811304e+16,
      "budget_used_percent": 39.4319470878113
    },
    {
      "type": "training",
      "description": "Training step 3315",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:38",
      "total_flops_so_far": 3.9443829203915496e+16,
      "budget_used_percent": 39.4438292039155
    },
    {
      "type": "training",
      "description": "Training step 3316",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:38",
      "total_flops_so_far": 3.945571132001969e+16,
      "budget_used_percent": 39.45571132001969
    },
    {
      "type": "training",
      "description": "Training step 3317",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:39",
      "total_flops_so_far": 3.946759343612388e+16,
      "budget_used_percent": 39.467593436123884
    },
    {
      "type": "training",
      "description": "Training step 3318",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:40",
      "total_flops_so_far": 3.947947555222807e+16,
      "budget_used_percent": 39.479475552228074
    },
    {
      "type": "training",
      "description": "Training step 3319",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:40",
      "total_flops_so_far": 3.9491357668332264e+16,
      "budget_used_percent": 39.49135766833226
    },
    {
      "type": "training",
      "description": "Training step 3320",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:41",
      "total_flops_so_far": 3.9503239784436456e+16,
      "budget_used_percent": 39.50323978443645
    },
    {
      "type": "training",
      "description": "Training step 3321",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:42",
      "total_flops_so_far": 3.951512190054065e+16,
      "budget_used_percent": 39.51512190054065
    },
    {
      "type": "training",
      "description": "Training step 3322",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:42",
      "total_flops_so_far": 3.952700401664484e+16,
      "budget_used_percent": 39.52700401664484
    },
    {
      "type": "training",
      "description": "Training step 3323",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:43",
      "total_flops_so_far": 3.953888613274903e+16,
      "budget_used_percent": 39.53888613274903
    },
    {
      "type": "training",
      "description": "Training step 3324",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:44",
      "total_flops_so_far": 3.9550768248853224e+16,
      "budget_used_percent": 39.550768248853224
    },
    {
      "type": "training",
      "description": "Training step 3325",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:44",
      "total_flops_so_far": 3.9562650364957416e+16,
      "budget_used_percent": 39.56265036495741
    },
    {
      "type": "training",
      "description": "Training step 3326",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:45",
      "total_flops_so_far": 3.957453248106161e+16,
      "budget_used_percent": 39.57453248106161
    },
    {
      "type": "training",
      "description": "Training step 3327",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:46",
      "total_flops_so_far": 3.95864145971658e+16,
      "budget_used_percent": 39.5864145971658
    },
    {
      "type": "training",
      "description": "Training step 3328",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:46",
      "total_flops_so_far": 3.959829671326999e+16,
      "budget_used_percent": 39.598296713269995
    },
    {
      "type": "training",
      "description": "Training step 3329",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:47",
      "total_flops_so_far": 3.9610178829374184e+16,
      "budget_used_percent": 39.610178829374185
    },
    {
      "type": "training",
      "description": "Training step 3330",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:47",
      "total_flops_so_far": 3.9622060945478376e+16,
      "budget_used_percent": 39.622060945478374
    },
    {
      "type": "training",
      "description": "Training step 3331",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:48",
      "total_flops_so_far": 3.963394306158257e+16,
      "budget_used_percent": 39.63394306158256
    },
    {
      "type": "training",
      "description": "Training step 3332",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:49",
      "total_flops_so_far": 3.964582517768676e+16,
      "budget_used_percent": 39.64582517768676
    },
    {
      "type": "training",
      "description": "Training step 3333",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:49",
      "total_flops_so_far": 3.965770729379095e+16,
      "budget_used_percent": 39.65770729379095
    },
    {
      "type": "training",
      "description": "Training step 3334",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:50",
      "total_flops_so_far": 3.9669589409895144e+16,
      "budget_used_percent": 39.669589409895146
    },
    {
      "type": "training",
      "description": "Training step 3335",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:51",
      "total_flops_so_far": 3.9681471525999336e+16,
      "budget_used_percent": 39.681471525999335
    },
    {
      "type": "training",
      "description": "Training step 3336",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:51",
      "total_flops_so_far": 3.969335364210353e+16,
      "budget_used_percent": 39.69335364210353
    },
    {
      "type": "training",
      "description": "Training step 3337",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:52",
      "total_flops_so_far": 3.970523575820772e+16,
      "budget_used_percent": 39.70523575820772
    },
    {
      "type": "training",
      "description": "Training step 3338",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:53",
      "total_flops_so_far": 3.971711787431191e+16,
      "budget_used_percent": 39.71711787431191
    },
    {
      "type": "training",
      "description": "Training step 3339",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:53",
      "total_flops_so_far": 3.9728999990416104e+16,
      "budget_used_percent": 39.7289999904161
    },
    {
      "type": "training",
      "description": "Training step 3340",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:54",
      "total_flops_so_far": 3.9740882106520296e+16,
      "budget_used_percent": 39.740882106520296
    },
    {
      "type": "training",
      "description": "Training step 3341",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:55",
      "total_flops_so_far": 3.975276422262449e+16,
      "budget_used_percent": 39.752764222624485
    },
    {
      "type": "training",
      "description": "Training step 3342",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:55",
      "total_flops_so_far": 3.976464633872868e+16,
      "budget_used_percent": 39.76464633872868
    },
    {
      "type": "training",
      "description": "Training step 3343",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:56",
      "total_flops_so_far": 3.977652845483287e+16,
      "budget_used_percent": 39.77652845483287
    },
    {
      "type": "training",
      "description": "Training step 3344",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:57",
      "total_flops_so_far": 3.9788410570937064e+16,
      "budget_used_percent": 39.78841057093707
    },
    {
      "type": "training",
      "description": "Training step 3345",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:57",
      "total_flops_so_far": 3.9800292687041256e+16,
      "budget_used_percent": 39.80029268704126
    },
    {
      "type": "training",
      "description": "Training step 3346",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:58",
      "total_flops_so_far": 3.981217480314545e+16,
      "budget_used_percent": 39.81217480314545
    },
    {
      "type": "training",
      "description": "Training step 3347",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:59",
      "total_flops_so_far": 3.982405691924964e+16,
      "budget_used_percent": 39.82405691924964
    },
    {
      "type": "training",
      "description": "Training step 3348",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:45:59",
      "total_flops_so_far": 3.983593903535383e+16,
      "budget_used_percent": 39.83593903535383
    },
    {
      "type": "training",
      "description": "Training step 3349",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:00",
      "total_flops_so_far": 3.9847821151458024e+16,
      "budget_used_percent": 39.84782115145802
    },
    {
      "type": "training",
      "description": "Training step 3350",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:01",
      "total_flops_so_far": 3.9859703267562216e+16,
      "budget_used_percent": 39.85970326756222
    },
    {
      "type": "training",
      "description": "Training step 3351",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:01",
      "total_flops_so_far": 3.987158538366641e+16,
      "budget_used_percent": 39.87158538366641
    },
    {
      "type": "training",
      "description": "Training step 3352",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:02",
      "total_flops_so_far": 3.98834674997706e+16,
      "budget_used_percent": 39.883467499770596
    },
    {
      "type": "training",
      "description": "Training step 3353",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:03",
      "total_flops_so_far": 3.989534961587479e+16,
      "budget_used_percent": 39.89534961587479
    },
    {
      "type": "training",
      "description": "Training step 3354",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:03",
      "total_flops_so_far": 3.9907231731978984e+16,
      "budget_used_percent": 39.90723173197898
    },
    {
      "type": "training",
      "description": "Training step 3355",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:04",
      "total_flops_so_far": 3.9919113848083176e+16,
      "budget_used_percent": 39.91911384808318
    },
    {
      "type": "training",
      "description": "Training step 3356",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:05",
      "total_flops_so_far": 3.993099596418737e+16,
      "budget_used_percent": 39.93099596418737
    },
    {
      "type": "training",
      "description": "Training step 3357",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:05",
      "total_flops_so_far": 3.994287808029156e+16,
      "budget_used_percent": 39.942878080291564
    },
    {
      "type": "training",
      "description": "Training step 3358",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:06",
      "total_flops_so_far": 3.995476019639575e+16,
      "budget_used_percent": 39.95476019639575
    },
    {
      "type": "training",
      "description": "Training step 3359",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:07",
      "total_flops_so_far": 3.9966642312499944e+16,
      "budget_used_percent": 39.96664231249994
    },
    {
      "type": "training",
      "description": "Training step 3360",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:07",
      "total_flops_so_far": 3.9978524428604136e+16,
      "budget_used_percent": 39.97852442860413
    },
    {
      "type": "training",
      "description": "Training step 3361",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:08",
      "total_flops_so_far": 3.999040654470833e+16,
      "budget_used_percent": 39.99040654470833
    },
    {
      "type": "training",
      "description": "Training step 3362",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:08",
      "total_flops_so_far": 4.000228866081252e+16,
      "budget_used_percent": 40.00228866081252
    },
    {
      "type": "training",
      "description": "Training step 3363",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:09",
      "total_flops_so_far": 4.001417077691671e+16,
      "budget_used_percent": 40.014170776916714
    },
    {
      "type": "training",
      "description": "Training step 3364",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:10",
      "total_flops_so_far": 4.0026052893020904e+16,
      "budget_used_percent": 40.0260528930209
    },
    {
      "type": "training",
      "description": "Training step 3365",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:10",
      "total_flops_so_far": 4.0037935009125096e+16,
      "budget_used_percent": 40.0379350091251
    },
    {
      "type": "training",
      "description": "Training step 3366",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:11",
      "total_flops_so_far": 4.004981712522929e+16,
      "budget_used_percent": 40.04981712522929
    },
    {
      "type": "training",
      "description": "Training step 3367",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:12",
      "total_flops_so_far": 4.006169924133348e+16,
      "budget_used_percent": 40.06169924133348
    },
    {
      "type": "training",
      "description": "Training step 3368",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:12",
      "total_flops_so_far": 4.007358135743767e+16,
      "budget_used_percent": 40.07358135743767
    },
    {
      "type": "training",
      "description": "Training step 3369",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:13",
      "total_flops_so_far": 4.0085463473541864e+16,
      "budget_used_percent": 40.085463473541864
    },
    {
      "type": "training",
      "description": "Training step 3370",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:14",
      "total_flops_so_far": 4.0097345589646056e+16,
      "budget_used_percent": 40.097345589646054
    },
    {
      "type": "training",
      "description": "Training step 3371",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:14",
      "total_flops_so_far": 4.010922770575025e+16,
      "budget_used_percent": 40.10922770575025
    },
    {
      "type": "training",
      "description": "Training step 3372",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:15",
      "total_flops_so_far": 4.012110982185444e+16,
      "budget_used_percent": 40.12110982185444
    },
    {
      "type": "training",
      "description": "Training step 3373",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:16",
      "total_flops_so_far": 4.013299193795863e+16,
      "budget_used_percent": 40.132991937958636
    },
    {
      "type": "training",
      "description": "Training step 3374",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:17",
      "total_flops_so_far": 4.0144874054062824e+16,
      "budget_used_percent": 40.144874054062825
    },
    {
      "type": "training",
      "description": "Training step 3375",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:17",
      "total_flops_so_far": 4.0156756170167016e+16,
      "budget_used_percent": 40.15675617016702
    },
    {
      "type": "training",
      "description": "Training step 3376",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:18",
      "total_flops_so_far": 4.016863828627121e+16,
      "budget_used_percent": 40.16863828627121
    },
    {
      "type": "training",
      "description": "Training step 3377",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:19",
      "total_flops_so_far": 4.01805204023754e+16,
      "budget_used_percent": 40.1805204023754
    },
    {
      "type": "training",
      "description": "Training step 3378",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:19",
      "total_flops_so_far": 4.019240251847959e+16,
      "budget_used_percent": 40.19240251847959
    },
    {
      "type": "training",
      "description": "Training step 3379",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:20",
      "total_flops_so_far": 4.0204284634583784e+16,
      "budget_used_percent": 40.204284634583786
    },
    {
      "type": "training",
      "description": "Training step 3380",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:21",
      "total_flops_so_far": 4.0216166750687976e+16,
      "budget_used_percent": 40.216166750687975
    },
    {
      "type": "training",
      "description": "Training step 3381",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:21",
      "total_flops_so_far": 4.022804886679217e+16,
      "budget_used_percent": 40.228048866792165
    },
    {
      "type": "training",
      "description": "Training step 3382",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:22",
      "total_flops_so_far": 4.023993098289636e+16,
      "budget_used_percent": 40.23993098289636
    },
    {
      "type": "training",
      "description": "Training step 3383",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:23",
      "total_flops_so_far": 4.025181309900055e+16,
      "budget_used_percent": 40.25181309900055
    },
    {
      "type": "training",
      "description": "Training step 3384",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:23",
      "total_flops_so_far": 4.0263695215104744e+16,
      "budget_used_percent": 40.26369521510475
    },
    {
      "type": "training",
      "description": "Training step 3385",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:24",
      "total_flops_so_far": 4.0275577331208936e+16,
      "budget_used_percent": 40.275577331208936
    },
    {
      "type": "training",
      "description": "Training step 3386",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:25",
      "total_flops_so_far": 4.028745944731313e+16,
      "budget_used_percent": 40.287459447313125
    },
    {
      "type": "training",
      "description": "Training step 3387",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:25",
      "total_flops_so_far": 4.029934156341732e+16,
      "budget_used_percent": 40.299341563417315
    },
    {
      "type": "training",
      "description": "Training step 3388",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:26",
      "total_flops_so_far": 4.031122367952151e+16,
      "budget_used_percent": 40.31122367952151
    },
    {
      "type": "training",
      "description": "Training step 3389",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:27",
      "total_flops_so_far": 4.0323105795625704e+16,
      "budget_used_percent": 40.3231057956257
    },
    {
      "type": "training",
      "description": "Training step 3390",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:27",
      "total_flops_so_far": 4.0334987911729896e+16,
      "budget_used_percent": 40.3349879117299
    },
    {
      "type": "training",
      "description": "Training step 3391",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:28",
      "total_flops_so_far": 4.034687002783409e+16,
      "budget_used_percent": 40.346870027834086
    },
    {
      "type": "training",
      "description": "Training step 3392",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:29",
      "total_flops_so_far": 4.035875214393828e+16,
      "budget_used_percent": 40.35875214393828
    },
    {
      "type": "training",
      "description": "Training step 3393",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:29",
      "total_flops_so_far": 4.037063426004247e+16,
      "budget_used_percent": 40.37063426004247
    },
    {
      "type": "training",
      "description": "Training step 3394",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:30",
      "total_flops_so_far": 4.0382516376146664e+16,
      "budget_used_percent": 40.38251637614667
    },
    {
      "type": "training",
      "description": "Training step 3395",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:30",
      "total_flops_so_far": 4.0394398492250856e+16,
      "budget_used_percent": 40.39439849225086
    },
    {
      "type": "training",
      "description": "Training step 3396",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:31",
      "total_flops_so_far": 4.040628060835505e+16,
      "budget_used_percent": 40.40628060835505
    },
    {
      "type": "training",
      "description": "Training step 3397",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:32",
      "total_flops_so_far": 4.041816272445924e+16,
      "budget_used_percent": 40.418162724459236
    },
    {
      "type": "training",
      "description": "Training step 3398",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:32",
      "total_flops_so_far": 4.043004484056343e+16,
      "budget_used_percent": 40.43004484056343
    },
    {
      "type": "training",
      "description": "Training step 3399",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:33",
      "total_flops_so_far": 4.0441926956667624e+16,
      "budget_used_percent": 40.44192695666762
    },
    {
      "type": "training",
      "description": "Training step 3400",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:34",
      "total_flops_so_far": 4.0453809072771816e+16,
      "budget_used_percent": 40.45380907277182
    },
    {
      "type": "training",
      "description": "Training step 3401",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:34",
      "total_flops_so_far": 4.046569118887601e+16,
      "budget_used_percent": 40.46569118887601
    },
    {
      "type": "training",
      "description": "Training step 3402",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:35",
      "total_flops_so_far": 4.04775733049802e+16,
      "budget_used_percent": 40.477573304980204
    },
    {
      "type": "training",
      "description": "Training step 3403",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:36",
      "total_flops_so_far": 4.048945542108439e+16,
      "budget_used_percent": 40.48945542108439
    },
    {
      "type": "training",
      "description": "Training step 3404",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:36",
      "total_flops_so_far": 4.0501337537188584e+16,
      "budget_used_percent": 40.50133753718859
    },
    {
      "type": "training",
      "description": "Training step 3405",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:37",
      "total_flops_so_far": 4.0513219653292776e+16,
      "budget_used_percent": 40.51321965329278
    },
    {
      "type": "training",
      "description": "Training step 3406",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:38",
      "total_flops_so_far": 4.052510176939697e+16,
      "budget_used_percent": 40.52510176939697
    },
    {
      "type": "training",
      "description": "Training step 3407",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:38",
      "total_flops_so_far": 4.053698388550116e+16,
      "budget_used_percent": 40.53698388550116
    },
    {
      "type": "training",
      "description": "Training step 3408",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:39",
      "total_flops_so_far": 4.054886600160535e+16,
      "budget_used_percent": 40.54886600160535
    },
    {
      "type": "training",
      "description": "Training step 3409",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:40",
      "total_flops_so_far": 4.0560748117709544e+16,
      "budget_used_percent": 40.560748117709544
    },
    {
      "type": "training",
      "description": "Training step 3410",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:40",
      "total_flops_so_far": 4.0572630233813736e+16,
      "budget_used_percent": 40.57263023381373
    },
    {
      "type": "training",
      "description": "Training step 3411",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:41",
      "total_flops_so_far": 4.058451234991793e+16,
      "budget_used_percent": 40.58451234991793
    },
    {
      "type": "training",
      "description": "Training step 3412",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:42",
      "total_flops_so_far": 4.059639446602212e+16,
      "budget_used_percent": 40.59639446602212
    },
    {
      "type": "training",
      "description": "Training step 3413",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:42",
      "total_flops_so_far": 4.060827658212631e+16,
      "budget_used_percent": 40.608276582126315
    },
    {
      "type": "training",
      "description": "Training step 3414",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:43",
      "total_flops_so_far": 4.0620158698230504e+16,
      "budget_used_percent": 40.620158698230505
    },
    {
      "type": "training",
      "description": "Training step 3415",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:44",
      "total_flops_so_far": 4.0632040814334696e+16,
      "budget_used_percent": 40.632040814334694
    },
    {
      "type": "training",
      "description": "Training step 3416",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:44",
      "total_flops_so_far": 4.064392293043889e+16,
      "budget_used_percent": 40.64392293043888
    },
    {
      "type": "training",
      "description": "Training step 3417",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:45",
      "total_flops_so_far": 4.065580504654308e+16,
      "budget_used_percent": 40.65580504654308
    },
    {
      "type": "training",
      "description": "Training step 3418",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:46",
      "total_flops_so_far": 4.066768716264727e+16,
      "budget_used_percent": 40.66768716264727
    },
    {
      "type": "training",
      "description": "Training step 3419",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:46",
      "total_flops_so_far": 4.0679569278751464e+16,
      "budget_used_percent": 40.679569278751465
    },
    {
      "type": "training",
      "description": "Training step 3420",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:47",
      "total_flops_so_far": 4.0691451394855656e+16,
      "budget_used_percent": 40.691451394855655
    },
    {
      "type": "training",
      "description": "Training step 3421",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:47",
      "total_flops_so_far": 4.070333351095985e+16,
      "budget_used_percent": 40.70333351095985
    },
    {
      "type": "training",
      "description": "Training step 3422",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:48",
      "total_flops_so_far": 4.071521562706404e+16,
      "budget_used_percent": 40.71521562706404
    },
    {
      "type": "training",
      "description": "Training step 3423",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:49",
      "total_flops_so_far": 4.072709774316823e+16,
      "budget_used_percent": 40.72709774316824
    },
    {
      "type": "training",
      "description": "Training step 3424",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:49",
      "total_flops_so_far": 4.0738979859272424e+16,
      "budget_used_percent": 40.738979859272426
    },
    {
      "type": "training",
      "description": "Training step 3425",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:50",
      "total_flops_so_far": 4.0750861975376616e+16,
      "budget_used_percent": 40.750861975376615
    },
    {
      "type": "training",
      "description": "Training step 3426",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:51",
      "total_flops_so_far": 4.076274409148081e+16,
      "budget_used_percent": 40.762744091480805
    },
    {
      "type": "training",
      "description": "Training step 3427",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:51",
      "total_flops_so_far": 4.0774626207585e+16,
      "budget_used_percent": 40.774626207585
    },
    {
      "type": "training",
      "description": "Training step 3428",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:52",
      "total_flops_so_far": 4.078650832368919e+16,
      "budget_used_percent": 40.78650832368919
    },
    {
      "type": "training",
      "description": "Training step 3429",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:53",
      "total_flops_so_far": 4.0798390439793384e+16,
      "budget_used_percent": 40.79839043979339
    },
    {
      "type": "training",
      "description": "Training step 3430",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:53",
      "total_flops_so_far": 4.0810272555897576e+16,
      "budget_used_percent": 40.810272555897576
    },
    {
      "type": "training",
      "description": "Training step 3431",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:54",
      "total_flops_so_far": 4.082215467200177e+16,
      "budget_used_percent": 40.82215467200177
    },
    {
      "type": "training",
      "description": "Training step 3432",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:55",
      "total_flops_so_far": 4.083403678810596e+16,
      "budget_used_percent": 40.83403678810596
    },
    {
      "type": "training",
      "description": "Training step 3433",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:55",
      "total_flops_so_far": 4.084591890421015e+16,
      "budget_used_percent": 40.84591890421015
    },
    {
      "type": "training",
      "description": "Training step 3434",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:56",
      "total_flops_so_far": 4.0857801020314344e+16,
      "budget_used_percent": 40.85780102031434
    },
    {
      "type": "training",
      "description": "Training step 3435",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:57",
      "total_flops_so_far": 4.0869683136418536e+16,
      "budget_used_percent": 40.86968313641854
    },
    {
      "type": "training",
      "description": "Training step 3436",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:57",
      "total_flops_so_far": 4.088156525252273e+16,
      "budget_used_percent": 40.881565252522726
    },
    {
      "type": "training",
      "description": "Training step 3437",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:58",
      "total_flops_so_far": 4.089344736862692e+16,
      "budget_used_percent": 40.893447368626916
    },
    {
      "type": "training",
      "description": "Training step 3438",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:59",
      "total_flops_so_far": 4.090532948473111e+16,
      "budget_used_percent": 40.90532948473111
    },
    {
      "type": "training",
      "description": "Training step 3439",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:46:59",
      "total_flops_so_far": 4.0917211600835304e+16,
      "budget_used_percent": 40.9172116008353
    },
    {
      "type": "training",
      "description": "Training step 3440",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:00",
      "total_flops_so_far": 4.0929093716939496e+16,
      "budget_used_percent": 40.9290937169395
    },
    {
      "type": "training",
      "description": "Training step 3441",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:01",
      "total_flops_so_far": 4.094097583304369e+16,
      "budget_used_percent": 40.94097583304369
    },
    {
      "type": "training",
      "description": "Training step 3442",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:01",
      "total_flops_so_far": 4.095285794914788e+16,
      "budget_used_percent": 40.952857949147884
    },
    {
      "type": "training",
      "description": "Training step 3443",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:02",
      "total_flops_so_far": 4.096474006525207e+16,
      "budget_used_percent": 40.96474006525207
    },
    {
      "type": "training",
      "description": "Training step 3444",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:02",
      "total_flops_so_far": 4.0976622181356264e+16,
      "budget_used_percent": 40.97662218135626
    },
    {
      "type": "training",
      "description": "Training step 3445",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:03",
      "total_flops_so_far": 4.0988504297460456e+16,
      "budget_used_percent": 40.98850429746045
    },
    {
      "type": "training",
      "description": "Training step 3446",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:04",
      "total_flops_so_far": 4.100038641356465e+16,
      "budget_used_percent": 41.00038641356465
    },
    {
      "type": "training",
      "description": "Training step 3447",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:04",
      "total_flops_so_far": 4.101226852966884e+16,
      "budget_used_percent": 41.01226852966884
    },
    {
      "type": "training",
      "description": "Training step 3448",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:05",
      "total_flops_so_far": 4.102415064577303e+16,
      "budget_used_percent": 41.024150645773034
    },
    {
      "type": "training",
      "description": "Training step 3449",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:06",
      "total_flops_so_far": 4.1036032761877224e+16,
      "budget_used_percent": 41.03603276187722
    },
    {
      "type": "training",
      "description": "Training step 3450",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:06",
      "total_flops_so_far": 4.1047914877981416e+16,
      "budget_used_percent": 41.04791487798142
    },
    {
      "type": "training",
      "description": "Training step 3451",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:07",
      "total_flops_so_far": 4.105979699408561e+16,
      "budget_used_percent": 41.05979699408561
    },
    {
      "type": "training",
      "description": "Training step 3452",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:08",
      "total_flops_so_far": 4.10716791101898e+16,
      "budget_used_percent": 41.0716791101898
    },
    {
      "type": "training",
      "description": "Training step 3453",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:08",
      "total_flops_so_far": 4.108356122629399e+16,
      "budget_used_percent": 41.08356122629399
    },
    {
      "type": "training",
      "description": "Training step 3454",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:09",
      "total_flops_so_far": 4.1095443342398184e+16,
      "budget_used_percent": 41.095443342398184
    },
    {
      "type": "training",
      "description": "Training step 3455",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:10",
      "total_flops_so_far": 4.1107325458502376e+16,
      "budget_used_percent": 41.10732545850237
    },
    {
      "type": "training",
      "description": "Training step 3456",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:11",
      "total_flops_so_far": 4.111920757460657e+16,
      "budget_used_percent": 41.11920757460657
    },
    {
      "type": "training",
      "description": "Training step 3457",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:11",
      "total_flops_so_far": 4.113108969071076e+16,
      "budget_used_percent": 41.13108969071076
    },
    {
      "type": "training",
      "description": "Training step 3458",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:12",
      "total_flops_so_far": 4.114297180681495e+16,
      "budget_used_percent": 41.142971806814955
    },
    {
      "type": "training",
      "description": "Training step 3459",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:13",
      "total_flops_so_far": 4.1154853922919144e+16,
      "budget_used_percent": 41.154853922919145
    },
    {
      "type": "training",
      "description": "Training step 3460",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:13",
      "total_flops_so_far": 4.1166736039023336e+16,
      "budget_used_percent": 41.16673603902334
    },
    {
      "type": "training",
      "description": "Training step 3461",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:14",
      "total_flops_so_far": 4.117861815512753e+16,
      "budget_used_percent": 41.17861815512753
    },
    {
      "type": "training",
      "description": "Training step 3462",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:15",
      "total_flops_so_far": 4.119050027123172e+16,
      "budget_used_percent": 41.19050027123172
    },
    {
      "type": "training",
      "description": "Training step 3463",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:15",
      "total_flops_so_far": 4.120238238733591e+16,
      "budget_used_percent": 41.20238238733591
    },
    {
      "type": "training",
      "description": "Training step 3464",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:16",
      "total_flops_so_far": 4.1214264503440104e+16,
      "budget_used_percent": 41.214264503440106
    },
    {
      "type": "training",
      "description": "Training step 3465",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:17",
      "total_flops_so_far": 4.1226146619544296e+16,
      "budget_used_percent": 41.226146619544295
    },
    {
      "type": "training",
      "description": "Training step 3466",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:17",
      "total_flops_so_far": 4.123802873564849e+16,
      "budget_used_percent": 41.238028735648484
    },
    {
      "type": "training",
      "description": "Training step 3467",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:18",
      "total_flops_so_far": 4.124991085175268e+16,
      "budget_used_percent": 41.24991085175268
    },
    {
      "type": "training",
      "description": "Training step 3468",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:19",
      "total_flops_so_far": 4.126179296785687e+16,
      "budget_used_percent": 41.26179296785687
    },
    {
      "type": "training",
      "description": "Training step 3469",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:19",
      "total_flops_so_far": 4.1273675083961064e+16,
      "budget_used_percent": 41.273675083961066
    },
    {
      "type": "training",
      "description": "Training step 3470",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:20",
      "total_flops_so_far": 4.1285557200065256e+16,
      "budget_used_percent": 41.285557200065256
    },
    {
      "type": "training",
      "description": "Training step 3471",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:21",
      "total_flops_so_far": 4.129743931616945e+16,
      "budget_used_percent": 41.297439316169445
    },
    {
      "type": "training",
      "description": "Training step 3472",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:21",
      "total_flops_so_far": 4.130932143227364e+16,
      "budget_used_percent": 41.309321432273634
    },
    {
      "type": "training",
      "description": "Training step 3473",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:22",
      "total_flops_so_far": 4.132120354837783e+16,
      "budget_used_percent": 41.32120354837783
    },
    {
      "type": "training",
      "description": "Training step 3474",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:23",
      "total_flops_so_far": 4.1333085664482024e+16,
      "budget_used_percent": 41.33308566448202
    },
    {
      "type": "training",
      "description": "Training step 3475",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:23",
      "total_flops_so_far": 4.1344967780586216e+16,
      "budget_used_percent": 41.34496778058622
    },
    {
      "type": "training",
      "description": "Training step 3476",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:24",
      "total_flops_so_far": 4.135684989669041e+16,
      "budget_used_percent": 41.356849896690406
    },
    {
      "type": "training",
      "description": "Training step 3477",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:25",
      "total_flops_so_far": 4.13687320127946e+16,
      "budget_used_percent": 41.3687320127946
    },
    {
      "type": "training",
      "description": "Training step 3478",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:25",
      "total_flops_so_far": 4.138061412889879e+16,
      "budget_used_percent": 41.38061412889879
    },
    {
      "type": "training",
      "description": "Training step 3479",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:26",
      "total_flops_so_far": 4.1392496245002984e+16,
      "budget_used_percent": 41.39249624500299
    },
    {
      "type": "training",
      "description": "Training step 3480",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:27",
      "total_flops_so_far": 4.1404378361107176e+16,
      "budget_used_percent": 41.40437836110718
    },
    {
      "type": "training",
      "description": "Training step 3481",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:27",
      "total_flops_so_far": 4.141626047721137e+16,
      "budget_used_percent": 41.41626047721137
    },
    {
      "type": "training",
      "description": "Training step 3482",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:28",
      "total_flops_so_far": 4.142814259331556e+16,
      "budget_used_percent": 41.428142593315556
    },
    {
      "type": "training",
      "description": "Training step 3483",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:28",
      "total_flops_so_far": 4.144002470941975e+16,
      "budget_used_percent": 41.44002470941975
    },
    {
      "type": "training",
      "description": "Training step 3484",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:29",
      "total_flops_so_far": 4.1451906825523944e+16,
      "budget_used_percent": 41.45190682552394
    },
    {
      "type": "training",
      "description": "Training step 3485",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:30",
      "total_flops_so_far": 4.1463788941628136e+16,
      "budget_used_percent": 41.46378894162814
    },
    {
      "type": "training",
      "description": "Training step 3486",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:30",
      "total_flops_so_far": 4.147567105773233e+16,
      "budget_used_percent": 41.47567105773233
    },
    {
      "type": "training",
      "description": "Training step 3487",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:31",
      "total_flops_so_far": 4.148755317383652e+16,
      "budget_used_percent": 41.487553173836524
    },
    {
      "type": "training",
      "description": "Training step 3488",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:32",
      "total_flops_so_far": 4.149943528994071e+16,
      "budget_used_percent": 41.49943528994071
    },
    {
      "type": "training",
      "description": "Training step 3489",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:32",
      "total_flops_so_far": 4.1511317406044904e+16,
      "budget_used_percent": 41.51131740604491
    },
    {
      "type": "training",
      "description": "Training step 3490",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:33",
      "total_flops_so_far": 4.1523199522149096e+16,
      "budget_used_percent": 41.5231995221491
    },
    {
      "type": "training",
      "description": "Training step 3491",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:34",
      "total_flops_so_far": 4.153508163825329e+16,
      "budget_used_percent": 41.53508163825329
    },
    {
      "type": "training",
      "description": "Training step 3492",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:34",
      "total_flops_so_far": 4.154696375435748e+16,
      "budget_used_percent": 41.54696375435748
    },
    {
      "type": "training",
      "description": "Training step 3493",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:35",
      "total_flops_so_far": 4.155884587046167e+16,
      "budget_used_percent": 41.55884587046167
    },
    {
      "type": "training",
      "description": "Training step 3494",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:36",
      "total_flops_so_far": 4.1570727986565864e+16,
      "budget_used_percent": 41.57072798656586
    },
    {
      "type": "training",
      "description": "Training step 3495",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:36",
      "total_flops_so_far": 4.1582610102670056e+16,
      "budget_used_percent": 41.58261010267005
    },
    {
      "type": "training",
      "description": "Training step 3496",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:37",
      "total_flops_so_far": 4.159449221877425e+16,
      "budget_used_percent": 41.59449221877425
    },
    {
      "type": "training",
      "description": "Training step 3497",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:38",
      "total_flops_so_far": 4.160637433487844e+16,
      "budget_used_percent": 41.60637433487844
    },
    {
      "type": "training",
      "description": "Training step 3498",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:38",
      "total_flops_so_far": 4.161825645098263e+16,
      "budget_used_percent": 41.618256450982635
    },
    {
      "type": "training",
      "description": "Training step 3499",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:39",
      "total_flops_so_far": 4.1630138567086824e+16,
      "budget_used_percent": 41.630138567086824
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:47",
      "total_flops_so_far": 4.163084919622468e+16,
      "budget_used_percent": 41.63084919622468
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 714333709232.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:47:54",
      "total_flops_so_far": 4.163156352993391e+16,
      "budget_used_percent": 41.63156352993391
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:48:02",
      "total_flops_so_far": 4.163227601099726e+16,
      "budget_used_percent": 41.632276010997266
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:48:09",
      "total_flops_so_far": 4.163298664013512e+16,
      "budget_used_percent": 41.63298664013512
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 713407296244.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:48:16",
      "total_flops_so_far": 4.163370004743136e+16,
      "budget_used_percent": 41.63370004743136
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 5",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:48:24",
      "total_flops_so_far": 4.163441067656922e+16,
      "budget_used_percent": 41.63441067656922
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 6",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:48:31",
      "total_flops_so_far": 4.163512315763257e+16,
      "budget_used_percent": 41.63512315763257
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 7",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:48:39",
      "total_flops_so_far": 4.163583563869592e+16,
      "budget_used_percent": 41.63583563869592
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 8",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:48:46",
      "total_flops_so_far": 4.163654811975927e+16,
      "budget_used_percent": 41.63654811975927
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 9",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:48:53",
      "total_flops_so_far": 4.163726060082262e+16,
      "budget_used_percent": 41.63726060082262
    },
    {
      "type": "training",
      "description": "Training step 3500",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:48:54",
      "total_flops_so_far": 4.164914271692682e+16,
      "budget_used_percent": 41.64914271692682
    },
    {
      "type": "training",
      "description": "Training step 3501",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:48:54",
      "total_flops_so_far": 4.166102483303101e+16,
      "budget_used_percent": 41.66102483303101
    },
    {
      "type": "training",
      "description": "Training step 3502",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:48:55",
      "total_flops_so_far": 4.16729069491352e+16,
      "budget_used_percent": 41.6729069491352
    },
    {
      "type": "training",
      "description": "Training step 3503",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:48:56",
      "total_flops_so_far": 4.168478906523939e+16,
      "budget_used_percent": 41.68478906523939
    },
    {
      "type": "training",
      "description": "Training step 3504",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:48:56",
      "total_flops_so_far": 4.169667118134358e+16,
      "budget_used_percent": 41.69667118134359
    },
    {
      "type": "training",
      "description": "Training step 3505",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:48:57",
      "total_flops_so_far": 4.170855329744778e+16,
      "budget_used_percent": 41.70855329744778
    },
    {
      "type": "training",
      "description": "Training step 3506",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:48:58",
      "total_flops_so_far": 4.172043541355197e+16,
      "budget_used_percent": 41.72043541355197
    },
    {
      "type": "training",
      "description": "Training step 3507",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:48:58",
      "total_flops_so_far": 4.173231752965616e+16,
      "budget_used_percent": 41.73231752965616
    },
    {
      "type": "training",
      "description": "Training step 3508",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:48:59",
      "total_flops_so_far": 4.174419964576035e+16,
      "budget_used_percent": 41.74419964576035
    },
    {
      "type": "training",
      "description": "Training step 3509",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:00",
      "total_flops_so_far": 4.175608176186454e+16,
      "budget_used_percent": 41.75608176186454
    },
    {
      "type": "training",
      "description": "Training step 3510",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:00",
      "total_flops_so_far": 4.176796387796874e+16,
      "budget_used_percent": 41.76796387796873
    },
    {
      "type": "training",
      "description": "Training step 3511",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:01",
      "total_flops_so_far": 4.177984599407293e+16,
      "budget_used_percent": 41.77984599407293
    },
    {
      "type": "training",
      "description": "Training step 3512",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:02",
      "total_flops_so_far": 4.179172811017712e+16,
      "budget_used_percent": 41.79172811017712
    },
    {
      "type": "training",
      "description": "Training step 3513",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:02",
      "total_flops_so_far": 4.180361022628131e+16,
      "budget_used_percent": 41.803610226281315
    },
    {
      "type": "training",
      "description": "Training step 3514",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:03",
      "total_flops_so_far": 4.18154923423855e+16,
      "budget_used_percent": 41.815492342385504
    },
    {
      "type": "training",
      "description": "Training step 3515",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:04",
      "total_flops_so_far": 4.18273744584897e+16,
      "budget_used_percent": 41.8273744584897
    },
    {
      "type": "training",
      "description": "Training step 3516",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:04",
      "total_flops_so_far": 4.183925657459389e+16,
      "budget_used_percent": 41.83925657459389
    },
    {
      "type": "training",
      "description": "Training step 3517",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:05",
      "total_flops_so_far": 4.185113869069808e+16,
      "budget_used_percent": 41.85113869069808
    },
    {
      "type": "training",
      "description": "Training step 3518",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:06",
      "total_flops_so_far": 4.186302080680227e+16,
      "budget_used_percent": 41.86302080680227
    },
    {
      "type": "training",
      "description": "Training step 3519",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:06",
      "total_flops_so_far": 4.187490292290646e+16,
      "budget_used_percent": 41.874902922906465
    },
    {
      "type": "training",
      "description": "Training step 3520",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:07",
      "total_flops_so_far": 4.188678503901066e+16,
      "budget_used_percent": 41.886785039010654
    },
    {
      "type": "training",
      "description": "Training step 3521",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:08",
      "total_flops_so_far": 4.189866715511485e+16,
      "budget_used_percent": 41.89866715511485
    },
    {
      "type": "training",
      "description": "Training step 3522",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:08",
      "total_flops_so_far": 4.191054927121904e+16,
      "budget_used_percent": 41.91054927121904
    },
    {
      "type": "training",
      "description": "Training step 3523",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:09",
      "total_flops_so_far": 4.192243138732323e+16,
      "budget_used_percent": 41.922431387323236
    },
    {
      "type": "training",
      "description": "Training step 3524",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:10",
      "total_flops_so_far": 4.193431350342742e+16,
      "budget_used_percent": 41.934313503427425
    },
    {
      "type": "training",
      "description": "Training step 3525",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:10",
      "total_flops_so_far": 4.194619561953162e+16,
      "budget_used_percent": 41.946195619531615
    },
    {
      "type": "training",
      "description": "Training step 3526",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:11",
      "total_flops_so_far": 4.195807773563581e+16,
      "budget_used_percent": 41.958077735635804
    },
    {
      "type": "training",
      "description": "Training step 3527",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:12",
      "total_flops_so_far": 4.196995985174e+16,
      "budget_used_percent": 41.96995985174
    },
    {
      "type": "training",
      "description": "Training step 3528",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:12",
      "total_flops_so_far": 4.198184196784419e+16,
      "budget_used_percent": 41.98184196784419
    },
    {
      "type": "training",
      "description": "Training step 3529",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:13",
      "total_flops_so_far": 4.199372408394838e+16,
      "budget_used_percent": 41.993724083948386
    },
    {
      "type": "training",
      "description": "Training step 3530",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:14",
      "total_flops_so_far": 4.200560620005258e+16,
      "budget_used_percent": 42.005606200052576
    },
    {
      "type": "training",
      "description": "Training step 3531",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:14",
      "total_flops_so_far": 4.201748831615677e+16,
      "budget_used_percent": 42.01748831615677
    },
    {
      "type": "training",
      "description": "Training step 3532",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:15",
      "total_flops_so_far": 4.202937043226096e+16,
      "budget_used_percent": 42.02937043226096
    },
    {
      "type": "training",
      "description": "Training step 3533",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:16",
      "total_flops_so_far": 4.204125254836515e+16,
      "budget_used_percent": 42.04125254836516
    },
    {
      "type": "training",
      "description": "Training step 3534",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:16",
      "total_flops_so_far": 4.205313466446934e+16,
      "budget_used_percent": 42.05313466446935
    },
    {
      "type": "training",
      "description": "Training step 3535",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:17",
      "total_flops_so_far": 4.206501678057354e+16,
      "budget_used_percent": 42.065016780573536
    },
    {
      "type": "training",
      "description": "Training step 3536",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:18",
      "total_flops_so_far": 4.207689889667773e+16,
      "budget_used_percent": 42.076898896677726
    },
    {
      "type": "training",
      "description": "Training step 3537",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:18",
      "total_flops_so_far": 4.208878101278192e+16,
      "budget_used_percent": 42.088781012781915
    },
    {
      "type": "training",
      "description": "Training step 3538",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:19",
      "total_flops_so_far": 4.210066312888611e+16,
      "budget_used_percent": 42.10066312888611
    },
    {
      "type": "training",
      "description": "Training step 3539",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:20",
      "total_flops_so_far": 4.21125452449903e+16,
      "budget_used_percent": 42.1125452449903
    },
    {
      "type": "training",
      "description": "Training step 3540",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:20",
      "total_flops_so_far": 4.21244273610945e+16,
      "budget_used_percent": 42.1244273610945
    },
    {
      "type": "training",
      "description": "Training step 3541",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:21",
      "total_flops_so_far": 4.213630947719869e+16,
      "budget_used_percent": 42.13630947719869
    },
    {
      "type": "training",
      "description": "Training step 3542",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:22",
      "total_flops_so_far": 4.214819159330288e+16,
      "budget_used_percent": 42.14819159330288
    },
    {
      "type": "training",
      "description": "Training step 3543",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:22",
      "total_flops_so_far": 4.216007370940707e+16,
      "budget_used_percent": 42.16007370940707
    },
    {
      "type": "training",
      "description": "Training step 3544",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:23",
      "total_flops_so_far": 4.217195582551126e+16,
      "budget_used_percent": 42.17195582551126
    },
    {
      "type": "training",
      "description": "Training step 3545",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:24",
      "total_flops_so_far": 4.218383794161546e+16,
      "budget_used_percent": 42.18383794161545
    },
    {
      "type": "training",
      "description": "Training step 3546",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:24",
      "total_flops_so_far": 4.219572005771965e+16,
      "budget_used_percent": 42.19572005771965
    },
    {
      "type": "training",
      "description": "Training step 3547",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:25",
      "total_flops_so_far": 4.220760217382384e+16,
      "budget_used_percent": 42.20760217382384
    },
    {
      "type": "training",
      "description": "Training step 3548",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:26",
      "total_flops_so_far": 4.221948428992803e+16,
      "budget_used_percent": 42.21948428992803
    },
    {
      "type": "training",
      "description": "Training step 3549",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:26",
      "total_flops_so_far": 4.223136640603222e+16,
      "budget_used_percent": 42.23136640603222
    },
    {
      "type": "training",
      "description": "Training step 3550",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:27",
      "total_flops_so_far": 4.224324852213642e+16,
      "budget_used_percent": 42.24324852213642
    },
    {
      "type": "training",
      "description": "Training step 3551",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:28",
      "total_flops_so_far": 4.225513063824061e+16,
      "budget_used_percent": 42.25513063824061
    },
    {
      "type": "training",
      "description": "Training step 3552",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:28",
      "total_flops_so_far": 4.22670127543448e+16,
      "budget_used_percent": 42.267012754344805
    },
    {
      "type": "training",
      "description": "Training step 3553",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:29",
      "total_flops_so_far": 4.227889487044899e+16,
      "budget_used_percent": 42.278894870448994
    },
    {
      "type": "training",
      "description": "Training step 3554",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:30",
      "total_flops_so_far": 4.229077698655318e+16,
      "budget_used_percent": 42.29077698655318
    },
    {
      "type": "training",
      "description": "Training step 3555",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:30",
      "total_flops_so_far": 4.230265910265738e+16,
      "budget_used_percent": 42.30265910265737
    },
    {
      "type": "training",
      "description": "Training step 3556",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:31",
      "total_flops_so_far": 4.231454121876157e+16,
      "budget_used_percent": 42.31454121876157
    },
    {
      "type": "training",
      "description": "Training step 3557",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:31",
      "total_flops_so_far": 4.232642333486576e+16,
      "budget_used_percent": 42.32642333486576
    },
    {
      "type": "training",
      "description": "Training step 3558",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:32",
      "total_flops_so_far": 4.233830545096995e+16,
      "budget_used_percent": 42.338305450969955
    },
    {
      "type": "training",
      "description": "Training step 3559",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:33",
      "total_flops_so_far": 4.235018756707414e+16,
      "budget_used_percent": 42.350187567074144
    },
    {
      "type": "training",
      "description": "Training step 3560",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:33",
      "total_flops_so_far": 4.236206968317834e+16,
      "budget_used_percent": 42.36206968317834
    },
    {
      "type": "training",
      "description": "Training step 3561",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:34",
      "total_flops_so_far": 4.237395179928253e+16,
      "budget_used_percent": 42.37395179928253
    },
    {
      "type": "training",
      "description": "Training step 3562",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:35",
      "total_flops_so_far": 4.238583391538672e+16,
      "budget_used_percent": 42.38583391538672
    },
    {
      "type": "training",
      "description": "Training step 3563",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:35",
      "total_flops_so_far": 4.239771603149091e+16,
      "budget_used_percent": 42.39771603149091
    },
    {
      "type": "training",
      "description": "Training step 3564",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:36",
      "total_flops_so_far": 4.24095981475951e+16,
      "budget_used_percent": 42.409598147595105
    },
    {
      "type": "training",
      "description": "Training step 3565",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:37",
      "total_flops_so_far": 4.24214802636993e+16,
      "budget_used_percent": 42.421480263699294
    },
    {
      "type": "training",
      "description": "Training step 3566",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:37",
      "total_flops_so_far": 4.243336237980349e+16,
      "budget_used_percent": 42.433362379803484
    },
    {
      "type": "training",
      "description": "Training step 3567",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:38",
      "total_flops_so_far": 4.244524449590768e+16,
      "budget_used_percent": 42.44524449590768
    },
    {
      "type": "training",
      "description": "Training step 3568",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:39",
      "total_flops_so_far": 4.245712661201187e+16,
      "budget_used_percent": 42.45712661201187
    },
    {
      "type": "training",
      "description": "Training step 3569",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:39",
      "total_flops_so_far": 4.246900872811606e+16,
      "budget_used_percent": 42.469008728116066
    },
    {
      "type": "training",
      "description": "Training step 3570",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:40",
      "total_flops_so_far": 4.248089084422026e+16,
      "budget_used_percent": 42.480890844220255
    },
    {
      "type": "training",
      "description": "Training step 3571",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:41",
      "total_flops_so_far": 4.249277296032445e+16,
      "budget_used_percent": 42.49277296032445
    },
    {
      "type": "training",
      "description": "Training step 3572",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:41",
      "total_flops_so_far": 4.250465507642864e+16,
      "budget_used_percent": 42.50465507642864
    },
    {
      "type": "training",
      "description": "Training step 3573",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:42",
      "total_flops_so_far": 4.251653719253283e+16,
      "budget_used_percent": 42.51653719253283
    },
    {
      "type": "training",
      "description": "Training step 3574",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:43",
      "total_flops_so_far": 4.252841930863702e+16,
      "budget_used_percent": 42.52841930863702
    },
    {
      "type": "training",
      "description": "Training step 3575",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:44",
      "total_flops_so_far": 4.254030142474122e+16,
      "budget_used_percent": 42.540301424741216
    },
    {
      "type": "training",
      "description": "Training step 3576",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:44",
      "total_flops_so_far": 4.255218354084541e+16,
      "budget_used_percent": 42.552183540845405
    },
    {
      "type": "training",
      "description": "Training step 3577",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:45",
      "total_flops_so_far": 4.25640656569496e+16,
      "budget_used_percent": 42.5640656569496
    },
    {
      "type": "training",
      "description": "Training step 3578",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:46",
      "total_flops_so_far": 4.257594777305379e+16,
      "budget_used_percent": 42.57594777305379
    },
    {
      "type": "training",
      "description": "Training step 3579",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:46",
      "total_flops_so_far": 4.258782988915798e+16,
      "budget_used_percent": 42.58782988915799
    },
    {
      "type": "training",
      "description": "Training step 3580",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:47",
      "total_flops_so_far": 4.259971200526218e+16,
      "budget_used_percent": 42.59971200526218
    },
    {
      "type": "training",
      "description": "Training step 3581",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:48",
      "total_flops_so_far": 4.261159412136637e+16,
      "budget_used_percent": 42.611594121366366
    },
    {
      "type": "training",
      "description": "Training step 3582",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:48",
      "total_flops_so_far": 4.262347623747056e+16,
      "budget_used_percent": 42.623476237470555
    },
    {
      "type": "training",
      "description": "Training step 3583",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:49",
      "total_flops_so_far": 4.263535835357475e+16,
      "budget_used_percent": 42.63535835357475
    },
    {
      "type": "training",
      "description": "Training step 3584",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:50",
      "total_flops_so_far": 4.264724046967894e+16,
      "budget_used_percent": 42.64724046967894
    },
    {
      "type": "training",
      "description": "Training step 3585",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:50",
      "total_flops_so_far": 4.265912258578314e+16,
      "budget_used_percent": 42.65912258578314
    },
    {
      "type": "training",
      "description": "Training step 3586",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:51",
      "total_flops_so_far": 4.267100470188733e+16,
      "budget_used_percent": 42.67100470188733
    },
    {
      "type": "training",
      "description": "Training step 3587",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:52",
      "total_flops_so_far": 4.268288681799152e+16,
      "budget_used_percent": 42.68288681799152
    },
    {
      "type": "training",
      "description": "Training step 3588",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:52",
      "total_flops_so_far": 4.269476893409571e+16,
      "budget_used_percent": 42.69476893409571
    },
    {
      "type": "training",
      "description": "Training step 3589",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:53",
      "total_flops_so_far": 4.27066510501999e+16,
      "budget_used_percent": 42.70665105019991
    },
    {
      "type": "training",
      "description": "Training step 3590",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:54",
      "total_flops_so_far": 4.27185331663041e+16,
      "budget_used_percent": 42.7185331663041
    },
    {
      "type": "training",
      "description": "Training step 3591",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:54",
      "total_flops_so_far": 4.273041528240829e+16,
      "budget_used_percent": 42.73041528240829
    },
    {
      "type": "training",
      "description": "Training step 3592",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:55",
      "total_flops_so_far": 4.274229739851248e+16,
      "budget_used_percent": 42.74229739851248
    },
    {
      "type": "training",
      "description": "Training step 3593",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:56",
      "total_flops_so_far": 4.275417951461667e+16,
      "budget_used_percent": 42.75417951461667
    },
    {
      "type": "training",
      "description": "Training step 3594",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:56",
      "total_flops_so_far": 4.276606163072086e+16,
      "budget_used_percent": 42.76606163072086
    },
    {
      "type": "training",
      "description": "Training step 3595",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:57",
      "total_flops_so_far": 4.277794374682506e+16,
      "budget_used_percent": 42.77794374682505
    },
    {
      "type": "training",
      "description": "Training step 3596",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:58",
      "total_flops_so_far": 4.278982586292925e+16,
      "budget_used_percent": 42.78982586292925
    },
    {
      "type": "training",
      "description": "Training step 3597",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:58",
      "total_flops_so_far": 4.280170797903344e+16,
      "budget_used_percent": 42.80170797903344
    },
    {
      "type": "training",
      "description": "Training step 3598",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:49:59",
      "total_flops_so_far": 4.281359009513763e+16,
      "budget_used_percent": 42.813590095137634
    },
    {
      "type": "training",
      "description": "Training step 3599",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:00",
      "total_flops_so_far": 4.282547221124182e+16,
      "budget_used_percent": 42.825472211241824
    },
    {
      "type": "training",
      "description": "Training step 3600",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:00",
      "total_flops_so_far": 4.283735432734602e+16,
      "budget_used_percent": 42.83735432734602
    },
    {
      "type": "training",
      "description": "Training step 3601",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:01",
      "total_flops_so_far": 4.284923644345021e+16,
      "budget_used_percent": 42.84923644345021
    },
    {
      "type": "training",
      "description": "Training step 3602",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:02",
      "total_flops_so_far": 4.28611185595544e+16,
      "budget_used_percent": 42.8611185595544
    },
    {
      "type": "training",
      "description": "Training step 3603",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:02",
      "total_flops_so_far": 4.287300067565859e+16,
      "budget_used_percent": 42.87300067565859
    },
    {
      "type": "training",
      "description": "Training step 3604",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:03",
      "total_flops_so_far": 4.288488279176278e+16,
      "budget_used_percent": 42.884882791762784
    },
    {
      "type": "training",
      "description": "Training step 3605",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:04",
      "total_flops_so_far": 4.289676490786698e+16,
      "budget_used_percent": 42.896764907866974
    },
    {
      "type": "training",
      "description": "Training step 3606",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:04",
      "total_flops_so_far": 4.290864702397117e+16,
      "budget_used_percent": 42.90864702397117
    },
    {
      "type": "training",
      "description": "Training step 3607",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:05",
      "total_flops_so_far": 4.292052914007536e+16,
      "budget_used_percent": 42.92052914007536
    },
    {
      "type": "training",
      "description": "Training step 3608",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:06",
      "total_flops_so_far": 4.293241125617955e+16,
      "budget_used_percent": 42.932411256179556
    },
    {
      "type": "training",
      "description": "Training step 3609",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:06",
      "total_flops_so_far": 4.294429337228374e+16,
      "budget_used_percent": 42.944293372283745
    },
    {
      "type": "training",
      "description": "Training step 3610",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:07",
      "total_flops_so_far": 4.295617548838794e+16,
      "budget_used_percent": 42.956175488387935
    },
    {
      "type": "training",
      "description": "Training step 3611",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:08",
      "total_flops_so_far": 4.296805760449213e+16,
      "budget_used_percent": 42.968057604492124
    },
    {
      "type": "training",
      "description": "Training step 3612",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:08",
      "total_flops_so_far": 4.297993972059632e+16,
      "budget_used_percent": 42.97993972059632
    },
    {
      "type": "training",
      "description": "Training step 3613",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:09",
      "total_flops_so_far": 4.299182183670051e+16,
      "budget_used_percent": 42.99182183670051
    },
    {
      "type": "training",
      "description": "Training step 3614",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:10",
      "total_flops_so_far": 4.30037039528047e+16,
      "budget_used_percent": 43.003703952804706
    },
    {
      "type": "training",
      "description": "Training step 3615",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:10",
      "total_flops_so_far": 4.30155860689089e+16,
      "budget_used_percent": 43.015586068908895
    },
    {
      "type": "training",
      "description": "Training step 3616",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:11",
      "total_flops_so_far": 4.302746818501309e+16,
      "budget_used_percent": 43.02746818501309
    },
    {
      "type": "training",
      "description": "Training step 3617",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:12",
      "total_flops_so_far": 4.303935030111728e+16,
      "budget_used_percent": 43.03935030111728
    },
    {
      "type": "training",
      "description": "Training step 3618",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:12",
      "total_flops_so_far": 4.305123241722147e+16,
      "budget_used_percent": 43.05123241722148
    },
    {
      "type": "training",
      "description": "Training step 3619",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:13",
      "total_flops_so_far": 4.306311453332566e+16,
      "budget_used_percent": 43.06311453332567
    },
    {
      "type": "training",
      "description": "Training step 3620",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:14",
      "total_flops_so_far": 4.307499664942986e+16,
      "budget_used_percent": 43.074996649429856
    },
    {
      "type": "training",
      "description": "Training step 3621",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:14",
      "total_flops_so_far": 4.308687876553405e+16,
      "budget_used_percent": 43.086878765534045
    },
    {
      "type": "training",
      "description": "Training step 3622",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:15",
      "total_flops_so_far": 4.309876088163824e+16,
      "budget_used_percent": 43.098760881638235
    },
    {
      "type": "training",
      "description": "Training step 3623",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:16",
      "total_flops_so_far": 4.311064299774243e+16,
      "budget_used_percent": 43.11064299774243
    },
    {
      "type": "training",
      "description": "Training step 3624",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:16",
      "total_flops_so_far": 4.312252511384662e+16,
      "budget_used_percent": 43.12252511384662
    },
    {
      "type": "training",
      "description": "Training step 3625",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:17",
      "total_flops_so_far": 4.313440722995082e+16,
      "budget_used_percent": 43.13440722995082
    },
    {
      "type": "training",
      "description": "Training step 3626",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:18",
      "total_flops_so_far": 4.314628934605501e+16,
      "budget_used_percent": 43.146289346055006
    },
    {
      "type": "training",
      "description": "Training step 3627",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:19",
      "total_flops_so_far": 4.31581714621592e+16,
      "budget_used_percent": 43.1581714621592
    },
    {
      "type": "training",
      "description": "Training step 3628",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:19",
      "total_flops_so_far": 4.317005357826339e+16,
      "budget_used_percent": 43.17005357826339
    },
    {
      "type": "training",
      "description": "Training step 3629",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:20",
      "total_flops_so_far": 4.318193569436758e+16,
      "budget_used_percent": 43.18193569436758
    },
    {
      "type": "training",
      "description": "Training step 3630",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:20",
      "total_flops_so_far": 4.319381781047178e+16,
      "budget_used_percent": 43.19381781047177
    },
    {
      "type": "training",
      "description": "Training step 3631",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:21",
      "total_flops_so_far": 4.320569992657597e+16,
      "budget_used_percent": 43.20569992657597
    },
    {
      "type": "training",
      "description": "Training step 3632",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:22",
      "total_flops_so_far": 4.321758204268016e+16,
      "budget_used_percent": 43.21758204268016
    },
    {
      "type": "training",
      "description": "Training step 3633",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:22",
      "total_flops_so_far": 4.322946415878435e+16,
      "budget_used_percent": 43.22946415878435
    },
    {
      "type": "training",
      "description": "Training step 3634",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:23",
      "total_flops_so_far": 4.324134627488854e+16,
      "budget_used_percent": 43.24134627488854
    },
    {
      "type": "training",
      "description": "Training step 3635",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:24",
      "total_flops_so_far": 4.325322839099274e+16,
      "budget_used_percent": 43.25322839099274
    },
    {
      "type": "training",
      "description": "Training step 3636",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:24",
      "total_flops_so_far": 4.326511050709693e+16,
      "budget_used_percent": 43.26511050709693
    },
    {
      "type": "training",
      "description": "Training step 3637",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:25",
      "total_flops_so_far": 4.327699262320112e+16,
      "budget_used_percent": 43.276992623201124
    },
    {
      "type": "training",
      "description": "Training step 3638",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:26",
      "total_flops_so_far": 4.328887473930531e+16,
      "budget_used_percent": 43.288874739305314
    },
    {
      "type": "training",
      "description": "Training step 3639",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:26",
      "total_flops_so_far": 4.33007568554095e+16,
      "budget_used_percent": 43.3007568554095
    },
    {
      "type": "training",
      "description": "Training step 3640",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:27",
      "total_flops_so_far": 4.33126389715137e+16,
      "budget_used_percent": 43.31263897151369
    },
    {
      "type": "training",
      "description": "Training step 3641",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:28",
      "total_flops_so_far": 4.332452108761789e+16,
      "budget_used_percent": 43.32452108761789
    },
    {
      "type": "training",
      "description": "Training step 3642",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:28",
      "total_flops_so_far": 4.333640320372208e+16,
      "budget_used_percent": 43.33640320372208
    },
    {
      "type": "training",
      "description": "Training step 3643",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:29",
      "total_flops_so_far": 4.334828531982627e+16,
      "budget_used_percent": 43.348285319826275
    },
    {
      "type": "training",
      "description": "Training step 3644",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:30",
      "total_flops_so_far": 4.336016743593046e+16,
      "budget_used_percent": 43.360167435930464
    },
    {
      "type": "training",
      "description": "Training step 3645",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:30",
      "total_flops_so_far": 4.337204955203466e+16,
      "budget_used_percent": 43.37204955203466
    },
    {
      "type": "training",
      "description": "Training step 3646",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:31",
      "total_flops_so_far": 4.338393166813885e+16,
      "budget_used_percent": 43.38393166813885
    },
    {
      "type": "training",
      "description": "Training step 3647",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:32",
      "total_flops_so_far": 4.339581378424304e+16,
      "budget_used_percent": 43.395813784243046
    },
    {
      "type": "training",
      "description": "Training step 3648",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:32",
      "total_flops_so_far": 4.340769590034723e+16,
      "budget_used_percent": 43.407695900347235
    },
    {
      "type": "training",
      "description": "Training step 3649",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:33",
      "total_flops_so_far": 4.341957801645142e+16,
      "budget_used_percent": 43.419578016451425
    },
    {
      "type": "training",
      "description": "Training step 3650",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:34",
      "total_flops_so_far": 4.343146013255562e+16,
      "budget_used_percent": 43.431460132555614
    },
    {
      "type": "training",
      "description": "Training step 3651",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:34",
      "total_flops_so_far": 4.344334224865981e+16,
      "budget_used_percent": 43.4433422486598
    },
    {
      "type": "training",
      "description": "Training step 3652",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:35",
      "total_flops_so_far": 4.3455224364764e+16,
      "budget_used_percent": 43.455224364764
    },
    {
      "type": "training",
      "description": "Training step 3653",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:36",
      "total_flops_so_far": 4.346710648086819e+16,
      "budget_used_percent": 43.46710648086819
    },
    {
      "type": "training",
      "description": "Training step 3654",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:36",
      "total_flops_so_far": 4.347898859697238e+16,
      "budget_used_percent": 43.478988596972385
    },
    {
      "type": "training",
      "description": "Training step 3655",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:37",
      "total_flops_so_far": 4.349087071307658e+16,
      "budget_used_percent": 43.490870713076575
    },
    {
      "type": "training",
      "description": "Training step 3656",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:38",
      "total_flops_so_far": 4.350275282918077e+16,
      "budget_used_percent": 43.50275282918077
    },
    {
      "type": "training",
      "description": "Training step 3657",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:38",
      "total_flops_so_far": 4.351463494528496e+16,
      "budget_used_percent": 43.51463494528496
    },
    {
      "type": "training",
      "description": "Training step 3658",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:39",
      "total_flops_so_far": 4.352651706138915e+16,
      "budget_used_percent": 43.52651706138915
    },
    {
      "type": "training",
      "description": "Training step 3659",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:40",
      "total_flops_so_far": 4.353839917749334e+16,
      "budget_used_percent": 43.53839917749334
    },
    {
      "type": "training",
      "description": "Training step 3660",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:40",
      "total_flops_so_far": 4.355028129359754e+16,
      "budget_used_percent": 43.550281293597536
    },
    {
      "type": "training",
      "description": "Training step 3661",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:41",
      "total_flops_so_far": 4.356216340970173e+16,
      "budget_used_percent": 43.562163409701725
    },
    {
      "type": "training",
      "description": "Training step 3662",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:42",
      "total_flops_so_far": 4.357404552580592e+16,
      "budget_used_percent": 43.57404552580592
    },
    {
      "type": "training",
      "description": "Training step 3663",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:42",
      "total_flops_so_far": 4.358592764191011e+16,
      "budget_used_percent": 43.58592764191011
    },
    {
      "type": "training",
      "description": "Training step 3664",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:43",
      "total_flops_so_far": 4.35978097580143e+16,
      "budget_used_percent": 43.59780975801431
    },
    {
      "type": "training",
      "description": "Training step 3665",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:44",
      "total_flops_so_far": 4.36096918741185e+16,
      "budget_used_percent": 43.6096918741185
    },
    {
      "type": "training",
      "description": "Training step 3666",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:44",
      "total_flops_so_far": 4.362157399022269e+16,
      "budget_used_percent": 43.62157399022269
    },
    {
      "type": "training",
      "description": "Training step 3667",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:45",
      "total_flops_so_far": 4.363345610632688e+16,
      "budget_used_percent": 43.63345610632688
    },
    {
      "type": "training",
      "description": "Training step 3668",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:46",
      "total_flops_so_far": 4.364533822243107e+16,
      "budget_used_percent": 43.64533822243107
    },
    {
      "type": "training",
      "description": "Training step 3669",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:46",
      "total_flops_so_far": 4.365722033853526e+16,
      "budget_used_percent": 43.65722033853526
    },
    {
      "type": "training",
      "description": "Training step 3670",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:47",
      "total_flops_so_far": 4.366910245463946e+16,
      "budget_used_percent": 43.66910245463946
    },
    {
      "type": "training",
      "description": "Training step 3671",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:48",
      "total_flops_so_far": 4.368098457074365e+16,
      "budget_used_percent": 43.68098457074365
    },
    {
      "type": "training",
      "description": "Training step 3672",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:48",
      "total_flops_so_far": 4.369286668684784e+16,
      "budget_used_percent": 43.69286668684784
    },
    {
      "type": "training",
      "description": "Training step 3673",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:49",
      "total_flops_so_far": 4.370474880295203e+16,
      "budget_used_percent": 43.70474880295203
    },
    {
      "type": "training",
      "description": "Training step 3674",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:50",
      "total_flops_so_far": 4.371663091905622e+16,
      "budget_used_percent": 43.71663091905623
    },
    {
      "type": "training",
      "description": "Training step 3675",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:50",
      "total_flops_so_far": 4.372851303516042e+16,
      "budget_used_percent": 43.72851303516042
    },
    {
      "type": "training",
      "description": "Training step 3676",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:51",
      "total_flops_so_far": 4.374039515126461e+16,
      "budget_used_percent": 43.74039515126461
    },
    {
      "type": "training",
      "description": "Training step 3677",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:52",
      "total_flops_so_far": 4.37522772673688e+16,
      "budget_used_percent": 43.7522772673688
    },
    {
      "type": "training",
      "description": "Training step 3678",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:52",
      "total_flops_so_far": 4.376415938347299e+16,
      "budget_used_percent": 43.76415938347299
    },
    {
      "type": "training",
      "description": "Training step 3679",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:53",
      "total_flops_so_far": 4.377604149957718e+16,
      "budget_used_percent": 43.77604149957718
    },
    {
      "type": "training",
      "description": "Training step 3680",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:54",
      "total_flops_so_far": 4.378792361568138e+16,
      "budget_used_percent": 43.78792361568137
    },
    {
      "type": "training",
      "description": "Training step 3681",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:54",
      "total_flops_so_far": 4.379980573178557e+16,
      "budget_used_percent": 43.79980573178557
    },
    {
      "type": "training",
      "description": "Training step 3682",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:55",
      "total_flops_so_far": 4.381168784788976e+16,
      "budget_used_percent": 43.81168784788976
    },
    {
      "type": "training",
      "description": "Training step 3683",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:55",
      "total_flops_so_far": 4.382356996399395e+16,
      "budget_used_percent": 43.823569963993954
    },
    {
      "type": "training",
      "description": "Training step 3684",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:56",
      "total_flops_so_far": 4.383545208009814e+16,
      "budget_used_percent": 43.83545208009814
    },
    {
      "type": "training",
      "description": "Training step 3685",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:57",
      "total_flops_so_far": 4.384733419620234e+16,
      "budget_used_percent": 43.84733419620234
    },
    {
      "type": "training",
      "description": "Training step 3686",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:57",
      "total_flops_so_far": 4.385921631230653e+16,
      "budget_used_percent": 43.85921631230653
    },
    {
      "type": "training",
      "description": "Training step 3687",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:58",
      "total_flops_so_far": 4.387109842841072e+16,
      "budget_used_percent": 43.87109842841072
    },
    {
      "type": "training",
      "description": "Training step 3688",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:59",
      "total_flops_so_far": 4.388298054451491e+16,
      "budget_used_percent": 43.88298054451491
    },
    {
      "type": "training",
      "description": "Training step 3689",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:50:59",
      "total_flops_so_far": 4.38948626606191e+16,
      "budget_used_percent": 43.894862660619104
    },
    {
      "type": "training",
      "description": "Training step 3690",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:00",
      "total_flops_so_far": 4.39067447767233e+16,
      "budget_used_percent": 43.90674477672329
    },
    {
      "type": "training",
      "description": "Training step 3691",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:01",
      "total_flops_so_far": 4.391862689282749e+16,
      "budget_used_percent": 43.91862689282749
    },
    {
      "type": "training",
      "description": "Training step 3692",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:02",
      "total_flops_so_far": 4.393050900893168e+16,
      "budget_used_percent": 43.93050900893168
    },
    {
      "type": "training",
      "description": "Training step 3693",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:02",
      "total_flops_so_far": 4.394239112503587e+16,
      "budget_used_percent": 43.942391125035876
    },
    {
      "type": "training",
      "description": "Training step 3694",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:03",
      "total_flops_so_far": 4.395427324114006e+16,
      "budget_used_percent": 43.954273241140065
    },
    {
      "type": "training",
      "description": "Training step 3695",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:04",
      "total_flops_so_far": 4.396615535724426e+16,
      "budget_used_percent": 43.966155357244254
    },
    {
      "type": "training",
      "description": "Training step 3696",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:04",
      "total_flops_so_far": 4.397803747334845e+16,
      "budget_used_percent": 43.978037473348444
    },
    {
      "type": "training",
      "description": "Training step 3697",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:05",
      "total_flops_so_far": 4.398991958945264e+16,
      "budget_used_percent": 43.98991958945264
    },
    {
      "type": "training",
      "description": "Training step 3698",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:06",
      "total_flops_so_far": 4.400180170555683e+16,
      "budget_used_percent": 44.00180170555683
    },
    {
      "type": "training",
      "description": "Training step 3699",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:06",
      "total_flops_so_far": 4.401368382166102e+16,
      "budget_used_percent": 44.013683821661026
    },
    {
      "type": "training",
      "description": "Training step 3700",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:07",
      "total_flops_so_far": 4.402556593776522e+16,
      "budget_used_percent": 44.025565937765215
    },
    {
      "type": "training",
      "description": "Training step 3701",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:08",
      "total_flops_so_far": 4.403744805386941e+16,
      "budget_used_percent": 44.03744805386941
    },
    {
      "type": "training",
      "description": "Training step 3702",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:08",
      "total_flops_so_far": 4.40493301699736e+16,
      "budget_used_percent": 44.0493301699736
    },
    {
      "type": "training",
      "description": "Training step 3703",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:09",
      "total_flops_so_far": 4.406121228607779e+16,
      "budget_used_percent": 44.0612122860778
    },
    {
      "type": "training",
      "description": "Training step 3704",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:10",
      "total_flops_so_far": 4.407309440218198e+16,
      "budget_used_percent": 44.07309440218199
    },
    {
      "type": "training",
      "description": "Training step 3705",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:10",
      "total_flops_so_far": 4.408497651828618e+16,
      "budget_used_percent": 44.084976518286176
    },
    {
      "type": "training",
      "description": "Training step 3706",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:11",
      "total_flops_so_far": 4.409685863439037e+16,
      "budget_used_percent": 44.096858634390365
    },
    {
      "type": "training",
      "description": "Training step 3707",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:12",
      "total_flops_so_far": 4.410874075049456e+16,
      "budget_used_percent": 44.108740750494555
    },
    {
      "type": "training",
      "description": "Training step 3708",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:12",
      "total_flops_so_far": 4.412062286659875e+16,
      "budget_used_percent": 44.12062286659875
    },
    {
      "type": "training",
      "description": "Training step 3709",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:13",
      "total_flops_so_far": 4.413250498270294e+16,
      "budget_used_percent": 44.13250498270294
    },
    {
      "type": "training",
      "description": "Training step 3710",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:14",
      "total_flops_so_far": 4.414438709880714e+16,
      "budget_used_percent": 44.14438709880714
    },
    {
      "type": "training",
      "description": "Training step 3711",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:14",
      "total_flops_so_far": 4.415626921491133e+16,
      "budget_used_percent": 44.156269214911326
    },
    {
      "type": "training",
      "description": "Training step 3712",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:15",
      "total_flops_so_far": 4.416815133101552e+16,
      "budget_used_percent": 44.16815133101552
    },
    {
      "type": "training",
      "description": "Training step 3713",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:16",
      "total_flops_so_far": 4.418003344711971e+16,
      "budget_used_percent": 44.18003344711971
    },
    {
      "type": "training",
      "description": "Training step 3714",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:16",
      "total_flops_so_far": 4.41919155632239e+16,
      "budget_used_percent": 44.1919155632239
    },
    {
      "type": "training",
      "description": "Training step 3715",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:17",
      "total_flops_so_far": 4.42037976793281e+16,
      "budget_used_percent": 44.20379767932809
    },
    {
      "type": "training",
      "description": "Training step 3716",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:18",
      "total_flops_so_far": 4.421567979543229e+16,
      "budget_used_percent": 44.21567979543229
    },
    {
      "type": "training",
      "description": "Training step 3717",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:18",
      "total_flops_so_far": 4.422756191153648e+16,
      "budget_used_percent": 44.227561911536476
    },
    {
      "type": "training",
      "description": "Training step 3718",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:19",
      "total_flops_so_far": 4.423944402764067e+16,
      "budget_used_percent": 44.23944402764067
    },
    {
      "type": "training",
      "description": "Training step 3719",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:20",
      "total_flops_so_far": 4.425132614374486e+16,
      "budget_used_percent": 44.25132614374486
    },
    {
      "type": "training",
      "description": "Training step 3720",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:20",
      "total_flops_so_far": 4.426320825984906e+16,
      "budget_used_percent": 44.26320825984906
    },
    {
      "type": "training",
      "description": "Training step 3721",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:21",
      "total_flops_so_far": 4.427509037595325e+16,
      "budget_used_percent": 44.27509037595325
    },
    {
      "type": "training",
      "description": "Training step 3722",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:22",
      "total_flops_so_far": 4.428697249205744e+16,
      "budget_used_percent": 44.286972492057444
    },
    {
      "type": "training",
      "description": "Training step 3723",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:22",
      "total_flops_so_far": 4.429885460816163e+16,
      "budget_used_percent": 44.29885460816163
    },
    {
      "type": "training",
      "description": "Training step 3724",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:23",
      "total_flops_so_far": 4.431073672426582e+16,
      "budget_used_percent": 44.31073672426582
    },
    {
      "type": "training",
      "description": "Training step 3725",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:24",
      "total_flops_so_far": 4.432261884037002e+16,
      "budget_used_percent": 44.32261884037001
    },
    {
      "type": "training",
      "description": "Training step 3726",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:24",
      "total_flops_so_far": 4.433450095647421e+16,
      "budget_used_percent": 44.33450095647421
    },
    {
      "type": "training",
      "description": "Training step 3727",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:25",
      "total_flops_so_far": 4.43463830725784e+16,
      "budget_used_percent": 44.3463830725784
    },
    {
      "type": "training",
      "description": "Training step 3728",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:26",
      "total_flops_so_far": 4.435826518868259e+16,
      "budget_used_percent": 44.358265188682594
    },
    {
      "type": "training",
      "description": "Training step 3729",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:26",
      "total_flops_so_far": 4.437014730478678e+16,
      "budget_used_percent": 44.370147304786784
    },
    {
      "type": "training",
      "description": "Training step 3730",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:27",
      "total_flops_so_far": 4.438202942089098e+16,
      "budget_used_percent": 44.38202942089098
    },
    {
      "type": "training",
      "description": "Training step 3731",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:28",
      "total_flops_so_far": 4.439391153699517e+16,
      "budget_used_percent": 44.39391153699517
    },
    {
      "type": "training",
      "description": "Training step 3732",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:28",
      "total_flops_so_far": 4.440579365309936e+16,
      "budget_used_percent": 44.405793653099366
    },
    {
      "type": "training",
      "description": "Training step 3733",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:29",
      "total_flops_so_far": 4.441767576920355e+16,
      "budget_used_percent": 44.417675769203555
    },
    {
      "type": "training",
      "description": "Training step 3734",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:30",
      "total_flops_so_far": 4.442955788530774e+16,
      "budget_used_percent": 44.429557885307744
    },
    {
      "type": "training",
      "description": "Training step 3735",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:30",
      "total_flops_so_far": 4.444144000141194e+16,
      "budget_used_percent": 44.441440001411934
    },
    {
      "type": "training",
      "description": "Training step 3736",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:31",
      "total_flops_so_far": 4.445332211751613e+16,
      "budget_used_percent": 44.45332211751612
    },
    {
      "type": "training",
      "description": "Training step 3737",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:32",
      "total_flops_so_far": 4.446520423362032e+16,
      "budget_used_percent": 44.46520423362032
    },
    {
      "type": "training",
      "description": "Training step 3738",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:32",
      "total_flops_so_far": 4.447708634972451e+16,
      "budget_used_percent": 44.47708634972451
    },
    {
      "type": "training",
      "description": "Training step 3739",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:33",
      "total_flops_so_far": 4.44889684658287e+16,
      "budget_used_percent": 44.488968465828705
    },
    {
      "type": "training",
      "description": "Training step 3740",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:34",
      "total_flops_so_far": 4.45008505819329e+16,
      "budget_used_percent": 44.500850581932895
    },
    {
      "type": "training",
      "description": "Training step 3741",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:34",
      "total_flops_so_far": 4.451273269803709e+16,
      "budget_used_percent": 44.51273269803709
    },
    {
      "type": "training",
      "description": "Training step 3742",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:35",
      "total_flops_so_far": 4.452461481414128e+16,
      "budget_used_percent": 44.52461481414128
    },
    {
      "type": "training",
      "description": "Training step 3743",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:36",
      "total_flops_so_far": 4.453649693024547e+16,
      "budget_used_percent": 44.53649693024547
    },
    {
      "type": "training",
      "description": "Training step 3744",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:36",
      "total_flops_so_far": 4.454837904634966e+16,
      "budget_used_percent": 44.54837904634966
    },
    {
      "type": "training",
      "description": "Training step 3745",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:37",
      "total_flops_so_far": 4.456026116245386e+16,
      "budget_used_percent": 44.560261162453855
    },
    {
      "type": "training",
      "description": "Training step 3746",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:38",
      "total_flops_so_far": 4.457214327855805e+16,
      "budget_used_percent": 44.572143278558045
    },
    {
      "type": "training",
      "description": "Training step 3747",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:39",
      "total_flops_so_far": 4.458402539466224e+16,
      "budget_used_percent": 44.58402539466224
    },
    {
      "type": "training",
      "description": "Training step 3748",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:39",
      "total_flops_so_far": 4.459590751076643e+16,
      "budget_used_percent": 44.59590751076643
    },
    {
      "type": "training",
      "description": "Training step 3749",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:40",
      "total_flops_so_far": 4.460778962687062e+16,
      "budget_used_percent": 44.60778962687063
    },
    {
      "type": "training",
      "description": "Training step 3750",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:41",
      "total_flops_so_far": 4.461967174297482e+16,
      "budget_used_percent": 44.619671742974816
    },
    {
      "type": "training",
      "description": "Training step 3751",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:41",
      "total_flops_so_far": 4.463155385907901e+16,
      "budget_used_percent": 44.63155385907901
    },
    {
      "type": "training",
      "description": "Training step 3752",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:42",
      "total_flops_so_far": 4.46434359751832e+16,
      "budget_used_percent": 44.6434359751832
    },
    {
      "type": "training",
      "description": "Training step 3753",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:43",
      "total_flops_so_far": 4.465531809128739e+16,
      "budget_used_percent": 44.65531809128739
    },
    {
      "type": "training",
      "description": "Training step 3754",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:43",
      "total_flops_so_far": 4.466720020739158e+16,
      "budget_used_percent": 44.66720020739158
    },
    {
      "type": "training",
      "description": "Training step 3755",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:44",
      "total_flops_so_far": 4.467908232349578e+16,
      "budget_used_percent": 44.67908232349578
    },
    {
      "type": "training",
      "description": "Training step 3756",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:45",
      "total_flops_so_far": 4.469096443959997e+16,
      "budget_used_percent": 44.690964439599966
    },
    {
      "type": "training",
      "description": "Training step 3757",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:45",
      "total_flops_so_far": 4.470284655570416e+16,
      "budget_used_percent": 44.70284655570416
    },
    {
      "type": "training",
      "description": "Training step 3758",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:46",
      "total_flops_so_far": 4.471472867180835e+16,
      "budget_used_percent": 44.71472867180835
    },
    {
      "type": "training",
      "description": "Training step 3759",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:47",
      "total_flops_so_far": 4.472661078791254e+16,
      "budget_used_percent": 44.72661078791255
    },
    {
      "type": "training",
      "description": "Training step 3760",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:47",
      "total_flops_so_far": 4.473849290401674e+16,
      "budget_used_percent": 44.73849290401674
    },
    {
      "type": "training",
      "description": "Training step 3761",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:48",
      "total_flops_so_far": 4.475037502012093e+16,
      "budget_used_percent": 44.75037502012093
    },
    {
      "type": "training",
      "description": "Training step 3762",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:49",
      "total_flops_so_far": 4.476225713622512e+16,
      "budget_used_percent": 44.76225713622512
    },
    {
      "type": "training",
      "description": "Training step 3763",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:49",
      "total_flops_so_far": 4.477413925232931e+16,
      "budget_used_percent": 44.77413925232931
    },
    {
      "type": "training",
      "description": "Training step 3764",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:50",
      "total_flops_so_far": 4.47860213684335e+16,
      "budget_used_percent": 44.7860213684335
    },
    {
      "type": "training",
      "description": "Training step 3765",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:51",
      "total_flops_so_far": 4.47979034845377e+16,
      "budget_used_percent": 44.79790348453769
    },
    {
      "type": "training",
      "description": "Training step 3766",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:51",
      "total_flops_so_far": 4.480978560064189e+16,
      "budget_used_percent": 44.80978560064189
    },
    {
      "type": "training",
      "description": "Training step 3767",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:52",
      "total_flops_so_far": 4.482166771674608e+16,
      "budget_used_percent": 44.82166771674608
    },
    {
      "type": "training",
      "description": "Training step 3768",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:53",
      "total_flops_so_far": 4.483354983285027e+16,
      "budget_used_percent": 44.833549832850274
    },
    {
      "type": "training",
      "description": "Training step 3769",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:53",
      "total_flops_so_far": 4.484543194895446e+16,
      "budget_used_percent": 44.84543194895446
    },
    {
      "type": "training",
      "description": "Training step 3770",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:54",
      "total_flops_so_far": 4.485731406505866e+16,
      "budget_used_percent": 44.85731406505866
    },
    {
      "type": "training",
      "description": "Training step 3771",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:55",
      "total_flops_so_far": 4.486919618116285e+16,
      "budget_used_percent": 44.86919618116285
    },
    {
      "type": "training",
      "description": "Training step 3772",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:55",
      "total_flops_so_far": 4.488107829726704e+16,
      "budget_used_percent": 44.88107829726704
    },
    {
      "type": "training",
      "description": "Training step 3773",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:56",
      "total_flops_so_far": 4.489296041337123e+16,
      "budget_used_percent": 44.89296041337123
    },
    {
      "type": "training",
      "description": "Training step 3774",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:57",
      "total_flops_so_far": 4.490484252947542e+16,
      "budget_used_percent": 44.904842529475424
    },
    {
      "type": "training",
      "description": "Training step 3775",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:57",
      "total_flops_so_far": 4.491672464557962e+16,
      "budget_used_percent": 44.91672464557961
    },
    {
      "type": "training",
      "description": "Training step 3776",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:58",
      "total_flops_so_far": 4.492860676168381e+16,
      "budget_used_percent": 44.92860676168381
    },
    {
      "type": "training",
      "description": "Training step 3777",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:59",
      "total_flops_so_far": 4.4940488877788e+16,
      "budget_used_percent": 44.940488877788
    },
    {
      "type": "training",
      "description": "Training step 3778",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:51:59",
      "total_flops_so_far": 4.495237099389219e+16,
      "budget_used_percent": 44.952370993892195
    },
    {
      "type": "training",
      "description": "Training step 3779",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:00",
      "total_flops_so_far": 4.496425310999638e+16,
      "budget_used_percent": 44.964253109996385
    },
    {
      "type": "training",
      "description": "Training step 3780",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:01",
      "total_flops_so_far": 4.497613522610058e+16,
      "budget_used_percent": 44.97613522610058
    },
    {
      "type": "training",
      "description": "Training step 3781",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:01",
      "total_flops_so_far": 4.498801734220477e+16,
      "budget_used_percent": 44.98801734220477
    },
    {
      "type": "training",
      "description": "Training step 3782",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:02",
      "total_flops_so_far": 4.499989945830896e+16,
      "budget_used_percent": 44.99989945830896
    },
    {
      "type": "training",
      "description": "Training step 3783",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:03",
      "total_flops_so_far": 4.501178157441315e+16,
      "budget_used_percent": 45.01178157441315
    },
    {
      "type": "training",
      "description": "Training step 3784",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:03",
      "total_flops_so_far": 4.502366369051734e+16,
      "budget_used_percent": 45.023663690517346
    },
    {
      "type": "training",
      "description": "Training step 3785",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:04",
      "total_flops_so_far": 4.503554580662154e+16,
      "budget_used_percent": 45.035545806621535
    },
    {
      "type": "training",
      "description": "Training step 3786",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:05",
      "total_flops_so_far": 4.504742792272573e+16,
      "budget_used_percent": 45.04742792272573
    },
    {
      "type": "training",
      "description": "Training step 3787",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:05",
      "total_flops_so_far": 4.505931003882992e+16,
      "budget_used_percent": 45.05931003882992
    },
    {
      "type": "training",
      "description": "Training step 3788",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:06",
      "total_flops_so_far": 4.507119215493411e+16,
      "budget_used_percent": 45.07119215493412
    },
    {
      "type": "training",
      "description": "Training step 3789",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:07",
      "total_flops_so_far": 4.50830742710383e+16,
      "budget_used_percent": 45.083074271038306
    },
    {
      "type": "training",
      "description": "Training step 3790",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:07",
      "total_flops_so_far": 4.50949563871425e+16,
      "budget_used_percent": 45.094956387142496
    },
    {
      "type": "training",
      "description": "Training step 3791",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:08",
      "total_flops_so_far": 4.510683850324669e+16,
      "budget_used_percent": 45.106838503246685
    },
    {
      "type": "training",
      "description": "Training step 3792",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:09",
      "total_flops_so_far": 4.511872061935088e+16,
      "budget_used_percent": 45.11872061935088
    },
    {
      "type": "training",
      "description": "Training step 3793",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:09",
      "total_flops_so_far": 4.513060273545507e+16,
      "budget_used_percent": 45.13060273545507
    },
    {
      "type": "training",
      "description": "Training step 3794",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:10",
      "total_flops_so_far": 4.514248485155926e+16,
      "budget_used_percent": 45.14248485155926
    },
    {
      "type": "training",
      "description": "Training step 3795",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:11",
      "total_flops_so_far": 4.515436696766346e+16,
      "budget_used_percent": 45.15436696766346
    },
    {
      "type": "training",
      "description": "Training step 3796",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:11",
      "total_flops_so_far": 4.516624908376765e+16,
      "budget_used_percent": 45.166249083767646
    },
    {
      "type": "training",
      "description": "Training step 3797",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:12",
      "total_flops_so_far": 4.517813119987184e+16,
      "budget_used_percent": 45.17813119987184
    },
    {
      "type": "training",
      "description": "Training step 3798",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:13",
      "total_flops_so_far": 4.519001331597603e+16,
      "budget_used_percent": 45.19001331597603
    },
    {
      "type": "training",
      "description": "Training step 3799",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:13",
      "total_flops_so_far": 4.520189543208022e+16,
      "budget_used_percent": 45.20189543208023
    },
    {
      "type": "training",
      "description": "Training step 3800",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:14",
      "total_flops_so_far": 4.521377754818442e+16,
      "budget_used_percent": 45.21377754818442
    },
    {
      "type": "training",
      "description": "Training step 3801",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:15",
      "total_flops_so_far": 4.522565966428861e+16,
      "budget_used_percent": 45.22565966428861
    },
    {
      "type": "training",
      "description": "Training step 3802",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:15",
      "total_flops_so_far": 4.52375417803928e+16,
      "budget_used_percent": 45.237541780392796
    },
    {
      "type": "training",
      "description": "Training step 3803",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:16",
      "total_flops_so_far": 4.524942389649699e+16,
      "budget_used_percent": 45.24942389649699
    },
    {
      "type": "training",
      "description": "Training step 3804",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:17",
      "total_flops_so_far": 4.526130601260118e+16,
      "budget_used_percent": 45.26130601260118
    },
    {
      "type": "training",
      "description": "Training step 3805",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:17",
      "total_flops_so_far": 4.527318812870538e+16,
      "budget_used_percent": 45.27318812870538
    },
    {
      "type": "training",
      "description": "Training step 3806",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:18",
      "total_flops_so_far": 4.528507024480957e+16,
      "budget_used_percent": 45.28507024480957
    },
    {
      "type": "training",
      "description": "Training step 3807",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:19",
      "total_flops_so_far": 4.529695236091376e+16,
      "budget_used_percent": 45.296952360913764
    },
    {
      "type": "training",
      "description": "Training step 3808",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:20",
      "total_flops_so_far": 4.530883447701795e+16,
      "budget_used_percent": 45.30883447701795
    },
    {
      "type": "training",
      "description": "Training step 3809",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:20",
      "total_flops_so_far": 4.532071659312214e+16,
      "budget_used_percent": 45.32071659312214
    },
    {
      "type": "training",
      "description": "Training step 3810",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:21",
      "total_flops_so_far": 4.533259870922634e+16,
      "budget_used_percent": 45.33259870922633
    },
    {
      "type": "training",
      "description": "Training step 3811",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:22",
      "total_flops_so_far": 4.534448082533053e+16,
      "budget_used_percent": 45.34448082533053
    },
    {
      "type": "training",
      "description": "Training step 3812",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:22",
      "total_flops_so_far": 4.535636294143472e+16,
      "budget_used_percent": 45.35636294143472
    },
    {
      "type": "training",
      "description": "Training step 3813",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:23",
      "total_flops_so_far": 4.536824505753891e+16,
      "budget_used_percent": 45.368245057538914
    },
    {
      "type": "training",
      "description": "Training step 3814",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:24",
      "total_flops_so_far": 4.53801271736431e+16,
      "budget_used_percent": 45.3801271736431
    },
    {
      "type": "training",
      "description": "Training step 3815",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:24",
      "total_flops_so_far": 4.53920092897473e+16,
      "budget_used_percent": 45.3920092897473
    },
    {
      "type": "training",
      "description": "Training step 3816",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:25",
      "total_flops_so_far": 4.540389140585149e+16,
      "budget_used_percent": 45.40389140585149
    },
    {
      "type": "training",
      "description": "Training step 3817",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:26",
      "total_flops_so_far": 4.541577352195568e+16,
      "budget_used_percent": 45.415773521955686
    },
    {
      "type": "training",
      "description": "Training step 3818",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:26",
      "total_flops_so_far": 4.542765563805987e+16,
      "budget_used_percent": 45.427655638059875
    },
    {
      "type": "training",
      "description": "Training step 3819",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:27",
      "total_flops_so_far": 4.543953775416406e+16,
      "budget_used_percent": 45.439537754164064
    },
    {
      "type": "training",
      "description": "Training step 3820",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:28",
      "total_flops_so_far": 4.545141987026826e+16,
      "budget_used_percent": 45.45141987026825
    },
    {
      "type": "training",
      "description": "Training step 3821",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:28",
      "total_flops_so_far": 4.546330198637245e+16,
      "budget_used_percent": 45.46330198637244
    },
    {
      "type": "training",
      "description": "Training step 3822",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:29",
      "total_flops_so_far": 4.547518410247664e+16,
      "budget_used_percent": 45.47518410247664
    },
    {
      "type": "training",
      "description": "Training step 3823",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:30",
      "total_flops_so_far": 4.548706621858083e+16,
      "budget_used_percent": 45.48706621858083
    },
    {
      "type": "training",
      "description": "Training step 3824",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:30",
      "total_flops_so_far": 4.549894833468502e+16,
      "budget_used_percent": 45.498948334685025
    },
    {
      "type": "training",
      "description": "Training step 3825",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:31",
      "total_flops_so_far": 4.551083045078922e+16,
      "budget_used_percent": 45.510830450789214
    },
    {
      "type": "training",
      "description": "Training step 3826",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:32",
      "total_flops_so_far": 4.552271256689341e+16,
      "budget_used_percent": 45.52271256689341
    },
    {
      "type": "training",
      "description": "Training step 3827",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:32",
      "total_flops_so_far": 4.55345946829976e+16,
      "budget_used_percent": 45.5345946829976
    },
    {
      "type": "training",
      "description": "Training step 3828",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:33",
      "total_flops_so_far": 4.554647679910179e+16,
      "budget_used_percent": 45.54647679910179
    },
    {
      "type": "training",
      "description": "Training step 3829",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:34",
      "total_flops_so_far": 4.555835891520598e+16,
      "budget_used_percent": 45.55835891520598
    },
    {
      "type": "training",
      "description": "Training step 3830",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:34",
      "total_flops_so_far": 4.557024103131018e+16,
      "budget_used_percent": 45.570241031310175
    },
    {
      "type": "training",
      "description": "Training step 3831",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:35",
      "total_flops_so_far": 4.558212314741437e+16,
      "budget_used_percent": 45.582123147414364
    },
    {
      "type": "training",
      "description": "Training step 3832",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:36",
      "total_flops_so_far": 4.559400526351856e+16,
      "budget_used_percent": 45.59400526351856
    },
    {
      "type": "training",
      "description": "Training step 3833",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:36",
      "total_flops_so_far": 4.560588737962275e+16,
      "budget_used_percent": 45.60588737962275
    },
    {
      "type": "training",
      "description": "Training step 3834",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:37",
      "total_flops_so_far": 4.561776949572694e+16,
      "budget_used_percent": 45.61776949572695
    },
    {
      "type": "training",
      "description": "Training step 3835",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:38",
      "total_flops_so_far": 4.562965161183114e+16,
      "budget_used_percent": 45.629651611831136
    },
    {
      "type": "training",
      "description": "Training step 3836",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:38",
      "total_flops_so_far": 4.564153372793533e+16,
      "budget_used_percent": 45.64153372793533
    },
    {
      "type": "training",
      "description": "Training step 3837",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:39",
      "total_flops_so_far": 4.565341584403952e+16,
      "budget_used_percent": 45.65341584403952
    },
    {
      "type": "training",
      "description": "Training step 3838",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:40",
      "total_flops_so_far": 4.566529796014371e+16,
      "budget_used_percent": 45.66529796014371
    },
    {
      "type": "training",
      "description": "Training step 3839",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:40",
      "total_flops_so_far": 4.56771800762479e+16,
      "budget_used_percent": 45.6771800762479
    },
    {
      "type": "training",
      "description": "Training step 3840",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:41",
      "total_flops_so_far": 4.56890621923521e+16,
      "budget_used_percent": 45.6890621923521
    },
    {
      "type": "training",
      "description": "Training step 3841",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:42",
      "total_flops_so_far": 4.570094430845629e+16,
      "budget_used_percent": 45.700944308456286
    },
    {
      "type": "training",
      "description": "Training step 3842",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:42",
      "total_flops_so_far": 4.571282642456048e+16,
      "budget_used_percent": 45.71282642456048
    },
    {
      "type": "training",
      "description": "Training step 3843",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:43",
      "total_flops_so_far": 4.572470854066467e+16,
      "budget_used_percent": 45.72470854066467
    },
    {
      "type": "training",
      "description": "Training step 3844",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:44",
      "total_flops_so_far": 4.573659065676886e+16,
      "budget_used_percent": 45.73659065676887
    },
    {
      "type": "training",
      "description": "Training step 3845",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:44",
      "total_flops_so_far": 4.574847277287306e+16,
      "budget_used_percent": 45.74847277287306
    },
    {
      "type": "training",
      "description": "Training step 3846",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:45",
      "total_flops_so_far": 4.576035488897725e+16,
      "budget_used_percent": 45.76035488897725
    },
    {
      "type": "training",
      "description": "Training step 3847",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:46",
      "total_flops_so_far": 4.577223700508144e+16,
      "budget_used_percent": 45.772237005081436
    },
    {
      "type": "training",
      "description": "Training step 3848",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:46",
      "total_flops_so_far": 4.578411912118563e+16,
      "budget_used_percent": 45.78411912118563
    },
    {
      "type": "training",
      "description": "Training step 3849",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:47",
      "total_flops_so_far": 4.579600123728982e+16,
      "budget_used_percent": 45.79600123728982
    },
    {
      "type": "training",
      "description": "Training step 3850",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:48",
      "total_flops_so_far": 4.580788335339402e+16,
      "budget_used_percent": 45.80788335339401
    },
    {
      "type": "training",
      "description": "Training step 3851",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:48",
      "total_flops_so_far": 4.581976546949821e+16,
      "budget_used_percent": 45.81976546949821
    },
    {
      "type": "training",
      "description": "Training step 3852",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:49",
      "total_flops_so_far": 4.58316475856024e+16,
      "budget_used_percent": 45.8316475856024
    },
    {
      "type": "training",
      "description": "Training step 3853",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:50",
      "total_flops_so_far": 4.584352970170659e+16,
      "budget_used_percent": 45.84352970170659
    },
    {
      "type": "training",
      "description": "Training step 3854",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:50",
      "total_flops_so_far": 4.585541181781078e+16,
      "budget_used_percent": 45.85541181781078
    },
    {
      "type": "training",
      "description": "Training step 3855",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:51",
      "total_flops_so_far": 4.586729393391498e+16,
      "budget_used_percent": 45.86729393391498
    },
    {
      "type": "training",
      "description": "Training step 3856",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:52",
      "total_flops_so_far": 4.587917605001917e+16,
      "budget_used_percent": 45.87917605001917
    },
    {
      "type": "training",
      "description": "Training step 3857",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:53",
      "total_flops_so_far": 4.589105816612336e+16,
      "budget_used_percent": 45.89105816612336
    },
    {
      "type": "training",
      "description": "Training step 3858",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:53",
      "total_flops_so_far": 4.590294028222755e+16,
      "budget_used_percent": 45.90294028222755
    },
    {
      "type": "training",
      "description": "Training step 3859",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:54",
      "total_flops_so_far": 4.591482239833174e+16,
      "budget_used_percent": 45.914822398331744
    },
    {
      "type": "training",
      "description": "Training step 3860",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:55",
      "total_flops_so_far": 4.592670451443594e+16,
      "budget_used_percent": 45.92670451443593
    },
    {
      "type": "training",
      "description": "Training step 3861",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:55",
      "total_flops_so_far": 4.593858663054013e+16,
      "budget_used_percent": 45.93858663054013
    },
    {
      "type": "training",
      "description": "Training step 3862",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:56",
      "total_flops_so_far": 4.595046874664432e+16,
      "budget_used_percent": 45.95046874664432
    },
    {
      "type": "training",
      "description": "Training step 3863",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:57",
      "total_flops_so_far": 4.596235086274851e+16,
      "budget_used_percent": 45.962350862748515
    },
    {
      "type": "training",
      "description": "Training step 3864",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:57",
      "total_flops_so_far": 4.59742329788527e+16,
      "budget_used_percent": 45.974232978852704
    },
    {
      "type": "training",
      "description": "Training step 3865",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:58",
      "total_flops_so_far": 4.59861150949569e+16,
      "budget_used_percent": 45.9861150949569
    },
    {
      "type": "training",
      "description": "Training step 3866",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:59",
      "total_flops_so_far": 4.599799721106109e+16,
      "budget_used_percent": 45.99799721106109
    },
    {
      "type": "training",
      "description": "Training step 3867",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:52:59",
      "total_flops_so_far": 4.600987932716528e+16,
      "budget_used_percent": 46.00987932716528
    },
    {
      "type": "training",
      "description": "Training step 3868",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:00",
      "total_flops_so_far": 4.602176144326947e+16,
      "budget_used_percent": 46.02176144326947
    },
    {
      "type": "training",
      "description": "Training step 3869",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:01",
      "total_flops_so_far": 4.603364355937366e+16,
      "budget_used_percent": 46.033643559373665
    },
    {
      "type": "training",
      "description": "Training step 3870",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:01",
      "total_flops_so_far": 4.604552567547786e+16,
      "budget_used_percent": 46.045525675477855
    },
    {
      "type": "training",
      "description": "Training step 3871",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:02",
      "total_flops_so_far": 4.605740779158205e+16,
      "budget_used_percent": 46.05740779158205
    },
    {
      "type": "training",
      "description": "Training step 3872",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:03",
      "total_flops_so_far": 4.606928990768624e+16,
      "budget_used_percent": 46.06928990768624
    },
    {
      "type": "training",
      "description": "Training step 3873",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:03",
      "total_flops_so_far": 4.608117202379043e+16,
      "budget_used_percent": 46.08117202379044
    },
    {
      "type": "training",
      "description": "Training step 3874",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:04",
      "total_flops_so_far": 4.609305413989462e+16,
      "budget_used_percent": 46.093054139894626
    },
    {
      "type": "training",
      "description": "Training step 3875",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:05",
      "total_flops_so_far": 4.610493625599882e+16,
      "budget_used_percent": 46.104936255998815
    },
    {
      "type": "training",
      "description": "Training step 3876",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:05",
      "total_flops_so_far": 4.611681837210301e+16,
      "budget_used_percent": 46.116818372103005
    },
    {
      "type": "training",
      "description": "Training step 3877",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:06",
      "total_flops_so_far": 4.61287004882072e+16,
      "budget_used_percent": 46.1287004882072
    },
    {
      "type": "training",
      "description": "Training step 3878",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:07",
      "total_flops_so_far": 4.614058260431139e+16,
      "budget_used_percent": 46.14058260431139
    },
    {
      "type": "training",
      "description": "Training step 3879",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:07",
      "total_flops_so_far": 4.615246472041558e+16,
      "budget_used_percent": 46.15246472041558
    },
    {
      "type": "training",
      "description": "Training step 3880",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:08",
      "total_flops_so_far": 4.616434683651978e+16,
      "budget_used_percent": 46.164346836519776
    },
    {
      "type": "training",
      "description": "Training step 3881",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:09",
      "total_flops_so_far": 4.617622895262397e+16,
      "budget_used_percent": 46.176228952623966
    },
    {
      "type": "training",
      "description": "Training step 3882",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:09",
      "total_flops_so_far": 4.618811106872816e+16,
      "budget_used_percent": 46.18811106872816
    },
    {
      "type": "training",
      "description": "Training step 3883",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:10",
      "total_flops_so_far": 4.619999318483235e+16,
      "budget_used_percent": 46.19999318483235
    },
    {
      "type": "training",
      "description": "Training step 3884",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:11",
      "total_flops_so_far": 4.621187530093654e+16,
      "budget_used_percent": 46.21187530093655
    },
    {
      "type": "training",
      "description": "Training step 3885",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:11",
      "total_flops_so_far": 4.622375741704074e+16,
      "budget_used_percent": 46.22375741704074
    },
    {
      "type": "training",
      "description": "Training step 3886",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:12",
      "total_flops_so_far": 4.623563953314493e+16,
      "budget_used_percent": 46.235639533144926
    },
    {
      "type": "training",
      "description": "Training step 3887",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:13",
      "total_flops_so_far": 4.624752164924912e+16,
      "budget_used_percent": 46.247521649249116
    },
    {
      "type": "training",
      "description": "Training step 3888",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:13",
      "total_flops_so_far": 4.625940376535331e+16,
      "budget_used_percent": 46.25940376535331
    },
    {
      "type": "training",
      "description": "Training step 3889",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:14",
      "total_flops_so_far": 4.62712858814575e+16,
      "budget_used_percent": 46.2712858814575
    },
    {
      "type": "training",
      "description": "Training step 3890",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:15",
      "total_flops_so_far": 4.62831679975617e+16,
      "budget_used_percent": 46.2831679975617
    },
    {
      "type": "training",
      "description": "Training step 3891",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:15",
      "total_flops_so_far": 4.629505011366589e+16,
      "budget_used_percent": 46.29505011366589
    },
    {
      "type": "training",
      "description": "Training step 3892",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:16",
      "total_flops_so_far": 4.630693222977008e+16,
      "budget_used_percent": 46.306932229770084
    },
    {
      "type": "training",
      "description": "Training step 3893",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:17",
      "total_flops_so_far": 4.631881434587427e+16,
      "budget_used_percent": 46.31881434587427
    },
    {
      "type": "training",
      "description": "Training step 3894",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:17",
      "total_flops_so_far": 4.633069646197846e+16,
      "budget_used_percent": 46.33069646197846
    },
    {
      "type": "training",
      "description": "Training step 3895",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:18",
      "total_flops_so_far": 4.634257857808266e+16,
      "budget_used_percent": 46.34257857808265
    },
    {
      "type": "training",
      "description": "Training step 3896",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:19",
      "total_flops_so_far": 4.635446069418685e+16,
      "budget_used_percent": 46.35446069418685
    },
    {
      "type": "training",
      "description": "Training step 3897",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:19",
      "total_flops_so_far": 4.636634281029104e+16,
      "budget_used_percent": 46.36634281029104
    },
    {
      "type": "training",
      "description": "Training step 3898",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:20",
      "total_flops_so_far": 4.637822492639523e+16,
      "budget_used_percent": 46.378224926395234
    },
    {
      "type": "training",
      "description": "Training step 3899",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:21",
      "total_flops_so_far": 4.639010704249942e+16,
      "budget_used_percent": 46.39010704249942
    },
    {
      "type": "training",
      "description": "Training step 3900",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:21",
      "total_flops_so_far": 4.640198915860362e+16,
      "budget_used_percent": 46.40198915860362
    },
    {
      "type": "training",
      "description": "Training step 3901",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:22",
      "total_flops_so_far": 4.641387127470781e+16,
      "budget_used_percent": 46.41387127470781
    },
    {
      "type": "training",
      "description": "Training step 3902",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:23",
      "total_flops_so_far": 4.6425753390812e+16,
      "budget_used_percent": 46.425753390812005
    },
    {
      "type": "training",
      "description": "Training step 3903",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:23",
      "total_flops_so_far": 4.643763550691619e+16,
      "budget_used_percent": 46.437635506916195
    },
    {
      "type": "training",
      "description": "Training step 3904",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:24",
      "total_flops_so_far": 4.644951762302038e+16,
      "budget_used_percent": 46.449517623020384
    },
    {
      "type": "training",
      "description": "Training step 3905",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:25",
      "total_flops_so_far": 4.646139973912458e+16,
      "budget_used_percent": 46.46139973912457
    },
    {
      "type": "training",
      "description": "Training step 3906",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:26",
      "total_flops_so_far": 4.647328185522877e+16,
      "budget_used_percent": 46.47328185522877
    },
    {
      "type": "training",
      "description": "Training step 3907",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:26",
      "total_flops_so_far": 4.648516397133296e+16,
      "budget_used_percent": 46.48516397133296
    },
    {
      "type": "training",
      "description": "Training step 3908",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:27",
      "total_flops_so_far": 4.649704608743715e+16,
      "budget_used_percent": 46.49704608743715
    },
    {
      "type": "training",
      "description": "Training step 3909",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:28",
      "total_flops_so_far": 4.650892820354134e+16,
      "budget_used_percent": 46.508928203541345
    },
    {
      "type": "training",
      "description": "Training step 3910",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:28",
      "total_flops_so_far": 4.652081031964554e+16,
      "budget_used_percent": 46.520810319645534
    },
    {
      "type": "training",
      "description": "Training step 3911",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:29",
      "total_flops_so_far": 4.653269243574973e+16,
      "budget_used_percent": 46.53269243574973
    },
    {
      "type": "training",
      "description": "Training step 3912",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:30",
      "total_flops_so_far": 4.654457455185392e+16,
      "budget_used_percent": 46.54457455185392
    },
    {
      "type": "training",
      "description": "Training step 3913",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:30",
      "total_flops_so_far": 4.655645666795811e+16,
      "budget_used_percent": 46.556456667958116
    },
    {
      "type": "training",
      "description": "Training step 3914",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:31",
      "total_flops_so_far": 4.65683387840623e+16,
      "budget_used_percent": 46.568338784062306
    },
    {
      "type": "training",
      "description": "Training step 3915",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:32",
      "total_flops_so_far": 4.65802209001665e+16,
      "budget_used_percent": 46.580220900166495
    },
    {
      "type": "training",
      "description": "Training step 3916",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:32",
      "total_flops_so_far": 4.659210301627069e+16,
      "budget_used_percent": 46.592103016270684
    },
    {
      "type": "training",
      "description": "Training step 3917",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:33",
      "total_flops_so_far": 4.660398513237488e+16,
      "budget_used_percent": 46.60398513237488
    },
    {
      "type": "training",
      "description": "Training step 3918",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:34",
      "total_flops_so_far": 4.661586724847907e+16,
      "budget_used_percent": 46.61586724847907
    },
    {
      "type": "training",
      "description": "Training step 3919",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:34",
      "total_flops_so_far": 4.662774936458326e+16,
      "budget_used_percent": 46.627749364583266
    },
    {
      "type": "training",
      "description": "Training step 3920",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:35",
      "total_flops_so_far": 4.663963148068746e+16,
      "budget_used_percent": 46.639631480687456
    },
    {
      "type": "training",
      "description": "Training step 3921",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:36",
      "total_flops_so_far": 4.665151359679165e+16,
      "budget_used_percent": 46.65151359679165
    },
    {
      "type": "training",
      "description": "Training step 3922",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:37",
      "total_flops_so_far": 4.666339571289584e+16,
      "budget_used_percent": 46.66339571289584
    },
    {
      "type": "training",
      "description": "Training step 3923",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:37",
      "total_flops_so_far": 4.667527782900003e+16,
      "budget_used_percent": 46.67527782900003
    },
    {
      "type": "training",
      "description": "Training step 3924",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:38",
      "total_flops_so_far": 4.668715994510422e+16,
      "budget_used_percent": 46.68715994510422
    },
    {
      "type": "training",
      "description": "Training step 3925",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:39",
      "total_flops_so_far": 4.669904206120842e+16,
      "budget_used_percent": 46.69904206120842
    },
    {
      "type": "training",
      "description": "Training step 3926",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:39",
      "total_flops_so_far": 4.671092417731261e+16,
      "budget_used_percent": 46.710924177312606
    },
    {
      "type": "training",
      "description": "Training step 3927",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:40",
      "total_flops_so_far": 4.67228062934168e+16,
      "budget_used_percent": 46.7228062934168
    },
    {
      "type": "training",
      "description": "Training step 3928",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:41",
      "total_flops_so_far": 4.673468840952099e+16,
      "budget_used_percent": 46.73468840952099
    },
    {
      "type": "training",
      "description": "Training step 3929",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:41",
      "total_flops_so_far": 4.674657052562518e+16,
      "budget_used_percent": 46.74657052562519
    },
    {
      "type": "training",
      "description": "Training step 3930",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:42",
      "total_flops_so_far": 4.675845264172938e+16,
      "budget_used_percent": 46.75845264172938
    },
    {
      "type": "training",
      "description": "Training step 3931",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:43",
      "total_flops_so_far": 4.677033475783357e+16,
      "budget_used_percent": 46.770334757833574
    },
    {
      "type": "training",
      "description": "Training step 3932",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:43",
      "total_flops_so_far": 4.678221687393776e+16,
      "budget_used_percent": 46.78221687393776
    },
    {
      "type": "training",
      "description": "Training step 3933",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:44",
      "total_flops_so_far": 4.679409899004195e+16,
      "budget_used_percent": 46.79409899004195
    },
    {
      "type": "training",
      "description": "Training step 3934",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:45",
      "total_flops_so_far": 4.680598110614614e+16,
      "budget_used_percent": 46.80598110614614
    },
    {
      "type": "training",
      "description": "Training step 3935",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:45",
      "total_flops_so_far": 4.681786322225034e+16,
      "budget_used_percent": 46.81786322225033
    },
    {
      "type": "training",
      "description": "Training step 3936",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:46",
      "total_flops_so_far": 4.682974533835453e+16,
      "budget_used_percent": 46.82974533835453
    },
    {
      "type": "training",
      "description": "Training step 3937",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:47",
      "total_flops_so_far": 4.684162745445872e+16,
      "budget_used_percent": 46.84162745445872
    },
    {
      "type": "training",
      "description": "Training step 3938",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:47",
      "total_flops_so_far": 4.685350957056291e+16,
      "budget_used_percent": 46.85350957056291
    },
    {
      "type": "training",
      "description": "Training step 3939",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:48",
      "total_flops_so_far": 4.68653916866671e+16,
      "budget_used_percent": 46.8653916866671
    },
    {
      "type": "training",
      "description": "Training step 3940",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:49",
      "total_flops_so_far": 4.68772738027713e+16,
      "budget_used_percent": 46.8772738027713
    },
    {
      "type": "training",
      "description": "Training step 3941",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:49",
      "total_flops_so_far": 4.688915591887549e+16,
      "budget_used_percent": 46.88915591887549
    },
    {
      "type": "training",
      "description": "Training step 3942",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:50",
      "total_flops_so_far": 4.690103803497968e+16,
      "budget_used_percent": 46.90103803497968
    },
    {
      "type": "training",
      "description": "Training step 3943",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:51",
      "total_flops_so_far": 4.691292015108387e+16,
      "budget_used_percent": 46.91292015108387
    },
    {
      "type": "training",
      "description": "Training step 3944",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:51",
      "total_flops_so_far": 4.692480226718806e+16,
      "budget_used_percent": 46.92480226718806
    },
    {
      "type": "training",
      "description": "Training step 3945",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:52",
      "total_flops_so_far": 4.693668438329226e+16,
      "budget_used_percent": 46.93668438329225
    },
    {
      "type": "training",
      "description": "Training step 3946",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:52",
      "total_flops_so_far": 4.694856649939645e+16,
      "budget_used_percent": 46.94856649939645
    },
    {
      "type": "training",
      "description": "Training step 3947",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:53",
      "total_flops_so_far": 4.696044861550064e+16,
      "budget_used_percent": 46.96044861550064
    },
    {
      "type": "training",
      "description": "Training step 3948",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:54",
      "total_flops_so_far": 4.697233073160483e+16,
      "budget_used_percent": 46.972330731604835
    },
    {
      "type": "training",
      "description": "Training step 3949",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:54",
      "total_flops_so_far": 4.698421284770902e+16,
      "budget_used_percent": 46.984212847709024
    },
    {
      "type": "training",
      "description": "Training step 3950",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:55",
      "total_flops_so_far": 4.699609496381322e+16,
      "budget_used_percent": 46.99609496381322
    },
    {
      "type": "training",
      "description": "Training step 3951",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:56",
      "total_flops_so_far": 4.700797707991741e+16,
      "budget_used_percent": 47.00797707991741
    },
    {
      "type": "training",
      "description": "Training step 3952",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:56",
      "total_flops_so_far": 4.70198591960216e+16,
      "budget_used_percent": 47.0198591960216
    },
    {
      "type": "training",
      "description": "Training step 3953",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:57",
      "total_flops_so_far": 4.703174131212579e+16,
      "budget_used_percent": 47.03174131212579
    },
    {
      "type": "training",
      "description": "Training step 3954",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:58",
      "total_flops_so_far": 4.704362342822998e+16,
      "budget_used_percent": 47.043623428229985
    },
    {
      "type": "training",
      "description": "Training step 3955",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:58",
      "total_flops_so_far": 4.705550554433418e+16,
      "budget_used_percent": 47.055505544334174
    },
    {
      "type": "training",
      "description": "Training step 3956",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:53:59",
      "total_flops_so_far": 4.706738766043837e+16,
      "budget_used_percent": 47.06738766043837
    },
    {
      "type": "training",
      "description": "Training step 3957",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:00",
      "total_flops_so_far": 4.707926977654256e+16,
      "budget_used_percent": 47.07926977654256
    },
    {
      "type": "training",
      "description": "Training step 3958",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:01",
      "total_flops_so_far": 4.709115189264675e+16,
      "budget_used_percent": 47.09115189264676
    },
    {
      "type": "training",
      "description": "Training step 3959",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:01",
      "total_flops_so_far": 4.710303400875094e+16,
      "budget_used_percent": 47.103034008750946
    },
    {
      "type": "training",
      "description": "Training step 3960",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:02",
      "total_flops_so_far": 4.711491612485514e+16,
      "budget_used_percent": 47.114916124855135
    },
    {
      "type": "training",
      "description": "Training step 3961",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:03",
      "total_flops_so_far": 4.712679824095933e+16,
      "budget_used_percent": 47.126798240959324
    },
    {
      "type": "training",
      "description": "Training step 3962",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:03",
      "total_flops_so_far": 4.713868035706352e+16,
      "budget_used_percent": 47.13868035706352
    },
    {
      "type": "training",
      "description": "Training step 3963",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:04",
      "total_flops_so_far": 4.715056247316771e+16,
      "budget_used_percent": 47.15056247316771
    },
    {
      "type": "training",
      "description": "Training step 3964",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:05",
      "total_flops_so_far": 4.71624445892719e+16,
      "budget_used_percent": 47.1624445892719
    },
    {
      "type": "training",
      "description": "Training step 3965",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:05",
      "total_flops_so_far": 4.71743267053761e+16,
      "budget_used_percent": 47.174326705376096
    },
    {
      "type": "training",
      "description": "Training step 3966",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:06",
      "total_flops_so_far": 4.718620882148029e+16,
      "budget_used_percent": 47.186208821480285
    },
    {
      "type": "training",
      "description": "Training step 3967",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:07",
      "total_flops_so_far": 4.719809093758448e+16,
      "budget_used_percent": 47.19809093758448
    },
    {
      "type": "training",
      "description": "Training step 3968",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:07",
      "total_flops_so_far": 4.720997305368867e+16,
      "budget_used_percent": 47.20997305368867
    },
    {
      "type": "training",
      "description": "Training step 3969",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:08",
      "total_flops_so_far": 4.722185516979286e+16,
      "budget_used_percent": 47.22185516979287
    },
    {
      "type": "training",
      "description": "Training step 3970",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:09",
      "total_flops_so_far": 4.723373728589706e+16,
      "budget_used_percent": 47.23373728589706
    },
    {
      "type": "training",
      "description": "Training step 3971",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:09",
      "total_flops_so_far": 4.724561940200125e+16,
      "budget_used_percent": 47.245619402001246
    },
    {
      "type": "training",
      "description": "Training step 3972",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:10",
      "total_flops_so_far": 4.725750151810544e+16,
      "budget_used_percent": 47.257501518105435
    },
    {
      "type": "training",
      "description": "Training step 3973",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:11",
      "total_flops_so_far": 4.726938363420963e+16,
      "budget_used_percent": 47.26938363420963
    },
    {
      "type": "training",
      "description": "Training step 3974",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:11",
      "total_flops_so_far": 4.728126575031382e+16,
      "budget_used_percent": 47.28126575031382
    },
    {
      "type": "training",
      "description": "Training step 3975",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:12",
      "total_flops_so_far": 4.729314786641802e+16,
      "budget_used_percent": 47.29314786641802
    },
    {
      "type": "training",
      "description": "Training step 3976",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:13",
      "total_flops_so_far": 4.730502998252221e+16,
      "budget_used_percent": 47.30502998252221
    },
    {
      "type": "training",
      "description": "Training step 3977",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:13",
      "total_flops_so_far": 4.73169120986264e+16,
      "budget_used_percent": 47.3169120986264
    },
    {
      "type": "training",
      "description": "Training step 3978",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:14",
      "total_flops_so_far": 4.732879421473059e+16,
      "budget_used_percent": 47.32879421473059
    },
    {
      "type": "training",
      "description": "Training step 3979",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:15",
      "total_flops_so_far": 4.734067633083478e+16,
      "budget_used_percent": 47.34067633083478
    },
    {
      "type": "training",
      "description": "Training step 3980",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:15",
      "total_flops_so_far": 4.735255844693898e+16,
      "budget_used_percent": 47.35255844693897
    },
    {
      "type": "training",
      "description": "Training step 3981",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:16",
      "total_flops_so_far": 4.736444056304317e+16,
      "budget_used_percent": 47.36444056304317
    },
    {
      "type": "training",
      "description": "Training step 3982",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:17",
      "total_flops_so_far": 4.737632267914736e+16,
      "budget_used_percent": 47.37632267914736
    },
    {
      "type": "training",
      "description": "Training step 3983",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:17",
      "total_flops_so_far": 4.738820479525155e+16,
      "budget_used_percent": 47.38820479525155
    },
    {
      "type": "training",
      "description": "Training step 3984",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:18",
      "total_flops_so_far": 4.740008691135574e+16,
      "budget_used_percent": 47.40008691135574
    },
    {
      "type": "training",
      "description": "Training step 3985",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:19",
      "total_flops_so_far": 4.741196902745994e+16,
      "budget_used_percent": 47.41196902745994
    },
    {
      "type": "training",
      "description": "Training step 3986",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:19",
      "total_flops_so_far": 4.742385114356413e+16,
      "budget_used_percent": 47.42385114356413
    },
    {
      "type": "training",
      "description": "Training step 3987",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:20",
      "total_flops_so_far": 4.743573325966832e+16,
      "budget_used_percent": 47.435733259668325
    },
    {
      "type": "training",
      "description": "Training step 3988",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:21",
      "total_flops_so_far": 4.744761537577251e+16,
      "budget_used_percent": 47.447615375772514
    },
    {
      "type": "training",
      "description": "Training step 3989",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:22",
      "total_flops_so_far": 4.74594974918767e+16,
      "budget_used_percent": 47.459497491876704
    },
    {
      "type": "training",
      "description": "Training step 3990",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:22",
      "total_flops_so_far": 4.74713796079809e+16,
      "budget_used_percent": 47.47137960798089
    },
    {
      "type": "training",
      "description": "Training step 3991",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:23",
      "total_flops_so_far": 4.748326172408509e+16,
      "budget_used_percent": 47.48326172408509
    },
    {
      "type": "training",
      "description": "Training step 3992",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:24",
      "total_flops_so_far": 4.749514384018928e+16,
      "budget_used_percent": 47.49514384018928
    },
    {
      "type": "training",
      "description": "Training step 3993",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:24",
      "total_flops_so_far": 4.750702595629347e+16,
      "budget_used_percent": 47.50702595629347
    },
    {
      "type": "training",
      "description": "Training step 3994",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:25",
      "total_flops_so_far": 4.751890807239766e+16,
      "budget_used_percent": 47.518908072397664
    },
    {
      "type": "training",
      "description": "Training step 3995",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:26",
      "total_flops_so_far": 4.753079018850186e+16,
      "budget_used_percent": 47.530790188501854
    },
    {
      "type": "training",
      "description": "Training step 3996",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:26",
      "total_flops_so_far": 4.754267230460605e+16,
      "budget_used_percent": 47.54267230460605
    },
    {
      "type": "training",
      "description": "Training step 3997",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:27",
      "total_flops_so_far": 4.755455442071024e+16,
      "budget_used_percent": 47.55455442071024
    },
    {
      "type": "training",
      "description": "Training step 3998",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:28",
      "total_flops_so_far": 4.756643653681443e+16,
      "budget_used_percent": 47.566436536814436
    },
    {
      "type": "training",
      "description": "Training step 3999",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:28",
      "total_flops_so_far": 4.757831865291862e+16,
      "budget_used_percent": 47.578318652918625
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:36",
      "total_flops_so_far": 4.757902928205648e+16,
      "budget_used_percent": 47.57902928205648
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 714333709232.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:44",
      "total_flops_so_far": 4.757974361576571e+16,
      "budget_used_percent": 47.579743615765715
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:51",
      "total_flops_so_far": 4.7580456096829064e+16,
      "budget_used_percent": 47.58045609682907
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:54:59",
      "total_flops_so_far": 4.758116672596692e+16,
      "budget_used_percent": 47.58116672596692
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 713407296244.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:06",
      "total_flops_so_far": 4.758188013326317e+16,
      "budget_used_percent": 47.58188013326317
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 5",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:13",
      "total_flops_so_far": 4.758259076240102e+16,
      "budget_used_percent": 47.582590762401026
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 6",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:21",
      "total_flops_so_far": 4.7583303243464376e+16,
      "budget_used_percent": 47.58330324346438
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 7",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:28",
      "total_flops_so_far": 4.758401572452773e+16,
      "budget_used_percent": 47.58401572452773
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 8",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:35",
      "total_flops_so_far": 4.758472820559108e+16,
      "budget_used_percent": 47.584728205591084
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 9",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:43",
      "total_flops_so_far": 4.758544068665443e+16,
      "budget_used_percent": 47.58544068665444
    },
    {
      "type": "training",
      "description": "Training step 4000",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:43",
      "total_flops_so_far": 4.759732280275862e+16,
      "budget_used_percent": 47.597322802758626
    },
    {
      "type": "training",
      "description": "Training step 4001",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:44",
      "total_flops_so_far": 4.760920491886282e+16,
      "budget_used_percent": 47.609204918862815
    },
    {
      "type": "training",
      "description": "Training step 4002",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:44",
      "total_flops_so_far": 4.762108703496701e+16,
      "budget_used_percent": 47.621087034967005
    },
    {
      "type": "training",
      "description": "Training step 4003",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:45",
      "total_flops_so_far": 4.76329691510712e+16,
      "budget_used_percent": 47.6329691510712
    },
    {
      "type": "training",
      "description": "Training step 4004",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:46",
      "total_flops_so_far": 4.764485126717539e+16,
      "budget_used_percent": 47.64485126717539
    },
    {
      "type": "training",
      "description": "Training step 4005",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:46",
      "total_flops_so_far": 4.765673338327958e+16,
      "budget_used_percent": 47.65673338327958
    },
    {
      "type": "training",
      "description": "Training step 4006",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:47",
      "total_flops_so_far": 4.766861549938378e+16,
      "budget_used_percent": 47.668615499383776
    },
    {
      "type": "training",
      "description": "Training step 4007",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:48",
      "total_flops_so_far": 4.768049761548797e+16,
      "budget_used_percent": 47.680497615487965
    },
    {
      "type": "training",
      "description": "Training step 4008",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:48",
      "total_flops_so_far": 4.769237973159216e+16,
      "budget_used_percent": 47.69237973159216
    },
    {
      "type": "training",
      "description": "Training step 4009",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:49",
      "total_flops_so_far": 4.770426184769635e+16,
      "budget_used_percent": 47.70426184769635
    },
    {
      "type": "training",
      "description": "Training step 4010",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:50",
      "total_flops_so_far": 4.771614396380054e+16,
      "budget_used_percent": 47.71614396380055
    },
    {
      "type": "training",
      "description": "Training step 4011",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:50",
      "total_flops_so_far": 4.772802607990474e+16,
      "budget_used_percent": 47.72802607990474
    },
    {
      "type": "training",
      "description": "Training step 4012",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:51",
      "total_flops_so_far": 4.773990819600893e+16,
      "budget_used_percent": 47.739908196008926
    },
    {
      "type": "training",
      "description": "Training step 4013",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:52",
      "total_flops_so_far": 4.775179031211312e+16,
      "budget_used_percent": 47.751790312113116
    },
    {
      "type": "training",
      "description": "Training step 4014",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:52",
      "total_flops_so_far": 4.776367242821731e+16,
      "budget_used_percent": 47.76367242821731
    },
    {
      "type": "training",
      "description": "Training step 4015",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:53",
      "total_flops_so_far": 4.77755545443215e+16,
      "budget_used_percent": 47.7755545443215
    },
    {
      "type": "training",
      "description": "Training step 4016",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:54",
      "total_flops_so_far": 4.77874366604257e+16,
      "budget_used_percent": 47.7874366604257
    },
    {
      "type": "training",
      "description": "Training step 4017",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:54",
      "total_flops_so_far": 4.779931877652989e+16,
      "budget_used_percent": 47.79931877652989
    },
    {
      "type": "training",
      "description": "Training step 4018",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:55",
      "total_flops_so_far": 4.781120089263408e+16,
      "budget_used_percent": 47.81120089263408
    },
    {
      "type": "training",
      "description": "Training step 4019",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:56",
      "total_flops_so_far": 4.782308300873827e+16,
      "budget_used_percent": 47.82308300873827
    },
    {
      "type": "training",
      "description": "Training step 4020",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:56",
      "total_flops_so_far": 4.783496512484246e+16,
      "budget_used_percent": 47.83496512484247
    },
    {
      "type": "training",
      "description": "Training step 4021",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:57",
      "total_flops_so_far": 4.784684724094666e+16,
      "budget_used_percent": 47.84684724094666
    },
    {
      "type": "training",
      "description": "Training step 4022",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:58",
      "total_flops_so_far": 4.785872935705085e+16,
      "budget_used_percent": 47.85872935705085
    },
    {
      "type": "training",
      "description": "Training step 4023",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:58",
      "total_flops_so_far": 4.787061147315504e+16,
      "budget_used_percent": 47.87061147315504
    },
    {
      "type": "training",
      "description": "Training step 4024",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:55:59",
      "total_flops_so_far": 4.788249358925923e+16,
      "budget_used_percent": 47.882493589259234
    },
    {
      "type": "training",
      "description": "Training step 4025",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:00",
      "total_flops_so_far": 4.789437570536342e+16,
      "budget_used_percent": 47.89437570536342
    },
    {
      "type": "training",
      "description": "Training step 4026",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:01",
      "total_flops_so_far": 4.790625782146762e+16,
      "budget_used_percent": 47.90625782146762
    },
    {
      "type": "training",
      "description": "Training step 4027",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:01",
      "total_flops_so_far": 4.791813993757181e+16,
      "budget_used_percent": 47.91813993757181
    },
    {
      "type": "training",
      "description": "Training step 4028",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:02",
      "total_flops_so_far": 4.7930022053676e+16,
      "budget_used_percent": 47.930022053676005
    },
    {
      "type": "training",
      "description": "Training step 4029",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:03",
      "total_flops_so_far": 4.794190416978019e+16,
      "budget_used_percent": 47.941904169780194
    },
    {
      "type": "training",
      "description": "Training step 4030",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:03",
      "total_flops_so_far": 4.795378628588438e+16,
      "budget_used_percent": 47.953786285884384
    },
    {
      "type": "training",
      "description": "Training step 4031",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:04",
      "total_flops_so_far": 4.796566840198858e+16,
      "budget_used_percent": 47.96566840198857
    },
    {
      "type": "training",
      "description": "Training step 4032",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:05",
      "total_flops_so_far": 4.797755051809277e+16,
      "budget_used_percent": 47.97755051809276
    },
    {
      "type": "training",
      "description": "Training step 4033",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:05",
      "total_flops_so_far": 4.798943263419696e+16,
      "budget_used_percent": 47.98943263419696
    },
    {
      "type": "training",
      "description": "Training step 4034",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:06",
      "total_flops_so_far": 4.800131475030115e+16,
      "budget_used_percent": 48.00131475030115
    },
    {
      "type": "training",
      "description": "Training step 4035",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:07",
      "total_flops_so_far": 4.801319686640534e+16,
      "budget_used_percent": 48.013196866405345
    },
    {
      "type": "training",
      "description": "Training step 4036",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:07",
      "total_flops_so_far": 4.802507898250954e+16,
      "budget_used_percent": 48.025078982509534
    },
    {
      "type": "training",
      "description": "Training step 4037",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:08",
      "total_flops_so_far": 4.803696109861373e+16,
      "budget_used_percent": 48.03696109861373
    },
    {
      "type": "training",
      "description": "Training step 4038",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:09",
      "total_flops_so_far": 4.804884321471792e+16,
      "budget_used_percent": 48.04884321471792
    },
    {
      "type": "training",
      "description": "Training step 4039",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:09",
      "total_flops_so_far": 4.806072533082211e+16,
      "budget_used_percent": 48.060725330822116
    },
    {
      "type": "training",
      "description": "Training step 4040",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:10",
      "total_flops_so_far": 4.80726074469263e+16,
      "budget_used_percent": 48.072607446926305
    },
    {
      "type": "training",
      "description": "Training step 4041",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:11",
      "total_flops_so_far": 4.80844895630305e+16,
      "budget_used_percent": 48.084489563030495
    },
    {
      "type": "training",
      "description": "Training step 4042",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:11",
      "total_flops_so_far": 4.809637167913469e+16,
      "budget_used_percent": 48.096371679134684
    },
    {
      "type": "training",
      "description": "Training step 4043",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:12",
      "total_flops_so_far": 4.810825379523888e+16,
      "budget_used_percent": 48.10825379523888
    },
    {
      "type": "training",
      "description": "Training step 4044",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:13",
      "total_flops_so_far": 4.812013591134307e+16,
      "budget_used_percent": 48.12013591134307
    },
    {
      "type": "training",
      "description": "Training step 4045",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:13",
      "total_flops_so_far": 4.813201802744726e+16,
      "budget_used_percent": 48.132018027447266
    },
    {
      "type": "training",
      "description": "Training step 4046",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:14",
      "total_flops_so_far": 4.814390014355146e+16,
      "budget_used_percent": 48.143900143551456
    },
    {
      "type": "training",
      "description": "Training step 4047",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:15",
      "total_flops_so_far": 4.815578225965565e+16,
      "budget_used_percent": 48.15578225965565
    },
    {
      "type": "training",
      "description": "Training step 4048",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:16",
      "total_flops_so_far": 4.816766437575984e+16,
      "budget_used_percent": 48.16766437575984
    },
    {
      "type": "training",
      "description": "Training step 4049",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:16",
      "total_flops_so_far": 4.817954649186403e+16,
      "budget_used_percent": 48.17954649186403
    },
    {
      "type": "training",
      "description": "Training step 4050",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:17",
      "total_flops_so_far": 4.819142860796822e+16,
      "budget_used_percent": 48.19142860796822
    },
    {
      "type": "training",
      "description": "Training step 4051",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:18",
      "total_flops_so_far": 4.820331072407242e+16,
      "budget_used_percent": 48.203310724072416
    },
    {
      "type": "training",
      "description": "Training step 4052",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:18",
      "total_flops_so_far": 4.821519284017661e+16,
      "budget_used_percent": 48.215192840176606
    },
    {
      "type": "training",
      "description": "Training step 4053",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:19",
      "total_flops_so_far": 4.82270749562808e+16,
      "budget_used_percent": 48.2270749562808
    },
    {
      "type": "training",
      "description": "Training step 4054",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:20",
      "total_flops_so_far": 4.823895707238499e+16,
      "budget_used_percent": 48.23895707238499
    },
    {
      "type": "training",
      "description": "Training step 4055",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:20",
      "total_flops_so_far": 4.825083918848918e+16,
      "budget_used_percent": 48.25083918848919
    },
    {
      "type": "training",
      "description": "Training step 4056",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:21",
      "total_flops_so_far": 4.826272130459338e+16,
      "budget_used_percent": 48.26272130459338
    },
    {
      "type": "training",
      "description": "Training step 4057",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:22",
      "total_flops_so_far": 4.827460342069757e+16,
      "budget_used_percent": 48.274603420697574
    },
    {
      "type": "training",
      "description": "Training step 4058",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:22",
      "total_flops_so_far": 4.828648553680176e+16,
      "budget_used_percent": 48.28648553680176
    },
    {
      "type": "training",
      "description": "Training step 4059",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:23",
      "total_flops_so_far": 4.829836765290595e+16,
      "budget_used_percent": 48.29836765290595
    },
    {
      "type": "training",
      "description": "Training step 4060",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:24",
      "total_flops_so_far": 4.831024976901014e+16,
      "budget_used_percent": 48.31024976901014
    },
    {
      "type": "training",
      "description": "Training step 4061",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:24",
      "total_flops_so_far": 4.832213188511434e+16,
      "budget_used_percent": 48.32213188511433
    },
    {
      "type": "training",
      "description": "Training step 4062",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:25",
      "total_flops_so_far": 4.833401400121853e+16,
      "budget_used_percent": 48.33401400121853
    },
    {
      "type": "training",
      "description": "Training step 4063",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:26",
      "total_flops_so_far": 4.834589611732272e+16,
      "budget_used_percent": 48.34589611732272
    },
    {
      "type": "training",
      "description": "Training step 4064",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:26",
      "total_flops_so_far": 4.835777823342691e+16,
      "budget_used_percent": 48.35777823342691
    },
    {
      "type": "training",
      "description": "Training step 4065",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:27",
      "total_flops_so_far": 4.83696603495311e+16,
      "budget_used_percent": 48.3696603495311
    },
    {
      "type": "training",
      "description": "Training step 4066",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:28",
      "total_flops_so_far": 4.83815424656353e+16,
      "budget_used_percent": 48.3815424656353
    },
    {
      "type": "training",
      "description": "Training step 4067",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:28",
      "total_flops_so_far": 4.839342458173949e+16,
      "budget_used_percent": 48.39342458173949
    },
    {
      "type": "training",
      "description": "Training step 4068",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:29",
      "total_flops_so_far": 4.840530669784368e+16,
      "budget_used_percent": 48.40530669784368
    },
    {
      "type": "training",
      "description": "Training step 4069",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:30",
      "total_flops_so_far": 4.841718881394787e+16,
      "budget_used_percent": 48.41718881394787
    },
    {
      "type": "training",
      "description": "Training step 4070",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:30",
      "total_flops_so_far": 4.842907093005206e+16,
      "budget_used_percent": 48.42907093005206
    },
    {
      "type": "training",
      "description": "Training step 4071",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:31",
      "total_flops_so_far": 4.844095304615626e+16,
      "budget_used_percent": 48.44095304615625
    },
    {
      "type": "training",
      "description": "Training step 4072",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:32",
      "total_flops_so_far": 4.845283516226045e+16,
      "budget_used_percent": 48.45283516226045
    },
    {
      "type": "training",
      "description": "Training step 4073",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:32",
      "total_flops_so_far": 4.846471727836464e+16,
      "budget_used_percent": 48.46471727836464
    },
    {
      "type": "training",
      "description": "Training step 4074",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:33",
      "total_flops_so_far": 4.847659939446883e+16,
      "budget_used_percent": 48.476599394468835
    },
    {
      "type": "training",
      "description": "Training step 4075",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:34",
      "total_flops_so_far": 4.848848151057302e+16,
      "budget_used_percent": 48.488481510573024
    },
    {
      "type": "training",
      "description": "Training step 4076",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:34",
      "total_flops_so_far": 4.850036362667722e+16,
      "budget_used_percent": 48.50036362667722
    },
    {
      "type": "training",
      "description": "Training step 4077",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:35",
      "total_flops_so_far": 4.851224574278141e+16,
      "budget_used_percent": 48.51224574278141
    },
    {
      "type": "training",
      "description": "Training step 4078",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:36",
      "total_flops_so_far": 4.85241278588856e+16,
      "budget_used_percent": 48.5241278588856
    },
    {
      "type": "training",
      "description": "Training step 4079",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:37",
      "total_flops_so_far": 4.853600997498979e+16,
      "budget_used_percent": 48.53600997498979
    },
    {
      "type": "training",
      "description": "Training step 4080",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:37",
      "total_flops_so_far": 4.854789209109398e+16,
      "budget_used_percent": 48.547892091093985
    },
    {
      "type": "training",
      "description": "Training step 4081",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:38",
      "total_flops_so_far": 4.855977420719818e+16,
      "budget_used_percent": 48.559774207198174
    },
    {
      "type": "training",
      "description": "Training step 4082",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:39",
      "total_flops_so_far": 4.857165632330237e+16,
      "budget_used_percent": 48.57165632330237
    },
    {
      "type": "training",
      "description": "Training step 4083",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:39",
      "total_flops_so_far": 4.858353843940656e+16,
      "budget_used_percent": 48.58353843940656
    },
    {
      "type": "training",
      "description": "Training step 4084",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:40",
      "total_flops_so_far": 4.859542055551075e+16,
      "budget_used_percent": 48.595420555510756
    },
    {
      "type": "training",
      "description": "Training step 4085",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:41",
      "total_flops_so_far": 4.860730267161494e+16,
      "budget_used_percent": 48.607302671614946
    },
    {
      "type": "training",
      "description": "Training step 4086",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:41",
      "total_flops_so_far": 4.861918478771914e+16,
      "budget_used_percent": 48.619184787719135
    },
    {
      "type": "training",
      "description": "Training step 4087",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:42",
      "total_flops_so_far": 4.863106690382333e+16,
      "budget_used_percent": 48.631066903823324
    },
    {
      "type": "training",
      "description": "Training step 4088",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:43",
      "total_flops_so_far": 4.864294901992752e+16,
      "budget_used_percent": 48.64294901992752
    },
    {
      "type": "training",
      "description": "Training step 4089",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:43",
      "total_flops_so_far": 4.865483113603171e+16,
      "budget_used_percent": 48.65483113603171
    },
    {
      "type": "training",
      "description": "Training step 4090",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:44",
      "total_flops_so_far": 4.86667132521359e+16,
      "budget_used_percent": 48.6667132521359
    },
    {
      "type": "training",
      "description": "Training step 4091",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:45",
      "total_flops_so_far": 4.86785953682401e+16,
      "budget_used_percent": 48.678595368240096
    },
    {
      "type": "training",
      "description": "Training step 4092",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:45",
      "total_flops_so_far": 4.869047748434429e+16,
      "budget_used_percent": 48.690477484344285
    },
    {
      "type": "training",
      "description": "Training step 4093",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:46",
      "total_flops_so_far": 4.870235960044848e+16,
      "budget_used_percent": 48.70235960044848
    },
    {
      "type": "training",
      "description": "Training step 4094",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:47",
      "total_flops_so_far": 4.871424171655267e+16,
      "budget_used_percent": 48.71424171655267
    },
    {
      "type": "training",
      "description": "Training step 4095",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:47",
      "total_flops_so_far": 4.872612383265686e+16,
      "budget_used_percent": 48.72612383265687
    },
    {
      "type": "training",
      "description": "Training step 4096",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:48",
      "total_flops_so_far": 4.873800594876106e+16,
      "budget_used_percent": 48.73800594876106
    },
    {
      "type": "training",
      "description": "Training step 4097",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:49",
      "total_flops_so_far": 4.874988806486525e+16,
      "budget_used_percent": 48.749888064865246
    },
    {
      "type": "training",
      "description": "Training step 4098",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:49",
      "total_flops_so_far": 4.876177018096944e+16,
      "budget_used_percent": 48.761770180969435
    },
    {
      "type": "training",
      "description": "Training step 4099",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:50",
      "total_flops_so_far": 4.877365229707363e+16,
      "budget_used_percent": 48.77365229707363
    },
    {
      "type": "training",
      "description": "Training step 4100",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:51",
      "total_flops_so_far": 4.878553441317782e+16,
      "budget_used_percent": 48.78553441317782
    },
    {
      "type": "training",
      "description": "Training step 4101",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:52",
      "total_flops_so_far": 4.879741652928202e+16,
      "budget_used_percent": 48.79741652928202
    },
    {
      "type": "training",
      "description": "Training step 4102",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:52",
      "total_flops_so_far": 4.880929864538621e+16,
      "budget_used_percent": 48.80929864538621
    },
    {
      "type": "training",
      "description": "Training step 4103",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:53",
      "total_flops_so_far": 4.88211807614904e+16,
      "budget_used_percent": 48.8211807614904
    },
    {
      "type": "training",
      "description": "Training step 4104",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:54",
      "total_flops_so_far": 4.883306287759459e+16,
      "budget_used_percent": 48.83306287759459
    },
    {
      "type": "training",
      "description": "Training step 4105",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:54",
      "total_flops_so_far": 4.884494499369878e+16,
      "budget_used_percent": 48.84494499369879
    },
    {
      "type": "training",
      "description": "Training step 4106",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:55",
      "total_flops_so_far": 4.885682710980298e+16,
      "budget_used_percent": 48.85682710980298
    },
    {
      "type": "training",
      "description": "Training step 4107",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:56",
      "total_flops_so_far": 4.886870922590717e+16,
      "budget_used_percent": 48.86870922590717
    },
    {
      "type": "training",
      "description": "Training step 4108",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:56",
      "total_flops_so_far": 4.888059134201136e+16,
      "budget_used_percent": 48.88059134201136
    },
    {
      "type": "training",
      "description": "Training step 4109",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:57",
      "total_flops_so_far": 4.889247345811555e+16,
      "budget_used_percent": 48.89247345811555
    },
    {
      "type": "training",
      "description": "Training step 4110",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:58",
      "total_flops_so_far": 4.890435557421974e+16,
      "budget_used_percent": 48.90435557421974
    },
    {
      "type": "training",
      "description": "Training step 4111",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:58",
      "total_flops_so_far": 4.891623769032394e+16,
      "budget_used_percent": 48.91623769032394
    },
    {
      "type": "training",
      "description": "Training step 4112",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:56:59",
      "total_flops_so_far": 4.892811980642813e+16,
      "budget_used_percent": 48.92811980642813
    },
    {
      "type": "training",
      "description": "Training step 4113",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:00",
      "total_flops_so_far": 4.894000192253232e+16,
      "budget_used_percent": 48.940001922532325
    },
    {
      "type": "training",
      "description": "Training step 4114",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:00",
      "total_flops_so_far": 4.895188403863651e+16,
      "budget_used_percent": 48.951884038636514
    },
    {
      "type": "training",
      "description": "Training step 4115",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:01",
      "total_flops_so_far": 4.89637661547407e+16,
      "budget_used_percent": 48.9637661547407
    },
    {
      "type": "training",
      "description": "Training step 4116",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:02",
      "total_flops_so_far": 4.89756482708449e+16,
      "budget_used_percent": 48.97564827084489
    },
    {
      "type": "training",
      "description": "Training step 4117",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:02",
      "total_flops_so_far": 4.898753038694909e+16,
      "budget_used_percent": 48.98753038694908
    },
    {
      "type": "training",
      "description": "Training step 4118",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:03",
      "total_flops_so_far": 4.899941250305328e+16,
      "budget_used_percent": 48.99941250305328
    },
    {
      "type": "training",
      "description": "Training step 4119",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:04",
      "total_flops_so_far": 4.901129461915747e+16,
      "budget_used_percent": 49.01129461915747
    },
    {
      "type": "training",
      "description": "Training step 4120",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:05",
      "total_flops_so_far": 4.902317673526166e+16,
      "budget_used_percent": 49.023176735261664
    },
    {
      "type": "training",
      "description": "Training step 4121",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:05",
      "total_flops_so_far": 4.903505885136586e+16,
      "budget_used_percent": 49.035058851365854
    },
    {
      "type": "training",
      "description": "Training step 4122",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:06",
      "total_flops_so_far": 4.904694096747005e+16,
      "budget_used_percent": 49.04694096747005
    },
    {
      "type": "training",
      "description": "Training step 4123",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:07",
      "total_flops_so_far": 4.905882308357424e+16,
      "budget_used_percent": 49.05882308357424
    },
    {
      "type": "training",
      "description": "Training step 4124",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:07",
      "total_flops_so_far": 4.907070519967843e+16,
      "budget_used_percent": 49.070705199678436
    },
    {
      "type": "training",
      "description": "Training step 4125",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:08",
      "total_flops_so_far": 4.908258731578262e+16,
      "budget_used_percent": 49.082587315782625
    },
    {
      "type": "training",
      "description": "Training step 4126",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:09",
      "total_flops_so_far": 4.909446943188682e+16,
      "budget_used_percent": 49.094469431886814
    },
    {
      "type": "training",
      "description": "Training step 4127",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:09",
      "total_flops_so_far": 4.910635154799101e+16,
      "budget_used_percent": 49.106351547991004
    },
    {
      "type": "training",
      "description": "Training step 4128",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:10",
      "total_flops_so_far": 4.91182336640952e+16,
      "budget_used_percent": 49.1182336640952
    },
    {
      "type": "training",
      "description": "Training step 4129",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:11",
      "total_flops_so_far": 4.913011578019939e+16,
      "budget_used_percent": 49.13011578019939
    },
    {
      "type": "training",
      "description": "Training step 4130",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:11",
      "total_flops_so_far": 4.914199789630358e+16,
      "budget_used_percent": 49.141997896303586
    },
    {
      "type": "training",
      "description": "Training step 4131",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:12",
      "total_flops_so_far": 4.915388001240778e+16,
      "budget_used_percent": 49.153880012407775
    },
    {
      "type": "training",
      "description": "Training step 4132",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:13",
      "total_flops_so_far": 4.916576212851197e+16,
      "budget_used_percent": 49.16576212851197
    },
    {
      "type": "training",
      "description": "Training step 4133",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:13",
      "total_flops_so_far": 4.917764424461616e+16,
      "budget_used_percent": 49.17764424461616
    },
    {
      "type": "training",
      "description": "Training step 4134",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:14",
      "total_flops_so_far": 4.918952636072035e+16,
      "budget_used_percent": 49.18952636072035
    },
    {
      "type": "training",
      "description": "Training step 4135",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:15",
      "total_flops_so_far": 4.920140847682454e+16,
      "budget_used_percent": 49.20140847682454
    },
    {
      "type": "training",
      "description": "Training step 4136",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:15",
      "total_flops_so_far": 4.921329059292874e+16,
      "budget_used_percent": 49.213290592928736
    },
    {
      "type": "training",
      "description": "Training step 4137",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:16",
      "total_flops_so_far": 4.922517270903293e+16,
      "budget_used_percent": 49.225172709032925
    },
    {
      "type": "training",
      "description": "Training step 4138",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:17",
      "total_flops_so_far": 4.923705482513712e+16,
      "budget_used_percent": 49.23705482513712
    },
    {
      "type": "training",
      "description": "Training step 4139",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:17",
      "total_flops_so_far": 4.924893694124131e+16,
      "budget_used_percent": 49.24893694124131
    },
    {
      "type": "training",
      "description": "Training step 4140",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:18",
      "total_flops_so_far": 4.92608190573455e+16,
      "budget_used_percent": 49.26081905734551
    },
    {
      "type": "training",
      "description": "Training step 4141",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:19",
      "total_flops_so_far": 4.92727011734497e+16,
      "budget_used_percent": 49.2727011734497
    },
    {
      "type": "training",
      "description": "Training step 4142",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:19",
      "total_flops_so_far": 4.928458328955389e+16,
      "budget_used_percent": 49.28458328955389
    },
    {
      "type": "training",
      "description": "Training step 4143",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:20",
      "total_flops_so_far": 4.929646540565808e+16,
      "budget_used_percent": 49.29646540565808
    },
    {
      "type": "training",
      "description": "Training step 4144",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:21",
      "total_flops_so_far": 4.930834752176227e+16,
      "budget_used_percent": 49.30834752176227
    },
    {
      "type": "training",
      "description": "Training step 4145",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:21",
      "total_flops_so_far": 4.932022963786646e+16,
      "budget_used_percent": 49.32022963786646
    },
    {
      "type": "training",
      "description": "Training step 4146",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:22",
      "total_flops_so_far": 4.933211175397066e+16,
      "budget_used_percent": 49.33211175397065
    },
    {
      "type": "training",
      "description": "Training step 4147",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:23",
      "total_flops_so_far": 4.934399387007485e+16,
      "budget_used_percent": 49.34399387007485
    },
    {
      "type": "training",
      "description": "Training step 4148",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:23",
      "total_flops_so_far": 4.935587598617904e+16,
      "budget_used_percent": 49.355875986179036
    },
    {
      "type": "training",
      "description": "Training step 4149",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:24",
      "total_flops_so_far": 4.936775810228323e+16,
      "budget_used_percent": 49.36775810228323
    },
    {
      "type": "training",
      "description": "Training step 4150",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:25",
      "total_flops_so_far": 4.937964021838742e+16,
      "budget_used_percent": 49.37964021838742
    },
    {
      "type": "training",
      "description": "Training step 4151",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:25",
      "total_flops_so_far": 4.939152233449162e+16,
      "budget_used_percent": 49.39152233449162
    },
    {
      "type": "training",
      "description": "Training step 4152",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:26",
      "total_flops_so_far": 4.940340445059581e+16,
      "budget_used_percent": 49.40340445059581
    },
    {
      "type": "training",
      "description": "Training step 4153",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:27",
      "total_flops_so_far": 4.94152865667e+16,
      "budget_used_percent": 49.415286566700004
    },
    {
      "type": "training",
      "description": "Training step 4154",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:27",
      "total_flops_so_far": 4.942716868280419e+16,
      "budget_used_percent": 49.427168682804194
    },
    {
      "type": "training",
      "description": "Training step 4155",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:28",
      "total_flops_so_far": 4.943905079890838e+16,
      "budget_used_percent": 49.43905079890838
    },
    {
      "type": "training",
      "description": "Training step 4156",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:29",
      "total_flops_so_far": 4.945093291501258e+16,
      "budget_used_percent": 49.45093291501257
    },
    {
      "type": "training",
      "description": "Training step 4157",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:30",
      "total_flops_so_far": 4.946281503111677e+16,
      "budget_used_percent": 49.46281503111677
    },
    {
      "type": "training",
      "description": "Training step 4158",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:30",
      "total_flops_so_far": 4.947469714722096e+16,
      "budget_used_percent": 49.47469714722096
    },
    {
      "type": "training",
      "description": "Training step 4159",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:31",
      "total_flops_so_far": 4.948657926332515e+16,
      "budget_used_percent": 49.486579263325154
    },
    {
      "type": "training",
      "description": "Training step 4160",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:32",
      "total_flops_so_far": 4.949846137942934e+16,
      "budget_used_percent": 49.498461379429344
    },
    {
      "type": "training",
      "description": "Training step 4161",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:32",
      "total_flops_so_far": 4.951034349553354e+16,
      "budget_used_percent": 49.51034349553354
    },
    {
      "type": "training",
      "description": "Training step 4162",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:33",
      "total_flops_so_far": 4.952222561163773e+16,
      "budget_used_percent": 49.52222561163773
    },
    {
      "type": "training",
      "description": "Training step 4163",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:34",
      "total_flops_so_far": 4.953410772774192e+16,
      "budget_used_percent": 49.53410772774192
    },
    {
      "type": "training",
      "description": "Training step 4164",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:34",
      "total_flops_so_far": 4.954598984384611e+16,
      "budget_used_percent": 49.54598984384611
    },
    {
      "type": "training",
      "description": "Training step 4165",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:35",
      "total_flops_so_far": 4.95578719599503e+16,
      "budget_used_percent": 49.557871959950305
    },
    {
      "type": "training",
      "description": "Training step 4166",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:36",
      "total_flops_so_far": 4.95697540760545e+16,
      "budget_used_percent": 49.569754076054494
    },
    {
      "type": "training",
      "description": "Training step 4167",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:36",
      "total_flops_so_far": 4.958163619215869e+16,
      "budget_used_percent": 49.58163619215869
    },
    {
      "type": "training",
      "description": "Training step 4168",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:37",
      "total_flops_so_far": 4.959351830826288e+16,
      "budget_used_percent": 49.59351830826288
    },
    {
      "type": "training",
      "description": "Training step 4169",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:38",
      "total_flops_so_far": 4.960540042436707e+16,
      "budget_used_percent": 49.605400424367076
    },
    {
      "type": "training",
      "description": "Training step 4170",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:38",
      "total_flops_so_far": 4.961728254047126e+16,
      "budget_used_percent": 49.617282540471265
    },
    {
      "type": "training",
      "description": "Training step 4171",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:39",
      "total_flops_so_far": 4.962916465657546e+16,
      "budget_used_percent": 49.62916465657546
    },
    {
      "type": "training",
      "description": "Training step 4172",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:40",
      "total_flops_so_far": 4.964104677267965e+16,
      "budget_used_percent": 49.64104677267965
    },
    {
      "type": "training",
      "description": "Training step 4173",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:40",
      "total_flops_so_far": 4.965292888878384e+16,
      "budget_used_percent": 49.65292888878384
    },
    {
      "type": "training",
      "description": "Training step 4174",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:41",
      "total_flops_so_far": 4.966481100488803e+16,
      "budget_used_percent": 49.66481100488803
    },
    {
      "type": "training",
      "description": "Training step 4175",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:42",
      "total_flops_so_far": 4.967669312099222e+16,
      "budget_used_percent": 49.67669312099222
    },
    {
      "type": "training",
      "description": "Training step 4176",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:42",
      "total_flops_so_far": 4.968857523709642e+16,
      "budget_used_percent": 49.688575237096416
    },
    {
      "type": "training",
      "description": "Training step 4177",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:43",
      "total_flops_so_far": 4.970045735320061e+16,
      "budget_used_percent": 49.700457353200605
    },
    {
      "type": "training",
      "description": "Training step 4178",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:44",
      "total_flops_so_far": 4.97123394693048e+16,
      "budget_used_percent": 49.7123394693048
    },
    {
      "type": "training",
      "description": "Training step 4179",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:44",
      "total_flops_so_far": 4.972422158540899e+16,
      "budget_used_percent": 49.72422158540899
    },
    {
      "type": "training",
      "description": "Training step 4180",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:45",
      "total_flops_so_far": 4.973610370151318e+16,
      "budget_used_percent": 49.73610370151319
    },
    {
      "type": "training",
      "description": "Training step 4181",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:46",
      "total_flops_so_far": 4.974798581761738e+16,
      "budget_used_percent": 49.747985817617376
    },
    {
      "type": "training",
      "description": "Training step 4182",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:46",
      "total_flops_so_far": 4.975986793372157e+16,
      "budget_used_percent": 49.759867933721566
    },
    {
      "type": "training",
      "description": "Training step 4183",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:47",
      "total_flops_so_far": 4.977175004982576e+16,
      "budget_used_percent": 49.771750049825755
    },
    {
      "type": "training",
      "description": "Training step 4184",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:48",
      "total_flops_so_far": 4.978363216592995e+16,
      "budget_used_percent": 49.78363216592995
    },
    {
      "type": "training",
      "description": "Training step 4185",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:48",
      "total_flops_so_far": 4.979551428203414e+16,
      "budget_used_percent": 49.79551428203414
    },
    {
      "type": "training",
      "description": "Training step 4186",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:49",
      "total_flops_so_far": 4.980739639813834e+16,
      "budget_used_percent": 49.80739639813834
    },
    {
      "type": "training",
      "description": "Training step 4187",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:50",
      "total_flops_so_far": 4.981927851424253e+16,
      "budget_used_percent": 49.81927851424253
    },
    {
      "type": "training",
      "description": "Training step 4188",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:50",
      "total_flops_so_far": 4.983116063034672e+16,
      "budget_used_percent": 49.83116063034672
    },
    {
      "type": "training",
      "description": "Training step 4189",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:51",
      "total_flops_so_far": 4.984304274645091e+16,
      "budget_used_percent": 49.84304274645091
    },
    {
      "type": "training",
      "description": "Training step 4190",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:52",
      "total_flops_so_far": 4.98549248625551e+16,
      "budget_used_percent": 49.85492486255511
    },
    {
      "type": "training",
      "description": "Training step 4191",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:53",
      "total_flops_so_far": 4.98668069786593e+16,
      "budget_used_percent": 49.8668069786593
    },
    {
      "type": "training",
      "description": "Training step 4192",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:53",
      "total_flops_so_far": 4.987868909476349e+16,
      "budget_used_percent": 49.87868909476349
    },
    {
      "type": "training",
      "description": "Training step 4193",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:54",
      "total_flops_so_far": 4.989057121086768e+16,
      "budget_used_percent": 49.89057121086768
    },
    {
      "type": "training",
      "description": "Training step 4194",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:55",
      "total_flops_so_far": 4.990245332697187e+16,
      "budget_used_percent": 49.90245332697187
    },
    {
      "type": "training",
      "description": "Training step 4195",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:56",
      "total_flops_so_far": 4.991433544307606e+16,
      "budget_used_percent": 49.91433544307606
    },
    {
      "type": "training",
      "description": "Training step 4196",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:56",
      "total_flops_so_far": 4.992621755918026e+16,
      "budget_used_percent": 49.92621755918026
    },
    {
      "type": "training",
      "description": "Training step 4197",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:57",
      "total_flops_so_far": 4.993809967528445e+16,
      "budget_used_percent": 49.93809967528445
    },
    {
      "type": "training",
      "description": "Training step 4198",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:58",
      "total_flops_so_far": 4.994998179138864e+16,
      "budget_used_percent": 49.949981791388645
    },
    {
      "type": "training",
      "description": "Training step 4199",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:58",
      "total_flops_so_far": 4.996186390749283e+16,
      "budget_used_percent": 49.961863907492834
    },
    {
      "type": "training",
      "description": "Training step 4200",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:57:59",
      "total_flops_so_far": 4.997374602359702e+16,
      "budget_used_percent": 49.97374602359702
    },
    {
      "type": "training",
      "description": "Training step 4201",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:00",
      "total_flops_so_far": 4.998562813970122e+16,
      "budget_used_percent": 49.98562813970121
    },
    {
      "type": "training",
      "description": "Training step 4202",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:00",
      "total_flops_so_far": 4.999751025580541e+16,
      "budget_used_percent": 49.99751025580541
    },
    {
      "type": "training",
      "description": "Training step 4203",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:01",
      "total_flops_so_far": 5.00093923719096e+16,
      "budget_used_percent": 50.0093923719096
    },
    {
      "type": "training",
      "description": "Training step 4204",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:02",
      "total_flops_so_far": 5.002127448801379e+16,
      "budget_used_percent": 50.02127448801379
    },
    {
      "type": "training",
      "description": "Training step 4205",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:02",
      "total_flops_so_far": 5.003315660411798e+16,
      "budget_used_percent": 50.03315660411798
    },
    {
      "type": "training",
      "description": "Training step 4206",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:03",
      "total_flops_so_far": 5.004503872022218e+16,
      "budget_used_percent": 50.04503872022218
    },
    {
      "type": "training",
      "description": "Training step 4207",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:04",
      "total_flops_so_far": 5.005692083632637e+16,
      "budget_used_percent": 50.05692083632637
    },
    {
      "type": "training",
      "description": "Training step 4208",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:04",
      "total_flops_so_far": 5.006880295243056e+16,
      "budget_used_percent": 50.06880295243056
    },
    {
      "type": "training",
      "description": "Training step 4209",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:05",
      "total_flops_so_far": 5.008068506853475e+16,
      "budget_used_percent": 50.08068506853475
    },
    {
      "type": "training",
      "description": "Training step 4210",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:05",
      "total_flops_so_far": 5.009256718463894e+16,
      "budget_used_percent": 50.092567184638945
    },
    {
      "type": "training",
      "description": "Training step 4211",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:06",
      "total_flops_so_far": 5.010444930074314e+16,
      "budget_used_percent": 50.104449300743134
    },
    {
      "type": "training",
      "description": "Training step 4212",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:07",
      "total_flops_so_far": 5.011633141684733e+16,
      "budget_used_percent": 50.11633141684732
    },
    {
      "type": "training",
      "description": "Training step 4213",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:07",
      "total_flops_so_far": 5.012821353295152e+16,
      "budget_used_percent": 50.12821353295151
    },
    {
      "type": "training",
      "description": "Training step 4214",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:08",
      "total_flops_so_far": 5.014009564905571e+16,
      "budget_used_percent": 50.140095649055716
    },
    {
      "type": "training",
      "description": "Training step 4215",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:09",
      "total_flops_so_far": 5.01519777651599e+16,
      "budget_used_percent": 50.151977765159906
    },
    {
      "type": "training",
      "description": "Training step 4216",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:09",
      "total_flops_so_far": 5.01638598812641e+16,
      "budget_used_percent": 50.163859881264095
    },
    {
      "type": "training",
      "description": "Training step 4217",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:10",
      "total_flops_so_far": 5.017574199736829e+16,
      "budget_used_percent": 50.175741997368284
    },
    {
      "type": "training",
      "description": "Training step 4218",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:11",
      "total_flops_so_far": 5.018762411347248e+16,
      "budget_used_percent": 50.18762411347248
    },
    {
      "type": "training",
      "description": "Training step 4219",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:12",
      "total_flops_so_far": 5.019950622957667e+16,
      "budget_used_percent": 50.19950622957667
    },
    {
      "type": "training",
      "description": "Training step 4220",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:12",
      "total_flops_so_far": 5.021138834568086e+16,
      "budget_used_percent": 50.21138834568086
    },
    {
      "type": "training",
      "description": "Training step 4221",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:13",
      "total_flops_so_far": 5.022327046178506e+16,
      "budget_used_percent": 50.22327046178505
    },
    {
      "type": "training",
      "description": "Training step 4222",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:14",
      "total_flops_so_far": 5.023515257788925e+16,
      "budget_used_percent": 50.23515257788925
    },
    {
      "type": "training",
      "description": "Training step 4223",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:14",
      "total_flops_so_far": 5.024703469399344e+16,
      "budget_used_percent": 50.24703469399344
    },
    {
      "type": "training",
      "description": "Training step 4224",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:15",
      "total_flops_so_far": 5.025891681009763e+16,
      "budget_used_percent": 50.25891681009763
    },
    {
      "type": "training",
      "description": "Training step 4225",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:16",
      "total_flops_so_far": 5.027079892620182e+16,
      "budget_used_percent": 50.27079892620182
    },
    {
      "type": "training",
      "description": "Training step 4226",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:16",
      "total_flops_so_far": 5.028268104230602e+16,
      "budget_used_percent": 50.282681042306024
    },
    {
      "type": "training",
      "description": "Training step 4227",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:17",
      "total_flops_so_far": 5.029456315841021e+16,
      "budget_used_percent": 50.29456315841021
    },
    {
      "type": "training",
      "description": "Training step 4228",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:18",
      "total_flops_so_far": 5.03064452745144e+16,
      "budget_used_percent": 50.3064452745144
    },
    {
      "type": "training",
      "description": "Training step 4229",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:18",
      "total_flops_so_far": 5.031832739061859e+16,
      "budget_used_percent": 50.31832739061859
    },
    {
      "type": "training",
      "description": "Training step 4230",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:19",
      "total_flops_so_far": 5.033020950672278e+16,
      "budget_used_percent": 50.33020950672279
    },
    {
      "type": "training",
      "description": "Training step 4231",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:20",
      "total_flops_so_far": 5.034209162282698e+16,
      "budget_used_percent": 50.34209162282698
    },
    {
      "type": "training",
      "description": "Training step 4232",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:20",
      "total_flops_so_far": 5.035397373893117e+16,
      "budget_used_percent": 50.35397373893117
    },
    {
      "type": "training",
      "description": "Training step 4233",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:21",
      "total_flops_so_far": 5.036585585503536e+16,
      "budget_used_percent": 50.365855855035356
    },
    {
      "type": "training",
      "description": "Training step 4234",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:22",
      "total_flops_so_far": 5.037773797113955e+16,
      "budget_used_percent": 50.377737971139545
    },
    {
      "type": "training",
      "description": "Training step 4235",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:22",
      "total_flops_so_far": 5.038962008724374e+16,
      "budget_used_percent": 50.38962008724375
    },
    {
      "type": "training",
      "description": "Training step 4236",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:23",
      "total_flops_so_far": 5.040150220334794e+16,
      "budget_used_percent": 50.40150220334794
    },
    {
      "type": "training",
      "description": "Training step 4237",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:24",
      "total_flops_so_far": 5.041338431945213e+16,
      "budget_used_percent": 50.41338431945213
    },
    {
      "type": "training",
      "description": "Training step 4238",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:24",
      "total_flops_so_far": 5.042526643555632e+16,
      "budget_used_percent": 50.42526643555632
    },
    {
      "type": "training",
      "description": "Training step 4239",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:25",
      "total_flops_so_far": 5.043714855166051e+16,
      "budget_used_percent": 50.43714855166051
    },
    {
      "type": "training",
      "description": "Training step 4240",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:26",
      "total_flops_so_far": 5.04490306677647e+16,
      "budget_used_percent": 50.4490306677647
    },
    {
      "type": "training",
      "description": "Training step 4241",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:26",
      "total_flops_so_far": 5.04609127838689e+16,
      "budget_used_percent": 50.46091278386889
    },
    {
      "type": "training",
      "description": "Training step 4242",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:27",
      "total_flops_so_far": 5.047279489997309e+16,
      "budget_used_percent": 50.47279489997308
    },
    {
      "type": "training",
      "description": "Training step 4243",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:28",
      "total_flops_so_far": 5.048467701607728e+16,
      "budget_used_percent": 50.484677016077285
    },
    {
      "type": "training",
      "description": "Training step 4244",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:28",
      "total_flops_so_far": 5.049655913218147e+16,
      "budget_used_percent": 50.496559132181474
    },
    {
      "type": "training",
      "description": "Training step 4245",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:29",
      "total_flops_so_far": 5.050844124828566e+16,
      "budget_used_percent": 50.50844124828566
    },
    {
      "type": "training",
      "description": "Training step 4246",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:30",
      "total_flops_so_far": 5.052032336438986e+16,
      "budget_used_percent": 50.52032336438985
    },
    {
      "type": "training",
      "description": "Training step 4247",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:30",
      "total_flops_so_far": 5.053220548049405e+16,
      "budget_used_percent": 50.53220548049405
    },
    {
      "type": "training",
      "description": "Training step 4248",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:31",
      "total_flops_so_far": 5.054408759659824e+16,
      "budget_used_percent": 50.54408759659824
    },
    {
      "type": "training",
      "description": "Training step 4249",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:32",
      "total_flops_so_far": 5.055596971270243e+16,
      "budget_used_percent": 50.55596971270243
    },
    {
      "type": "training",
      "description": "Training step 4250",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:33",
      "total_flops_so_far": 5.056785182880662e+16,
      "budget_used_percent": 50.56785182880662
    },
    {
      "type": "training",
      "description": "Training step 4251",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:33",
      "total_flops_so_far": 5.057973394491082e+16,
      "budget_used_percent": 50.57973394491082
    },
    {
      "type": "training",
      "description": "Training step 4252",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:34",
      "total_flops_so_far": 5.059161606101501e+16,
      "budget_used_percent": 50.59161606101501
    },
    {
      "type": "training",
      "description": "Training step 4253",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:35",
      "total_flops_so_far": 5.06034981771192e+16,
      "budget_used_percent": 50.6034981771192
    },
    {
      "type": "training",
      "description": "Training step 4254",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:35",
      "total_flops_so_far": 5.061538029322339e+16,
      "budget_used_percent": 50.61538029322339
    },
    {
      "type": "training",
      "description": "Training step 4255",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:36",
      "total_flops_so_far": 5.062726240932758e+16,
      "budget_used_percent": 50.62726240932759
    },
    {
      "type": "training",
      "description": "Training step 4256",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:37",
      "total_flops_so_far": 5.063914452543178e+16,
      "budget_used_percent": 50.63914452543178
    },
    {
      "type": "training",
      "description": "Training step 4257",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:37",
      "total_flops_so_far": 5.065102664153597e+16,
      "budget_used_percent": 50.65102664153597
    },
    {
      "type": "training",
      "description": "Training step 4258",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:38",
      "total_flops_so_far": 5.066290875764016e+16,
      "budget_used_percent": 50.66290875764016
    },
    {
      "type": "training",
      "description": "Training step 4259",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:39",
      "total_flops_so_far": 5.067479087374435e+16,
      "budget_used_percent": 50.67479087374436
    },
    {
      "type": "training",
      "description": "Training step 4260",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:39",
      "total_flops_so_far": 5.068667298984854e+16,
      "budget_used_percent": 50.686672989848546
    },
    {
      "type": "training",
      "description": "Training step 4261",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:40",
      "total_flops_so_far": 5.069855510595274e+16,
      "budget_used_percent": 50.698555105952735
    },
    {
      "type": "training",
      "description": "Training step 4262",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:41",
      "total_flops_so_far": 5.071043722205693e+16,
      "budget_used_percent": 50.710437222056925
    },
    {
      "type": "training",
      "description": "Training step 4263",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:41",
      "total_flops_so_far": 5.072231933816112e+16,
      "budget_used_percent": 50.722319338161114
    },
    {
      "type": "training",
      "description": "Training step 4264",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:42",
      "total_flops_so_far": 5.073420145426531e+16,
      "budget_used_percent": 50.73420145426532
    },
    {
      "type": "training",
      "description": "Training step 4265",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:43",
      "total_flops_so_far": 5.07460835703695e+16,
      "budget_used_percent": 50.74608357036951
    },
    {
      "type": "training",
      "description": "Training step 4266",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:43",
      "total_flops_so_far": 5.07579656864737e+16,
      "budget_used_percent": 50.757965686473696
    },
    {
      "type": "training",
      "description": "Training step 4267",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:44",
      "total_flops_so_far": 5.076984780257789e+16,
      "budget_used_percent": 50.769847802577885
    },
    {
      "type": "training",
      "description": "Training step 4268",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:45",
      "total_flops_so_far": 5.078172991868208e+16,
      "budget_used_percent": 50.78172991868208
    },
    {
      "type": "training",
      "description": "Training step 4269",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:45",
      "total_flops_so_far": 5.079361203478627e+16,
      "budget_used_percent": 50.79361203478627
    },
    {
      "type": "training",
      "description": "Training step 4270",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:46",
      "total_flops_so_far": 5.080549415089046e+16,
      "budget_used_percent": 50.80549415089046
    },
    {
      "type": "training",
      "description": "Training step 4271",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:47",
      "total_flops_so_far": 5.081737626699466e+16,
      "budget_used_percent": 50.81737626699465
    },
    {
      "type": "training",
      "description": "Training step 4272",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:47",
      "total_flops_so_far": 5.082925838309885e+16,
      "budget_used_percent": 50.82925838309885
    },
    {
      "type": "training",
      "description": "Training step 4273",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:48",
      "total_flops_so_far": 5.084114049920304e+16,
      "budget_used_percent": 50.84114049920304
    },
    {
      "type": "training",
      "description": "Training step 4274",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:49",
      "total_flops_so_far": 5.085302261530723e+16,
      "budget_used_percent": 50.85302261530723
    },
    {
      "type": "training",
      "description": "Training step 4275",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:50",
      "total_flops_so_far": 5.086490473141142e+16,
      "budget_used_percent": 50.86490473141142
    },
    {
      "type": "training",
      "description": "Training step 4276",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:50",
      "total_flops_so_far": 5.087678684751562e+16,
      "budget_used_percent": 50.87678684751562
    },
    {
      "type": "training",
      "description": "Training step 4277",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:51",
      "total_flops_so_far": 5.088866896361981e+16,
      "budget_used_percent": 50.88866896361981
    },
    {
      "type": "training",
      "description": "Training step 4278",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:52",
      "total_flops_so_far": 5.0900551079724e+16,
      "budget_used_percent": 50.900551079723996
    },
    {
      "type": "training",
      "description": "Training step 4279",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:52",
      "total_flops_so_far": 5.091243319582819e+16,
      "budget_used_percent": 50.912433195828186
    },
    {
      "type": "training",
      "description": "Training step 4280",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:53",
      "total_flops_so_far": 5.092431531193238e+16,
      "budget_used_percent": 50.92431531193239
    },
    {
      "type": "training",
      "description": "Training step 4281",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:54",
      "total_flops_so_far": 5.093619742803658e+16,
      "budget_used_percent": 50.93619742803658
    },
    {
      "type": "training",
      "description": "Training step 4282",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:54",
      "total_flops_so_far": 5.094807954414077e+16,
      "budget_used_percent": 50.94807954414077
    },
    {
      "type": "training",
      "description": "Training step 4283",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:55",
      "total_flops_so_far": 5.095996166024496e+16,
      "budget_used_percent": 50.95996166024496
    },
    {
      "type": "training",
      "description": "Training step 4284",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:56",
      "total_flops_so_far": 5.097184377634915e+16,
      "budget_used_percent": 50.97184377634916
    },
    {
      "type": "training",
      "description": "Training step 4285",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:56",
      "total_flops_so_far": 5.098372589245334e+16,
      "budget_used_percent": 50.98372589245335
    },
    {
      "type": "training",
      "description": "Training step 4286",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:57",
      "total_flops_so_far": 5.099560800855754e+16,
      "budget_used_percent": 50.99560800855754
    },
    {
      "type": "training",
      "description": "Training step 4287",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:58",
      "total_flops_so_far": 5.100749012466173e+16,
      "budget_used_percent": 51.00749012466173
    },
    {
      "type": "training",
      "description": "Training step 4288",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:58",
      "total_flops_so_far": 5.101937224076592e+16,
      "budget_used_percent": 51.01937224076592
    },
    {
      "type": "training",
      "description": "Training step 4289",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:58:59",
      "total_flops_so_far": 5.103125435687011e+16,
      "budget_used_percent": 51.031254356870114
    },
    {
      "type": "training",
      "description": "Training step 4290",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:00",
      "total_flops_so_far": 5.10431364729743e+16,
      "budget_used_percent": 51.043136472974304
    },
    {
      "type": "training",
      "description": "Training step 4291",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:00",
      "total_flops_so_far": 5.10550185890785e+16,
      "budget_used_percent": 51.05501858907849
    },
    {
      "type": "training",
      "description": "Training step 4292",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:01",
      "total_flops_so_far": 5.106690070518269e+16,
      "budget_used_percent": 51.06690070518268
    },
    {
      "type": "training",
      "description": "Training step 4293",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:02",
      "total_flops_so_far": 5.107878282128688e+16,
      "budget_used_percent": 51.078782821286886
    },
    {
      "type": "training",
      "description": "Training step 4294",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:02",
      "total_flops_so_far": 5.109066493739107e+16,
      "budget_used_percent": 51.090664937391075
    },
    {
      "type": "training",
      "description": "Training step 4295",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:03",
      "total_flops_so_far": 5.110254705349526e+16,
      "budget_used_percent": 51.102547053495265
    },
    {
      "type": "training",
      "description": "Training step 4296",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:04",
      "total_flops_so_far": 5.111442916959946e+16,
      "budget_used_percent": 51.114429169599454
    },
    {
      "type": "training",
      "description": "Training step 4297",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:04",
      "total_flops_so_far": 5.112631128570365e+16,
      "budget_used_percent": 51.12631128570365
    },
    {
      "type": "training",
      "description": "Training step 4298",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:05",
      "total_flops_so_far": 5.113819340180784e+16,
      "budget_used_percent": 51.13819340180784
    },
    {
      "type": "training",
      "description": "Training step 4299",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:06",
      "total_flops_so_far": 5.115007551791203e+16,
      "budget_used_percent": 51.15007551791203
    },
    {
      "type": "training",
      "description": "Training step 4300",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:06",
      "total_flops_so_far": 5.116195763401622e+16,
      "budget_used_percent": 51.16195763401622
    },
    {
      "type": "training",
      "description": "Training step 4301",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:07",
      "total_flops_so_far": 5.117383975012042e+16,
      "budget_used_percent": 51.17383975012042
    },
    {
      "type": "training",
      "description": "Training step 4302",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:08",
      "total_flops_so_far": 5.118572186622461e+16,
      "budget_used_percent": 51.18572186622461
    },
    {
      "type": "training",
      "description": "Training step 4303",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:09",
      "total_flops_so_far": 5.11976039823288e+16,
      "budget_used_percent": 51.1976039823288
    },
    {
      "type": "training",
      "description": "Training step 4304",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:09",
      "total_flops_so_far": 5.120948609843299e+16,
      "budget_used_percent": 51.20948609843299
    },
    {
      "type": "training",
      "description": "Training step 4305",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:10",
      "total_flops_so_far": 5.122136821453718e+16,
      "budget_used_percent": 51.221368214537186
    },
    {
      "type": "training",
      "description": "Training step 4306",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:11",
      "total_flops_so_far": 5.123325033064138e+16,
      "budget_used_percent": 51.233250330641376
    },
    {
      "type": "training",
      "description": "Training step 4307",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:11",
      "total_flops_so_far": 5.124513244674557e+16,
      "budget_used_percent": 51.245132446745565
    },
    {
      "type": "training",
      "description": "Training step 4308",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:12",
      "total_flops_so_far": 5.125701456284976e+16,
      "budget_used_percent": 51.257014562849754
    },
    {
      "type": "training",
      "description": "Training step 4309",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:13",
      "total_flops_so_far": 5.126889667895395e+16,
      "budget_used_percent": 51.26889667895396
    },
    {
      "type": "training",
      "description": "Training step 4310",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:13",
      "total_flops_so_far": 5.128077879505814e+16,
      "budget_used_percent": 51.28077879505815
    },
    {
      "type": "training",
      "description": "Training step 4311",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:14",
      "total_flops_so_far": 5.129266091116234e+16,
      "budget_used_percent": 51.292660911162336
    },
    {
      "type": "training",
      "description": "Training step 4312",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:15",
      "total_flops_so_far": 5.130454302726653e+16,
      "budget_used_percent": 51.304543027266526
    },
    {
      "type": "training",
      "description": "Training step 4313",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:15",
      "total_flops_so_far": 5.131642514337072e+16,
      "budget_used_percent": 51.31642514337072
    },
    {
      "type": "training",
      "description": "Training step 4314",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:16",
      "total_flops_so_far": 5.132830725947491e+16,
      "budget_used_percent": 51.32830725947491
    },
    {
      "type": "training",
      "description": "Training step 4315",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:17",
      "total_flops_so_far": 5.13401893755791e+16,
      "budget_used_percent": 51.3401893755791
    },
    {
      "type": "training",
      "description": "Training step 4316",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:18",
      "total_flops_so_far": 5.13520714916833e+16,
      "budget_used_percent": 51.35207149168329
    },
    {
      "type": "training",
      "description": "Training step 4317",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:18",
      "total_flops_so_far": 5.136395360778749e+16,
      "budget_used_percent": 51.36395360778748
    },
    {
      "type": "training",
      "description": "Training step 4318",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:19",
      "total_flops_so_far": 5.137583572389168e+16,
      "budget_used_percent": 51.37583572389168
    },
    {
      "type": "training",
      "description": "Training step 4319",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:20",
      "total_flops_so_far": 5.138771783999587e+16,
      "budget_used_percent": 51.38771783999587
    },
    {
      "type": "training",
      "description": "Training step 4320",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:20",
      "total_flops_so_far": 5.139959995610006e+16,
      "budget_used_percent": 51.39959995610006
    },
    {
      "type": "training",
      "description": "Training step 4321",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:21",
      "total_flops_so_far": 5.141148207220426e+16,
      "budget_used_percent": 51.41148207220425
    },
    {
      "type": "training",
      "description": "Training step 4322",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:22",
      "total_flops_so_far": 5.142336418830845e+16,
      "budget_used_percent": 51.423364188308454
    },
    {
      "type": "training",
      "description": "Training step 4323",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:22",
      "total_flops_so_far": 5.143524630441264e+16,
      "budget_used_percent": 51.435246304412644
    },
    {
      "type": "training",
      "description": "Training step 4324",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:23",
      "total_flops_so_far": 5.144712842051683e+16,
      "budget_used_percent": 51.44712842051683
    },
    {
      "type": "training",
      "description": "Training step 4325",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:24",
      "total_flops_so_far": 5.145901053662102e+16,
      "budget_used_percent": 51.45901053662102
    },
    {
      "type": "training",
      "description": "Training step 4326",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:24",
      "total_flops_so_far": 5.147089265272522e+16,
      "budget_used_percent": 51.47089265272522
    },
    {
      "type": "training",
      "description": "Training step 4327",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:25",
      "total_flops_so_far": 5.148277476882941e+16,
      "budget_used_percent": 51.48277476882941
    },
    {
      "type": "training",
      "description": "Training step 4328",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:26",
      "total_flops_so_far": 5.14946568849336e+16,
      "budget_used_percent": 51.4946568849336
    },
    {
      "type": "training",
      "description": "Training step 4329",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:26",
      "total_flops_so_far": 5.150653900103779e+16,
      "budget_used_percent": 51.50653900103779
    },
    {
      "type": "training",
      "description": "Training step 4330",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:27",
      "total_flops_so_far": 5.151842111714198e+16,
      "budget_used_percent": 51.51842111714199
    },
    {
      "type": "training",
      "description": "Training step 4331",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:28",
      "total_flops_so_far": 5.153030323324618e+16,
      "budget_used_percent": 51.53030323324618
    },
    {
      "type": "training",
      "description": "Training step 4332",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:29",
      "total_flops_so_far": 5.154218534935037e+16,
      "budget_used_percent": 51.54218534935037
    },
    {
      "type": "training",
      "description": "Training step 4333",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:29",
      "total_flops_so_far": 5.155406746545456e+16,
      "budget_used_percent": 51.55406746545456
    },
    {
      "type": "training",
      "description": "Training step 4334",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:30",
      "total_flops_so_far": 5.156594958155875e+16,
      "budget_used_percent": 51.565949581558755
    },
    {
      "type": "training",
      "description": "Training step 4335",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:31",
      "total_flops_so_far": 5.157783169766294e+16,
      "budget_used_percent": 51.577831697662944
    },
    {
      "type": "training",
      "description": "Training step 4336",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:31",
      "total_flops_so_far": 5.158971381376714e+16,
      "budget_used_percent": 51.58971381376713
    },
    {
      "type": "training",
      "description": "Training step 4337",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:32",
      "total_flops_so_far": 5.160159592987133e+16,
      "budget_used_percent": 51.60159592987132
    },
    {
      "type": "training",
      "description": "Training step 4338",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:33",
      "total_flops_so_far": 5.161347804597552e+16,
      "budget_used_percent": 51.613478045975526
    },
    {
      "type": "training",
      "description": "Training step 4339",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:33",
      "total_flops_so_far": 5.162536016207971e+16,
      "budget_used_percent": 51.625360162079716
    },
    {
      "type": "training",
      "description": "Training step 4340",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:34",
      "total_flops_so_far": 5.16372422781839e+16,
      "budget_used_percent": 51.637242278183905
    },
    {
      "type": "training",
      "description": "Training step 4341",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:35",
      "total_flops_so_far": 5.16491243942881e+16,
      "budget_used_percent": 51.649124394288094
    },
    {
      "type": "training",
      "description": "Training step 4342",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:35",
      "total_flops_so_far": 5.166100651039229e+16,
      "budget_used_percent": 51.66100651039229
    },
    {
      "type": "training",
      "description": "Training step 4343",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:36",
      "total_flops_so_far": 5.167288862649648e+16,
      "budget_used_percent": 51.67288862649648
    },
    {
      "type": "training",
      "description": "Training step 4344",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:37",
      "total_flops_so_far": 5.168477074260067e+16,
      "budget_used_percent": 51.68477074260067
    },
    {
      "type": "training",
      "description": "Training step 4345",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:37",
      "total_flops_so_far": 5.169665285870486e+16,
      "budget_used_percent": 51.69665285870486
    },
    {
      "type": "training",
      "description": "Training step 4346",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:38",
      "total_flops_so_far": 5.170853497480906e+16,
      "budget_used_percent": 51.70853497480905
    },
    {
      "type": "training",
      "description": "Training step 4347",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:39",
      "total_flops_so_far": 5.172041709091325e+16,
      "budget_used_percent": 51.72041709091325
    },
    {
      "type": "training",
      "description": "Training step 4348",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:39",
      "total_flops_so_far": 5.173229920701744e+16,
      "budget_used_percent": 51.73229920701744
    },
    {
      "type": "training",
      "description": "Training step 4349",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:40",
      "total_flops_so_far": 5.174418132312163e+16,
      "budget_used_percent": 51.74418132312163
    },
    {
      "type": "training",
      "description": "Training step 4350",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:41",
      "total_flops_so_far": 5.175606343922582e+16,
      "budget_used_percent": 51.75606343922582
    },
    {
      "type": "training",
      "description": "Training step 4351",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:41",
      "total_flops_so_far": 5.176794555533002e+16,
      "budget_used_percent": 51.767945555330016
    },
    {
      "type": "training",
      "description": "Training step 4352",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:42",
      "total_flops_so_far": 5.177982767143421e+16,
      "budget_used_percent": 51.779827671434205
    },
    {
      "type": "training",
      "description": "Training step 4353",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:43",
      "total_flops_so_far": 5.17917097875384e+16,
      "budget_used_percent": 51.791709787538394
    },
    {
      "type": "training",
      "description": "Training step 4354",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:43",
      "total_flops_so_far": 5.180359190364259e+16,
      "budget_used_percent": 51.803591903642584
    },
    {
      "type": "training",
      "description": "Training step 4355",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:44",
      "total_flops_so_far": 5.181547401974678e+16,
      "budget_used_percent": 51.81547401974679
    },
    {
      "type": "training",
      "description": "Training step 4356",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:45",
      "total_flops_so_far": 5.182735613585098e+16,
      "budget_used_percent": 51.82735613585098
    },
    {
      "type": "training",
      "description": "Training step 4357",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:46",
      "total_flops_so_far": 5.183923825195517e+16,
      "budget_used_percent": 51.839238251955166
    },
    {
      "type": "training",
      "description": "Training step 4358",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:46",
      "total_flops_so_far": 5.185112036805936e+16,
      "budget_used_percent": 51.851120368059355
    },
    {
      "type": "training",
      "description": "Training step 4359",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:47",
      "total_flops_so_far": 5.186300248416355e+16,
      "budget_used_percent": 51.86300248416356
    },
    {
      "type": "training",
      "description": "Training step 4360",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:48",
      "total_flops_so_far": 5.187488460026774e+16,
      "budget_used_percent": 51.87488460026775
    },
    {
      "type": "training",
      "description": "Training step 4361",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:48",
      "total_flops_so_far": 5.188676671637194e+16,
      "budget_used_percent": 51.88676671637194
    },
    {
      "type": "training",
      "description": "Training step 4362",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:49",
      "total_flops_so_far": 5.189864883247613e+16,
      "budget_used_percent": 51.89864883247613
    },
    {
      "type": "training",
      "description": "Training step 4363",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:50",
      "total_flops_so_far": 5.191053094858032e+16,
      "budget_used_percent": 51.91053094858032
    },
    {
      "type": "training",
      "description": "Training step 4364",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:50",
      "total_flops_so_far": 5.192241306468451e+16,
      "budget_used_percent": 51.92241306468451
    },
    {
      "type": "training",
      "description": "Training step 4365",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:51",
      "total_flops_so_far": 5.19342951807887e+16,
      "budget_used_percent": 51.9342951807887
    },
    {
      "type": "training",
      "description": "Training step 4366",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:52",
      "total_flops_so_far": 5.19461772968929e+16,
      "budget_used_percent": 51.94617729689289
    },
    {
      "type": "training",
      "description": "Training step 4367",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:52",
      "total_flops_so_far": 5.195805941299709e+16,
      "budget_used_percent": 51.958059412997095
    },
    {
      "type": "training",
      "description": "Training step 4368",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:53",
      "total_flops_so_far": 5.196994152910128e+16,
      "budget_used_percent": 51.969941529101284
    },
    {
      "type": "training",
      "description": "Training step 4369",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:54",
      "total_flops_so_far": 5.198182364520547e+16,
      "budget_used_percent": 51.98182364520547
    },
    {
      "type": "training",
      "description": "Training step 4370",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:54",
      "total_flops_so_far": 5.199370576130966e+16,
      "budget_used_percent": 51.99370576130966
    },
    {
      "type": "training",
      "description": "Training step 4371",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:55",
      "total_flops_so_far": 5.200558787741386e+16,
      "budget_used_percent": 52.00558787741386
    },
    {
      "type": "training",
      "description": "Training step 4372",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:56",
      "total_flops_so_far": 5.201746999351805e+16,
      "budget_used_percent": 52.01746999351805
    },
    {
      "type": "training",
      "description": "Training step 4373",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:56",
      "total_flops_so_far": 5.202935210962224e+16,
      "budget_used_percent": 52.02935210962224
    },
    {
      "type": "training",
      "description": "Training step 4374",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:57",
      "total_flops_so_far": 5.204123422572643e+16,
      "budget_used_percent": 52.04123422572643
    },
    {
      "type": "training",
      "description": "Training step 4375",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:58",
      "total_flops_so_far": 5.205311634183062e+16,
      "budget_used_percent": 52.053116341830616
    },
    {
      "type": "training",
      "description": "Training step 4376",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:58",
      "total_flops_so_far": 5.206499845793482e+16,
      "budget_used_percent": 52.06499845793482
    },
    {
      "type": "training",
      "description": "Training step 4377",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 22:59:59",
      "total_flops_so_far": 5.207688057403901e+16,
      "budget_used_percent": 52.07688057403901
    },
    {
      "type": "training",
      "description": "Training step 4378",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:00",
      "total_flops_so_far": 5.20887626901432e+16,
      "budget_used_percent": 52.0887626901432
    },
    {
      "type": "training",
      "description": "Training step 4379",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:01",
      "total_flops_so_far": 5.210064480624739e+16,
      "budget_used_percent": 52.10064480624739
    },
    {
      "type": "training",
      "description": "Training step 4380",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:01",
      "total_flops_so_far": 5.211252692235158e+16,
      "budget_used_percent": 52.112526922351584
    },
    {
      "type": "training",
      "description": "Training step 4381",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:02",
      "total_flops_so_far": 5.212440903845578e+16,
      "budget_used_percent": 52.124409038455774
    },
    {
      "type": "training",
      "description": "Training step 4382",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:03",
      "total_flops_so_far": 5.213629115455997e+16,
      "budget_used_percent": 52.13629115455996
    },
    {
      "type": "training",
      "description": "Training step 4383",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:03",
      "total_flops_so_far": 5.214817327066416e+16,
      "budget_used_percent": 52.14817327066415
    },
    {
      "type": "training",
      "description": "Training step 4384",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:04",
      "total_flops_so_far": 5.216005538676835e+16,
      "budget_used_percent": 52.160055386768356
    },
    {
      "type": "training",
      "description": "Training step 4385",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:05",
      "total_flops_so_far": 5.217193750287254e+16,
      "budget_used_percent": 52.171937502872545
    },
    {
      "type": "training",
      "description": "Training step 4386",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:05",
      "total_flops_so_far": 5.218381961897674e+16,
      "budget_used_percent": 52.183819618976734
    },
    {
      "type": "training",
      "description": "Training step 4387",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:06",
      "total_flops_so_far": 5.219570173508093e+16,
      "budget_used_percent": 52.195701735080924
    },
    {
      "type": "training",
      "description": "Training step 4388",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:07",
      "total_flops_so_far": 5.220758385118512e+16,
      "budget_used_percent": 52.20758385118513
    },
    {
      "type": "training",
      "description": "Training step 4389",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:07",
      "total_flops_so_far": 5.221946596728931e+16,
      "budget_used_percent": 52.21946596728932
    },
    {
      "type": "training",
      "description": "Training step 4390",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:08",
      "total_flops_so_far": 5.22313480833935e+16,
      "budget_used_percent": 52.231348083393506
    },
    {
      "type": "training",
      "description": "Training step 4391",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:09",
      "total_flops_so_far": 5.22432301994977e+16,
      "budget_used_percent": 52.243230199497695
    },
    {
      "type": "training",
      "description": "Training step 4392",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:09",
      "total_flops_so_far": 5.225511231560189e+16,
      "budget_used_percent": 52.25511231560189
    },
    {
      "type": "training",
      "description": "Training step 4393",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:10",
      "total_flops_so_far": 5.226699443170608e+16,
      "budget_used_percent": 52.26699443170608
    },
    {
      "type": "training",
      "description": "Training step 4394",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:11",
      "total_flops_so_far": 5.227887654781027e+16,
      "budget_used_percent": 52.27887654781027
    },
    {
      "type": "training",
      "description": "Training step 4395",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:11",
      "total_flops_so_far": 5.229075866391446e+16,
      "budget_used_percent": 52.29075866391446
    },
    {
      "type": "training",
      "description": "Training step 4396",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:12",
      "total_flops_so_far": 5.230264078001866e+16,
      "budget_used_percent": 52.30264078001866
    },
    {
      "type": "training",
      "description": "Training step 4397",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:13",
      "total_flops_so_far": 5.231452289612285e+16,
      "budget_used_percent": 52.31452289612285
    },
    {
      "type": "training",
      "description": "Training step 4398",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:13",
      "total_flops_so_far": 5.232640501222704e+16,
      "budget_used_percent": 52.32640501222704
    },
    {
      "type": "training",
      "description": "Training step 4399",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:14",
      "total_flops_so_far": 5.233828712833123e+16,
      "budget_used_percent": 52.33828712833123
    },
    {
      "type": "training",
      "description": "Training step 4400",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:15",
      "total_flops_so_far": 5.235016924443542e+16,
      "budget_used_percent": 52.35016924443543
    },
    {
      "type": "training",
      "description": "Training step 4401",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:16",
      "total_flops_so_far": 5.236205136053962e+16,
      "budget_used_percent": 52.36205136053962
    },
    {
      "type": "training",
      "description": "Training step 4402",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:17",
      "total_flops_so_far": 5.237393347664381e+16,
      "budget_used_percent": 52.373933476643806
    },
    {
      "type": "training",
      "description": "Training step 4403",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:17",
      "total_flops_so_far": 5.2385815592748e+16,
      "budget_used_percent": 52.385815592747996
    },
    {
      "type": "training",
      "description": "Training step 4404",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:18",
      "total_flops_so_far": 5.239769770885219e+16,
      "budget_used_percent": 52.397697708852185
    },
    {
      "type": "training",
      "description": "Training step 4405",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:19",
      "total_flops_so_far": 5.240957982495638e+16,
      "budget_used_percent": 52.40957982495639
    },
    {
      "type": "training",
      "description": "Training step 4406",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:19",
      "total_flops_so_far": 5.242146194106058e+16,
      "budget_used_percent": 52.42146194106058
    },
    {
      "type": "training",
      "description": "Training step 4407",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:20",
      "total_flops_so_far": 5.243334405716477e+16,
      "budget_used_percent": 52.43334405716477
    },
    {
      "type": "training",
      "description": "Training step 4408",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:21",
      "total_flops_so_far": 5.244522617326896e+16,
      "budget_used_percent": 52.445226173268956
    },
    {
      "type": "training",
      "description": "Training step 4409",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:21",
      "total_flops_so_far": 5.245710828937315e+16,
      "budget_used_percent": 52.45710828937315
    },
    {
      "type": "training",
      "description": "Training step 4410",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:22",
      "total_flops_so_far": 5.246899040547734e+16,
      "budget_used_percent": 52.46899040547734
    },
    {
      "type": "training",
      "description": "Training step 4411",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:23",
      "total_flops_so_far": 5.248087252158154e+16,
      "budget_used_percent": 52.48087252158153
    },
    {
      "type": "training",
      "description": "Training step 4412",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:23",
      "total_flops_so_far": 5.249275463768573e+16,
      "budget_used_percent": 52.49275463768572
    },
    {
      "type": "training",
      "description": "Training step 4413",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:24",
      "total_flops_so_far": 5.250463675378992e+16,
      "budget_used_percent": 52.504636753789924
    },
    {
      "type": "training",
      "description": "Training step 4414",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:25",
      "total_flops_so_far": 5.251651886989411e+16,
      "budget_used_percent": 52.516518869894114
    },
    {
      "type": "training",
      "description": "Training step 4415",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:25",
      "total_flops_so_far": 5.25284009859983e+16,
      "budget_used_percent": 52.5284009859983
    },
    {
      "type": "training",
      "description": "Training step 4416",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:26",
      "total_flops_so_far": 5.25402831021025e+16,
      "budget_used_percent": 52.54028310210249
    },
    {
      "type": "training",
      "description": "Training step 4417",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:27",
      "total_flops_so_far": 5.255216521820669e+16,
      "budget_used_percent": 52.552165218206696
    },
    {
      "type": "training",
      "description": "Training step 4418",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:27",
      "total_flops_so_far": 5.256404733431088e+16,
      "budget_used_percent": 52.564047334310885
    },
    {
      "type": "training",
      "description": "Training step 4419",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:28",
      "total_flops_so_far": 5.257592945041507e+16,
      "budget_used_percent": 52.575929450415074
    },
    {
      "type": "training",
      "description": "Training step 4420",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:29",
      "total_flops_so_far": 5.258781156651926e+16,
      "budget_used_percent": 52.587811566519264
    },
    {
      "type": "training",
      "description": "Training step 4421",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:29",
      "total_flops_so_far": 5.259969368262346e+16,
      "budget_used_percent": 52.59969368262346
    },
    {
      "type": "training",
      "description": "Training step 4422",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:30",
      "total_flops_so_far": 5.261157579872765e+16,
      "budget_used_percent": 52.61157579872765
    },
    {
      "type": "training",
      "description": "Training step 4423",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:31",
      "total_flops_so_far": 5.262345791483184e+16,
      "budget_used_percent": 52.62345791483184
    },
    {
      "type": "training",
      "description": "Training step 4424",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:32",
      "total_flops_so_far": 5.263534003093603e+16,
      "budget_used_percent": 52.63534003093603
    },
    {
      "type": "training",
      "description": "Training step 4425",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:32",
      "total_flops_so_far": 5.264722214704022e+16,
      "budget_used_percent": 52.64722214704023
    },
    {
      "type": "training",
      "description": "Training step 4426",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:33",
      "total_flops_so_far": 5.265910426314442e+16,
      "budget_used_percent": 52.65910426314442
    },
    {
      "type": "training",
      "description": "Training step 4427",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:34",
      "total_flops_so_far": 5.267098637924861e+16,
      "budget_used_percent": 52.67098637924861
    },
    {
      "type": "training",
      "description": "Training step 4428",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:34",
      "total_flops_so_far": 5.26828684953528e+16,
      "budget_used_percent": 52.6828684953528
    },
    {
      "type": "training",
      "description": "Training step 4429",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:35",
      "total_flops_so_far": 5.269475061145699e+16,
      "budget_used_percent": 52.694750611456996
    },
    {
      "type": "training",
      "description": "Training step 4430",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:36",
      "total_flops_so_far": 5.270663272756118e+16,
      "budget_used_percent": 52.706632727561185
    },
    {
      "type": "training",
      "description": "Training step 4431",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:36",
      "total_flops_so_far": 5.271851484366538e+16,
      "budget_used_percent": 52.718514843665375
    },
    {
      "type": "training",
      "description": "Training step 4432",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:37",
      "total_flops_so_far": 5.273039695976957e+16,
      "budget_used_percent": 52.730396959769564
    },
    {
      "type": "training",
      "description": "Training step 4433",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:38",
      "total_flops_so_far": 5.274227907587376e+16,
      "budget_used_percent": 52.74227907587375
    },
    {
      "type": "training",
      "description": "Training step 4434",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:38",
      "total_flops_so_far": 5.275416119197795e+16,
      "budget_used_percent": 52.75416119197796
    },
    {
      "type": "training",
      "description": "Training step 4435",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:39",
      "total_flops_so_far": 5.276604330808214e+16,
      "budget_used_percent": 52.766043308082146
    },
    {
      "type": "training",
      "description": "Training step 4436",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:40",
      "total_flops_so_far": 5.277792542418634e+16,
      "budget_used_percent": 52.777925424186336
    },
    {
      "type": "training",
      "description": "Training step 4437",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:40",
      "total_flops_so_far": 5.278980754029053e+16,
      "budget_used_percent": 52.789807540290525
    },
    {
      "type": "training",
      "description": "Training step 4438",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:41",
      "total_flops_so_far": 5.280168965639472e+16,
      "budget_used_percent": 52.80168965639472
    },
    {
      "type": "training",
      "description": "Training step 4439",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:42",
      "total_flops_so_far": 5.281357177249891e+16,
      "budget_used_percent": 52.81357177249891
    },
    {
      "type": "training",
      "description": "Training step 4440",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:42",
      "total_flops_so_far": 5.28254538886031e+16,
      "budget_used_percent": 52.8254538886031
    },
    {
      "type": "training",
      "description": "Training step 4441",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:43",
      "total_flops_so_far": 5.28373360047073e+16,
      "budget_used_percent": 52.83733600470729
    },
    {
      "type": "training",
      "description": "Training step 4442",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:44",
      "total_flops_so_far": 5.284921812081149e+16,
      "budget_used_percent": 52.84921812081149
    },
    {
      "type": "training",
      "description": "Training step 4443",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:44",
      "total_flops_so_far": 5.286110023691568e+16,
      "budget_used_percent": 52.86110023691568
    },
    {
      "type": "training",
      "description": "Training step 4444",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:45",
      "total_flops_so_far": 5.287298235301987e+16,
      "budget_used_percent": 52.87298235301987
    },
    {
      "type": "training",
      "description": "Training step 4445",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:46",
      "total_flops_so_far": 5.288486446912406e+16,
      "budget_used_percent": 52.88486446912406
    },
    {
      "type": "training",
      "description": "Training step 4446",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:47",
      "total_flops_so_far": 5.289674658522826e+16,
      "budget_used_percent": 52.89674658522826
    },
    {
      "type": "training",
      "description": "Training step 4447",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:47",
      "total_flops_so_far": 5.290862870133245e+16,
      "budget_used_percent": 52.90862870133245
    },
    {
      "type": "training",
      "description": "Training step 4448",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:48",
      "total_flops_so_far": 5.292051081743664e+16,
      "budget_used_percent": 52.920510817436636
    },
    {
      "type": "training",
      "description": "Training step 4449",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:49",
      "total_flops_so_far": 5.293239293354083e+16,
      "budget_used_percent": 52.932392933540825
    },
    {
      "type": "training",
      "description": "Training step 4450",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:49",
      "total_flops_so_far": 5.294427504964502e+16,
      "budget_used_percent": 52.94427504964503
    },
    {
      "type": "training",
      "description": "Training step 4451",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:50",
      "total_flops_so_far": 5.295615716574922e+16,
      "budget_used_percent": 52.95615716574922
    },
    {
      "type": "training",
      "description": "Training step 4452",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:51",
      "total_flops_so_far": 5.296803928185341e+16,
      "budget_used_percent": 52.96803928185341
    },
    {
      "type": "training",
      "description": "Training step 4453",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:51",
      "total_flops_so_far": 5.29799213979576e+16,
      "budget_used_percent": 52.9799213979576
    },
    {
      "type": "training",
      "description": "Training step 4454",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:52",
      "total_flops_so_far": 5.299180351406179e+16,
      "budget_used_percent": 52.9918035140618
    },
    {
      "type": "training",
      "description": "Training step 4455",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:53",
      "total_flops_so_far": 5.300368563016598e+16,
      "budget_used_percent": 53.00368563016599
    },
    {
      "type": "training",
      "description": "Training step 4456",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:53",
      "total_flops_so_far": 5.301556774627018e+16,
      "budget_used_percent": 53.01556774627018
    },
    {
      "type": "training",
      "description": "Training step 4457",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:54",
      "total_flops_so_far": 5.302744986237437e+16,
      "budget_used_percent": 53.02744986237437
    },
    {
      "type": "training",
      "description": "Training step 4458",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:55",
      "total_flops_so_far": 5.303933197847856e+16,
      "budget_used_percent": 53.039331978478565
    },
    {
      "type": "training",
      "description": "Training step 4459",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:55",
      "total_flops_so_far": 5.305121409458275e+16,
      "budget_used_percent": 53.051214094582754
    },
    {
      "type": "training",
      "description": "Training step 4460",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:56",
      "total_flops_so_far": 5.306309621068694e+16,
      "budget_used_percent": 53.06309621068694
    },
    {
      "type": "training",
      "description": "Training step 4461",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:57",
      "total_flops_so_far": 5.307497832679114e+16,
      "budget_used_percent": 53.07497832679113
    },
    {
      "type": "training",
      "description": "Training step 4462",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:57",
      "total_flops_so_far": 5.308686044289533e+16,
      "budget_used_percent": 53.08686044289532
    },
    {
      "type": "training",
      "description": "Training step 4463",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:58",
      "total_flops_so_far": 5.309874255899952e+16,
      "budget_used_percent": 53.098742558999525
    },
    {
      "type": "training",
      "description": "Training step 4464",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:59",
      "total_flops_so_far": 5.311062467510371e+16,
      "budget_used_percent": 53.110624675103715
    },
    {
      "type": "training",
      "description": "Training step 4465",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:00:59",
      "total_flops_so_far": 5.31225067912079e+16,
      "budget_used_percent": 53.122506791207904
    },
    {
      "type": "training",
      "description": "Training step 4466",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:00",
      "total_flops_so_far": 5.31343889073121e+16,
      "budget_used_percent": 53.13438890731209
    },
    {
      "type": "training",
      "description": "Training step 4467",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:01",
      "total_flops_so_far": 5.314627102341629e+16,
      "budget_used_percent": 53.14627102341629
    },
    {
      "type": "training",
      "description": "Training step 4468",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:02",
      "total_flops_so_far": 5.315815313952048e+16,
      "budget_used_percent": 53.15815313952048
    },
    {
      "type": "training",
      "description": "Training step 4469",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:03",
      "total_flops_so_far": 5.317003525562467e+16,
      "budget_used_percent": 53.17003525562467
    },
    {
      "type": "training",
      "description": "Training step 4470",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:03",
      "total_flops_so_far": 5.318191737172886e+16,
      "budget_used_percent": 53.18191737172886
    },
    {
      "type": "training",
      "description": "Training step 4471",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:04",
      "total_flops_so_far": 5.319379948783306e+16,
      "budget_used_percent": 53.19379948783306
    },
    {
      "type": "training",
      "description": "Training step 4472",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:04",
      "total_flops_so_far": 5.320568160393725e+16,
      "budget_used_percent": 53.20568160393725
    },
    {
      "type": "training",
      "description": "Training step 4473",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:05",
      "total_flops_so_far": 5.321756372004144e+16,
      "budget_used_percent": 53.21756372004144
    },
    {
      "type": "training",
      "description": "Training step 4474",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:06",
      "total_flops_so_far": 5.322944583614563e+16,
      "budget_used_percent": 53.22944583614563
    },
    {
      "type": "training",
      "description": "Training step 4475",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:06",
      "total_flops_so_far": 5.324132795224982e+16,
      "budget_used_percent": 53.241327952249826
    },
    {
      "type": "training",
      "description": "Training step 4476",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:07",
      "total_flops_so_far": 5.325321006835402e+16,
      "budget_used_percent": 53.253210068354015
    },
    {
      "type": "training",
      "description": "Training step 4477",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:08",
      "total_flops_so_far": 5.326509218445821e+16,
      "budget_used_percent": 53.265092184458204
    },
    {
      "type": "training",
      "description": "Training step 4478",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:08",
      "total_flops_so_far": 5.32769743005624e+16,
      "budget_used_percent": 53.276974300562394
    },
    {
      "type": "training",
      "description": "Training step 4479",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:09",
      "total_flops_so_far": 5.328885641666659e+16,
      "budget_used_percent": 53.2888564166666
    },
    {
      "type": "training",
      "description": "Training step 4480",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:10",
      "total_flops_so_far": 5.330073853277078e+16,
      "budget_used_percent": 53.30073853277079
    },
    {
      "type": "training",
      "description": "Training step 4481",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:10",
      "total_flops_so_far": 5.331262064887498e+16,
      "budget_used_percent": 53.312620648874976
    },
    {
      "type": "training",
      "description": "Training step 4482",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:11",
      "total_flops_so_far": 5.332450276497917e+16,
      "budget_used_percent": 53.324502764979165
    },
    {
      "type": "training",
      "description": "Training step 4483",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:12",
      "total_flops_so_far": 5.333638488108336e+16,
      "budget_used_percent": 53.33638488108336
    },
    {
      "type": "training",
      "description": "Training step 4484",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:13",
      "total_flops_so_far": 5.334826699718755e+16,
      "budget_used_percent": 53.34826699718755
    },
    {
      "type": "training",
      "description": "Training step 4485",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:13",
      "total_flops_so_far": 5.336014911329174e+16,
      "budget_used_percent": 53.36014911329174
    },
    {
      "type": "training",
      "description": "Training step 4486",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:14",
      "total_flops_so_far": 5.337203122939594e+16,
      "budget_used_percent": 53.37203122939593
    },
    {
      "type": "training",
      "description": "Training step 4487",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:15",
      "total_flops_so_far": 5.338391334550013e+16,
      "budget_used_percent": 53.38391334550012
    },
    {
      "type": "training",
      "description": "Training step 4488",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:15",
      "total_flops_so_far": 5.339579546160432e+16,
      "budget_used_percent": 53.39579546160432
    },
    {
      "type": "training",
      "description": "Training step 4489",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:16",
      "total_flops_so_far": 5.340767757770851e+16,
      "budget_used_percent": 53.40767757770851
    },
    {
      "type": "training",
      "description": "Training step 4490",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:17",
      "total_flops_so_far": 5.34195596938127e+16,
      "budget_used_percent": 53.4195596938127
    },
    {
      "type": "training",
      "description": "Training step 4491",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:17",
      "total_flops_so_far": 5.34314418099169e+16,
      "budget_used_percent": 53.43144180991689
    },
    {
      "type": "training",
      "description": "Training step 4492",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:18",
      "total_flops_so_far": 5.344332392602109e+16,
      "budget_used_percent": 53.443323926021094
    },
    {
      "type": "training",
      "description": "Training step 4493",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:19",
      "total_flops_so_far": 5.345520604212528e+16,
      "budget_used_percent": 53.45520604212528
    },
    {
      "type": "training",
      "description": "Training step 4494",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:19",
      "total_flops_so_far": 5.346708815822947e+16,
      "budget_used_percent": 53.46708815822947
    },
    {
      "type": "training",
      "description": "Training step 4495",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:20",
      "total_flops_so_far": 5.347897027433366e+16,
      "budget_used_percent": 53.47897027433366
    },
    {
      "type": "training",
      "description": "Training step 4496",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:21",
      "total_flops_so_far": 5.349085239043786e+16,
      "budget_used_percent": 53.49085239043786
    },
    {
      "type": "training",
      "description": "Training step 4497",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:21",
      "total_flops_so_far": 5.350273450654205e+16,
      "budget_used_percent": 53.50273450654205
    },
    {
      "type": "training",
      "description": "Training step 4498",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:22",
      "total_flops_so_far": 5.351461662264624e+16,
      "budget_used_percent": 53.51461662264624
    },
    {
      "type": "training",
      "description": "Training step 4499",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:23",
      "total_flops_so_far": 5.352649873875043e+16,
      "budget_used_percent": 53.526498738750426
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:31",
      "total_flops_so_far": 5.352720936788829e+16,
      "budget_used_percent": 53.52720936788828
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 714333709232.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:38",
      "total_flops_so_far": 5.352792370159752e+16,
      "budget_used_percent": 53.52792370159752
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:46",
      "total_flops_so_far": 5.352863618266087e+16,
      "budget_used_percent": 53.52863618266087
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:01:53",
      "total_flops_so_far": 5.352934681179873e+16,
      "budget_used_percent": 53.52934681179873
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 713407296244.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:00",
      "total_flops_so_far": 5.353006021909498e+16,
      "budget_used_percent": 53.53006021909498
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 5",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:08",
      "total_flops_so_far": 5.353077084823283e+16,
      "budget_used_percent": 53.530770848232834
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 6",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:15",
      "total_flops_so_far": 5.3531483329296184e+16,
      "budget_used_percent": 53.53148332929618
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 7",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:22",
      "total_flops_so_far": 5.353219581035954e+16,
      "budget_used_percent": 53.53219581035954
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 8",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:30",
      "total_flops_so_far": 5.353290829142289e+16,
      "budget_used_percent": 53.532908291422885
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 9",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:37",
      "total_flops_so_far": 5.353362077248624e+16,
      "budget_used_percent": 53.53362077248624
    },
    {
      "type": "training",
      "description": "Training step 4500",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:38",
      "total_flops_so_far": 5.354550288859043e+16,
      "budget_used_percent": 53.545502888590434
    },
    {
      "type": "training",
      "description": "Training step 4501",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:38",
      "total_flops_so_far": 5.355738500469462e+16,
      "budget_used_percent": 53.55738500469462
    },
    {
      "type": "training",
      "description": "Training step 4502",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:39",
      "total_flops_so_far": 5.356926712079882e+16,
      "budget_used_percent": 53.56926712079881
    },
    {
      "type": "training",
      "description": "Training step 4503",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:40",
      "total_flops_so_far": 5.358114923690301e+16,
      "budget_used_percent": 53.581149236903
    },
    {
      "type": "training",
      "description": "Training step 4504",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:40",
      "total_flops_so_far": 5.35930313530072e+16,
      "budget_used_percent": 53.593031353007206
    },
    {
      "type": "training",
      "description": "Training step 4505",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:41",
      "total_flops_so_far": 5.360491346911139e+16,
      "budget_used_percent": 53.604913469111395
    },
    {
      "type": "training",
      "description": "Training step 4506",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:42",
      "total_flops_so_far": 5.361679558521558e+16,
      "budget_used_percent": 53.616795585215584
    },
    {
      "type": "training",
      "description": "Training step 4507",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:43",
      "total_flops_so_far": 5.362867770131978e+16,
      "budget_used_percent": 53.62867770131977
    },
    {
      "type": "training",
      "description": "Training step 4508",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:43",
      "total_flops_so_far": 5.364055981742397e+16,
      "budget_used_percent": 53.64055981742397
    },
    {
      "type": "training",
      "description": "Training step 4509",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:44",
      "total_flops_so_far": 5.365244193352816e+16,
      "budget_used_percent": 53.65244193352816
    },
    {
      "type": "training",
      "description": "Training step 4510",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:45",
      "total_flops_so_far": 5.366432404963235e+16,
      "budget_used_percent": 53.66432404963235
    },
    {
      "type": "training",
      "description": "Training step 4511",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:45",
      "total_flops_so_far": 5.367620616573654e+16,
      "budget_used_percent": 53.67620616573654
    },
    {
      "type": "training",
      "description": "Training step 4512",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:46",
      "total_flops_so_far": 5.368808828184074e+16,
      "budget_used_percent": 53.68808828184074
    },
    {
      "type": "training",
      "description": "Training step 4513",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:47",
      "total_flops_so_far": 5.369997039794493e+16,
      "budget_used_percent": 53.69997039794493
    },
    {
      "type": "training",
      "description": "Training step 4514",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:47",
      "total_flops_so_far": 5.371185251404912e+16,
      "budget_used_percent": 53.71185251404912
    },
    {
      "type": "training",
      "description": "Training step 4515",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:48",
      "total_flops_so_far": 5.372373463015331e+16,
      "budget_used_percent": 53.72373463015331
    },
    {
      "type": "training",
      "description": "Training step 4516",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:49",
      "total_flops_so_far": 5.37356167462575e+16,
      "budget_used_percent": 53.735616746257506
    },
    {
      "type": "training",
      "description": "Training step 4517",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:49",
      "total_flops_so_far": 5.37474988623617e+16,
      "budget_used_percent": 53.747498862361695
    },
    {
      "type": "training",
      "description": "Training step 4518",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:50",
      "total_flops_so_far": 5.375938097846589e+16,
      "budget_used_percent": 53.759380978465884
    },
    {
      "type": "training",
      "description": "Training step 4519",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:51",
      "total_flops_so_far": 5.377126309457008e+16,
      "budget_used_percent": 53.771263094570074
    },
    {
      "type": "training",
      "description": "Training step 4520",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:51",
      "total_flops_so_far": 5.378314521067427e+16,
      "budget_used_percent": 53.78314521067428
    },
    {
      "type": "training",
      "description": "Training step 4521",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:52",
      "total_flops_so_far": 5.379502732677846e+16,
      "budget_used_percent": 53.79502732677847
    },
    {
      "type": "training",
      "description": "Training step 4522",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:53",
      "total_flops_so_far": 5.380690944288266e+16,
      "budget_used_percent": 53.806909442882656
    },
    {
      "type": "training",
      "description": "Training step 4523",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:53",
      "total_flops_so_far": 5.381879155898685e+16,
      "budget_used_percent": 53.818791558986845
    },
    {
      "type": "training",
      "description": "Training step 4524",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:54",
      "total_flops_so_far": 5.383067367509104e+16,
      "budget_used_percent": 53.83067367509105
    },
    {
      "type": "training",
      "description": "Training step 4525",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:55",
      "total_flops_so_far": 5.384255579119523e+16,
      "budget_used_percent": 53.84255579119524
    },
    {
      "type": "training",
      "description": "Training step 4526",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:56",
      "total_flops_so_far": 5.385443790729942e+16,
      "budget_used_percent": 53.85443790729943
    },
    {
      "type": "training",
      "description": "Training step 4527",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:56",
      "total_flops_so_far": 5.386632002340362e+16,
      "budget_used_percent": 53.86632002340362
    },
    {
      "type": "training",
      "description": "Training step 4528",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:57",
      "total_flops_so_far": 5.387820213950781e+16,
      "budget_used_percent": 53.878202139507806
    },
    {
      "type": "training",
      "description": "Training step 4529",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:58",
      "total_flops_so_far": 5.3890084255612e+16,
      "budget_used_percent": 53.890084255612
    },
    {
      "type": "training",
      "description": "Training step 4530",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:58",
      "total_flops_so_far": 5.390196637171619e+16,
      "budget_used_percent": 53.90196637171619
    },
    {
      "type": "training",
      "description": "Training step 4531",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:02:59",
      "total_flops_so_far": 5.391384848782038e+16,
      "budget_used_percent": 53.91384848782038
    },
    {
      "type": "training",
      "description": "Training step 4532",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:00",
      "total_flops_so_far": 5.392573060392458e+16,
      "budget_used_percent": 53.92573060392457
    },
    {
      "type": "training",
      "description": "Training step 4533",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:00",
      "total_flops_so_far": 5.393761272002877e+16,
      "budget_used_percent": 53.937612720028774
    },
    {
      "type": "training",
      "description": "Training step 4534",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:01",
      "total_flops_so_far": 5.394949483613296e+16,
      "budget_used_percent": 53.94949483613296
    },
    {
      "type": "training",
      "description": "Training step 4535",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:02",
      "total_flops_so_far": 5.396137695223715e+16,
      "budget_used_percent": 53.96137695223715
    },
    {
      "type": "training",
      "description": "Training step 4536",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:02",
      "total_flops_so_far": 5.397325906834134e+16,
      "budget_used_percent": 53.97325906834134
    },
    {
      "type": "training",
      "description": "Training step 4537",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:03",
      "total_flops_so_far": 5.398514118444554e+16,
      "budget_used_percent": 53.98514118444554
    },
    {
      "type": "training",
      "description": "Training step 4538",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:04",
      "total_flops_so_far": 5.399702330054973e+16,
      "budget_used_percent": 53.99702330054973
    },
    {
      "type": "training",
      "description": "Training step 4539",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:05",
      "total_flops_so_far": 5.400890541665392e+16,
      "budget_used_percent": 54.00890541665392
    },
    {
      "type": "training",
      "description": "Training step 4540",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:05",
      "total_flops_so_far": 5.402078753275811e+16,
      "budget_used_percent": 54.020787532758106
    },
    {
      "type": "training",
      "description": "Training step 4541",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:06",
      "total_flops_so_far": 5.40326696488623e+16,
      "budget_used_percent": 54.03266964886231
    },
    {
      "type": "training",
      "description": "Training step 4542",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:07",
      "total_flops_so_far": 5.40445517649665e+16,
      "budget_used_percent": 54.0445517649665
    },
    {
      "type": "training",
      "description": "Training step 4543",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:07",
      "total_flops_so_far": 5.405643388107069e+16,
      "budget_used_percent": 54.05643388107069
    },
    {
      "type": "training",
      "description": "Training step 4544",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:08",
      "total_flops_so_far": 5.406831599717488e+16,
      "budget_used_percent": 54.06831599717488
    },
    {
      "type": "training",
      "description": "Training step 4545",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:09",
      "total_flops_so_far": 5.408019811327907e+16,
      "budget_used_percent": 54.080198113279074
    },
    {
      "type": "training",
      "description": "Training step 4546",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:10",
      "total_flops_so_far": 5.409208022938326e+16,
      "budget_used_percent": 54.092080229383264
    },
    {
      "type": "training",
      "description": "Training step 4547",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:10",
      "total_flops_so_far": 5.410396234548746e+16,
      "budget_used_percent": 54.10396234548745
    },
    {
      "type": "training",
      "description": "Training step 4548",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:11",
      "total_flops_so_far": 5.411584446159165e+16,
      "budget_used_percent": 54.11584446159164
    },
    {
      "type": "training",
      "description": "Training step 4549",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:12",
      "total_flops_so_far": 5.412772657769584e+16,
      "budget_used_percent": 54.127726577695846
    },
    {
      "type": "training",
      "description": "Training step 4550",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:12",
      "total_flops_so_far": 5.413960869380003e+16,
      "budget_used_percent": 54.139608693800035
    },
    {
      "type": "training",
      "description": "Training step 4551",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:13",
      "total_flops_so_far": 5.415149080990422e+16,
      "budget_used_percent": 54.151490809904224
    },
    {
      "type": "training",
      "description": "Training step 4552",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:14",
      "total_flops_so_far": 5.416337292600842e+16,
      "budget_used_percent": 54.163372926008414
    },
    {
      "type": "training",
      "description": "Training step 4553",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:14",
      "total_flops_so_far": 5.417525504211261e+16,
      "budget_used_percent": 54.17525504211261
    },
    {
      "type": "training",
      "description": "Training step 4554",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:15",
      "total_flops_so_far": 5.41871371582168e+16,
      "budget_used_percent": 54.1871371582168
    },
    {
      "type": "training",
      "description": "Training step 4555",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:16",
      "total_flops_so_far": 5.419901927432099e+16,
      "budget_used_percent": 54.19901927432099
    },
    {
      "type": "training",
      "description": "Training step 4556",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:16",
      "total_flops_so_far": 5.421090139042518e+16,
      "budget_used_percent": 54.21090139042518
    },
    {
      "type": "training",
      "description": "Training step 4557",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:17",
      "total_flops_so_far": 5.422278350652938e+16,
      "budget_used_percent": 54.22278350652937
    },
    {
      "type": "training",
      "description": "Training step 4558",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:18",
      "total_flops_so_far": 5.423466562263357e+16,
      "budget_used_percent": 54.23466562263357
    },
    {
      "type": "training",
      "description": "Training step 4559",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:18",
      "total_flops_so_far": 5.424654773873776e+16,
      "budget_used_percent": 54.24654773873776
    },
    {
      "type": "training",
      "description": "Training step 4560",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:19",
      "total_flops_so_far": 5.425842985484195e+16,
      "budget_used_percent": 54.25842985484195
    },
    {
      "type": "training",
      "description": "Training step 4561",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:20",
      "total_flops_so_far": 5.427031197094614e+16,
      "budget_used_percent": 54.27031197094614
    },
    {
      "type": "training",
      "description": "Training step 4562",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:21",
      "total_flops_so_far": 5.428219408705034e+16,
      "budget_used_percent": 54.28219408705034
    },
    {
      "type": "training",
      "description": "Training step 4563",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:21",
      "total_flops_so_far": 5.429407620315453e+16,
      "budget_used_percent": 54.29407620315453
    },
    {
      "type": "training",
      "description": "Training step 4564",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:22",
      "total_flops_so_far": 5.430595831925872e+16,
      "budget_used_percent": 54.30595831925872
    },
    {
      "type": "training",
      "description": "Training step 4565",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:23",
      "total_flops_so_far": 5.431784043536291e+16,
      "budget_used_percent": 54.31784043536291
    },
    {
      "type": "training",
      "description": "Training step 4566",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:23",
      "total_flops_so_far": 5.43297225514671e+16,
      "budget_used_percent": 54.32972255146711
    },
    {
      "type": "training",
      "description": "Training step 4567",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:24",
      "total_flops_so_far": 5.43416046675713e+16,
      "budget_used_percent": 54.341604667571296
    },
    {
      "type": "training",
      "description": "Training step 4568",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:25",
      "total_flops_so_far": 5.435348678367549e+16,
      "budget_used_percent": 54.353486783675486
    },
    {
      "type": "training",
      "description": "Training step 4569",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:25",
      "total_flops_so_far": 5.436536889977968e+16,
      "budget_used_percent": 54.365368899779675
    },
    {
      "type": "training",
      "description": "Training step 4570",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:26",
      "total_flops_so_far": 5.437725101588387e+16,
      "budget_used_percent": 54.37725101588388
    },
    {
      "type": "training",
      "description": "Training step 4571",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:27",
      "total_flops_so_far": 5.438913313198806e+16,
      "budget_used_percent": 54.38913313198807
    },
    {
      "type": "training",
      "description": "Training step 4572",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:27",
      "total_flops_so_far": 5.440101524809226e+16,
      "budget_used_percent": 54.40101524809226
    },
    {
      "type": "training",
      "description": "Training step 4573",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:28",
      "total_flops_so_far": 5.441289736419645e+16,
      "budget_used_percent": 54.412897364196446
    },
    {
      "type": "training",
      "description": "Training step 4574",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:29",
      "total_flops_so_far": 5.442477948030064e+16,
      "budget_used_percent": 54.42477948030064
    },
    {
      "type": "training",
      "description": "Training step 4575",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:30",
      "total_flops_so_far": 5.443666159640483e+16,
      "budget_used_percent": 54.43666159640483
    },
    {
      "type": "training",
      "description": "Training step 4576",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:30",
      "total_flops_so_far": 5.444854371250902e+16,
      "budget_used_percent": 54.44854371250902
    },
    {
      "type": "training",
      "description": "Training step 4577",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:31",
      "total_flops_so_far": 5.446042582861322e+16,
      "budget_used_percent": 54.46042582861321
    },
    {
      "type": "training",
      "description": "Training step 4578",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:32",
      "total_flops_so_far": 5.447230794471741e+16,
      "budget_used_percent": 54.472307944717414
    },
    {
      "type": "training",
      "description": "Training step 4579",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:33",
      "total_flops_so_far": 5.44841900608216e+16,
      "budget_used_percent": 54.484190060821604
    },
    {
      "type": "training",
      "description": "Training step 4580",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:33",
      "total_flops_so_far": 5.449607217692579e+16,
      "budget_used_percent": 54.49607217692579
    },
    {
      "type": "training",
      "description": "Training step 4581",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:34",
      "total_flops_so_far": 5.450795429302998e+16,
      "budget_used_percent": 54.50795429302998
    },
    {
      "type": "training",
      "description": "Training step 4582",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:35",
      "total_flops_so_far": 5.451983640913418e+16,
      "budget_used_percent": 54.51983640913418
    },
    {
      "type": "training",
      "description": "Training step 4583",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:35",
      "total_flops_so_far": 5.453171852523837e+16,
      "budget_used_percent": 54.53171852523837
    },
    {
      "type": "training",
      "description": "Training step 4584",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:36",
      "total_flops_so_far": 5.454360064134256e+16,
      "budget_used_percent": 54.54360064134256
    },
    {
      "type": "training",
      "description": "Training step 4585",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:37",
      "total_flops_so_far": 5.455548275744675e+16,
      "budget_used_percent": 54.55548275744675
    },
    {
      "type": "training",
      "description": "Training step 4586",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:37",
      "total_flops_so_far": 5.456736487355094e+16,
      "budget_used_percent": 54.567364873550936
    },
    {
      "type": "training",
      "description": "Training step 4587",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:38",
      "total_flops_so_far": 5.457924698965514e+16,
      "budget_used_percent": 54.57924698965514
    },
    {
      "type": "training",
      "description": "Training step 4588",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:39",
      "total_flops_so_far": 5.459112910575933e+16,
      "budget_used_percent": 54.59112910575933
    },
    {
      "type": "training",
      "description": "Training step 4589",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:39",
      "total_flops_so_far": 5.460301122186352e+16,
      "budget_used_percent": 54.60301122186352
    },
    {
      "type": "training",
      "description": "Training step 4590",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:40",
      "total_flops_so_far": 5.461489333796771e+16,
      "budget_used_percent": 54.61489333796771
    },
    {
      "type": "training",
      "description": "Training step 4591",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:41",
      "total_flops_so_far": 5.46267754540719e+16,
      "budget_used_percent": 54.626775454071904
    },
    {
      "type": "training",
      "description": "Training step 4592",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:41",
      "total_flops_so_far": 5.46386575701761e+16,
      "budget_used_percent": 54.63865757017609
    },
    {
      "type": "training",
      "description": "Training step 4593",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:42",
      "total_flops_so_far": 5.465053968628029e+16,
      "budget_used_percent": 54.65053968628028
    },
    {
      "type": "training",
      "description": "Training step 4594",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:43",
      "total_flops_so_far": 5.466242180238448e+16,
      "budget_used_percent": 54.66242180238447
    },
    {
      "type": "training",
      "description": "Training step 4595",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:44",
      "total_flops_so_far": 5.467430391848867e+16,
      "budget_used_percent": 54.674303918488675
    },
    {
      "type": "training",
      "description": "Training step 4596",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:44",
      "total_flops_so_far": 5.468618603459286e+16,
      "budget_used_percent": 54.686186034592865
    },
    {
      "type": "training",
      "description": "Training step 4597",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:45",
      "total_flops_so_far": 5.469806815069706e+16,
      "budget_used_percent": 54.698068150697054
    },
    {
      "type": "training",
      "description": "Training step 4598",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:46",
      "total_flops_so_far": 5.470995026680125e+16,
      "budget_used_percent": 54.70995026680124
    },
    {
      "type": "training",
      "description": "Training step 4599",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:46",
      "total_flops_so_far": 5.472183238290544e+16,
      "budget_used_percent": 54.72183238290545
    },
    {
      "type": "training",
      "description": "Training step 4600",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:47",
      "total_flops_so_far": 5.473371449900963e+16,
      "budget_used_percent": 54.733714499009636
    },
    {
      "type": "training",
      "description": "Training step 4601",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:48",
      "total_flops_so_far": 5.474559661511382e+16,
      "budget_used_percent": 54.745596615113826
    },
    {
      "type": "training",
      "description": "Training step 4602",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:48",
      "total_flops_so_far": 5.475747873121802e+16,
      "budget_used_percent": 54.757478731218015
    },
    {
      "type": "training",
      "description": "Training step 4603",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:49",
      "total_flops_so_far": 5.476936084732221e+16,
      "budget_used_percent": 54.76936084732221
    },
    {
      "type": "training",
      "description": "Training step 4604",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:50",
      "total_flops_so_far": 5.47812429634264e+16,
      "budget_used_percent": 54.7812429634264
    },
    {
      "type": "training",
      "description": "Training step 4605",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:50",
      "total_flops_so_far": 5.479312507953059e+16,
      "budget_used_percent": 54.79312507953059
    },
    {
      "type": "training",
      "description": "Training step 4606",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:51",
      "total_flops_so_far": 5.480500719563478e+16,
      "budget_used_percent": 54.80500719563478
    },
    {
      "type": "training",
      "description": "Training step 4607",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:52",
      "total_flops_so_far": 5.481688931173898e+16,
      "budget_used_percent": 54.81688931173898
    },
    {
      "type": "training",
      "description": "Training step 4608",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:52",
      "total_flops_so_far": 5.482877142784317e+16,
      "budget_used_percent": 54.82877142784317
    },
    {
      "type": "training",
      "description": "Training step 4609",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:53",
      "total_flops_so_far": 5.484065354394736e+16,
      "budget_used_percent": 54.84065354394736
    },
    {
      "type": "training",
      "description": "Training step 4610",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:54",
      "total_flops_so_far": 5.485253566005155e+16,
      "budget_used_percent": 54.85253566005155
    },
    {
      "type": "training",
      "description": "Training step 4611",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:55",
      "total_flops_so_far": 5.486441777615574e+16,
      "budget_used_percent": 54.86441777615575
    },
    {
      "type": "training",
      "description": "Training step 4612",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:56",
      "total_flops_so_far": 5.487629989225994e+16,
      "budget_used_percent": 54.87629989225994
    },
    {
      "type": "training",
      "description": "Training step 4613",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:56",
      "total_flops_so_far": 5.488818200836413e+16,
      "budget_used_percent": 54.888182008364126
    },
    {
      "type": "training",
      "description": "Training step 4614",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:57",
      "total_flops_so_far": 5.490006412446832e+16,
      "budget_used_percent": 54.900064124468315
    },
    {
      "type": "training",
      "description": "Training step 4615",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:58",
      "total_flops_so_far": 5.491194624057251e+16,
      "budget_used_percent": 54.911946240572505
    },
    {
      "type": "training",
      "description": "Training step 4616",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:58",
      "total_flops_so_far": 5.49238283566767e+16,
      "budget_used_percent": 54.92382835667671
    },
    {
      "type": "training",
      "description": "Training step 4617",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:03:59",
      "total_flops_so_far": 5.49357104727809e+16,
      "budget_used_percent": 54.9357104727809
    },
    {
      "type": "training",
      "description": "Training step 4618",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:00",
      "total_flops_so_far": 5.494759258888509e+16,
      "budget_used_percent": 54.94759258888509
    },
    {
      "type": "training",
      "description": "Training step 4619",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:00",
      "total_flops_so_far": 5.495947470498928e+16,
      "budget_used_percent": 54.959474704989276
    },
    {
      "type": "training",
      "description": "Training step 4620",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:01",
      "total_flops_so_far": 5.497135682109347e+16,
      "budget_used_percent": 54.97135682109347
    },
    {
      "type": "training",
      "description": "Training step 4621",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:02",
      "total_flops_so_far": 5.498323893719766e+16,
      "budget_used_percent": 54.98323893719766
    },
    {
      "type": "training",
      "description": "Training step 4622",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:02",
      "total_flops_so_far": 5.499512105330186e+16,
      "budget_used_percent": 54.99512105330185
    },
    {
      "type": "training",
      "description": "Training step 4623",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:03",
      "total_flops_so_far": 5.500700316940605e+16,
      "budget_used_percent": 55.00700316940604
    },
    {
      "type": "training",
      "description": "Training step 4624",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:04",
      "total_flops_so_far": 5.501888528551024e+16,
      "budget_used_percent": 55.018885285510244
    },
    {
      "type": "training",
      "description": "Training step 4625",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:04",
      "total_flops_so_far": 5.503076740161443e+16,
      "budget_used_percent": 55.03076740161443
    },
    {
      "type": "training",
      "description": "Training step 4626",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:05",
      "total_flops_so_far": 5.504264951771862e+16,
      "budget_used_percent": 55.04264951771862
    },
    {
      "type": "training",
      "description": "Training step 4627",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:06",
      "total_flops_so_far": 5.505453163382282e+16,
      "budget_used_percent": 55.05453163382281
    },
    {
      "type": "training",
      "description": "Training step 4628",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:07",
      "total_flops_so_far": 5.506641374992701e+16,
      "budget_used_percent": 55.066413749927015
    },
    {
      "type": "training",
      "description": "Training step 4629",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:07",
      "total_flops_so_far": 5.50782958660312e+16,
      "budget_used_percent": 55.078295866031205
    },
    {
      "type": "training",
      "description": "Training step 4630",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:08",
      "total_flops_so_far": 5.509017798213539e+16,
      "budget_used_percent": 55.090177982135394
    },
    {
      "type": "training",
      "description": "Training step 4631",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:09",
      "total_flops_so_far": 5.510206009823958e+16,
      "budget_used_percent": 55.10206009823958
    },
    {
      "type": "training",
      "description": "Training step 4632",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:09",
      "total_flops_so_far": 5.511394221434378e+16,
      "budget_used_percent": 55.11394221434378
    },
    {
      "type": "training",
      "description": "Training step 4633",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:10",
      "total_flops_so_far": 5.512582433044797e+16,
      "budget_used_percent": 55.12582433044797
    },
    {
      "type": "training",
      "description": "Training step 4634",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:11",
      "total_flops_so_far": 5.513770644655216e+16,
      "budget_used_percent": 55.13770644655216
    },
    {
      "type": "training",
      "description": "Training step 4635",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:11",
      "total_flops_so_far": 5.514958856265635e+16,
      "budget_used_percent": 55.14958856265635
    },
    {
      "type": "training",
      "description": "Training step 4636",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:12",
      "total_flops_so_far": 5.516147067876054e+16,
      "budget_used_percent": 55.16147067876055
    },
    {
      "type": "training",
      "description": "Training step 4637",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:13",
      "total_flops_so_far": 5.517335279486474e+16,
      "budget_used_percent": 55.17335279486474
    },
    {
      "type": "training",
      "description": "Training step 4638",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:13",
      "total_flops_so_far": 5.518523491096893e+16,
      "budget_used_percent": 55.18523491096893
    },
    {
      "type": "training",
      "description": "Training step 4639",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:14",
      "total_flops_so_far": 5.519711702707312e+16,
      "budget_used_percent": 55.19711702707312
    },
    {
      "type": "training",
      "description": "Training step 4640",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:15",
      "total_flops_so_far": 5.520899914317731e+16,
      "budget_used_percent": 55.208999143177316
    },
    {
      "type": "training",
      "description": "Training step 4641",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:15",
      "total_flops_so_far": 5.52208812592815e+16,
      "budget_used_percent": 55.220881259281505
    },
    {
      "type": "training",
      "description": "Training step 4642",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:16",
      "total_flops_so_far": 5.52327633753857e+16,
      "budget_used_percent": 55.232763375385694
    },
    {
      "type": "training",
      "description": "Training step 4643",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:17",
      "total_flops_so_far": 5.524464549148989e+16,
      "budget_used_percent": 55.244645491489884
    },
    {
      "type": "training",
      "description": "Training step 4644",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:18",
      "total_flops_so_far": 5.525652760759408e+16,
      "budget_used_percent": 55.25652760759407
    },
    {
      "type": "training",
      "description": "Training step 4645",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:18",
      "total_flops_so_far": 5.526840972369827e+16,
      "budget_used_percent": 55.26840972369828
    },
    {
      "type": "training",
      "description": "Training step 4646",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:19",
      "total_flops_so_far": 5.528029183980246e+16,
      "budget_used_percent": 55.280291839802466
    },
    {
      "type": "training",
      "description": "Training step 4647",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:20",
      "total_flops_so_far": 5.529217395590666e+16,
      "budget_used_percent": 55.292173955906655
    },
    {
      "type": "training",
      "description": "Training step 4648",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:21",
      "total_flops_so_far": 5.530405607201085e+16,
      "budget_used_percent": 55.304056072010845
    },
    {
      "type": "training",
      "description": "Training step 4649",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:21",
      "total_flops_so_far": 5.531593818811504e+16,
      "budget_used_percent": 55.31593818811504
    },
    {
      "type": "training",
      "description": "Training step 4650",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:22",
      "total_flops_so_far": 5.532782030421923e+16,
      "budget_used_percent": 55.32782030421923
    },
    {
      "type": "training",
      "description": "Training step 4651",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:23",
      "total_flops_so_far": 5.533970242032342e+16,
      "budget_used_percent": 55.33970242032342
    },
    {
      "type": "training",
      "description": "Training step 4652",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:23",
      "total_flops_so_far": 5.535158453642762e+16,
      "budget_used_percent": 55.35158453642761
    },
    {
      "type": "training",
      "description": "Training step 4653",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:24",
      "total_flops_so_far": 5.536346665253181e+16,
      "budget_used_percent": 55.36346665253181
    },
    {
      "type": "training",
      "description": "Training step 4654",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:25",
      "total_flops_so_far": 5.5375348768636e+16,
      "budget_used_percent": 55.375348768636
    },
    {
      "type": "training",
      "description": "Training step 4655",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:25",
      "total_flops_so_far": 5.538723088474019e+16,
      "budget_used_percent": 55.38723088474019
    },
    {
      "type": "training",
      "description": "Training step 4656",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:26",
      "total_flops_so_far": 5.539911300084438e+16,
      "budget_used_percent": 55.39911300084438
    },
    {
      "type": "training",
      "description": "Training step 4657",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:27",
      "total_flops_so_far": 5.541099511694858e+16,
      "budget_used_percent": 55.410995116948584
    },
    {
      "type": "training",
      "description": "Training step 4658",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:28",
      "total_flops_so_far": 5.542287723305277e+16,
      "budget_used_percent": 55.42287723305277
    },
    {
      "type": "training",
      "description": "Training step 4659",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:28",
      "total_flops_so_far": 5.543475934915696e+16,
      "budget_used_percent": 55.43475934915696
    },
    {
      "type": "training",
      "description": "Training step 4660",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:29",
      "total_flops_so_far": 5.544664146526115e+16,
      "budget_used_percent": 55.44664146526115
    },
    {
      "type": "training",
      "description": "Training step 4661",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:30",
      "total_flops_so_far": 5.545852358136534e+16,
      "budget_used_percent": 55.45852358136535
    },
    {
      "type": "training",
      "description": "Training step 4662",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:30",
      "total_flops_so_far": 5.547040569746954e+16,
      "budget_used_percent": 55.47040569746954
    },
    {
      "type": "training",
      "description": "Training step 4663",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:31",
      "total_flops_so_far": 5.548228781357373e+16,
      "budget_used_percent": 55.48228781357373
    },
    {
      "type": "training",
      "description": "Training step 4664",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:32",
      "total_flops_so_far": 5.549416992967792e+16,
      "budget_used_percent": 55.494169929677916
    },
    {
      "type": "training",
      "description": "Training step 4665",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:32",
      "total_flops_so_far": 5.550605204578211e+16,
      "budget_used_percent": 55.50605204578212
    },
    {
      "type": "training",
      "description": "Training step 4666",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:33",
      "total_flops_so_far": 5.55179341618863e+16,
      "budget_used_percent": 55.51793416188631
    },
    {
      "type": "training",
      "description": "Training step 4667",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:34",
      "total_flops_so_far": 5.55298162779905e+16,
      "budget_used_percent": 55.5298162779905
    },
    {
      "type": "training",
      "description": "Training step 4668",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:34",
      "total_flops_so_far": 5.554169839409469e+16,
      "budget_used_percent": 55.54169839409469
    },
    {
      "type": "training",
      "description": "Training step 4669",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:35",
      "total_flops_so_far": 5.555358051019888e+16,
      "budget_used_percent": 55.553580510198884
    },
    {
      "type": "training",
      "description": "Training step 4670",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:36",
      "total_flops_so_far": 5.556546262630307e+16,
      "budget_used_percent": 55.56546262630307
    },
    {
      "type": "training",
      "description": "Training step 4671",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:36",
      "total_flops_so_far": 5.557734474240726e+16,
      "budget_used_percent": 55.57734474240726
    },
    {
      "type": "training",
      "description": "Training step 4672",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:37",
      "total_flops_so_far": 5.558922685851146e+16,
      "budget_used_percent": 55.58922685851145
    },
    {
      "type": "training",
      "description": "Training step 4673",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:38",
      "total_flops_so_far": 5.560110897461565e+16,
      "budget_used_percent": 55.60110897461564
    },
    {
      "type": "training",
      "description": "Training step 4674",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:39",
      "total_flops_so_far": 5.561299109071984e+16,
      "budget_used_percent": 55.612991090719845
    },
    {
      "type": "training",
      "description": "Training step 4675",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:39",
      "total_flops_so_far": 5.562487320682403e+16,
      "budget_used_percent": 55.624873206824034
    },
    {
      "type": "training",
      "description": "Training step 4676",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:40",
      "total_flops_so_far": 5.563675532292822e+16,
      "budget_used_percent": 55.636755322928224
    },
    {
      "type": "training",
      "description": "Training step 4677",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:41",
      "total_flops_so_far": 5.564863743903242e+16,
      "budget_used_percent": 55.64863743903241
    },
    {
      "type": "training",
      "description": "Training step 4678",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:41",
      "total_flops_so_far": 5.566051955513661e+16,
      "budget_used_percent": 55.66051955513661
    },
    {
      "type": "training",
      "description": "Training step 4679",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:42",
      "total_flops_so_far": 5.56724016712408e+16,
      "budget_used_percent": 55.6724016712408
    },
    {
      "type": "training",
      "description": "Training step 4680",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:43",
      "total_flops_so_far": 5.568428378734499e+16,
      "budget_used_percent": 55.68428378734499
    },
    {
      "type": "training",
      "description": "Training step 4681",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:43",
      "total_flops_so_far": 5.569616590344918e+16,
      "budget_used_percent": 55.69616590344918
    },
    {
      "type": "training",
      "description": "Training step 4682",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:44",
      "total_flops_so_far": 5.570804801955338e+16,
      "budget_used_percent": 55.70804801955338
    },
    {
      "type": "training",
      "description": "Training step 4683",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:45",
      "total_flops_so_far": 5.571993013565757e+16,
      "budget_used_percent": 55.71993013565757
    },
    {
      "type": "training",
      "description": "Training step 4684",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:45",
      "total_flops_so_far": 5.573181225176176e+16,
      "budget_used_percent": 55.73181225176176
    },
    {
      "type": "training",
      "description": "Training step 4685",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:46",
      "total_flops_so_far": 5.574369436786595e+16,
      "budget_used_percent": 55.74369436786595
    },
    {
      "type": "training",
      "description": "Training step 4686",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:47",
      "total_flops_so_far": 5.575557648397014e+16,
      "budget_used_percent": 55.755576483970145
    },
    {
      "type": "training",
      "description": "Training step 4687",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:48",
      "total_flops_so_far": 5.576745860007434e+16,
      "budget_used_percent": 55.767458600074335
    },
    {
      "type": "training",
      "description": "Training step 4688",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:48",
      "total_flops_so_far": 5.577934071617853e+16,
      "budget_used_percent": 55.779340716178524
    },
    {
      "type": "training",
      "description": "Training step 4689",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:49",
      "total_flops_so_far": 5.579122283228272e+16,
      "budget_used_percent": 55.79122283228271
    },
    {
      "type": "training",
      "description": "Training step 4690",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:50",
      "total_flops_so_far": 5.580310494838691e+16,
      "budget_used_percent": 55.80310494838692
    },
    {
      "type": "training",
      "description": "Training step 4691",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:51",
      "total_flops_so_far": 5.58149870644911e+16,
      "budget_used_percent": 55.814987064491106
    },
    {
      "type": "training",
      "description": "Training step 4692",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:51",
      "total_flops_so_far": 5.58268691805953e+16,
      "budget_used_percent": 55.826869180595295
    },
    {
      "type": "training",
      "description": "Training step 4693",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:52",
      "total_flops_so_far": 5.583875129669949e+16,
      "budget_used_percent": 55.838751296699485
    },
    {
      "type": "training",
      "description": "Training step 4694",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:53",
      "total_flops_so_far": 5.585063341280368e+16,
      "budget_used_percent": 55.85063341280369
    },
    {
      "type": "training",
      "description": "Training step 4695",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:53",
      "total_flops_so_far": 5.586251552890787e+16,
      "budget_used_percent": 55.86251552890788
    },
    {
      "type": "training",
      "description": "Training step 4696",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:54",
      "total_flops_so_far": 5.587439764501206e+16,
      "budget_used_percent": 55.87439764501207
    },
    {
      "type": "training",
      "description": "Training step 4697",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:55",
      "total_flops_so_far": 5.588627976111626e+16,
      "budget_used_percent": 55.886279761116256
    },
    {
      "type": "training",
      "description": "Training step 4698",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:55",
      "total_flops_so_far": 5.589816187722045e+16,
      "budget_used_percent": 55.898161877220446
    },
    {
      "type": "training",
      "description": "Training step 4699",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:56",
      "total_flops_so_far": 5.591004399332464e+16,
      "budget_used_percent": 55.91004399332464
    },
    {
      "type": "training",
      "description": "Training step 4700",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:57",
      "total_flops_so_far": 5.592192610942883e+16,
      "budget_used_percent": 55.92192610942883
    },
    {
      "type": "training",
      "description": "Training step 4701",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:57",
      "total_flops_so_far": 5.593380822553302e+16,
      "budget_used_percent": 55.93380822553302
    },
    {
      "type": "training",
      "description": "Training step 4702",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:58",
      "total_flops_so_far": 5.594569034163722e+16,
      "budget_used_percent": 55.94569034163721
    },
    {
      "type": "training",
      "description": "Training step 4703",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:04:59",
      "total_flops_so_far": 5.595757245774141e+16,
      "budget_used_percent": 55.95757245774141
    },
    {
      "type": "training",
      "description": "Training step 4704",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:00",
      "total_flops_so_far": 5.59694545738456e+16,
      "budget_used_percent": 55.9694545738456
    },
    {
      "type": "training",
      "description": "Training step 4705",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:00",
      "total_flops_so_far": 5.598133668994979e+16,
      "budget_used_percent": 55.98133668994979
    },
    {
      "type": "training",
      "description": "Training step 4706",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:01",
      "total_flops_so_far": 5.599321880605398e+16,
      "budget_used_percent": 55.99321880605398
    },
    {
      "type": "training",
      "description": "Training step 4707",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:02",
      "total_flops_so_far": 5.600510092215818e+16,
      "budget_used_percent": 56.00510092215818
    },
    {
      "type": "training",
      "description": "Training step 4708",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:02",
      "total_flops_so_far": 5.601698303826237e+16,
      "budget_used_percent": 56.01698303826237
    },
    {
      "type": "training",
      "description": "Training step 4709",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:03",
      "total_flops_so_far": 5.602886515436656e+16,
      "budget_used_percent": 56.02886515436656
    },
    {
      "type": "training",
      "description": "Training step 4710",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:04",
      "total_flops_so_far": 5.604074727047075e+16,
      "budget_used_percent": 56.040747270470746
    },
    {
      "type": "training",
      "description": "Training step 4711",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:04",
      "total_flops_so_far": 5.605262938657494e+16,
      "budget_used_percent": 56.05262938657495
    },
    {
      "type": "training",
      "description": "Training step 4712",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:05",
      "total_flops_so_far": 5.606451150267914e+16,
      "budget_used_percent": 56.06451150267914
    },
    {
      "type": "training",
      "description": "Training step 4713",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:06",
      "total_flops_so_far": 5.607639361878333e+16,
      "budget_used_percent": 56.07639361878333
    },
    {
      "type": "training",
      "description": "Training step 4714",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:06",
      "total_flops_so_far": 5.608827573488752e+16,
      "budget_used_percent": 56.08827573488752
    },
    {
      "type": "training",
      "description": "Training step 4715",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:07",
      "total_flops_so_far": 5.610015785099171e+16,
      "budget_used_percent": 56.100157850991714
    },
    {
      "type": "training",
      "description": "Training step 4716",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:08",
      "total_flops_so_far": 5.61120399670959e+16,
      "budget_used_percent": 56.1120399670959
    },
    {
      "type": "training",
      "description": "Training step 4717",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:08",
      "total_flops_so_far": 5.61239220832001e+16,
      "budget_used_percent": 56.12392208320009
    },
    {
      "type": "training",
      "description": "Training step 4718",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:09",
      "total_flops_so_far": 5.613580419930429e+16,
      "budget_used_percent": 56.13580419930428
    },
    {
      "type": "training",
      "description": "Training step 4719",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:10",
      "total_flops_so_far": 5.614768631540848e+16,
      "budget_used_percent": 56.147686315408485
    },
    {
      "type": "training",
      "description": "Training step 4720",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:11",
      "total_flops_so_far": 5.615956843151267e+16,
      "budget_used_percent": 56.159568431512675
    },
    {
      "type": "training",
      "description": "Training step 4721",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:11",
      "total_flops_so_far": 5.617145054761686e+16,
      "budget_used_percent": 56.171450547616864
    },
    {
      "type": "training",
      "description": "Training step 4722",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:12",
      "total_flops_so_far": 5.618333266372106e+16,
      "budget_used_percent": 56.18333266372105
    },
    {
      "type": "training",
      "description": "Training step 4723",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:13",
      "total_flops_so_far": 5.619521477982525e+16,
      "budget_used_percent": 56.19521477982525
    },
    {
      "type": "training",
      "description": "Training step 4724",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:14",
      "total_flops_so_far": 5.620709689592944e+16,
      "budget_used_percent": 56.20709689592944
    },
    {
      "type": "training",
      "description": "Training step 4725",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:14",
      "total_flops_so_far": 5.621897901203363e+16,
      "budget_used_percent": 56.21897901203363
    },
    {
      "type": "training",
      "description": "Training step 4726",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:15",
      "total_flops_so_far": 5.623086112813782e+16,
      "budget_used_percent": 56.23086112813782
    },
    {
      "type": "training",
      "description": "Training step 4727",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:16",
      "total_flops_so_far": 5.624274324424202e+16,
      "budget_used_percent": 56.24274324424201
    },
    {
      "type": "training",
      "description": "Training step 4728",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:16",
      "total_flops_so_far": 5.625462536034621e+16,
      "budget_used_percent": 56.25462536034621
    },
    {
      "type": "training",
      "description": "Training step 4729",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:17",
      "total_flops_so_far": 5.62665074764504e+16,
      "budget_used_percent": 56.2665074764504
    },
    {
      "type": "training",
      "description": "Training step 4730",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:18",
      "total_flops_so_far": 5.627838959255459e+16,
      "budget_used_percent": 56.27838959255459
    },
    {
      "type": "training",
      "description": "Training step 4731",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:18",
      "total_flops_so_far": 5.629027170865878e+16,
      "budget_used_percent": 56.29027170865878
    },
    {
      "type": "training",
      "description": "Training step 4732",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:19",
      "total_flops_so_far": 5.630215382476298e+16,
      "budget_used_percent": 56.30215382476298
    },
    {
      "type": "training",
      "description": "Training step 4733",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:20",
      "total_flops_so_far": 5.631403594086717e+16,
      "budget_used_percent": 56.31403594086717
    },
    {
      "type": "training",
      "description": "Training step 4734",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:20",
      "total_flops_so_far": 5.632591805697136e+16,
      "budget_used_percent": 56.32591805697136
    },
    {
      "type": "training",
      "description": "Training step 4735",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:21",
      "total_flops_so_far": 5.633780017307555e+16,
      "budget_used_percent": 56.33780017307555
    },
    {
      "type": "training",
      "description": "Training step 4736",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:22",
      "total_flops_so_far": 5.634968228917974e+16,
      "budget_used_percent": 56.349682289179746
    },
    {
      "type": "training",
      "description": "Training step 4737",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:22",
      "total_flops_so_far": 5.636156440528394e+16,
      "budget_used_percent": 56.361564405283936
    },
    {
      "type": "training",
      "description": "Training step 4738",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:23",
      "total_flops_so_far": 5.637344652138813e+16,
      "budget_used_percent": 56.373446521388125
    },
    {
      "type": "training",
      "description": "Training step 4739",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:24",
      "total_flops_so_far": 5.638532863749232e+16,
      "budget_used_percent": 56.385328637492314
    },
    {
      "type": "training",
      "description": "Training step 4740",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:24",
      "total_flops_so_far": 5.639721075359651e+16,
      "budget_used_percent": 56.39721075359652
    },
    {
      "type": "training",
      "description": "Training step 4741",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:25",
      "total_flops_so_far": 5.64090928697007e+16,
      "budget_used_percent": 56.40909286970071
    },
    {
      "type": "training",
      "description": "Training step 4742",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:26",
      "total_flops_so_far": 5.64209749858049e+16,
      "budget_used_percent": 56.4209749858049
    },
    {
      "type": "training",
      "description": "Training step 4743",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:27",
      "total_flops_so_far": 5.643285710190909e+16,
      "budget_used_percent": 56.432857101909086
    },
    {
      "type": "training",
      "description": "Training step 4744",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:27",
      "total_flops_so_far": 5.644473921801328e+16,
      "budget_used_percent": 56.44473921801328
    },
    {
      "type": "training",
      "description": "Training step 4745",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:28",
      "total_flops_so_far": 5.645662133411747e+16,
      "budget_used_percent": 56.45662133411747
    },
    {
      "type": "training",
      "description": "Training step 4746",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:29",
      "total_flops_so_far": 5.646850345022166e+16,
      "budget_used_percent": 56.46850345022166
    },
    {
      "type": "training",
      "description": "Training step 4747",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:29",
      "total_flops_so_far": 5.648038556632586e+16,
      "budget_used_percent": 56.48038556632585
    },
    {
      "type": "training",
      "description": "Training step 4748",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:30",
      "total_flops_so_far": 5.649226768243005e+16,
      "budget_used_percent": 56.492267682430054
    },
    {
      "type": "training",
      "description": "Training step 4749",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:31",
      "total_flops_so_far": 5.650414979853424e+16,
      "budget_used_percent": 56.50414979853424
    },
    {
      "type": "training",
      "description": "Training step 4750",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:31",
      "total_flops_so_far": 5.651603191463843e+16,
      "budget_used_percent": 56.51603191463843
    },
    {
      "type": "training",
      "description": "Training step 4751",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:32",
      "total_flops_so_far": 5.652791403074262e+16,
      "budget_used_percent": 56.52791403074262
    },
    {
      "type": "training",
      "description": "Training step 4752",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:33",
      "total_flops_so_far": 5.653979614684682e+16,
      "budget_used_percent": 56.53979614684682
    },
    {
      "type": "training",
      "description": "Training step 4753",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:33",
      "total_flops_so_far": 5.655167826295101e+16,
      "budget_used_percent": 56.55167826295101
    },
    {
      "type": "training",
      "description": "Training step 4754",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:34",
      "total_flops_so_far": 5.65635603790552e+16,
      "budget_used_percent": 56.5635603790552
    },
    {
      "type": "training",
      "description": "Training step 4755",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:35",
      "total_flops_so_far": 5.657544249515939e+16,
      "budget_used_percent": 56.575442495159386
    },
    {
      "type": "training",
      "description": "Training step 4756",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:35",
      "total_flops_so_far": 5.658732461126358e+16,
      "budget_used_percent": 56.587324611263575
    },
    {
      "type": "training",
      "description": "Training step 4757",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:36",
      "total_flops_so_far": 5.659920672736778e+16,
      "budget_used_percent": 56.59920672736778
    },
    {
      "type": "training",
      "description": "Training step 4758",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:37",
      "total_flops_so_far": 5.661108884347197e+16,
      "budget_used_percent": 56.61108884347197
    },
    {
      "type": "training",
      "description": "Training step 4759",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:38",
      "total_flops_so_far": 5.662297095957616e+16,
      "budget_used_percent": 56.62297095957616
    },
    {
      "type": "training",
      "description": "Training step 4760",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:39",
      "total_flops_so_far": 5.663485307568035e+16,
      "budget_used_percent": 56.63485307568035
    },
    {
      "type": "training",
      "description": "Training step 4761",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:39",
      "total_flops_so_far": 5.664673519178454e+16,
      "budget_used_percent": 56.64673519178455
    },
    {
      "type": "training",
      "description": "Training step 4762",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:40",
      "total_flops_so_far": 5.665861730788874e+16,
      "budget_used_percent": 56.65861730788874
    },
    {
      "type": "training",
      "description": "Training step 4763",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:41",
      "total_flops_so_far": 5.667049942399293e+16,
      "budget_used_percent": 56.67049942399293
    },
    {
      "type": "training",
      "description": "Training step 4764",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:41",
      "total_flops_so_far": 5.668238154009712e+16,
      "budget_used_percent": 56.68238154009712
    },
    {
      "type": "training",
      "description": "Training step 4765",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:42",
      "total_flops_so_far": 5.669426365620131e+16,
      "budget_used_percent": 56.694263656201315
    },
    {
      "type": "training",
      "description": "Training step 4766",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:43",
      "total_flops_so_far": 5.67061457723055e+16,
      "budget_used_percent": 56.706145772305504
    },
    {
      "type": "training",
      "description": "Training step 4767",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:43",
      "total_flops_so_far": 5.67180278884097e+16,
      "budget_used_percent": 56.718027888409694
    },
    {
      "type": "training",
      "description": "Training step 4768",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:44",
      "total_flops_so_far": 5.672991000451389e+16,
      "budget_used_percent": 56.72991000451388
    },
    {
      "type": "training",
      "description": "Training step 4769",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:45",
      "total_flops_so_far": 5.674179212061808e+16,
      "budget_used_percent": 56.741792120618086
    },
    {
      "type": "training",
      "description": "Training step 4770",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:45",
      "total_flops_so_far": 5.675367423672227e+16,
      "budget_used_percent": 56.753674236722276
    },
    {
      "type": "training",
      "description": "Training step 4771",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:46",
      "total_flops_so_far": 5.676555635282646e+16,
      "budget_used_percent": 56.765556352826465
    },
    {
      "type": "training",
      "description": "Training step 4772",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:47",
      "total_flops_so_far": 5.677743846893066e+16,
      "budget_used_percent": 56.777438468930654
    },
    {
      "type": "training",
      "description": "Training step 4773",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:48",
      "total_flops_so_far": 5.678932058503485e+16,
      "budget_used_percent": 56.78932058503485
    },
    {
      "type": "training",
      "description": "Training step 4774",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:48",
      "total_flops_so_far": 5.680120270113904e+16,
      "budget_used_percent": 56.80120270113904
    },
    {
      "type": "training",
      "description": "Training step 4775",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:49",
      "total_flops_so_far": 5.681308481724323e+16,
      "budget_used_percent": 56.81308481724323
    },
    {
      "type": "training",
      "description": "Training step 4776",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:50",
      "total_flops_so_far": 5.682496693334742e+16,
      "budget_used_percent": 56.82496693334742
    },
    {
      "type": "training",
      "description": "Training step 4777",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:50",
      "total_flops_so_far": 5.683684904945162e+16,
      "budget_used_percent": 56.83684904945162
    },
    {
      "type": "training",
      "description": "Training step 4778",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:51",
      "total_flops_so_far": 5.684873116555581e+16,
      "budget_used_percent": 56.84873116555581
    },
    {
      "type": "training",
      "description": "Training step 4779",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:52",
      "total_flops_so_far": 5.686061328166e+16,
      "budget_used_percent": 56.86061328166
    },
    {
      "type": "training",
      "description": "Training step 4780",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:52",
      "total_flops_so_far": 5.687249539776419e+16,
      "budget_used_percent": 56.87249539776419
    },
    {
      "type": "training",
      "description": "Training step 4781",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:53",
      "total_flops_so_far": 5.688437751386838e+16,
      "budget_used_percent": 56.88437751386839
    },
    {
      "type": "training",
      "description": "Training step 4782",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:54",
      "total_flops_so_far": 5.689625962997258e+16,
      "budget_used_percent": 56.896259629972576
    },
    {
      "type": "training",
      "description": "Training step 4783",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:54",
      "total_flops_so_far": 5.690814174607677e+16,
      "budget_used_percent": 56.908141746076765
    },
    {
      "type": "training",
      "description": "Training step 4784",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:55",
      "total_flops_so_far": 5.692002386218096e+16,
      "budget_used_percent": 56.920023862180955
    },
    {
      "type": "training",
      "description": "Training step 4785",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:56",
      "total_flops_so_far": 5.693190597828515e+16,
      "budget_used_percent": 56.931905978285144
    },
    {
      "type": "training",
      "description": "Training step 4786",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:57",
      "total_flops_so_far": 5.694378809438934e+16,
      "budget_used_percent": 56.94378809438935
    },
    {
      "type": "training",
      "description": "Training step 4787",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:57",
      "total_flops_so_far": 5.695567021049354e+16,
      "budget_used_percent": 56.95567021049354
    },
    {
      "type": "training",
      "description": "Training step 4788",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:58",
      "total_flops_so_far": 5.696755232659773e+16,
      "budget_used_percent": 56.967552326597726
    },
    {
      "type": "training",
      "description": "Training step 4789",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:59",
      "total_flops_so_far": 5.697943444270192e+16,
      "budget_used_percent": 56.979434442701915
    },
    {
      "type": "training",
      "description": "Training step 4790",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:05:59",
      "total_flops_so_far": 5.699131655880611e+16,
      "budget_used_percent": 56.99131655880612
    },
    {
      "type": "training",
      "description": "Training step 4791",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:00",
      "total_flops_so_far": 5.70031986749103e+16,
      "budget_used_percent": 57.00319867491031
    },
    {
      "type": "training",
      "description": "Training step 4792",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:01",
      "total_flops_so_far": 5.70150807910145e+16,
      "budget_used_percent": 57.0150807910145
    },
    {
      "type": "training",
      "description": "Training step 4793",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:01",
      "total_flops_so_far": 5.702696290711869e+16,
      "budget_used_percent": 57.02696290711869
    },
    {
      "type": "training",
      "description": "Training step 4794",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:02",
      "total_flops_so_far": 5.703884502322288e+16,
      "budget_used_percent": 57.03884502322288
    },
    {
      "type": "training",
      "description": "Training step 4795",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:03",
      "total_flops_so_far": 5.705072713932707e+16,
      "budget_used_percent": 57.05072713932707
    },
    {
      "type": "training",
      "description": "Training step 4796",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:04",
      "total_flops_so_far": 5.706260925543126e+16,
      "budget_used_percent": 57.06260925543126
    },
    {
      "type": "training",
      "description": "Training step 4797",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:04",
      "total_flops_so_far": 5.707449137153546e+16,
      "budget_used_percent": 57.07449137153545
    },
    {
      "type": "training",
      "description": "Training step 4798",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:05",
      "total_flops_so_far": 5.708637348763965e+16,
      "budget_used_percent": 57.086373487639655
    },
    {
      "type": "training",
      "description": "Training step 4799",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:06",
      "total_flops_so_far": 5.709825560374384e+16,
      "budget_used_percent": 57.098255603743844
    },
    {
      "type": "training",
      "description": "Training step 4800",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:07",
      "total_flops_so_far": 5.711013771984803e+16,
      "budget_used_percent": 57.110137719848034
    },
    {
      "type": "training",
      "description": "Training step 4801",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:07",
      "total_flops_so_far": 5.712201983595222e+16,
      "budget_used_percent": 57.12201983595222
    },
    {
      "type": "training",
      "description": "Training step 4802",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:08",
      "total_flops_so_far": 5.713390195205642e+16,
      "budget_used_percent": 57.13390195205642
    },
    {
      "type": "training",
      "description": "Training step 4803",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:09",
      "total_flops_so_far": 5.714578406816061e+16,
      "budget_used_percent": 57.14578406816061
    },
    {
      "type": "training",
      "description": "Training step 4804",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:09",
      "total_flops_so_far": 5.71576661842648e+16,
      "budget_used_percent": 57.1576661842648
    },
    {
      "type": "training",
      "description": "Training step 4805",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:10",
      "total_flops_so_far": 5.716954830036899e+16,
      "budget_used_percent": 57.16954830036899
    },
    {
      "type": "training",
      "description": "Training step 4806",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:11",
      "total_flops_so_far": 5.718143041647318e+16,
      "budget_used_percent": 57.18143041647319
    },
    {
      "type": "training",
      "description": "Training step 4807",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:11",
      "total_flops_so_far": 5.719331253257738e+16,
      "budget_used_percent": 57.19331253257738
    },
    {
      "type": "training",
      "description": "Training step 4808",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:12",
      "total_flops_so_far": 5.720519464868157e+16,
      "budget_used_percent": 57.20519464868157
    },
    {
      "type": "training",
      "description": "Training step 4809",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:13",
      "total_flops_so_far": 5.721707676478576e+16,
      "budget_used_percent": 57.21707676478576
    },
    {
      "type": "training",
      "description": "Training step 4810",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:13",
      "total_flops_so_far": 5.722895888088995e+16,
      "budget_used_percent": 57.228958880889955
    },
    {
      "type": "training",
      "description": "Training step 4811",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:14",
      "total_flops_so_far": 5.724084099699414e+16,
      "budget_used_percent": 57.240840996994145
    },
    {
      "type": "training",
      "description": "Training step 4812",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:15",
      "total_flops_so_far": 5.725272311309834e+16,
      "budget_used_percent": 57.252723113098334
    },
    {
      "type": "training",
      "description": "Training step 4813",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:16",
      "total_flops_so_far": 5.726460522920253e+16,
      "budget_used_percent": 57.26460522920252
    },
    {
      "type": "training",
      "description": "Training step 4814",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:16",
      "total_flops_so_far": 5.727648734530672e+16,
      "budget_used_percent": 57.27648734530671
    },
    {
      "type": "training",
      "description": "Training step 4815",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:17",
      "total_flops_so_far": 5.728836946141091e+16,
      "budget_used_percent": 57.288369461410916
    },
    {
      "type": "training",
      "description": "Training step 4816",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:18",
      "total_flops_so_far": 5.73002515775151e+16,
      "budget_used_percent": 57.300251577515105
    },
    {
      "type": "training",
      "description": "Training step 4817",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:18",
      "total_flops_so_far": 5.73121336936193e+16,
      "budget_used_percent": 57.312133693619295
    },
    {
      "type": "training",
      "description": "Training step 4818",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:19",
      "total_flops_so_far": 5.732401580972349e+16,
      "budget_used_percent": 57.324015809723484
    },
    {
      "type": "training",
      "description": "Training step 4819",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:20",
      "total_flops_so_far": 5.733589792582768e+16,
      "budget_used_percent": 57.33589792582768
    },
    {
      "type": "training",
      "description": "Training step 4820",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:20",
      "total_flops_so_far": 5.734778004193187e+16,
      "budget_used_percent": 57.34778004193187
    },
    {
      "type": "training",
      "description": "Training step 4821",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:21",
      "total_flops_so_far": 5.735966215803606e+16,
      "budget_used_percent": 57.35966215803606
    },
    {
      "type": "training",
      "description": "Training step 4822",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:22",
      "total_flops_so_far": 5.737154427414026e+16,
      "budget_used_percent": 57.37154427414025
    },
    {
      "type": "training",
      "description": "Training step 4823",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:22",
      "total_flops_so_far": 5.738342639024445e+16,
      "budget_used_percent": 57.38342639024445
    },
    {
      "type": "training",
      "description": "Training step 4824",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:23",
      "total_flops_so_far": 5.739530850634864e+16,
      "budget_used_percent": 57.39530850634864
    },
    {
      "type": "training",
      "description": "Training step 4825",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:24",
      "total_flops_so_far": 5.740719062245283e+16,
      "budget_used_percent": 57.40719062245283
    },
    {
      "type": "training",
      "description": "Training step 4826",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:24",
      "total_flops_so_far": 5.741907273855702e+16,
      "budget_used_percent": 57.41907273855702
    },
    {
      "type": "training",
      "description": "Training step 4827",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:25",
      "total_flops_so_far": 5.743095485466122e+16,
      "budget_used_percent": 57.43095485466122
    },
    {
      "type": "training",
      "description": "Training step 4828",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:26",
      "total_flops_so_far": 5.744283697076541e+16,
      "budget_used_percent": 57.44283697076541
    },
    {
      "type": "training",
      "description": "Training step 4829",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:27",
      "total_flops_so_far": 5.74547190868696e+16,
      "budget_used_percent": 57.4547190868696
    },
    {
      "type": "training",
      "description": "Training step 4830",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:27",
      "total_flops_so_far": 5.746660120297379e+16,
      "budget_used_percent": 57.46660120297379
    },
    {
      "type": "training",
      "description": "Training step 4831",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:28",
      "total_flops_so_far": 5.747848331907798e+16,
      "budget_used_percent": 57.47848331907799
    },
    {
      "type": "training",
      "description": "Training step 4832",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:29",
      "total_flops_so_far": 5.749036543518218e+16,
      "budget_used_percent": 57.49036543518218
    },
    {
      "type": "training",
      "description": "Training step 4833",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:30",
      "total_flops_so_far": 5.750224755128637e+16,
      "budget_used_percent": 57.502247551286366
    },
    {
      "type": "training",
      "description": "Training step 4834",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:30",
      "total_flops_so_far": 5.751412966739056e+16,
      "budget_used_percent": 57.514129667390556
    },
    {
      "type": "training",
      "description": "Training step 4835",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:31",
      "total_flops_so_far": 5.752601178349475e+16,
      "budget_used_percent": 57.52601178349476
    },
    {
      "type": "training",
      "description": "Training step 4836",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:32",
      "total_flops_so_far": 5.753789389959894e+16,
      "budget_used_percent": 57.53789389959895
    },
    {
      "type": "training",
      "description": "Training step 4837",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:32",
      "total_flops_so_far": 5.754977601570314e+16,
      "budget_used_percent": 57.54977601570314
    },
    {
      "type": "training",
      "description": "Training step 4838",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:33",
      "total_flops_so_far": 5.756165813180733e+16,
      "budget_used_percent": 57.56165813180733
    },
    {
      "type": "training",
      "description": "Training step 4839",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:34",
      "total_flops_so_far": 5.757354024791152e+16,
      "budget_used_percent": 57.573540247911524
    },
    {
      "type": "training",
      "description": "Training step 4840",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:35",
      "total_flops_so_far": 5.758542236401571e+16,
      "budget_used_percent": 57.58542236401571
    },
    {
      "type": "training",
      "description": "Training step 4841",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:35",
      "total_flops_so_far": 5.75973044801199e+16,
      "budget_used_percent": 57.5973044801199
    },
    {
      "type": "training",
      "description": "Training step 4842",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:36",
      "total_flops_so_far": 5.76091865962241e+16,
      "budget_used_percent": 57.60918659622409
    },
    {
      "type": "training",
      "description": "Training step 4843",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:37",
      "total_flops_so_far": 5.762106871232829e+16,
      "budget_used_percent": 57.62106871232828
    },
    {
      "type": "training",
      "description": "Training step 4844",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:37",
      "total_flops_so_far": 5.763295082843248e+16,
      "budget_used_percent": 57.632950828432485
    },
    {
      "type": "training",
      "description": "Training step 4845",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:38",
      "total_flops_so_far": 5.764483294453667e+16,
      "budget_used_percent": 57.644832944536674
    },
    {
      "type": "training",
      "description": "Training step 4846",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:39",
      "total_flops_so_far": 5.765671506064086e+16,
      "budget_used_percent": 57.65671506064086
    },
    {
      "type": "training",
      "description": "Training step 4847",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:39",
      "total_flops_so_far": 5.766859717674506e+16,
      "budget_used_percent": 57.66859717674505
    },
    {
      "type": "training",
      "description": "Training step 4848",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:40",
      "total_flops_so_far": 5.768047929284925e+16,
      "budget_used_percent": 57.68047929284925
    },
    {
      "type": "training",
      "description": "Training step 4849",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:41",
      "total_flops_so_far": 5.769236140895344e+16,
      "budget_used_percent": 57.69236140895344
    },
    {
      "type": "training",
      "description": "Training step 4850",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:41",
      "total_flops_so_far": 5.770424352505763e+16,
      "budget_used_percent": 57.70424352505763
    },
    {
      "type": "training",
      "description": "Training step 4851",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:42",
      "total_flops_so_far": 5.771612564116182e+16,
      "budget_used_percent": 57.71612564116182
    },
    {
      "type": "training",
      "description": "Training step 4852",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:43",
      "total_flops_so_far": 5.772800775726602e+16,
      "budget_used_percent": 57.72800775726602
    },
    {
      "type": "training",
      "description": "Training step 4853",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:44",
      "total_flops_so_far": 5.773988987337021e+16,
      "budget_used_percent": 57.73988987337021
    },
    {
      "type": "training",
      "description": "Training step 4854",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:44",
      "total_flops_so_far": 5.77517719894744e+16,
      "budget_used_percent": 57.7517719894744
    },
    {
      "type": "training",
      "description": "Training step 4855",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:45",
      "total_flops_so_far": 5.776365410557859e+16,
      "budget_used_percent": 57.76365410557859
    },
    {
      "type": "training",
      "description": "Training step 4856",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:46",
      "total_flops_so_far": 5.777553622168278e+16,
      "budget_used_percent": 57.775536221682785
    },
    {
      "type": "training",
      "description": "Training step 4857",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:46",
      "total_flops_so_far": 5.778741833778698e+16,
      "budget_used_percent": 57.787418337786974
    },
    {
      "type": "training",
      "description": "Training step 4858",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:47",
      "total_flops_so_far": 5.779930045389117e+16,
      "budget_used_percent": 57.79930045389116
    },
    {
      "type": "training",
      "description": "Training step 4859",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:48",
      "total_flops_so_far": 5.781118256999536e+16,
      "budget_used_percent": 57.81118256999535
    },
    {
      "type": "training",
      "description": "Training step 4860",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:48",
      "total_flops_so_far": 5.782306468609955e+16,
      "budget_used_percent": 57.823064686099556
    },
    {
      "type": "training",
      "description": "Training step 4861",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:49",
      "total_flops_so_far": 5.783494680220374e+16,
      "budget_used_percent": 57.834946802203746
    },
    {
      "type": "training",
      "description": "Training step 4862",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:50",
      "total_flops_so_far": 5.784682891830794e+16,
      "budget_used_percent": 57.846828918307935
    },
    {
      "type": "training",
      "description": "Training step 4863",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:50",
      "total_flops_so_far": 5.785871103441213e+16,
      "budget_used_percent": 57.858711034412124
    },
    {
      "type": "training",
      "description": "Training step 4864",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:51",
      "total_flops_so_far": 5.787059315051632e+16,
      "budget_used_percent": 57.87059315051633
    },
    {
      "type": "training",
      "description": "Training step 4865",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:52",
      "total_flops_so_far": 5.788247526662051e+16,
      "budget_used_percent": 57.88247526662052
    },
    {
      "type": "training",
      "description": "Training step 4866",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:53",
      "total_flops_so_far": 5.78943573827247e+16,
      "budget_used_percent": 57.894357382724706
    },
    {
      "type": "training",
      "description": "Training step 4867",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:54",
      "total_flops_so_far": 5.79062394988289e+16,
      "budget_used_percent": 57.906239498828896
    },
    {
      "type": "training",
      "description": "Training step 4868",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:54",
      "total_flops_so_far": 5.791812161493309e+16,
      "budget_used_percent": 57.91812161493309
    },
    {
      "type": "training",
      "description": "Training step 4869",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:55",
      "total_flops_so_far": 5.793000373103728e+16,
      "budget_used_percent": 57.93000373103728
    },
    {
      "type": "training",
      "description": "Training step 4870",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:56",
      "total_flops_so_far": 5.794188584714147e+16,
      "budget_used_percent": 57.94188584714147
    },
    {
      "type": "training",
      "description": "Training step 4871",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:56",
      "total_flops_so_far": 5.795376796324566e+16,
      "budget_used_percent": 57.95376796324566
    },
    {
      "type": "training",
      "description": "Training step 4872",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:57",
      "total_flops_so_far": 5.796565007934986e+16,
      "budget_used_percent": 57.96565007934985
    },
    {
      "type": "training",
      "description": "Training step 4873",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:58",
      "total_flops_so_far": 5.797753219545405e+16,
      "budget_used_percent": 57.97753219545405
    },
    {
      "type": "training",
      "description": "Training step 4874",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:58",
      "total_flops_so_far": 5.798941431155824e+16,
      "budget_used_percent": 57.98941431155824
    },
    {
      "type": "training",
      "description": "Training step 4875",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:06:59",
      "total_flops_so_far": 5.800129642766243e+16,
      "budget_used_percent": 58.00129642766243
    },
    {
      "type": "training",
      "description": "Training step 4876",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:00",
      "total_flops_so_far": 5.801317854376662e+16,
      "budget_used_percent": 58.01317854376662
    },
    {
      "type": "training",
      "description": "Training step 4877",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:00",
      "total_flops_so_far": 5.802506065987082e+16,
      "budget_used_percent": 58.02506065987082
    },
    {
      "type": "training",
      "description": "Training step 4878",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:01",
      "total_flops_so_far": 5.803694277597501e+16,
      "budget_used_percent": 58.03694277597501
    },
    {
      "type": "training",
      "description": "Training step 4879",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:02",
      "total_flops_so_far": 5.80488248920792e+16,
      "budget_used_percent": 58.048824892079196
    },
    {
      "type": "training",
      "description": "Training step 4880",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:03",
      "total_flops_so_far": 5.806070700818339e+16,
      "budget_used_percent": 58.060707008183385
    },
    {
      "type": "training",
      "description": "Training step 4881",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:03",
      "total_flops_so_far": 5.807258912428758e+16,
      "budget_used_percent": 58.07258912428759
    },
    {
      "type": "training",
      "description": "Training step 4882",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:04",
      "total_flops_so_far": 5.808447124039178e+16,
      "budget_used_percent": 58.08447124039178
    },
    {
      "type": "training",
      "description": "Training step 4883",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:05",
      "total_flops_so_far": 5.809635335649597e+16,
      "budget_used_percent": 58.09635335649597
    },
    {
      "type": "training",
      "description": "Training step 4884",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:05",
      "total_flops_so_far": 5.810823547260016e+16,
      "budget_used_percent": 58.10823547260016
    },
    {
      "type": "training",
      "description": "Training step 4885",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:06",
      "total_flops_so_far": 5.812011758870435e+16,
      "budget_used_percent": 58.12011758870435
    },
    {
      "type": "training",
      "description": "Training step 4886",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:07",
      "total_flops_so_far": 5.813199970480854e+16,
      "budget_used_percent": 58.13199970480854
    },
    {
      "type": "training",
      "description": "Training step 4887",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:07",
      "total_flops_so_far": 5.814388182091274e+16,
      "budget_used_percent": 58.14388182091273
    },
    {
      "type": "training",
      "description": "Training step 4888",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:08",
      "total_flops_so_far": 5.815576393701693e+16,
      "budget_used_percent": 58.15576393701692
    },
    {
      "type": "training",
      "description": "Training step 4889",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:09",
      "total_flops_so_far": 5.816764605312112e+16,
      "budget_used_percent": 58.167646053121125
    },
    {
      "type": "training",
      "description": "Training step 4890",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:10",
      "total_flops_so_far": 5.817952816922531e+16,
      "budget_used_percent": 58.179528169225314
    },
    {
      "type": "training",
      "description": "Training step 4891",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:10",
      "total_flops_so_far": 5.81914102853295e+16,
      "budget_used_percent": 58.1914102853295
    },
    {
      "type": "training",
      "description": "Training step 4892",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:11",
      "total_flops_so_far": 5.82032924014337e+16,
      "budget_used_percent": 58.20329240143369
    },
    {
      "type": "training",
      "description": "Training step 4893",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:12",
      "total_flops_so_far": 5.821517451753789e+16,
      "budget_used_percent": 58.215174517537896
    },
    {
      "type": "training",
      "description": "Training step 4894",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:12",
      "total_flops_so_far": 5.822705663364208e+16,
      "budget_used_percent": 58.227056633642086
    },
    {
      "type": "training",
      "description": "Training step 4895",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:13",
      "total_flops_so_far": 5.823893874974627e+16,
      "budget_used_percent": 58.238938749746275
    },
    {
      "type": "training",
      "description": "Training step 4896",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:14",
      "total_flops_so_far": 5.825082086585046e+16,
      "budget_used_percent": 58.250820865850464
    },
    {
      "type": "training",
      "description": "Training step 4897",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:14",
      "total_flops_so_far": 5.826270298195466e+16,
      "budget_used_percent": 58.262702981954654
    },
    {
      "type": "training",
      "description": "Training step 4898",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:15",
      "total_flops_so_far": 5.827458509805885e+16,
      "budget_used_percent": 58.27458509805885
    },
    {
      "type": "training",
      "description": "Training step 4899",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:16",
      "total_flops_so_far": 5.828646721416304e+16,
      "budget_used_percent": 58.28646721416304
    },
    {
      "type": "training",
      "description": "Training step 4900",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:17",
      "total_flops_so_far": 5.829834933026723e+16,
      "budget_used_percent": 58.29834933026723
    },
    {
      "type": "training",
      "description": "Training step 4901",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:17",
      "total_flops_so_far": 5.831023144637142e+16,
      "budget_used_percent": 58.31023144637142
    },
    {
      "type": "training",
      "description": "Training step 4902",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:18",
      "total_flops_so_far": 5.832211356247562e+16,
      "budget_used_percent": 58.32211356247562
    },
    {
      "type": "training",
      "description": "Training step 4903",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:19",
      "total_flops_so_far": 5.833399567857981e+16,
      "budget_used_percent": 58.33399567857981
    },
    {
      "type": "training",
      "description": "Training step 4904",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:20",
      "total_flops_so_far": 5.8345877794684e+16,
      "budget_used_percent": 58.345877794684
    },
    {
      "type": "training",
      "description": "Training step 4905",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:20",
      "total_flops_so_far": 5.835775991078819e+16,
      "budget_used_percent": 58.35775991078819
    },
    {
      "type": "training",
      "description": "Training step 4906",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:21",
      "total_flops_so_far": 5.836964202689238e+16,
      "budget_used_percent": 58.369642026892386
    },
    {
      "type": "training",
      "description": "Training step 4907",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:22",
      "total_flops_so_far": 5.838152414299658e+16,
      "budget_used_percent": 58.381524142996575
    },
    {
      "type": "training",
      "description": "Training step 4908",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:22",
      "total_flops_so_far": 5.839340625910077e+16,
      "budget_used_percent": 58.393406259100765
    },
    {
      "type": "training",
      "description": "Training step 4909",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:23",
      "total_flops_so_far": 5.840528837520496e+16,
      "budget_used_percent": 58.405288375204954
    },
    {
      "type": "training",
      "description": "Training step 4910",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:24",
      "total_flops_so_far": 5.841717049130915e+16,
      "budget_used_percent": 58.41717049130916
    },
    {
      "type": "training",
      "description": "Training step 4911",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:24",
      "total_flops_so_far": 5.842905260741334e+16,
      "budget_used_percent": 58.42905260741335
    },
    {
      "type": "training",
      "description": "Training step 4912",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:25",
      "total_flops_so_far": 5.844093472351754e+16,
      "budget_used_percent": 58.440934723517536
    },
    {
      "type": "training",
      "description": "Training step 4913",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:26",
      "total_flops_so_far": 5.845281683962173e+16,
      "budget_used_percent": 58.452816839621725
    },
    {
      "type": "training",
      "description": "Training step 4914",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:27",
      "total_flops_so_far": 5.846469895572592e+16,
      "budget_used_percent": 58.46469895572592
    },
    {
      "type": "training",
      "description": "Training step 4915",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:27",
      "total_flops_so_far": 5.847658107183011e+16,
      "budget_used_percent": 58.47658107183011
    },
    {
      "type": "training",
      "description": "Training step 4916",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:28",
      "total_flops_so_far": 5.84884631879343e+16,
      "budget_used_percent": 58.4884631879343
    },
    {
      "type": "training",
      "description": "Training step 4917",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:29",
      "total_flops_so_far": 5.85003453040385e+16,
      "budget_used_percent": 58.50034530403849
    },
    {
      "type": "training",
      "description": "Training step 4918",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:29",
      "total_flops_so_far": 5.851222742014269e+16,
      "budget_used_percent": 58.51222742014269
    },
    {
      "type": "training",
      "description": "Training step 4919",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:30",
      "total_flops_so_far": 5.852410953624688e+16,
      "budget_used_percent": 58.52410953624688
    },
    {
      "type": "training",
      "description": "Training step 4920",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:31",
      "total_flops_so_far": 5.853599165235107e+16,
      "budget_used_percent": 58.53599165235107
    },
    {
      "type": "training",
      "description": "Training step 4921",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:31",
      "total_flops_so_far": 5.854787376845526e+16,
      "budget_used_percent": 58.54787376845526
    },
    {
      "type": "training",
      "description": "Training step 4922",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:32",
      "total_flops_so_far": 5.855975588455946e+16,
      "budget_used_percent": 58.559755884559465
    },
    {
      "type": "training",
      "description": "Training step 4923",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:33",
      "total_flops_so_far": 5.857163800066365e+16,
      "budget_used_percent": 58.571638000663654
    },
    {
      "type": "training",
      "description": "Training step 4924",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:33",
      "total_flops_so_far": 5.858352011676784e+16,
      "budget_used_percent": 58.58352011676784
    },
    {
      "type": "training",
      "description": "Training step 4925",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:34",
      "total_flops_so_far": 5.859540223287203e+16,
      "budget_used_percent": 58.59540223287203
    },
    {
      "type": "training",
      "description": "Training step 4926",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:35",
      "total_flops_so_far": 5.860728434897622e+16,
      "budget_used_percent": 58.60728434897622
    },
    {
      "type": "training",
      "description": "Training step 4927",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:36",
      "total_flops_so_far": 5.861916646508042e+16,
      "budget_used_percent": 58.61916646508042
    },
    {
      "type": "training",
      "description": "Training step 4928",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:36",
      "total_flops_so_far": 5.863104858118461e+16,
      "budget_used_percent": 58.63104858118461
    },
    {
      "type": "training",
      "description": "Training step 4929",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:37",
      "total_flops_so_far": 5.86429306972888e+16,
      "budget_used_percent": 58.6429306972888
    },
    {
      "type": "training",
      "description": "Training step 4930",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:38",
      "total_flops_so_far": 5.865481281339299e+16,
      "budget_used_percent": 58.65481281339299
    },
    {
      "type": "training",
      "description": "Training step 4931",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:39",
      "total_flops_so_far": 5.866669492949718e+16,
      "budget_used_percent": 58.66669492949719
    },
    {
      "type": "training",
      "description": "Training step 4932",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:39",
      "total_flops_so_far": 5.867857704560138e+16,
      "budget_used_percent": 58.67857704560138
    },
    {
      "type": "training",
      "description": "Training step 4933",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:40",
      "total_flops_so_far": 5.869045916170557e+16,
      "budget_used_percent": 58.69045916170557
    },
    {
      "type": "training",
      "description": "Training step 4934",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:41",
      "total_flops_so_far": 5.870234127780976e+16,
      "budget_used_percent": 58.70234127780976
    },
    {
      "type": "training",
      "description": "Training step 4935",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:41",
      "total_flops_so_far": 5.871422339391395e+16,
      "budget_used_percent": 58.714223393913954
    },
    {
      "type": "training",
      "description": "Training step 4936",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:42",
      "total_flops_so_far": 5.872610551001814e+16,
      "budget_used_percent": 58.726105510018144
    },
    {
      "type": "training",
      "description": "Training step 4937",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:43",
      "total_flops_so_far": 5.873798762612234e+16,
      "budget_used_percent": 58.73798762612233
    },
    {
      "type": "training",
      "description": "Training step 4938",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:44",
      "total_flops_so_far": 5.874986974222653e+16,
      "budget_used_percent": 58.74986974222652
    },
    {
      "type": "training",
      "description": "Training step 4939",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:44",
      "total_flops_so_far": 5.876175185833072e+16,
      "budget_used_percent": 58.761751858330726
    },
    {
      "type": "training",
      "description": "Training step 4940",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:45",
      "total_flops_so_far": 5.877363397443491e+16,
      "budget_used_percent": 58.773633974434915
    },
    {
      "type": "training",
      "description": "Training step 4941",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:46",
      "total_flops_so_far": 5.87855160905391e+16,
      "budget_used_percent": 58.785516090539105
    },
    {
      "type": "training",
      "description": "Training step 4942",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:46",
      "total_flops_so_far": 5.87973982066433e+16,
      "budget_used_percent": 58.797398206643294
    },
    {
      "type": "training",
      "description": "Training step 4943",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:47",
      "total_flops_so_far": 5.880928032274749e+16,
      "budget_used_percent": 58.80928032274749
    },
    {
      "type": "training",
      "description": "Training step 4944",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:48",
      "total_flops_so_far": 5.882116243885168e+16,
      "budget_used_percent": 58.82116243885168
    },
    {
      "type": "training",
      "description": "Training step 4945",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:48",
      "total_flops_so_far": 5.883304455495587e+16,
      "budget_used_percent": 58.83304455495587
    },
    {
      "type": "training",
      "description": "Training step 4946",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:49",
      "total_flops_so_far": 5.884492667106006e+16,
      "budget_used_percent": 58.84492667106006
    },
    {
      "type": "training",
      "description": "Training step 4947",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:50",
      "total_flops_so_far": 5.885680878716426e+16,
      "budget_used_percent": 58.85680878716426
    },
    {
      "type": "training",
      "description": "Training step 4948",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:50",
      "total_flops_so_far": 5.886869090326845e+16,
      "budget_used_percent": 58.86869090326845
    },
    {
      "type": "training",
      "description": "Training step 4949",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:51",
      "total_flops_so_far": 5.888057301937264e+16,
      "budget_used_percent": 58.88057301937264
    },
    {
      "type": "training",
      "description": "Training step 4950",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:52",
      "total_flops_so_far": 5.889245513547683e+16,
      "budget_used_percent": 58.89245513547683
    },
    {
      "type": "training",
      "description": "Training step 4951",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:53",
      "total_flops_so_far": 5.890433725158102e+16,
      "budget_used_percent": 58.904337251581026
    },
    {
      "type": "training",
      "description": "Training step 4952",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:53",
      "total_flops_so_far": 5.891621936768522e+16,
      "budget_used_percent": 58.916219367685216
    },
    {
      "type": "training",
      "description": "Training step 4953",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:54",
      "total_flops_so_far": 5.892810148378941e+16,
      "budget_used_percent": 58.928101483789405
    },
    {
      "type": "training",
      "description": "Training step 4954",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:55",
      "total_flops_so_far": 5.89399835998936e+16,
      "budget_used_percent": 58.939983599893594
    },
    {
      "type": "training",
      "description": "Training step 4955",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:55",
      "total_flops_so_far": 5.895186571599779e+16,
      "budget_used_percent": 58.95186571599778
    },
    {
      "type": "training",
      "description": "Training step 4956",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:56",
      "total_flops_so_far": 5.896374783210198e+16,
      "budget_used_percent": 58.96374783210199
    },
    {
      "type": "training",
      "description": "Training step 4957",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:57",
      "total_flops_so_far": 5.897562994820618e+16,
      "budget_used_percent": 58.975629948206176
    },
    {
      "type": "training",
      "description": "Training step 4958",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:57",
      "total_flops_so_far": 5.898751206431037e+16,
      "budget_used_percent": 58.987512064310366
    },
    {
      "type": "training",
      "description": "Training step 4959",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:58",
      "total_flops_so_far": 5.899939418041456e+16,
      "budget_used_percent": 58.999394180414555
    },
    {
      "type": "training",
      "description": "Training step 4960",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:07:59",
      "total_flops_so_far": 5.901127629651875e+16,
      "budget_used_percent": 59.01127629651876
    },
    {
      "type": "training",
      "description": "Training step 4961",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:00",
      "total_flops_so_far": 5.902315841262294e+16,
      "budget_used_percent": 59.02315841262295
    },
    {
      "type": "training",
      "description": "Training step 4962",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:00",
      "total_flops_so_far": 5.903504052872714e+16,
      "budget_used_percent": 59.03504052872714
    },
    {
      "type": "training",
      "description": "Training step 4963",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:01",
      "total_flops_so_far": 5.904692264483133e+16,
      "budget_used_percent": 59.04692264483133
    },
    {
      "type": "training",
      "description": "Training step 4964",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:02",
      "total_flops_so_far": 5.905880476093552e+16,
      "budget_used_percent": 59.05880476093552
    },
    {
      "type": "training",
      "description": "Training step 4965",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:03",
      "total_flops_so_far": 5.907068687703971e+16,
      "budget_used_percent": 59.07068687703971
    },
    {
      "type": "training",
      "description": "Training step 4966",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:03",
      "total_flops_so_far": 5.90825689931439e+16,
      "budget_used_percent": 59.0825689931439
    },
    {
      "type": "training",
      "description": "Training step 4967",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:04",
      "total_flops_so_far": 5.90944511092481e+16,
      "budget_used_percent": 59.09445110924809
    },
    {
      "type": "training",
      "description": "Training step 4968",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:05",
      "total_flops_so_far": 5.910633322535229e+16,
      "budget_used_percent": 59.106333225352294
    },
    {
      "type": "training",
      "description": "Training step 4969",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:05",
      "total_flops_so_far": 5.911821534145648e+16,
      "budget_used_percent": 59.118215341456484
    },
    {
      "type": "training",
      "description": "Training step 4970",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:06",
      "total_flops_so_far": 5.913009745756067e+16,
      "budget_used_percent": 59.13009745756067
    },
    {
      "type": "training",
      "description": "Training step 4971",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:07",
      "total_flops_so_far": 5.914197957366486e+16,
      "budget_used_percent": 59.14197957366486
    },
    {
      "type": "training",
      "description": "Training step 4972",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:07",
      "total_flops_so_far": 5.915386168976906e+16,
      "budget_used_percent": 59.15386168976906
    },
    {
      "type": "training",
      "description": "Training step 4973",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:08",
      "total_flops_so_far": 5.916574380587325e+16,
      "budget_used_percent": 59.16574380587325
    },
    {
      "type": "training",
      "description": "Training step 4974",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:09",
      "total_flops_so_far": 5.917762592197744e+16,
      "budget_used_percent": 59.17762592197744
    },
    {
      "type": "training",
      "description": "Training step 4975",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:10",
      "total_flops_so_far": 5.918950803808163e+16,
      "budget_used_percent": 59.18950803808163
    },
    {
      "type": "training",
      "description": "Training step 4976",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:10",
      "total_flops_so_far": 5.920139015418582e+16,
      "budget_used_percent": 59.20139015418583
    },
    {
      "type": "training",
      "description": "Training step 4977",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:11",
      "total_flops_so_far": 5.921327227029002e+16,
      "budget_used_percent": 59.21327227029002
    },
    {
      "type": "training",
      "description": "Training step 4978",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:12",
      "total_flops_so_far": 5.922515438639421e+16,
      "budget_used_percent": 59.22515438639421
    },
    {
      "type": "training",
      "description": "Training step 4979",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:12",
      "total_flops_so_far": 5.92370365024984e+16,
      "budget_used_percent": 59.2370365024984
    },
    {
      "type": "training",
      "description": "Training step 4980",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:13",
      "total_flops_so_far": 5.924891861860259e+16,
      "budget_used_percent": 59.248918618602595
    },
    {
      "type": "training",
      "description": "Training step 4981",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:14",
      "total_flops_so_far": 5.926080073470678e+16,
      "budget_used_percent": 59.260800734706784
    },
    {
      "type": "training",
      "description": "Training step 4982",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:14",
      "total_flops_so_far": 5.927268285081098e+16,
      "budget_used_percent": 59.27268285081097
    },
    {
      "type": "training",
      "description": "Training step 4983",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:15",
      "total_flops_so_far": 5.928456496691517e+16,
      "budget_used_percent": 59.28456496691516
    },
    {
      "type": "training",
      "description": "Training step 4984",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:16",
      "total_flops_so_far": 5.929644708301936e+16,
      "budget_used_percent": 59.29644708301935
    },
    {
      "type": "training",
      "description": "Training step 4985",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:17",
      "total_flops_so_far": 5.930832919912355e+16,
      "budget_used_percent": 59.308329199123555
    },
    {
      "type": "training",
      "description": "Training step 4986",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:17",
      "total_flops_so_far": 5.932021131522774e+16,
      "budget_used_percent": 59.320211315227745
    },
    {
      "type": "training",
      "description": "Training step 4987",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:18",
      "total_flops_so_far": 5.933209343133194e+16,
      "budget_used_percent": 59.332093431331934
    },
    {
      "type": "training",
      "description": "Training step 4988",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:19",
      "total_flops_so_far": 5.934397554743613e+16,
      "budget_used_percent": 59.34397554743612
    },
    {
      "type": "training",
      "description": "Training step 4989",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:19",
      "total_flops_so_far": 5.935585766354032e+16,
      "budget_used_percent": 59.35585766354032
    },
    {
      "type": "training",
      "description": "Training step 4990",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:20",
      "total_flops_so_far": 5.936773977964451e+16,
      "budget_used_percent": 59.36773977964451
    },
    {
      "type": "training",
      "description": "Training step 4991",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:21",
      "total_flops_so_far": 5.93796218957487e+16,
      "budget_used_percent": 59.3796218957487
    },
    {
      "type": "training",
      "description": "Training step 4992",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:21",
      "total_flops_so_far": 5.93915040118529e+16,
      "budget_used_percent": 59.39150401185289
    },
    {
      "type": "training",
      "description": "Training step 4993",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:22",
      "total_flops_so_far": 5.940338612795709e+16,
      "budget_used_percent": 59.40338612795709
    },
    {
      "type": "training",
      "description": "Training step 4994",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:23",
      "total_flops_so_far": 5.941526824406128e+16,
      "budget_used_percent": 59.41526824406128
    },
    {
      "type": "training",
      "description": "Training step 4995",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:24",
      "total_flops_so_far": 5.942715036016547e+16,
      "budget_used_percent": 59.42715036016547
    },
    {
      "type": "training",
      "description": "Training step 4996",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:25",
      "total_flops_so_far": 5.943903247626966e+16,
      "budget_used_percent": 59.43903247626966
    },
    {
      "type": "training",
      "description": "Training step 4997",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:25",
      "total_flops_so_far": 5.945091459237386e+16,
      "budget_used_percent": 59.45091459237386
    },
    {
      "type": "training",
      "description": "Training step 4998",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:26",
      "total_flops_so_far": 5.946279670847805e+16,
      "budget_used_percent": 59.46279670847805
    },
    {
      "type": "training",
      "description": "Training step 4999",
      "seq_len": 512,
      "batch_size": 8,
      "forward_flops": 3960705368064.0,
      "backward_flops": 7921410736128.0,
      "flops": 11882116104192.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:26",
      "total_flops_so_far": 5.947467882458224e+16,
      "budget_used_percent": 59.47467882458224
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:34",
      "total_flops_so_far": 5.94753894537201e+16,
      "budget_used_percent": 59.4753894537201
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 714333709232.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:42",
      "total_flops_so_far": 5.947610378742933e+16,
      "budget_used_percent": 59.476103787429324
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:49",
      "total_flops_so_far": 5.947681626849268e+16,
      "budget_used_percent": 59.47681626849268
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:08:57",
      "total_flops_so_far": 5.9477526897630536e+16,
      "budget_used_percent": 59.47752689763054
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 713407296244.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:09:04",
      "total_flops_so_far": 5.947824030492678e+16,
      "budget_used_percent": 59.478240304926786
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 5",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:09:11",
      "total_flops_so_far": 5.947895093406464e+16,
      "budget_used_percent": 59.47895093406464
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 6",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:09:19",
      "total_flops_so_far": 5.947966341512799e+16,
      "budget_used_percent": 59.479663415127995
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 7",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:09:26",
      "total_flops_so_far": 5.948037589619134e+16,
      "budget_used_percent": 59.48037589619134
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 8",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:09:33",
      "total_flops_so_far": 5.9481088377254696e+16,
      "budget_used_percent": 59.4810883772547
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 9",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:09:41",
      "total_flops_so_far": 5.948180085831805e+16,
      "budget_used_percent": 59.481800858318046
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:09:48",
      "total_flops_so_far": 5.94825114874559e+16,
      "budget_used_percent": 59.4825114874559
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 714333709232.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:09:56",
      "total_flops_so_far": 5.948322582116514e+16,
      "budget_used_percent": 59.48322582116513
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:10:03",
      "total_flops_so_far": 5.948393830222849e+16,
      "budget_used_percent": 59.48393830222849
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:10:10",
      "total_flops_so_far": 5.9484648931366344e+16,
      "budget_used_percent": 59.484648931366344
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 713407296244.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:10:18",
      "total_flops_so_far": 5.948536233866259e+16,
      "budget_used_percent": 59.4853623386626
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 5",
      "context_len": 600,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 710629137856.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:10:25",
      "total_flops_so_far": 5.948607296780045e+16,
      "budget_used_percent": 59.48607296780045
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 6",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:10:33",
      "total_flops_so_far": 5.94867854488638e+16,
      "budget_used_percent": 59.4867854488638
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 7",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:10:40",
      "total_flops_so_far": 5.948749792992715e+16,
      "budget_used_percent": 59.487497929927144
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 8",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:10:47",
      "total_flops_so_far": 5.9488210410990504e+16,
      "budget_used_percent": 59.488210410990504
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 9",
      "context_len": 602,
      "gen_len": 200,
      "batch_size": 1,
      "flops": 712481063352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 23:10:55",
      "total_flops_so_far": 5.948892289205386e+16,
      "budget_used_percent": 59.48892289205385
    }
  ],
  "total_flops": 5.948892289205386e+16,
  "budget_used_percent": 59.48892289205385
}