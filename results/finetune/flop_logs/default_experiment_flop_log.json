{
  "experiment_name": "default_experiment",
  "model_config": {
    "hidden_size": 896,
    "num_attention_heads": 14,
    "num_hidden_layers": 24,
    "intermediate_size": 4864,
    "head_dim": 64,
    "vocab_size": 151936,
    "lora_r": 8,
    "lora_target_modules": [
      "q_proj",
      "v_proj"
    ]
  },
  "max_budget": 1e+17,
  "start_time": "2025-03-23 16:38:45",
  "operations": [
    {
      "type": "training",
      "description": "Training step 0",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:38:52",
      "total_flops_so_far": 5941058052096.0,
      "budget_used_percent": 0.005941058052096
    },
    {
      "type": "training",
      "description": "Training step 1",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:38:53",
      "total_flops_so_far": 11882116104192.0,
      "budget_used_percent": 0.011882116104192
    },
    {
      "type": "training",
      "description": "Training step 2",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:38:53",
      "total_flops_so_far": 17823174156288.0,
      "budget_used_percent": 0.017823174156288
    },
    {
      "type": "training",
      "description": "Training step 3",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:38:54",
      "total_flops_so_far": 23764232208384.0,
      "budget_used_percent": 0.023764232208384
    },
    {
      "type": "training",
      "description": "Training step 4",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:38:54",
      "total_flops_so_far": 29705290260480.0,
      "budget_used_percent": 0.02970529026048
    },
    {
      "type": "training",
      "description": "Training step 5",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:38:54",
      "total_flops_so_far": 35646348312576.0,
      "budget_used_percent": 0.035646348312576
    },
    {
      "type": "training",
      "description": "Training step 6",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:38:54",
      "total_flops_so_far": 41587406364672.0,
      "budget_used_percent": 0.041587406364672
    },
    {
      "type": "training",
      "description": "Training step 7",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:38:55",
      "total_flops_so_far": 47528464416768.0,
      "budget_used_percent": 0.047528464416768
    },
    {
      "type": "training",
      "description": "Training step 8",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:38:55",
      "total_flops_so_far": 53469522468864.0,
      "budget_used_percent": 0.05346952246886401
    },
    {
      "type": "training",
      "description": "Training step 9",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:38:55",
      "total_flops_so_far": 59410580520960.0,
      "budget_used_percent": 0.05941058052096
    },
    {
      "type": "training",
      "description": "Training step 10",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:38:56",
      "total_flops_so_far": 65351638573056.0,
      "budget_used_percent": 0.06535163857305601
    },
    {
      "type": "training",
      "description": "Training step 11",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:38:56",
      "total_flops_so_far": 71292696625152.0,
      "budget_used_percent": 0.071292696625152
    },
    {
      "type": "training",
      "description": "Training step 12",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:38:56",
      "total_flops_so_far": 77233754677248.0,
      "budget_used_percent": 0.07723375467724801
    },
    {
      "type": "training",
      "description": "Training step 13",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:38:57",
      "total_flops_so_far": 83174812729344.0,
      "budget_used_percent": 0.083174812729344
    },
    {
      "type": "training",
      "description": "Training step 14",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:38:57",
      "total_flops_so_far": 89115870781440.0,
      "budget_used_percent": 0.08911587078144001
    },
    {
      "type": "training",
      "description": "Training step 15",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:38:57",
      "total_flops_so_far": 95056928833536.0,
      "budget_used_percent": 0.095056928833536
    },
    {
      "type": "training",
      "description": "Training step 16",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:38:57",
      "total_flops_so_far": 100997986885632.0,
      "budget_used_percent": 0.10099798688563201
    },
    {
      "type": "training",
      "description": "Training step 17",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:38:58",
      "total_flops_so_far": 106939044937728.0,
      "budget_used_percent": 0.10693904493772802
    },
    {
      "type": "training",
      "description": "Training step 18",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:38:58",
      "total_flops_so_far": 112880102989824.0,
      "budget_used_percent": 0.112880102989824
    },
    {
      "type": "training",
      "description": "Training step 19",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:38:58",
      "total_flops_so_far": 118821161041920.0,
      "budget_used_percent": 0.11882116104192
    },
    {
      "type": "training",
      "description": "Training step 20",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:38:59",
      "total_flops_so_far": 124762219094016.0,
      "budget_used_percent": 0.12476221909401601
    },
    {
      "type": "training",
      "description": "Training step 21",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:38:59",
      "total_flops_so_far": 130703277146112.0,
      "budget_used_percent": 0.13070327714611202
    },
    {
      "type": "training",
      "description": "Training step 22",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:38:59",
      "total_flops_so_far": 136644335198208.0,
      "budget_used_percent": 0.13664433519820798
    },
    {
      "type": "training",
      "description": "Training step 23",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:00",
      "total_flops_so_far": 142585393250304.0,
      "budget_used_percent": 0.142585393250304
    },
    {
      "type": "training",
      "description": "Training step 24",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:00",
      "total_flops_so_far": 148526451302400.0,
      "budget_used_percent": 0.1485264513024
    },
    {
      "type": "training",
      "description": "Training step 25",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:00",
      "total_flops_so_far": 154467509354496.0,
      "budget_used_percent": 0.15446750935449602
    },
    {
      "type": "training",
      "description": "Training step 26",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:01",
      "total_flops_so_far": 160408567406592.0,
      "budget_used_percent": 0.16040856740659198
    },
    {
      "type": "training",
      "description": "Training step 27",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:01",
      "total_flops_so_far": 166349625458688.0,
      "budget_used_percent": 0.166349625458688
    },
    {
      "type": "training",
      "description": "Training step 28",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:01",
      "total_flops_so_far": 172290683510784.0,
      "budget_used_percent": 0.172290683510784
    },
    {
      "type": "training",
      "description": "Training step 29",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:01",
      "total_flops_so_far": 178231741562880.0,
      "budget_used_percent": 0.17823174156288002
    },
    {
      "type": "training",
      "description": "Training step 30",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:02",
      "total_flops_so_far": 184172799614976.0,
      "budget_used_percent": 0.18417279961497598
    },
    {
      "type": "training",
      "description": "Training step 31",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:02",
      "total_flops_so_far": 190113857667072.0,
      "budget_used_percent": 0.190113857667072
    },
    {
      "type": "training",
      "description": "Training step 32",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:02",
      "total_flops_so_far": 196054915719168.0,
      "budget_used_percent": 0.196054915719168
    },
    {
      "type": "training",
      "description": "Training step 33",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:03",
      "total_flops_so_far": 201995973771264.0,
      "budget_used_percent": 0.20199597377126402
    },
    {
      "type": "training",
      "description": "Training step 34",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:03",
      "total_flops_so_far": 207937031823360.0,
      "budget_used_percent": 0.20793703182336
    },
    {
      "type": "training",
      "description": "Training step 35",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:03",
      "total_flops_so_far": 213878089875456.0,
      "budget_used_percent": 0.21387808987545603
    },
    {
      "type": "training",
      "description": "Training step 36",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:04",
      "total_flops_so_far": 219819147927552.0,
      "budget_used_percent": 0.21981914792755197
    },
    {
      "type": "training",
      "description": "Training step 37",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:04",
      "total_flops_so_far": 225760205979648.0,
      "budget_used_percent": 0.225760205979648
    },
    {
      "type": "training",
      "description": "Training step 38",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:04",
      "total_flops_so_far": 231701264031744.0,
      "budget_used_percent": 0.231701264031744
    },
    {
      "type": "training",
      "description": "Training step 39",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:04",
      "total_flops_so_far": 237642322083840.0,
      "budget_used_percent": 0.23764232208384
    },
    {
      "type": "training",
      "description": "Training step 40",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:05",
      "total_flops_so_far": 243583380135936.0,
      "budget_used_percent": 0.243583380135936
    },
    {
      "type": "training",
      "description": "Training step 41",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:05",
      "total_flops_so_far": 249524438188032.0,
      "budget_used_percent": 0.24952443818803202
    },
    {
      "type": "training",
      "description": "Training step 42",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:05",
      "total_flops_so_far": 255465496240128.0,
      "budget_used_percent": 0.25546549624012804
    },
    {
      "type": "training",
      "description": "Training step 43",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:06",
      "total_flops_so_far": 261406554292224.0,
      "budget_used_percent": 0.26140655429222404
    },
    {
      "type": "training",
      "description": "Training step 44",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:06",
      "total_flops_so_far": 267347612344320.0,
      "budget_used_percent": 0.26734761234432
    },
    {
      "type": "training",
      "description": "Training step 45",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:06",
      "total_flops_so_far": 273288670396416.0,
      "budget_used_percent": 0.27328867039641597
    },
    {
      "type": "training",
      "description": "Training step 46",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:07",
      "total_flops_so_far": 279229728448512.0,
      "budget_used_percent": 0.279229728448512
    },
    {
      "type": "training",
      "description": "Training step 47",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:07",
      "total_flops_so_far": 285170786500608.0,
      "budget_used_percent": 0.285170786500608
    },
    {
      "type": "training",
      "description": "Training step 48",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:07",
      "total_flops_so_far": 291111844552704.0,
      "budget_used_percent": 0.291111844552704
    },
    {
      "type": "training",
      "description": "Training step 49",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:07",
      "total_flops_so_far": 297052902604800.0,
      "budget_used_percent": 0.2970529026048
    },
    {
      "type": "training",
      "description": "Training step 50",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:08",
      "total_flops_so_far": 302993960656896.0,
      "budget_used_percent": 0.302993960656896
    },
    {
      "type": "training",
      "description": "Training step 51",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:08",
      "total_flops_so_far": 308935018708992.0,
      "budget_used_percent": 0.30893501870899204
    },
    {
      "type": "training",
      "description": "Training step 52",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:08",
      "total_flops_so_far": 314876076761088.0,
      "budget_used_percent": 0.314876076761088
    },
    {
      "type": "training",
      "description": "Training step 53",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:09",
      "total_flops_so_far": 320817134813184.0,
      "budget_used_percent": 0.32081713481318397
    },
    {
      "type": "training",
      "description": "Training step 54",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:09",
      "total_flops_so_far": 326758192865280.0,
      "budget_used_percent": 0.32675819286528
    },
    {
      "type": "training",
      "description": "Training step 55",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:09",
      "total_flops_so_far": 332699250917376.0,
      "budget_used_percent": 0.332699250917376
    },
    {
      "type": "training",
      "description": "Training step 56",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:10",
      "total_flops_so_far": 338640308969472.0,
      "budget_used_percent": 0.338640308969472
    },
    {
      "type": "training",
      "description": "Training step 57",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:10",
      "total_flops_so_far": 344581367021568.0,
      "budget_used_percent": 0.344581367021568
    },
    {
      "type": "training",
      "description": "Training step 58",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:10",
      "total_flops_so_far": 350522425073664.0,
      "budget_used_percent": 0.350522425073664
    },
    {
      "type": "training",
      "description": "Training step 59",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:11",
      "total_flops_so_far": 356463483125760.0,
      "budget_used_percent": 0.35646348312576004
    },
    {
      "type": "training",
      "description": "Training step 60",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:11",
      "total_flops_so_far": 362404541177856.0,
      "budget_used_percent": 0.362404541177856
    },
    {
      "type": "training",
      "description": "Training step 61",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:11",
      "total_flops_so_far": 368345599229952.0,
      "budget_used_percent": 0.36834559922995197
    },
    {
      "type": "training",
      "description": "Training step 62",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:11",
      "total_flops_so_far": 374286657282048.0,
      "budget_used_percent": 0.374286657282048
    },
    {
      "type": "training",
      "description": "Training step 63",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:12",
      "total_flops_so_far": 380227715334144.0,
      "budget_used_percent": 0.380227715334144
    },
    {
      "type": "training",
      "description": "Training step 64",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:12",
      "total_flops_so_far": 386168773386240.0,
      "budget_used_percent": 0.38616877338624
    },
    {
      "type": "training",
      "description": "Training step 65",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:12",
      "total_flops_so_far": 392109831438336.0,
      "budget_used_percent": 0.392109831438336
    },
    {
      "type": "training",
      "description": "Training step 66",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:13",
      "total_flops_so_far": 398050889490432.0,
      "budget_used_percent": 0.398050889490432
    },
    {
      "type": "training",
      "description": "Training step 67",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:13",
      "total_flops_so_far": 403991947542528.0,
      "budget_used_percent": 0.40399194754252804
    },
    {
      "type": "training",
      "description": "Training step 68",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:13",
      "total_flops_so_far": 409933005594624.0,
      "budget_used_percent": 0.409933005594624
    },
    {
      "type": "training",
      "description": "Training step 69",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:14",
      "total_flops_so_far": 415874063646720.0,
      "budget_used_percent": 0.41587406364672
    },
    {
      "type": "training",
      "description": "Training step 70",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:14",
      "total_flops_so_far": 421815121698816.0,
      "budget_used_percent": 0.42181512169881596
    },
    {
      "type": "training",
      "description": "Training step 71",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:14",
      "total_flops_so_far": 427756179750912.0,
      "budget_used_percent": 0.42775617975091207
    },
    {
      "type": "training",
      "description": "Training step 72",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:15",
      "total_flops_so_far": 433697237803008.0,
      "budget_used_percent": 0.433697237803008
    },
    {
      "type": "training",
      "description": "Training step 73",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:15",
      "total_flops_so_far": 439638295855104.0,
      "budget_used_percent": 0.43963829585510394
    },
    {
      "type": "training",
      "description": "Training step 74",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:15",
      "total_flops_so_far": 445579353907200.0,
      "budget_used_percent": 0.4455793539072
    },
    {
      "type": "training",
      "description": "Training step 75",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:15",
      "total_flops_so_far": 451520411959296.0,
      "budget_used_percent": 0.451520411959296
    },
    {
      "type": "training",
      "description": "Training step 76",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:16",
      "total_flops_so_far": 457461470011392.0,
      "budget_used_percent": 0.45746147001139204
    },
    {
      "type": "training",
      "description": "Training step 77",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:16",
      "total_flops_so_far": 463402528063488.0,
      "budget_used_percent": 0.463402528063488
    },
    {
      "type": "training",
      "description": "Training step 78",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:16",
      "total_flops_so_far": 469343586115584.0,
      "budget_used_percent": 0.469343586115584
    },
    {
      "type": "training",
      "description": "Training step 79",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:17",
      "total_flops_so_far": 475284644167680.0,
      "budget_used_percent": 0.47528464416768
    },
    {
      "type": "training",
      "description": "Training step 80",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:17",
      "total_flops_so_far": 481225702219776.0,
      "budget_used_percent": 0.48122570221977595
    },
    {
      "type": "training",
      "description": "Training step 81",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:17",
      "total_flops_so_far": 487166760271872.0,
      "budget_used_percent": 0.487166760271872
    },
    {
      "type": "training",
      "description": "Training step 82",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:18",
      "total_flops_so_far": 493107818323968.0,
      "budget_used_percent": 0.493107818323968
    },
    {
      "type": "training",
      "description": "Training step 83",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:18",
      "total_flops_so_far": 499048876376064.0,
      "budget_used_percent": 0.49904887637606404
    },
    {
      "type": "training",
      "description": "Training step 84",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:18",
      "total_flops_so_far": 504989934428160.0,
      "budget_used_percent": 0.50498993442816
    },
    {
      "type": "training",
      "description": "Training step 85",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:18",
      "total_flops_so_far": 510930992480256.0,
      "budget_used_percent": 0.5109309924802561
    },
    {
      "type": "training",
      "description": "Training step 86",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:19",
      "total_flops_so_far": 516872050532352.0,
      "budget_used_percent": 0.516872050532352
    },
    {
      "type": "training",
      "description": "Training step 87",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:19",
      "total_flops_so_far": 522813108584448.0,
      "budget_used_percent": 0.5228131085844481
    },
    {
      "type": "training",
      "description": "Training step 88",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:19",
      "total_flops_so_far": 528754166636544.0,
      "budget_used_percent": 0.528754166636544
    },
    {
      "type": "training",
      "description": "Training step 89",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:20",
      "total_flops_so_far": 534695224688640.0,
      "budget_used_percent": 0.53469522468864
    },
    {
      "type": "training",
      "description": "Training step 90",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:20",
      "total_flops_so_far": 540636282740736.0,
      "budget_used_percent": 0.540636282740736
    },
    {
      "type": "training",
      "description": "Training step 91",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:20",
      "total_flops_so_far": 546577340792832.0,
      "budget_used_percent": 0.5465773407928319
    },
    {
      "type": "training",
      "description": "Training step 92",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:21",
      "total_flops_so_far": 552518398844928.0,
      "budget_used_percent": 0.552518398844928
    },
    {
      "type": "training",
      "description": "Training step 93",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:21",
      "total_flops_so_far": 558459456897024.0,
      "budget_used_percent": 0.558459456897024
    },
    {
      "type": "training",
      "description": "Training step 94",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:21",
      "total_flops_so_far": 564400514949120.0,
      "budget_used_percent": 0.56440051494912
    },
    {
      "type": "training",
      "description": "Training step 95",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:22",
      "total_flops_so_far": 570341573001216.0,
      "budget_used_percent": 0.570341573001216
    },
    {
      "type": "training",
      "description": "Training step 96",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:22",
      "total_flops_so_far": 576282631053312.0,
      "budget_used_percent": 0.576282631053312
    },
    {
      "type": "training",
      "description": "Training step 97",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:22",
      "total_flops_so_far": 582223689105408.0,
      "budget_used_percent": 0.582223689105408
    },
    {
      "type": "training",
      "description": "Training step 98",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:22",
      "total_flops_so_far": 588164747157504.0,
      "budget_used_percent": 0.588164747157504
    },
    {
      "type": "training",
      "description": "Training step 99",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:23",
      "total_flops_so_far": 594105805209600.0,
      "budget_used_percent": 0.5941058052096
    },
    {
      "type": "training",
      "description": "Training step 100",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:23",
      "total_flops_so_far": 600046863261696.0,
      "budget_used_percent": 0.600046863261696
    },
    {
      "type": "training",
      "description": "Training step 101",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:23",
      "total_flops_so_far": 605987921313792.0,
      "budget_used_percent": 0.605987921313792
    },
    {
      "type": "training",
      "description": "Training step 102",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:24",
      "total_flops_so_far": 611928979365888.0,
      "budget_used_percent": 0.611928979365888
    },
    {
      "type": "training",
      "description": "Training step 103",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:24",
      "total_flops_so_far": 617870037417984.0,
      "budget_used_percent": 0.6178700374179841
    },
    {
      "type": "training",
      "description": "Training step 104",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:24",
      "total_flops_so_far": 623811095470080.0,
      "budget_used_percent": 0.62381109547008
    },
    {
      "type": "training",
      "description": "Training step 105",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:25",
      "total_flops_so_far": 629752153522176.0,
      "budget_used_percent": 0.629752153522176
    },
    {
      "type": "training",
      "description": "Training step 106",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:25",
      "total_flops_so_far": 635693211574272.0,
      "budget_used_percent": 0.635693211574272
    },
    {
      "type": "training",
      "description": "Training step 107",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:25",
      "total_flops_so_far": 641634269626368.0,
      "budget_used_percent": 0.6416342696263679
    },
    {
      "type": "training",
      "description": "Training step 108",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:25",
      "total_flops_so_far": 647575327678464.0,
      "budget_used_percent": 0.647575327678464
    },
    {
      "type": "training",
      "description": "Training step 109",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:26",
      "total_flops_so_far": 653516385730560.0,
      "budget_used_percent": 0.65351638573056
    },
    {
      "type": "training",
      "description": "Training step 110",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:26",
      "total_flops_so_far": 659457443782656.0,
      "budget_used_percent": 0.659457443782656
    },
    {
      "type": "training",
      "description": "Training step 111",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:26",
      "total_flops_so_far": 665398501834752.0,
      "budget_used_percent": 0.665398501834752
    },
    {
      "type": "training",
      "description": "Training step 112",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:27",
      "total_flops_so_far": 671339559886848.0,
      "budget_used_percent": 0.6713395598868479
    },
    {
      "type": "training",
      "description": "Training step 113",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:27",
      "total_flops_so_far": 677280617938944.0,
      "budget_used_percent": 0.677280617938944
    },
    {
      "type": "training",
      "description": "Training step 114",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:27",
      "total_flops_so_far": 683221675991040.0,
      "budget_used_percent": 0.68322167599104
    },
    {
      "type": "training",
      "description": "Training step 115",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:28",
      "total_flops_so_far": 689162734043136.0,
      "budget_used_percent": 0.689162734043136
    },
    {
      "type": "training",
      "description": "Training step 116",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:28",
      "total_flops_so_far": 695103792095232.0,
      "budget_used_percent": 0.695103792095232
    },
    {
      "type": "training",
      "description": "Training step 117",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:28",
      "total_flops_so_far": 701044850147328.0,
      "budget_used_percent": 0.701044850147328
    },
    {
      "type": "training",
      "description": "Training step 118",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:29",
      "total_flops_so_far": 706985908199424.0,
      "budget_used_percent": 0.706985908199424
    },
    {
      "type": "training",
      "description": "Training step 119",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:29",
      "total_flops_so_far": 712926966251520.0,
      "budget_used_percent": 0.7129269662515201
    },
    {
      "type": "training",
      "description": "Training step 120",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:29",
      "total_flops_so_far": 718868024303616.0,
      "budget_used_percent": 0.718868024303616
    },
    {
      "type": "training",
      "description": "Training step 121",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:29",
      "total_flops_so_far": 724809082355712.0,
      "budget_used_percent": 0.724809082355712
    },
    {
      "type": "training",
      "description": "Training step 122",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:30",
      "total_flops_so_far": 730750140407808.0,
      "budget_used_percent": 0.7307501404078081
    },
    {
      "type": "training",
      "description": "Training step 123",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:30",
      "total_flops_so_far": 736691198459904.0,
      "budget_used_percent": 0.7366911984599039
    },
    {
      "type": "training",
      "description": "Training step 124",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:30",
      "total_flops_so_far": 742632256512000.0,
      "budget_used_percent": 0.742632256512
    },
    {
      "type": "training",
      "description": "Training step 125",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:31",
      "total_flops_so_far": 748573314564096.0,
      "budget_used_percent": 0.748573314564096
    },
    {
      "type": "training",
      "description": "Training step 126",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:31",
      "total_flops_so_far": 754514372616192.0,
      "budget_used_percent": 0.754514372616192
    },
    {
      "type": "training",
      "description": "Training step 127",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:31",
      "total_flops_so_far": 760455430668288.0,
      "budget_used_percent": 0.760455430668288
    },
    {
      "type": "training",
      "description": "Training step 128",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:32",
      "total_flops_so_far": 766396488720384.0,
      "budget_used_percent": 0.7663964887203839
    },
    {
      "type": "training",
      "description": "Training step 129",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:32",
      "total_flops_so_far": 772337546772480.0,
      "budget_used_percent": 0.77233754677248
    },
    {
      "type": "training",
      "description": "Training step 130",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:32",
      "total_flops_so_far": 778278604824576.0,
      "budget_used_percent": 0.778278604824576
    },
    {
      "type": "training",
      "description": "Training step 131",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:33",
      "total_flops_so_far": 784219662876672.0,
      "budget_used_percent": 0.784219662876672
    },
    {
      "type": "training",
      "description": "Training step 132",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:33",
      "total_flops_so_far": 790160720928768.0,
      "budget_used_percent": 0.7901607209287681
    },
    {
      "type": "training",
      "description": "Training step 133",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:33",
      "total_flops_so_far": 796101778980864.0,
      "budget_used_percent": 0.796101778980864
    },
    {
      "type": "training",
      "description": "Training step 134",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:33",
      "total_flops_so_far": 802042837032960.0,
      "budget_used_percent": 0.80204283703296
    },
    {
      "type": "training",
      "description": "Training step 135",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:34",
      "total_flops_so_far": 807983895085056.0,
      "budget_used_percent": 0.8079838950850561
    },
    {
      "type": "training",
      "description": "Training step 136",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:34",
      "total_flops_so_far": 813924953137152.0,
      "budget_used_percent": 0.813924953137152
    },
    {
      "type": "training",
      "description": "Training step 137",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:34",
      "total_flops_so_far": 819866011189248.0,
      "budget_used_percent": 0.819866011189248
    },
    {
      "type": "training",
      "description": "Training step 138",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:35",
      "total_flops_so_far": 825807069241344.0,
      "budget_used_percent": 0.8258070692413441
    },
    {
      "type": "training",
      "description": "Training step 139",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:35",
      "total_flops_so_far": 831748127293440.0,
      "budget_used_percent": 0.83174812729344
    },
    {
      "type": "training",
      "description": "Training step 140",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:35",
      "total_flops_so_far": 837689185345536.0,
      "budget_used_percent": 0.8376891853455359
    },
    {
      "type": "training",
      "description": "Training step 141",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:36",
      "total_flops_so_far": 843630243397632.0,
      "budget_used_percent": 0.8436302433976319
    },
    {
      "type": "training",
      "description": "Training step 142",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:36",
      "total_flops_so_far": 849571301449728.0,
      "budget_used_percent": 0.849571301449728
    },
    {
      "type": "training",
      "description": "Training step 143",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:36",
      "total_flops_so_far": 855512359501824.0,
      "budget_used_percent": 0.8555123595018241
    },
    {
      "type": "training",
      "description": "Training step 144",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:37",
      "total_flops_so_far": 861453417553920.0,
      "budget_used_percent": 0.8614534175539199
    },
    {
      "type": "training",
      "description": "Training step 145",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:37",
      "total_flops_so_far": 867394475606016.0,
      "budget_used_percent": 0.867394475606016
    },
    {
      "type": "training",
      "description": "Training step 146",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:37",
      "total_flops_so_far": 873335533658112.0,
      "budget_used_percent": 0.873335533658112
    },
    {
      "type": "training",
      "description": "Training step 147",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:37",
      "total_flops_so_far": 879276591710208.0,
      "budget_used_percent": 0.8792765917102079
    },
    {
      "type": "training",
      "description": "Training step 148",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:38",
      "total_flops_so_far": 885217649762304.0,
      "budget_used_percent": 0.885217649762304
    },
    {
      "type": "training",
      "description": "Training step 149",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:38",
      "total_flops_so_far": 891158707814400.0,
      "budget_used_percent": 0.8911587078144
    },
    {
      "type": "training",
      "description": "Training step 150",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:38",
      "total_flops_so_far": 897099765866496.0,
      "budget_used_percent": 0.8970997658664961
    },
    {
      "type": "training",
      "description": "Training step 151",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:39",
      "total_flops_so_far": 903040823918592.0,
      "budget_used_percent": 0.903040823918592
    },
    {
      "type": "training",
      "description": "Training step 152",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:39",
      "total_flops_so_far": 908981881970688.0,
      "budget_used_percent": 0.908981881970688
    },
    {
      "type": "training",
      "description": "Training step 153",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:39",
      "total_flops_so_far": 914922940022784.0,
      "budget_used_percent": 0.9149229400227841
    },
    {
      "type": "training",
      "description": "Training step 154",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:40",
      "total_flops_so_far": 920863998074880.0,
      "budget_used_percent": 0.92086399807488
    },
    {
      "type": "training",
      "description": "Training step 155",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:40",
      "total_flops_so_far": 926805056126976.0,
      "budget_used_percent": 0.926805056126976
    },
    {
      "type": "training",
      "description": "Training step 156",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:40",
      "total_flops_so_far": 932746114179072.0,
      "budget_used_percent": 0.932746114179072
    },
    {
      "type": "training",
      "description": "Training step 157",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:41",
      "total_flops_so_far": 938687172231168.0,
      "budget_used_percent": 0.938687172231168
    },
    {
      "type": "training",
      "description": "Training step 158",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:41",
      "total_flops_so_far": 944628230283264.0,
      "budget_used_percent": 0.9446282302832639
    },
    {
      "type": "training",
      "description": "Training step 159",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:41",
      "total_flops_so_far": 950569288335360.0,
      "budget_used_percent": 0.95056928833536
    },
    {
      "type": "training",
      "description": "Training step 160",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:41",
      "total_flops_so_far": 956510346387456.0,
      "budget_used_percent": 0.956510346387456
    },
    {
      "type": "training",
      "description": "Training step 161",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:42",
      "total_flops_so_far": 962451404439552.0,
      "budget_used_percent": 0.9624514044395519
    },
    {
      "type": "training",
      "description": "Training step 162",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:42",
      "total_flops_so_far": 968392462491648.0,
      "budget_used_percent": 0.968392462491648
    },
    {
      "type": "training",
      "description": "Training step 163",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:42",
      "total_flops_so_far": 974333520543744.0,
      "budget_used_percent": 0.974333520543744
    },
    {
      "type": "training",
      "description": "Training step 164",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:43",
      "total_flops_so_far": 980274578595840.0,
      "budget_used_percent": 0.9802745785958401
    },
    {
      "type": "training",
      "description": "Training step 165",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:43",
      "total_flops_so_far": 986215636647936.0,
      "budget_used_percent": 0.986215636647936
    },
    {
      "type": "training",
      "description": "Training step 166",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:43",
      "total_flops_so_far": 992156694700032.0,
      "budget_used_percent": 0.992156694700032
    },
    {
      "type": "training",
      "description": "Training step 167",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:44",
      "total_flops_so_far": 998097752752128.0,
      "budget_used_percent": 0.9980977527521281
    },
    {
      "type": "training",
      "description": "Training step 168",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:44",
      "total_flops_so_far": 1004038810804224.0,
      "budget_used_percent": 1.004038810804224
    },
    {
      "type": "training",
      "description": "Training step 169",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:44",
      "total_flops_so_far": 1009979868856320.0,
      "budget_used_percent": 1.00997986885632
    },
    {
      "type": "training",
      "description": "Training step 170",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:44",
      "total_flops_so_far": 1015920926908416.0,
      "budget_used_percent": 1.015920926908416
    },
    {
      "type": "training",
      "description": "Training step 171",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:45",
      "total_flops_so_far": 1021861984960512.0,
      "budget_used_percent": 1.0218619849605122
    },
    {
      "type": "training",
      "description": "Training step 172",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:45",
      "total_flops_so_far": 1027803043012608.0,
      "budget_used_percent": 1.027803043012608
    },
    {
      "type": "training",
      "description": "Training step 173",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:45",
      "total_flops_so_far": 1033744101064704.0,
      "budget_used_percent": 1.033744101064704
    },
    {
      "type": "training",
      "description": "Training step 174",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:46",
      "total_flops_so_far": 1039685159116800.0,
      "budget_used_percent": 1.0396851591168
    },
    {
      "type": "training",
      "description": "Training step 175",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:46",
      "total_flops_so_far": 1045626217168896.0,
      "budget_used_percent": 1.0456262171688961
    },
    {
      "type": "training",
      "description": "Training step 176",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:46",
      "total_flops_so_far": 1051567275220992.0,
      "budget_used_percent": 1.051567275220992
    },
    {
      "type": "training",
      "description": "Training step 177",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:47",
      "total_flops_so_far": 1057508333273088.0,
      "budget_used_percent": 1.057508333273088
    },
    {
      "type": "training",
      "description": "Training step 178",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:47",
      "total_flops_so_far": 1063449391325184.0,
      "budget_used_percent": 1.0634493913251841
    },
    {
      "type": "training",
      "description": "Training step 179",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:47",
      "total_flops_so_far": 1069390449377280.0,
      "budget_used_percent": 1.06939044937728
    },
    {
      "type": "training",
      "description": "Training step 180",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:48",
      "total_flops_so_far": 1075331507429376.0,
      "budget_used_percent": 1.0753315074293759
    },
    {
      "type": "training",
      "description": "Training step 181",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:48",
      "total_flops_so_far": 1081272565481472.0,
      "budget_used_percent": 1.081272565481472
    },
    {
      "type": "training",
      "description": "Training step 182",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:48",
      "total_flops_so_far": 1087213623533568.0,
      "budget_used_percent": 1.087213623533568
    },
    {
      "type": "training",
      "description": "Training step 183",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:48",
      "total_flops_so_far": 1093154681585664.0,
      "budget_used_percent": 1.0931546815856639
    },
    {
      "type": "training",
      "description": "Training step 184",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:49",
      "total_flops_so_far": 1099095739637760.0,
      "budget_used_percent": 1.09909573963776
    },
    {
      "type": "training",
      "description": "Training step 185",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:49",
      "total_flops_so_far": 1105036797689856.0,
      "budget_used_percent": 1.105036797689856
    },
    {
      "type": "training",
      "description": "Training step 186",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:49",
      "total_flops_so_far": 1110977855741952.0,
      "budget_used_percent": 1.1109778557419518
    },
    {
      "type": "training",
      "description": "Training step 187",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:50",
      "total_flops_so_far": 1116918913794048.0,
      "budget_used_percent": 1.116918913794048
    },
    {
      "type": "training",
      "description": "Training step 188",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:50",
      "total_flops_so_far": 1122859971846144.0,
      "budget_used_percent": 1.122859971846144
    },
    {
      "type": "training",
      "description": "Training step 189",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:50",
      "total_flops_so_far": 1128801029898240.0,
      "budget_used_percent": 1.12880102989824
    },
    {
      "type": "training",
      "description": "Training step 190",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:51",
      "total_flops_so_far": 1134742087950336.0,
      "budget_used_percent": 1.134742087950336
    },
    {
      "type": "training",
      "description": "Training step 191",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:51",
      "total_flops_so_far": 1140683146002432.0,
      "budget_used_percent": 1.140683146002432
    },
    {
      "type": "training",
      "description": "Training step 192",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:51",
      "total_flops_so_far": 1146624204054528.0,
      "budget_used_percent": 1.146624204054528
    },
    {
      "type": "training",
      "description": "Training step 193",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:52",
      "total_flops_so_far": 1152565262106624.0,
      "budget_used_percent": 1.152565262106624
    },
    {
      "type": "training",
      "description": "Training step 194",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:52",
      "total_flops_so_far": 1158506320158720.0,
      "budget_used_percent": 1.15850632015872
    },
    {
      "type": "training",
      "description": "Training step 195",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:52",
      "total_flops_so_far": 1164447378210816.0,
      "budget_used_percent": 1.164447378210816
    },
    {
      "type": "training",
      "description": "Training step 196",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:52",
      "total_flops_so_far": 1170388436262912.0,
      "budget_used_percent": 1.170388436262912
    },
    {
      "type": "training",
      "description": "Training step 197",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:53",
      "total_flops_so_far": 1176329494315008.0,
      "budget_used_percent": 1.176329494315008
    },
    {
      "type": "training",
      "description": "Training step 198",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:53",
      "total_flops_so_far": 1182270552367104.0,
      "budget_used_percent": 1.182270552367104
    },
    {
      "type": "training",
      "description": "Training step 199",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:53",
      "total_flops_so_far": 1188211610419200.0,
      "budget_used_percent": 1.1882116104192
    },
    {
      "type": "training",
      "description": "Training step 200",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:54",
      "total_flops_so_far": 1194152668471296.0,
      "budget_used_percent": 1.194152668471296
    },
    {
      "type": "training",
      "description": "Training step 201",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:54",
      "total_flops_so_far": 1200093726523392.0,
      "budget_used_percent": 1.200093726523392
    },
    {
      "type": "training",
      "description": "Training step 202",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:54",
      "total_flops_so_far": 1206034784575488.0,
      "budget_used_percent": 1.206034784575488
    },
    {
      "type": "training",
      "description": "Training step 203",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:55",
      "total_flops_so_far": 1211975842627584.0,
      "budget_used_percent": 1.211975842627584
    },
    {
      "type": "training",
      "description": "Training step 204",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:55",
      "total_flops_so_far": 1217916900679680.0,
      "budget_used_percent": 1.21791690067968
    },
    {
      "type": "training",
      "description": "Training step 205",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:55",
      "total_flops_so_far": 1223857958731776.0,
      "budget_used_percent": 1.223857958731776
    },
    {
      "type": "training",
      "description": "Training step 206",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:56",
      "total_flops_so_far": 1229799016783872.0,
      "budget_used_percent": 1.229799016783872
    },
    {
      "type": "training",
      "description": "Training step 207",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:56",
      "total_flops_so_far": 1235740074835968.0,
      "budget_used_percent": 1.2357400748359681
    },
    {
      "type": "training",
      "description": "Training step 208",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:56",
      "total_flops_so_far": 1241681132888064.0,
      "budget_used_percent": 1.241681132888064
    },
    {
      "type": "training",
      "description": "Training step 209",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:56",
      "total_flops_so_far": 1247622190940160.0,
      "budget_used_percent": 1.24762219094016
    },
    {
      "type": "training",
      "description": "Training step 210",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:57",
      "total_flops_so_far": 1253563248992256.0,
      "budget_used_percent": 1.2535632489922561
    },
    {
      "type": "training",
      "description": "Training step 211",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:57",
      "total_flops_so_far": 1259504307044352.0,
      "budget_used_percent": 1.259504307044352
    },
    {
      "type": "training",
      "description": "Training step 212",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:57",
      "total_flops_so_far": 1265445365096448.0,
      "budget_used_percent": 1.265445365096448
    },
    {
      "type": "training",
      "description": "Training step 213",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:58",
      "total_flops_so_far": 1271386423148544.0,
      "budget_used_percent": 1.271386423148544
    },
    {
      "type": "training",
      "description": "Training step 214",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:58",
      "total_flops_so_far": 1277327481200640.0,
      "budget_used_percent": 1.27732748120064
    },
    {
      "type": "training",
      "description": "Training step 215",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:58",
      "total_flops_so_far": 1283268539252736.0,
      "budget_used_percent": 1.2832685392527359
    },
    {
      "type": "training",
      "description": "Training step 216",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:59",
      "total_flops_so_far": 1289209597304832.0,
      "budget_used_percent": 1.289209597304832
    },
    {
      "type": "training",
      "description": "Training step 217",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:59",
      "total_flops_so_far": 1295150655356928.0,
      "budget_used_percent": 1.295150655356928
    },
    {
      "type": "training",
      "description": "Training step 218",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:39:59",
      "total_flops_so_far": 1301091713409024.0,
      "budget_used_percent": 1.3010917134090239
    },
    {
      "type": "training",
      "description": "Training step 219",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:00",
      "total_flops_so_far": 1307032771461120.0,
      "budget_used_percent": 1.30703277146112
    },
    {
      "type": "training",
      "description": "Training step 220",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:00",
      "total_flops_so_far": 1312973829513216.0,
      "budget_used_percent": 1.312973829513216
    },
    {
      "type": "training",
      "description": "Training step 221",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:00",
      "total_flops_so_far": 1318914887565312.0,
      "budget_used_percent": 1.318914887565312
    },
    {
      "type": "training",
      "description": "Training step 222",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:00",
      "total_flops_so_far": 1324855945617408.0,
      "budget_used_percent": 1.324855945617408
    },
    {
      "type": "training",
      "description": "Training step 223",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:01",
      "total_flops_so_far": 1330797003669504.0,
      "budget_used_percent": 1.330797003669504
    },
    {
      "type": "training",
      "description": "Training step 224",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:01",
      "total_flops_so_far": 1336738061721600.0,
      "budget_used_percent": 1.3367380617216
    },
    {
      "type": "training",
      "description": "Training step 225",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:01",
      "total_flops_so_far": 1342679119773696.0,
      "budget_used_percent": 1.3426791197736958
    },
    {
      "type": "training",
      "description": "Training step 226",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:02",
      "total_flops_so_far": 1348620177825792.0,
      "budget_used_percent": 1.348620177825792
    },
    {
      "type": "training",
      "description": "Training step 227",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:02",
      "total_flops_so_far": 1354561235877888.0,
      "budget_used_percent": 1.354561235877888
    },
    {
      "type": "training",
      "description": "Training step 228",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:02",
      "total_flops_so_far": 1360502293929984.0,
      "budget_used_percent": 1.360502293929984
    },
    {
      "type": "training",
      "description": "Training step 229",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:03",
      "total_flops_so_far": 1366443351982080.0,
      "budget_used_percent": 1.36644335198208
    },
    {
      "type": "training",
      "description": "Training step 230",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:03",
      "total_flops_so_far": 1372384410034176.0,
      "budget_used_percent": 1.372384410034176
    },
    {
      "type": "training",
      "description": "Training step 231",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:03",
      "total_flops_so_far": 1378325468086272.0,
      "budget_used_percent": 1.378325468086272
    },
    {
      "type": "training",
      "description": "Training step 232",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:04",
      "total_flops_so_far": 1384266526138368.0,
      "budget_used_percent": 1.384266526138368
    },
    {
      "type": "training",
      "description": "Training step 233",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:04",
      "total_flops_so_far": 1390207584190464.0,
      "budget_used_percent": 1.390207584190464
    },
    {
      "type": "training",
      "description": "Training step 234",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:04",
      "total_flops_so_far": 1396148642242560.0,
      "budget_used_percent": 1.39614864224256
    },
    {
      "type": "training",
      "description": "Training step 235",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:05",
      "total_flops_so_far": 1402089700294656.0,
      "budget_used_percent": 1.402089700294656
    },
    {
      "type": "training",
      "description": "Training step 236",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:05",
      "total_flops_so_far": 1408030758346752.0,
      "budget_used_percent": 1.408030758346752
    },
    {
      "type": "training",
      "description": "Training step 237",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:05",
      "total_flops_so_far": 1413971816398848.0,
      "budget_used_percent": 1.413971816398848
    },
    {
      "type": "training",
      "description": "Training step 238",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:05",
      "total_flops_so_far": 1419912874450944.0,
      "budget_used_percent": 1.419912874450944
    },
    {
      "type": "training",
      "description": "Training step 239",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:06",
      "total_flops_so_far": 1425853932503040.0,
      "budget_used_percent": 1.4258539325030402
    },
    {
      "type": "training",
      "description": "Training step 240",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:06",
      "total_flops_so_far": 1431794990555136.0,
      "budget_used_percent": 1.431794990555136
    },
    {
      "type": "training",
      "description": "Training step 241",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:06",
      "total_flops_so_far": 1437736048607232.0,
      "budget_used_percent": 1.437736048607232
    },
    {
      "type": "training",
      "description": "Training step 242",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:07",
      "total_flops_so_far": 1443677106659328.0,
      "budget_used_percent": 1.4436771066593281
    },
    {
      "type": "training",
      "description": "Training step 243",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:07",
      "total_flops_so_far": 1449618164711424.0,
      "budget_used_percent": 1.449618164711424
    },
    {
      "type": "training",
      "description": "Training step 244",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:07",
      "total_flops_so_far": 1455559222763520.0,
      "budget_used_percent": 1.45555922276352
    },
    {
      "type": "training",
      "description": "Training step 245",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:08",
      "total_flops_so_far": 1461500280815616.0,
      "budget_used_percent": 1.4615002808156161
    },
    {
      "type": "training",
      "description": "Training step 246",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:08",
      "total_flops_so_far": 1467441338867712.0,
      "budget_used_percent": 1.467441338867712
    },
    {
      "type": "training",
      "description": "Training step 247",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:08",
      "total_flops_so_far": 1473382396919808.0,
      "budget_used_percent": 1.4733823969198079
    },
    {
      "type": "training",
      "description": "Training step 248",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:09",
      "total_flops_so_far": 1479323454971904.0,
      "budget_used_percent": 1.479323454971904
    },
    {
      "type": "training",
      "description": "Training step 249",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:09",
      "total_flops_so_far": 1485264513024000.0,
      "budget_used_percent": 1.485264513024
    },
    {
      "type": "training",
      "description": "Training step 250",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:09",
      "total_flops_so_far": 1491205571076096.0,
      "budget_used_percent": 1.4912055710760959
    },
    {
      "type": "training",
      "description": "Training step 251",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:09",
      "total_flops_so_far": 1497146629128192.0,
      "budget_used_percent": 1.497146629128192
    },
    {
      "type": "training",
      "description": "Training step 252",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:10",
      "total_flops_so_far": 1503087687180288.0,
      "budget_used_percent": 1.503087687180288
    },
    {
      "type": "training",
      "description": "Training step 253",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:10",
      "total_flops_so_far": 1509028745232384.0,
      "budget_used_percent": 1.509028745232384
    },
    {
      "type": "training",
      "description": "Training step 254",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:10",
      "total_flops_so_far": 1514969803284480.0,
      "budget_used_percent": 1.51496980328448
    },
    {
      "type": "training",
      "description": "Training step 255",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:11",
      "total_flops_so_far": 1520910861336576.0,
      "budget_used_percent": 1.520910861336576
    },
    {
      "type": "training",
      "description": "Training step 256",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:11",
      "total_flops_so_far": 1526851919388672.0,
      "budget_used_percent": 1.526851919388672
    },
    {
      "type": "training",
      "description": "Training step 257",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:11",
      "total_flops_so_far": 1532792977440768.0,
      "budget_used_percent": 1.5327929774407678
    },
    {
      "type": "training",
      "description": "Training step 258",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:12",
      "total_flops_so_far": 1538734035492864.0,
      "budget_used_percent": 1.538734035492864
    },
    {
      "type": "training",
      "description": "Training step 259",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:12",
      "total_flops_so_far": 1544675093544960.0,
      "budget_used_percent": 1.54467509354496
    },
    {
      "type": "training",
      "description": "Training step 260",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:12",
      "total_flops_so_far": 1550616151597056.0,
      "budget_used_percent": 1.550616151597056
    },
    {
      "type": "training",
      "description": "Training step 261",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:13",
      "total_flops_so_far": 1556557209649152.0,
      "budget_used_percent": 1.556557209649152
    },
    {
      "type": "training",
      "description": "Training step 262",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:13",
      "total_flops_so_far": 1562498267701248.0,
      "budget_used_percent": 1.562498267701248
    },
    {
      "type": "training",
      "description": "Training step 263",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:13",
      "total_flops_so_far": 1568439325753344.0,
      "budget_used_percent": 1.568439325753344
    },
    {
      "type": "training",
      "description": "Training step 264",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:13",
      "total_flops_so_far": 1574380383805440.0,
      "budget_used_percent": 1.57438038380544
    },
    {
      "type": "training",
      "description": "Training step 265",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:14",
      "total_flops_so_far": 1580321441857536.0,
      "budget_used_percent": 1.5803214418575362
    },
    {
      "type": "training",
      "description": "Training step 266",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:14",
      "total_flops_so_far": 1586262499909632.0,
      "budget_used_percent": 1.5862624999096318
    },
    {
      "type": "training",
      "description": "Training step 267",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:14",
      "total_flops_so_far": 1592203557961728.0,
      "budget_used_percent": 1.592203557961728
    },
    {
      "type": "training",
      "description": "Training step 268",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:15",
      "total_flops_so_far": 1598144616013824.0,
      "budget_used_percent": 1.598144616013824
    },
    {
      "type": "training",
      "description": "Training step 269",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:15",
      "total_flops_so_far": 1604085674065920.0,
      "budget_used_percent": 1.60408567406592
    },
    {
      "type": "training",
      "description": "Training step 270",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:15",
      "total_flops_so_far": 1610026732118016.0,
      "budget_used_percent": 1.610026732118016
    },
    {
      "type": "training",
      "description": "Training step 271",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:16",
      "total_flops_so_far": 1615967790170112.0,
      "budget_used_percent": 1.6159677901701122
    },
    {
      "type": "training",
      "description": "Training step 272",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:16",
      "total_flops_so_far": 1621908848222208.0,
      "budget_used_percent": 1.6219088482222082
    },
    {
      "type": "training",
      "description": "Training step 273",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:16",
      "total_flops_so_far": 1627849906274304.0,
      "budget_used_percent": 1.627849906274304
    },
    {
      "type": "training",
      "description": "Training step 274",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:17",
      "total_flops_so_far": 1633790964326400.0,
      "budget_used_percent": 1.6337909643264
    },
    {
      "type": "training",
      "description": "Training step 275",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:17",
      "total_flops_so_far": 1639732022378496.0,
      "budget_used_percent": 1.639732022378496
    },
    {
      "type": "training",
      "description": "Training step 276",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:17",
      "total_flops_so_far": 1645673080430592.0,
      "budget_used_percent": 1.645673080430592
    },
    {
      "type": "training",
      "description": "Training step 277",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:17",
      "total_flops_so_far": 1651614138482688.0,
      "budget_used_percent": 1.6516141384826881
    },
    {
      "type": "training",
      "description": "Training step 278",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:18",
      "total_flops_so_far": 1657555196534784.0,
      "budget_used_percent": 1.6575551965347841
    },
    {
      "type": "training",
      "description": "Training step 279",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:18",
      "total_flops_so_far": 1663496254586880.0,
      "budget_used_percent": 1.66349625458688
    },
    {
      "type": "training",
      "description": "Training step 280",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:18",
      "total_flops_so_far": 1669437312638976.0,
      "budget_used_percent": 1.669437312638976
    },
    {
      "type": "training",
      "description": "Training step 281",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:19",
      "total_flops_so_far": 1675378370691072.0,
      "budget_used_percent": 1.6753783706910719
    },
    {
      "type": "training",
      "description": "Training step 282",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:19",
      "total_flops_so_far": 1681319428743168.0,
      "budget_used_percent": 1.6813194287431679
    },
    {
      "type": "training",
      "description": "Training step 283",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:19",
      "total_flops_so_far": 1687260486795264.0,
      "budget_used_percent": 1.6872604867952639
    },
    {
      "type": "training",
      "description": "Training step 284",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:20",
      "total_flops_so_far": 1693201544847360.0,
      "budget_used_percent": 1.69320154484736
    },
    {
      "type": "training",
      "description": "Training step 285",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:20",
      "total_flops_so_far": 1699142602899456.0,
      "budget_used_percent": 1.699142602899456
    },
    {
      "type": "training",
      "description": "Training step 286",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:20",
      "total_flops_so_far": 1705083660951552.0,
      "budget_used_percent": 1.705083660951552
    },
    {
      "type": "training",
      "description": "Training step 287",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:21",
      "total_flops_so_far": 1711024719003648.0,
      "budget_used_percent": 1.7110247190036483
    },
    {
      "type": "training",
      "description": "Training step 288",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:21",
      "total_flops_so_far": 1716965777055744.0,
      "budget_used_percent": 1.7169657770557438
    },
    {
      "type": "training",
      "description": "Training step 289",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:21",
      "total_flops_so_far": 1722906835107840.0,
      "budget_used_percent": 1.7229068351078398
    },
    {
      "type": "training",
      "description": "Training step 290",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:22",
      "total_flops_so_far": 1728847893159936.0,
      "budget_used_percent": 1.728847893159936
    },
    {
      "type": "training",
      "description": "Training step 291",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:22",
      "total_flops_so_far": 1734788951212032.0,
      "budget_used_percent": 1.734788951212032
    },
    {
      "type": "training",
      "description": "Training step 292",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:22",
      "total_flops_so_far": 1740730009264128.0,
      "budget_used_percent": 1.740730009264128
    },
    {
      "type": "training",
      "description": "Training step 293",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:22",
      "total_flops_so_far": 1746671067316224.0,
      "budget_used_percent": 1.746671067316224
    },
    {
      "type": "training",
      "description": "Training step 294",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:23",
      "total_flops_so_far": 1752612125368320.0,
      "budget_used_percent": 1.7526121253683202
    },
    {
      "type": "training",
      "description": "Training step 295",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:23",
      "total_flops_so_far": 1758553183420416.0,
      "budget_used_percent": 1.7585531834204158
    },
    {
      "type": "training",
      "description": "Training step 296",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:23",
      "total_flops_so_far": 1764494241472512.0,
      "budget_used_percent": 1.764494241472512
    },
    {
      "type": "training",
      "description": "Training step 297",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:24",
      "total_flops_so_far": 1770435299524608.0,
      "budget_used_percent": 1.770435299524608
    },
    {
      "type": "training",
      "description": "Training step 298",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:24",
      "total_flops_so_far": 1776376357576704.0,
      "budget_used_percent": 1.776376357576704
    },
    {
      "type": "training",
      "description": "Training step 299",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:24",
      "total_flops_so_far": 1782317415628800.0,
      "budget_used_percent": 1.7823174156288
    },
    {
      "type": "training",
      "description": "Training step 300",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:25",
      "total_flops_so_far": 1788258473680896.0,
      "budget_used_percent": 1.7882584736808962
    },
    {
      "type": "training",
      "description": "Training step 301",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:25",
      "total_flops_so_far": 1794199531732992.0,
      "budget_used_percent": 1.7941995317329922
    },
    {
      "type": "training",
      "description": "Training step 302",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:25",
      "total_flops_so_far": 1800140589785088.0,
      "budget_used_percent": 1.800140589785088
    },
    {
      "type": "training",
      "description": "Training step 303",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:26",
      "total_flops_so_far": 1806081647837184.0,
      "budget_used_percent": 1.806081647837184
    },
    {
      "type": "training",
      "description": "Training step 304",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:26",
      "total_flops_so_far": 1812022705889280.0,
      "budget_used_percent": 1.81202270588928
    },
    {
      "type": "training",
      "description": "Training step 305",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:26",
      "total_flops_so_far": 1817963763941376.0,
      "budget_used_percent": 1.817963763941376
    },
    {
      "type": "training",
      "description": "Training step 306",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:26",
      "total_flops_so_far": 1823904821993472.0,
      "budget_used_percent": 1.8239048219934721
    },
    {
      "type": "training",
      "description": "Training step 307",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:27",
      "total_flops_so_far": 1829845880045568.0,
      "budget_used_percent": 1.8298458800455681
    },
    {
      "type": "training",
      "description": "Training step 308",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:27",
      "total_flops_so_far": 1835786938097664.0,
      "budget_used_percent": 1.8357869380976641
    },
    {
      "type": "training",
      "description": "Training step 309",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:27",
      "total_flops_so_far": 1841727996149760.0,
      "budget_used_percent": 1.84172799614976
    },
    {
      "type": "training",
      "description": "Training step 310",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:28",
      "total_flops_so_far": 1847669054201856.0,
      "budget_used_percent": 1.847669054201856
    },
    {
      "type": "training",
      "description": "Training step 311",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:28",
      "total_flops_so_far": 1853610112253952.0,
      "budget_used_percent": 1.853610112253952
    },
    {
      "type": "training",
      "description": "Training step 312",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:28",
      "total_flops_so_far": 1859551170306048.0,
      "budget_used_percent": 1.859551170306048
    },
    {
      "type": "training",
      "description": "Training step 313",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:29",
      "total_flops_so_far": 1865492228358144.0,
      "budget_used_percent": 1.865492228358144
    },
    {
      "type": "training",
      "description": "Training step 314",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:29",
      "total_flops_so_far": 1871433286410240.0,
      "budget_used_percent": 1.87143328641024
    },
    {
      "type": "training",
      "description": "Training step 315",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:29",
      "total_flops_so_far": 1877374344462336.0,
      "budget_used_percent": 1.877374344462336
    },
    {
      "type": "training",
      "description": "Training step 316",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:30",
      "total_flops_so_far": 1883315402514432.0,
      "budget_used_percent": 1.8833154025144319
    },
    {
      "type": "training",
      "description": "Training step 317",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:30",
      "total_flops_so_far": 1889256460566528.0,
      "budget_used_percent": 1.8892564605665279
    },
    {
      "type": "training",
      "description": "Training step 318",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:30",
      "total_flops_so_far": 1895197518618624.0,
      "budget_used_percent": 1.8951975186186238
    },
    {
      "type": "training",
      "description": "Training step 319",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:30",
      "total_flops_so_far": 1901138576670720.0,
      "budget_used_percent": 1.90113857667072
    },
    {
      "type": "training",
      "description": "Training step 320",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:31",
      "total_flops_so_far": 1907079634722816.0,
      "budget_used_percent": 1.907079634722816
    },
    {
      "type": "training",
      "description": "Training step 321",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:31",
      "total_flops_so_far": 1913020692774912.0,
      "budget_used_percent": 1.913020692774912
    },
    {
      "type": "training",
      "description": "Training step 322",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:31",
      "total_flops_so_far": 1918961750827008.0,
      "budget_used_percent": 1.918961750827008
    },
    {
      "type": "training",
      "description": "Training step 323",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:32",
      "total_flops_so_far": 1924902808879104.0,
      "budget_used_percent": 1.9249028088791038
    },
    {
      "type": "training",
      "description": "Training step 324",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:32",
      "total_flops_so_far": 1930843866931200.0,
      "budget_used_percent": 1.9308438669311998
    },
    {
      "type": "training",
      "description": "Training step 325",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:32",
      "total_flops_so_far": 1936784924983296.0,
      "budget_used_percent": 1.936784924983296
    },
    {
      "type": "training",
      "description": "Training step 326",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:33",
      "total_flops_so_far": 1942725983035392.0,
      "budget_used_percent": 1.942725983035392
    },
    {
      "type": "training",
      "description": "Training step 327",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:33",
      "total_flops_so_far": 1948667041087488.0,
      "budget_used_percent": 1.948667041087488
    },
    {
      "type": "training",
      "description": "Training step 328",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:33",
      "total_flops_so_far": 1954608099139584.0,
      "budget_used_percent": 1.954608099139584
    },
    {
      "type": "training",
      "description": "Training step 329",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:34",
      "total_flops_so_far": 1960549157191680.0,
      "budget_used_percent": 1.9605491571916802
    },
    {
      "type": "training",
      "description": "Training step 330",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:34",
      "total_flops_so_far": 1966490215243776.0,
      "budget_used_percent": 1.9664902152437758
    },
    {
      "type": "training",
      "description": "Training step 331",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:34",
      "total_flops_so_far": 1972431273295872.0,
      "budget_used_percent": 1.972431273295872
    },
    {
      "type": "training",
      "description": "Training step 332",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:35",
      "total_flops_so_far": 1978372331347968.0,
      "budget_used_percent": 1.978372331347968
    },
    {
      "type": "training",
      "description": "Training step 333",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:35",
      "total_flops_so_far": 1984313389400064.0,
      "budget_used_percent": 1.984313389400064
    },
    {
      "type": "training",
      "description": "Training step 334",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:35",
      "total_flops_so_far": 1990254447452160.0,
      "budget_used_percent": 1.99025444745216
    },
    {
      "type": "training",
      "description": "Training step 335",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:35",
      "total_flops_so_far": 1996195505504256.0,
      "budget_used_percent": 1.9961955055042562
    },
    {
      "type": "training",
      "description": "Training step 336",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:36",
      "total_flops_so_far": 2002136563556352.0,
      "budget_used_percent": 2.002136563556352
    },
    {
      "type": "training",
      "description": "Training step 337",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:36",
      "total_flops_so_far": 2008077621608448.0,
      "budget_used_percent": 2.008077621608448
    },
    {
      "type": "training",
      "description": "Training step 338",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:36",
      "total_flops_so_far": 2014018679660544.0,
      "budget_used_percent": 2.014018679660544
    },
    {
      "type": "training",
      "description": "Training step 339",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:37",
      "total_flops_so_far": 2019959737712640.0,
      "budget_used_percent": 2.01995973771264
    },
    {
      "type": "training",
      "description": "Training step 340",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:37",
      "total_flops_so_far": 2025900795764736.0,
      "budget_used_percent": 2.025900795764736
    },
    {
      "type": "training",
      "description": "Training step 341",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:37",
      "total_flops_so_far": 2031841853816832.0,
      "budget_used_percent": 2.031841853816832
    },
    {
      "type": "training",
      "description": "Training step 342",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:38",
      "total_flops_so_far": 2037782911868928.0,
      "budget_used_percent": 2.037782911868928
    },
    {
      "type": "training",
      "description": "Training step 343",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:38",
      "total_flops_so_far": 2043723969921024.0,
      "budget_used_percent": 2.0437239699210243
    },
    {
      "type": "training",
      "description": "Training step 344",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:38",
      "total_flops_so_far": 2049665027973120.0,
      "budget_used_percent": 2.0496650279731203
    },
    {
      "type": "training",
      "description": "Training step 345",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:39",
      "total_flops_so_far": 2055606086025216.0,
      "budget_used_percent": 2.055606086025216
    },
    {
      "type": "training",
      "description": "Training step 346",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:39",
      "total_flops_so_far": 2061547144077312.0,
      "budget_used_percent": 2.061547144077312
    },
    {
      "type": "training",
      "description": "Training step 347",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:39",
      "total_flops_so_far": 2067488202129408.0,
      "budget_used_percent": 2.067488202129408
    },
    {
      "type": "training",
      "description": "Training step 348",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:40",
      "total_flops_so_far": 2073429260181504.0,
      "budget_used_percent": 2.073429260181504
    },
    {
      "type": "training",
      "description": "Training step 349",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:40",
      "total_flops_so_far": 2079370318233600.0,
      "budget_used_percent": 2.0793703182336
    },
    {
      "type": "training",
      "description": "Training step 350",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:40",
      "total_flops_so_far": 2085311376285696.0,
      "budget_used_percent": 2.0853113762856963
    },
    {
      "type": "training",
      "description": "Training step 351",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:40",
      "total_flops_so_far": 2091252434337792.0,
      "budget_used_percent": 2.0912524343377923
    },
    {
      "type": "training",
      "description": "Training step 352",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:41",
      "total_flops_so_far": 2097193492389888.0,
      "budget_used_percent": 2.097193492389888
    },
    {
      "type": "training",
      "description": "Training step 353",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:41",
      "total_flops_so_far": 2103134550441984.0,
      "budget_used_percent": 2.103134550441984
    },
    {
      "type": "training",
      "description": "Training step 354",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:41",
      "total_flops_so_far": 2109075608494080.0,
      "budget_used_percent": 2.10907560849408
    },
    {
      "type": "training",
      "description": "Training step 355",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:42",
      "total_flops_so_far": 2115016666546176.0,
      "budget_used_percent": 2.115016666546176
    },
    {
      "type": "training",
      "description": "Training step 356",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:42",
      "total_flops_so_far": 2120957724598272.0,
      "budget_used_percent": 2.1209577245982723
    },
    {
      "type": "training",
      "description": "Training step 357",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:42",
      "total_flops_so_far": 2126898782650368.0,
      "budget_used_percent": 2.1268987826503682
    },
    {
      "type": "training",
      "description": "Training step 358",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:43",
      "total_flops_so_far": 2132839840702464.0,
      "budget_used_percent": 2.1328398407024642
    },
    {
      "type": "training",
      "description": "Training step 359",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:43",
      "total_flops_so_far": 2138780898754560.0,
      "budget_used_percent": 2.13878089875456
    },
    {
      "type": "training",
      "description": "Training step 360",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:43",
      "total_flops_so_far": 2144721956806656.0,
      "budget_used_percent": 2.144721956806656
    },
    {
      "type": "training",
      "description": "Training step 361",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:44",
      "total_flops_so_far": 2150663014858752.0,
      "budget_used_percent": 2.1506630148587518
    },
    {
      "type": "training",
      "description": "Training step 362",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:44",
      "total_flops_so_far": 2156604072910848.0,
      "budget_used_percent": 2.156604072910848
    },
    {
      "type": "training",
      "description": "Training step 363",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:44",
      "total_flops_so_far": 2162545130962944.0,
      "budget_used_percent": 2.162545130962944
    },
    {
      "type": "training",
      "description": "Training step 364",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:44",
      "total_flops_so_far": 2168486189015040.0,
      "budget_used_percent": 2.16848618901504
    },
    {
      "type": "training",
      "description": "Training step 365",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:45",
      "total_flops_so_far": 2174427247067136.0,
      "budget_used_percent": 2.174427247067136
    },
    {
      "type": "training",
      "description": "Training step 366",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:45",
      "total_flops_so_far": 2180368305119232.0,
      "budget_used_percent": 2.1803683051192317
    },
    {
      "type": "training",
      "description": "Training step 367",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:45",
      "total_flops_so_far": 2186309363171328.0,
      "budget_used_percent": 2.1863093631713277
    },
    {
      "type": "training",
      "description": "Training step 368",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:46",
      "total_flops_so_far": 2192250421223424.0,
      "budget_used_percent": 2.1922504212234237
    },
    {
      "type": "training",
      "description": "Training step 369",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:46",
      "total_flops_so_far": 2198191479275520.0,
      "budget_used_percent": 2.19819147927552
    },
    {
      "type": "training",
      "description": "Training step 370",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:46",
      "total_flops_so_far": 2204132537327616.0,
      "budget_used_percent": 2.204132537327616
    },
    {
      "type": "training",
      "description": "Training step 371",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:47",
      "total_flops_so_far": 2210073595379712.0,
      "budget_used_percent": 2.210073595379712
    },
    {
      "type": "training",
      "description": "Training step 372",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:47",
      "total_flops_so_far": 2216014653431808.0,
      "budget_used_percent": 2.216014653431808
    },
    {
      "type": "training",
      "description": "Training step 373",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:47",
      "total_flops_so_far": 2221955711483904.0,
      "budget_used_percent": 2.2219557114839037
    },
    {
      "type": "training",
      "description": "Training step 374",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:48",
      "total_flops_so_far": 2227896769536000.0,
      "budget_used_percent": 2.2278967695359997
    },
    {
      "type": "training",
      "description": "Training step 375",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:48",
      "total_flops_so_far": 2233837827588096.0,
      "budget_used_percent": 2.233837827588096
    },
    {
      "type": "training",
      "description": "Training step 376",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:48",
      "total_flops_so_far": 2239778885640192.0,
      "budget_used_percent": 2.239778885640192
    },
    {
      "type": "training",
      "description": "Training step 377",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:49",
      "total_flops_so_far": 2245719943692288.0,
      "budget_used_percent": 2.245719943692288
    },
    {
      "type": "training",
      "description": "Training step 378",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:49",
      "total_flops_so_far": 2251661001744384.0,
      "budget_used_percent": 2.251661001744384
    },
    {
      "type": "training",
      "description": "Training step 379",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:49",
      "total_flops_so_far": 2257602059796480.0,
      "budget_used_percent": 2.25760205979648
    },
    {
      "type": "training",
      "description": "Training step 380",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:49",
      "total_flops_so_far": 2263543117848576.0,
      "budget_used_percent": 2.2635431178485756
    },
    {
      "type": "training",
      "description": "Training step 381",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:50",
      "total_flops_so_far": 2269484175900672.0,
      "budget_used_percent": 2.269484175900672
    },
    {
      "type": "training",
      "description": "Training step 382",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:50",
      "total_flops_so_far": 2275425233952768.0,
      "budget_used_percent": 2.275425233952768
    },
    {
      "type": "training",
      "description": "Training step 383",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:50",
      "total_flops_so_far": 2281366292004864.0,
      "budget_used_percent": 2.281366292004864
    },
    {
      "type": "training",
      "description": "Training step 384",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:51",
      "total_flops_so_far": 2287307350056960.0,
      "budget_used_percent": 2.28730735005696
    },
    {
      "type": "training",
      "description": "Training step 385",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:51",
      "total_flops_so_far": 2293248408109056.0,
      "budget_used_percent": 2.293248408109056
    },
    {
      "type": "training",
      "description": "Training step 386",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:51",
      "total_flops_so_far": 2299189466161152.0,
      "budget_used_percent": 2.299189466161152
    },
    {
      "type": "training",
      "description": "Training step 387",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:52",
      "total_flops_so_far": 2305130524213248.0,
      "budget_used_percent": 2.305130524213248
    },
    {
      "type": "training",
      "description": "Training step 388",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:52",
      "total_flops_so_far": 2311071582265344.0,
      "budget_used_percent": 2.311071582265344
    },
    {
      "type": "training",
      "description": "Training step 389",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:52",
      "total_flops_so_far": 2317012640317440.0,
      "budget_used_percent": 2.31701264031744
    },
    {
      "type": "training",
      "description": "Training step 390",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:53",
      "total_flops_so_far": 2322953698369536.0,
      "budget_used_percent": 2.322953698369536
    },
    {
      "type": "training",
      "description": "Training step 391",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:53",
      "total_flops_so_far": 2328894756421632.0,
      "budget_used_percent": 2.328894756421632
    },
    {
      "type": "training",
      "description": "Training step 392",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:53",
      "total_flops_so_far": 2334835814473728.0,
      "budget_used_percent": 2.334835814473728
    },
    {
      "type": "training",
      "description": "Training step 393",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:54",
      "total_flops_so_far": 2340776872525824.0,
      "budget_used_percent": 2.340776872525824
    },
    {
      "type": "training",
      "description": "Training step 394",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:54",
      "total_flops_so_far": 2346717930577920.0,
      "budget_used_percent": 2.34671793057792
    },
    {
      "type": "training",
      "description": "Training step 395",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:54",
      "total_flops_so_far": 2352658988630016.0,
      "budget_used_percent": 2.352658988630016
    },
    {
      "type": "training",
      "description": "Training step 396",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:54",
      "total_flops_so_far": 2358600046682112.0,
      "budget_used_percent": 2.358600046682112
    },
    {
      "type": "training",
      "description": "Training step 397",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:55",
      "total_flops_so_far": 2364541104734208.0,
      "budget_used_percent": 2.364541104734208
    },
    {
      "type": "training",
      "description": "Training step 398",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:55",
      "total_flops_so_far": 2370482162786304.0,
      "budget_used_percent": 2.370482162786304
    },
    {
      "type": "training",
      "description": "Training step 399",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:55",
      "total_flops_so_far": 2376423220838400.0,
      "budget_used_percent": 2.3764232208384
    },
    {
      "type": "training",
      "description": "Training step 400",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:56",
      "total_flops_so_far": 2382364278890496.0,
      "budget_used_percent": 2.382364278890496
    },
    {
      "type": "training",
      "description": "Training step 401",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:56",
      "total_flops_so_far": 2388305336942592.0,
      "budget_used_percent": 2.388305336942592
    },
    {
      "type": "training",
      "description": "Training step 402",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:56",
      "total_flops_so_far": 2394246394994688.0,
      "budget_used_percent": 2.394246394994688
    },
    {
      "type": "training",
      "description": "Training step 403",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:57",
      "total_flops_so_far": 2400187453046784.0,
      "budget_used_percent": 2.400187453046784
    },
    {
      "type": "training",
      "description": "Training step 404",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:57",
      "total_flops_so_far": 2406128511098880.0,
      "budget_used_percent": 2.40612851109888
    },
    {
      "type": "training",
      "description": "Training step 405",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:57",
      "total_flops_so_far": 2412069569150976.0,
      "budget_used_percent": 2.412069569150976
    },
    {
      "type": "training",
      "description": "Training step 406",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:58",
      "total_flops_so_far": 2418010627203072.0,
      "budget_used_percent": 2.418010627203072
    },
    {
      "type": "training",
      "description": "Training step 407",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:58",
      "total_flops_so_far": 2423951685255168.0,
      "budget_used_percent": 2.423951685255168
    },
    {
      "type": "training",
      "description": "Training step 408",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:58",
      "total_flops_so_far": 2429892743307264.0,
      "budget_used_percent": 2.4298927433072643
    },
    {
      "type": "training",
      "description": "Training step 409",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:59",
      "total_flops_so_far": 2435833801359360.0,
      "budget_used_percent": 2.43583380135936
    },
    {
      "type": "training",
      "description": "Training step 410",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:59",
      "total_flops_so_far": 2441774859411456.0,
      "budget_used_percent": 2.441774859411456
    },
    {
      "type": "training",
      "description": "Training step 411",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:59",
      "total_flops_so_far": 2447715917463552.0,
      "budget_used_percent": 2.447715917463552
    },
    {
      "type": "training",
      "description": "Training step 412",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:40:59",
      "total_flops_so_far": 2453656975515648.0,
      "budget_used_percent": 2.453656975515648
    },
    {
      "type": "training",
      "description": "Training step 413",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:00",
      "total_flops_so_far": 2459598033567744.0,
      "budget_used_percent": 2.459598033567744
    },
    {
      "type": "training",
      "description": "Training step 414",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:00",
      "total_flops_so_far": 2465539091619840.0,
      "budget_used_percent": 2.4655390916198403
    },
    {
      "type": "training",
      "description": "Training step 415",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:00",
      "total_flops_so_far": 2471480149671936.0,
      "budget_used_percent": 2.4714801496719363
    },
    {
      "type": "training",
      "description": "Training step 416",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:01",
      "total_flops_so_far": 2477421207724032.0,
      "budget_used_percent": 2.477421207724032
    },
    {
      "type": "training",
      "description": "Training step 417",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:01",
      "total_flops_so_far": 2483362265776128.0,
      "budget_used_percent": 2.483362265776128
    },
    {
      "type": "training",
      "description": "Training step 418",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:01",
      "total_flops_so_far": 2489303323828224.0,
      "budget_used_percent": 2.489303323828224
    },
    {
      "type": "training",
      "description": "Training step 419",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:02",
      "total_flops_so_far": 2495244381880320.0,
      "budget_used_percent": 2.49524438188032
    },
    {
      "type": "training",
      "description": "Training step 420",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:02",
      "total_flops_so_far": 2501185439932416.0,
      "budget_used_percent": 2.5011854399324163
    },
    {
      "type": "training",
      "description": "Training step 421",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:02",
      "total_flops_so_far": 2507126497984512.0,
      "budget_used_percent": 2.5071264979845123
    },
    {
      "type": "training",
      "description": "Training step 422",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:03",
      "total_flops_so_far": 2513067556036608.0,
      "budget_used_percent": 2.5130675560366083
    },
    {
      "type": "training",
      "description": "Training step 423",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:03",
      "total_flops_so_far": 2519008614088704.0,
      "budget_used_percent": 2.519008614088704
    },
    {
      "type": "training",
      "description": "Training step 424",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:03",
      "total_flops_so_far": 2524949672140800.0,
      "budget_used_percent": 2.5249496721408
    },
    {
      "type": "training",
      "description": "Training step 425",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:04",
      "total_flops_so_far": 2530890730192896.0,
      "budget_used_percent": 2.530890730192896
    },
    {
      "type": "training",
      "description": "Training step 426",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:04",
      "total_flops_so_far": 2536831788244992.0,
      "budget_used_percent": 2.5368317882449922
    },
    {
      "type": "training",
      "description": "Training step 427",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:04",
      "total_flops_so_far": 2542772846297088.0,
      "budget_used_percent": 2.542772846297088
    },
    {
      "type": "training",
      "description": "Training step 428",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:04",
      "total_flops_so_far": 2548713904349184.0,
      "budget_used_percent": 2.548713904349184
    },
    {
      "type": "training",
      "description": "Training step 429",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:05",
      "total_flops_so_far": 2554654962401280.0,
      "budget_used_percent": 2.55465496240128
    },
    {
      "type": "training",
      "description": "Training step 430",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:05",
      "total_flops_so_far": 2560596020453376.0,
      "budget_used_percent": 2.5605960204533758
    },
    {
      "type": "training",
      "description": "Training step 431",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:05",
      "total_flops_so_far": 2566537078505472.0,
      "budget_used_percent": 2.5665370785054717
    },
    {
      "type": "training",
      "description": "Training step 432",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:06",
      "total_flops_so_far": 2572478136557568.0,
      "budget_used_percent": 2.5724781365575677
    },
    {
      "type": "training",
      "description": "Training step 433",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:06",
      "total_flops_so_far": 2578419194609664.0,
      "budget_used_percent": 2.578419194609664
    },
    {
      "type": "training",
      "description": "Training step 434",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:06",
      "total_flops_so_far": 2584360252661760.0,
      "budget_used_percent": 2.58436025266176
    },
    {
      "type": "training",
      "description": "Training step 435",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:07",
      "total_flops_so_far": 2590301310713856.0,
      "budget_used_percent": 2.590301310713856
    },
    {
      "type": "training",
      "description": "Training step 436",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:07",
      "total_flops_so_far": 2596242368765952.0,
      "budget_used_percent": 2.596242368765952
    },
    {
      "type": "training",
      "description": "Training step 437",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:07",
      "total_flops_so_far": 2602183426818048.0,
      "budget_used_percent": 2.6021834268180477
    },
    {
      "type": "training",
      "description": "Training step 438",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:08",
      "total_flops_so_far": 2608124484870144.0,
      "budget_used_percent": 2.6081244848701437
    },
    {
      "type": "training",
      "description": "Training step 439",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:08",
      "total_flops_so_far": 2614065542922240.0,
      "budget_used_percent": 2.61406554292224
    },
    {
      "type": "training",
      "description": "Training step 440",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:08",
      "total_flops_so_far": 2620006600974336.0,
      "budget_used_percent": 2.620006600974336
    },
    {
      "type": "training",
      "description": "Training step 441",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:09",
      "total_flops_so_far": 2625947659026432.0,
      "budget_used_percent": 2.625947659026432
    },
    {
      "type": "training",
      "description": "Training step 442",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:09",
      "total_flops_so_far": 2631888717078528.0,
      "budget_used_percent": 2.631888717078528
    },
    {
      "type": "training",
      "description": "Training step 443",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:09",
      "total_flops_so_far": 2637829775130624.0,
      "budget_used_percent": 2.637829775130624
    },
    {
      "type": "training",
      "description": "Training step 444",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:10",
      "total_flops_so_far": 2643770833182720.0,
      "budget_used_percent": 2.6437708331827197
    },
    {
      "type": "training",
      "description": "Training step 445",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:10",
      "total_flops_so_far": 2649711891234816.0,
      "budget_used_percent": 2.649711891234816
    },
    {
      "type": "training",
      "description": "Training step 446",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:10",
      "total_flops_so_far": 2655652949286912.0,
      "budget_used_percent": 2.655652949286912
    },
    {
      "type": "training",
      "description": "Training step 447",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:10",
      "total_flops_so_far": 2661594007339008.0,
      "budget_used_percent": 2.661594007339008
    },
    {
      "type": "training",
      "description": "Training step 448",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:11",
      "total_flops_so_far": 2667535065391104.0,
      "budget_used_percent": 2.667535065391104
    },
    {
      "type": "training",
      "description": "Training step 449",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:11",
      "total_flops_so_far": 2673476123443200.0,
      "budget_used_percent": 2.6734761234432
    },
    {
      "type": "training",
      "description": "Training step 450",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:11",
      "total_flops_so_far": 2679417181495296.0,
      "budget_used_percent": 2.679417181495296
    },
    {
      "type": "training",
      "description": "Training step 451",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:12",
      "total_flops_so_far": 2685358239547392.0,
      "budget_used_percent": 2.6853582395473916
    },
    {
      "type": "training",
      "description": "Training step 452",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:12",
      "total_flops_so_far": 2691299297599488.0,
      "budget_used_percent": 2.691299297599488
    },
    {
      "type": "training",
      "description": "Training step 453",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:12",
      "total_flops_so_far": 2697240355651584.0,
      "budget_used_percent": 2.697240355651584
    },
    {
      "type": "training",
      "description": "Training step 454",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:13",
      "total_flops_so_far": 2703181413703680.0,
      "budget_used_percent": 2.70318141370368
    },
    {
      "type": "training",
      "description": "Training step 455",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:13",
      "total_flops_so_far": 2709122471755776.0,
      "budget_used_percent": 2.709122471755776
    },
    {
      "type": "training",
      "description": "Training step 456",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:13",
      "total_flops_so_far": 2715063529807872.0,
      "budget_used_percent": 2.715063529807872
    },
    {
      "type": "training",
      "description": "Training step 457",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:14",
      "total_flops_so_far": 2721004587859968.0,
      "budget_used_percent": 2.721004587859968
    },
    {
      "type": "training",
      "description": "Training step 458",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:14",
      "total_flops_so_far": 2726945645912064.0,
      "budget_used_percent": 2.726945645912064
    },
    {
      "type": "training",
      "description": "Training step 459",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:14",
      "total_flops_so_far": 2732886703964160.0,
      "budget_used_percent": 2.73288670396416
    },
    {
      "type": "training",
      "description": "Training step 460",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:15",
      "total_flops_so_far": 2738827762016256.0,
      "budget_used_percent": 2.738827762016256
    },
    {
      "type": "training",
      "description": "Training step 461",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:15",
      "total_flops_so_far": 2744768820068352.0,
      "budget_used_percent": 2.744768820068352
    },
    {
      "type": "training",
      "description": "Training step 462",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:15",
      "total_flops_so_far": 2750709878120448.0,
      "budget_used_percent": 2.750709878120448
    },
    {
      "type": "training",
      "description": "Training step 463",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:15",
      "total_flops_so_far": 2756650936172544.0,
      "budget_used_percent": 2.756650936172544
    },
    {
      "type": "training",
      "description": "Training step 464",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:16",
      "total_flops_so_far": 2762591994224640.0,
      "budget_used_percent": 2.76259199422464
    },
    {
      "type": "training",
      "description": "Training step 465",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:16",
      "total_flops_so_far": 2768533052276736.0,
      "budget_used_percent": 2.768533052276736
    },
    {
      "type": "training",
      "description": "Training step 466",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:16",
      "total_flops_so_far": 2774474110328832.0,
      "budget_used_percent": 2.774474110328832
    },
    {
      "type": "training",
      "description": "Training step 467",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:17",
      "total_flops_so_far": 2780415168380928.0,
      "budget_used_percent": 2.780415168380928
    },
    {
      "type": "training",
      "description": "Training step 468",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:17",
      "total_flops_so_far": 2786356226433024.0,
      "budget_used_percent": 2.786356226433024
    },
    {
      "type": "training",
      "description": "Training step 469",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:17",
      "total_flops_so_far": 2792297284485120.0,
      "budget_used_percent": 2.79229728448512
    },
    {
      "type": "training",
      "description": "Training step 470",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:18",
      "total_flops_so_far": 2798238342537216.0,
      "budget_used_percent": 2.798238342537216
    },
    {
      "type": "training",
      "description": "Training step 471",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:18",
      "total_flops_so_far": 2804179400589312.0,
      "budget_used_percent": 2.804179400589312
    },
    {
      "type": "training",
      "description": "Training step 472",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:18",
      "total_flops_so_far": 2810120458641408.0,
      "budget_used_percent": 2.8101204586414084
    },
    {
      "type": "training",
      "description": "Training step 473",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:19",
      "total_flops_so_far": 2816061516693504.0,
      "budget_used_percent": 2.816061516693504
    },
    {
      "type": "training",
      "description": "Training step 474",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:19",
      "total_flops_so_far": 2822002574745600.0,
      "budget_used_percent": 2.8220025747456
    },
    {
      "type": "training",
      "description": "Training step 475",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:19",
      "total_flops_so_far": 2827943632797696.0,
      "budget_used_percent": 2.827943632797696
    },
    {
      "type": "training",
      "description": "Training step 476",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:20",
      "total_flops_so_far": 2833884690849792.0,
      "budget_used_percent": 2.833884690849792
    },
    {
      "type": "training",
      "description": "Training step 477",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:20",
      "total_flops_so_far": 2839825748901888.0,
      "budget_used_percent": 2.839825748901888
    },
    {
      "type": "training",
      "description": "Training step 478",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:20",
      "total_flops_so_far": 2845766806953984.0,
      "budget_used_percent": 2.8457668069539843
    },
    {
      "type": "training",
      "description": "Training step 479",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:20",
      "total_flops_so_far": 2851707865006080.0,
      "budget_used_percent": 2.8517078650060803
    },
    {
      "type": "training",
      "description": "Training step 480",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:21",
      "total_flops_so_far": 2857648923058176.0,
      "budget_used_percent": 2.857648923058176
    },
    {
      "type": "training",
      "description": "Training step 481",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:21",
      "total_flops_so_far": 2863589981110272.0,
      "budget_used_percent": 2.863589981110272
    },
    {
      "type": "training",
      "description": "Training step 482",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:21",
      "total_flops_so_far": 2869531039162368.0,
      "budget_used_percent": 2.869531039162368
    },
    {
      "type": "training",
      "description": "Training step 483",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:22",
      "total_flops_so_far": 2875472097214464.0,
      "budget_used_percent": 2.875472097214464
    },
    {
      "type": "training",
      "description": "Training step 484",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:22",
      "total_flops_so_far": 2881413155266560.0,
      "budget_used_percent": 2.88141315526656
    },
    {
      "type": "training",
      "description": "Training step 485",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:22",
      "total_flops_so_far": 2887354213318656.0,
      "budget_used_percent": 2.8873542133186563
    },
    {
      "type": "training",
      "description": "Training step 486",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:23",
      "total_flops_so_far": 2893295271370752.0,
      "budget_used_percent": 2.8932952713707523
    },
    {
      "type": "training",
      "description": "Training step 487",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:23",
      "total_flops_so_far": 2899236329422848.0,
      "budget_used_percent": 2.899236329422848
    },
    {
      "type": "training",
      "description": "Training step 488",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:23",
      "total_flops_so_far": 2905177387474944.0,
      "budget_used_percent": 2.905177387474944
    },
    {
      "type": "training",
      "description": "Training step 489",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:24",
      "total_flops_so_far": 2911118445527040.0,
      "budget_used_percent": 2.91111844552704
    },
    {
      "type": "training",
      "description": "Training step 490",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:24",
      "total_flops_so_far": 2917059503579136.0,
      "budget_used_percent": 2.917059503579136
    },
    {
      "type": "training",
      "description": "Training step 491",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:24",
      "total_flops_so_far": 2923000561631232.0,
      "budget_used_percent": 2.9230005616312322
    },
    {
      "type": "training",
      "description": "Training step 492",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:25",
      "total_flops_so_far": 2928941619683328.0,
      "budget_used_percent": 2.9289416196833282
    },
    {
      "type": "training",
      "description": "Training step 493",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:25",
      "total_flops_so_far": 2934882677735424.0,
      "budget_used_percent": 2.934882677735424
    },
    {
      "type": "training",
      "description": "Training step 494",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:25",
      "total_flops_so_far": 2940823735787520.0,
      "budget_used_percent": 2.9408237357875198
    },
    {
      "type": "training",
      "description": "Training step 495",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:26",
      "total_flops_so_far": 2946764793839616.0,
      "budget_used_percent": 2.9467647938396158
    },
    {
      "type": "training",
      "description": "Training step 496",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:26",
      "total_flops_so_far": 2952705851891712.0,
      "budget_used_percent": 2.9527058518917118
    },
    {
      "type": "training",
      "description": "Training step 497",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:26",
      "total_flops_so_far": 2958646909943808.0,
      "budget_used_percent": 2.958646909943808
    },
    {
      "type": "training",
      "description": "Training step 498",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:26",
      "total_flops_so_far": 2964587967995904.0,
      "budget_used_percent": 2.964587967995904
    },
    {
      "type": "training",
      "description": "Training step 499",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:27",
      "total_flops_so_far": 2970529026048000.0,
      "budget_used_percent": 2.970529026048
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 552526568288.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:29",
      "total_flops_so_far": 2971081552616288.0,
      "budget_used_percent": 2.9710815526162877
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 556130834256.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:31",
      "total_flops_so_far": 2971637683450544.0,
      "budget_used_percent": 2.971637683450544
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 554328341080.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:32",
      "total_flops_so_far": 2972192011791624.0,
      "budget_used_percent": 2.9721920117916243
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 552526568288.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:34",
      "total_flops_so_far": 2972744538359912.0,
      "budget_used_percent": 2.972744538359912
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 555229497620.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:36",
      "total_flops_so_far": 2973299767857532.0,
      "budget_used_percent": 2.973299767857532
    },
    {
      "type": "training",
      "description": "Training step 500",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:36",
      "total_flops_so_far": 2979240825909628.0,
      "budget_used_percent": 2.9792408259096277
    },
    {
      "type": "training",
      "description": "Training step 501",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:36",
      "total_flops_so_far": 2985181883961724.0,
      "budget_used_percent": 2.985181883961724
    },
    {
      "type": "training",
      "description": "Training step 502",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:37",
      "total_flops_so_far": 2991122942013820.0,
      "budget_used_percent": 2.99112294201382
    },
    {
      "type": "training",
      "description": "Training step 503",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:37",
      "total_flops_so_far": 2997064000065916.0,
      "budget_used_percent": 2.997064000065916
    },
    {
      "type": "training",
      "description": "Training step 504",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:37",
      "total_flops_so_far": 3003005058118012.0,
      "budget_used_percent": 3.003005058118012
    },
    {
      "type": "training",
      "description": "Training step 505",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:38",
      "total_flops_so_far": 3008946116170108.0,
      "budget_used_percent": 3.008946116170108
    },
    {
      "type": "training",
      "description": "Training step 506",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:38",
      "total_flops_so_far": 3014887174222204.0,
      "budget_used_percent": 3.014887174222204
    },
    {
      "type": "training",
      "description": "Training step 507",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:38",
      "total_flops_so_far": 3020828232274300.0,
      "budget_used_percent": 3.0208282322742996
    },
    {
      "type": "training",
      "description": "Training step 508",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:39",
      "total_flops_so_far": 3026769290326396.0,
      "budget_used_percent": 3.026769290326396
    },
    {
      "type": "training",
      "description": "Training step 509",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:39",
      "total_flops_so_far": 3032710348378492.0,
      "budget_used_percent": 3.032710348378492
    },
    {
      "type": "training",
      "description": "Training step 510",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:39",
      "total_flops_so_far": 3038651406430588.0,
      "budget_used_percent": 3.038651406430588
    },
    {
      "type": "training",
      "description": "Training step 511",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:40",
      "total_flops_so_far": 3044592464482684.0,
      "budget_used_percent": 3.044592464482684
    },
    {
      "type": "training",
      "description": "Training step 512",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:40",
      "total_flops_so_far": 3050533522534780.0,
      "budget_used_percent": 3.05053352253478
    },
    {
      "type": "training",
      "description": "Training step 513",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:40",
      "total_flops_so_far": 3056474580586876.0,
      "budget_used_percent": 3.056474580586876
    },
    {
      "type": "training",
      "description": "Training step 514",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:40",
      "total_flops_so_far": 3062415638638972.0,
      "budget_used_percent": 3.062415638638972
    },
    {
      "type": "training",
      "description": "Training step 515",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:41",
      "total_flops_so_far": 3068356696691068.0,
      "budget_used_percent": 3.068356696691068
    },
    {
      "type": "training",
      "description": "Training step 516",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:41",
      "total_flops_so_far": 3074297754743164.0,
      "budget_used_percent": 3.074297754743164
    },
    {
      "type": "training",
      "description": "Training step 517",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:41",
      "total_flops_so_far": 3080238812795260.0,
      "budget_used_percent": 3.08023881279526
    },
    {
      "type": "training",
      "description": "Training step 518",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:42",
      "total_flops_so_far": 3086179870847356.0,
      "budget_used_percent": 3.086179870847356
    },
    {
      "type": "training",
      "description": "Training step 519",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:42",
      "total_flops_so_far": 3092120928899452.0,
      "budget_used_percent": 3.092120928899452
    },
    {
      "type": "training",
      "description": "Training step 520",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:42",
      "total_flops_so_far": 3098061986951548.0,
      "budget_used_percent": 3.098061986951548
    },
    {
      "type": "training",
      "description": "Training step 521",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:43",
      "total_flops_so_far": 3104003045003644.0,
      "budget_used_percent": 3.104003045003644
    },
    {
      "type": "training",
      "description": "Training step 522",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:43",
      "total_flops_so_far": 3109944103055740.0,
      "budget_used_percent": 3.10994410305574
    },
    {
      "type": "training",
      "description": "Training step 523",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:43",
      "total_flops_so_far": 3115885161107836.0,
      "budget_used_percent": 3.115885161107836
    },
    {
      "type": "training",
      "description": "Training step 524",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:44",
      "total_flops_so_far": 3121826219159932.0,
      "budget_used_percent": 3.121826219159932
    },
    {
      "type": "training",
      "description": "Training step 525",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:44",
      "total_flops_so_far": 3127767277212028.0,
      "budget_used_percent": 3.1277672772120275
    },
    {
      "type": "training",
      "description": "Training step 526",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:44",
      "total_flops_so_far": 3133708335264124.0,
      "budget_used_percent": 3.1337083352641235
    },
    {
      "type": "training",
      "description": "Training step 527",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:45",
      "total_flops_so_far": 3139649393316220.0,
      "budget_used_percent": 3.13964939331622
    },
    {
      "type": "training",
      "description": "Training step 528",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:45",
      "total_flops_so_far": 3145590451368316.0,
      "budget_used_percent": 3.145590451368316
    },
    {
      "type": "training",
      "description": "Training step 529",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:45",
      "total_flops_so_far": 3151531509420412.0,
      "budget_used_percent": 3.151531509420412
    },
    {
      "type": "training",
      "description": "Training step 530",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:46",
      "total_flops_so_far": 3157472567472508.0,
      "budget_used_percent": 3.157472567472508
    },
    {
      "type": "training",
      "description": "Training step 531",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:46",
      "total_flops_so_far": 3163413625524604.0,
      "budget_used_percent": 3.163413625524604
    },
    {
      "type": "training",
      "description": "Training step 532",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:46",
      "total_flops_so_far": 3169354683576700.0,
      "budget_used_percent": 3.1693546835767
    },
    {
      "type": "training",
      "description": "Training step 533",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:46",
      "total_flops_so_far": 3175295741628796.0,
      "budget_used_percent": 3.175295741628796
    },
    {
      "type": "training",
      "description": "Training step 534",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:47",
      "total_flops_so_far": 3181236799680892.0,
      "budget_used_percent": 3.1812367996808923
    },
    {
      "type": "training",
      "description": "Training step 535",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:47",
      "total_flops_so_far": 3187177857732988.0,
      "budget_used_percent": 3.1871778577329883
    },
    {
      "type": "training",
      "description": "Training step 536",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:47",
      "total_flops_so_far": 3193118915785084.0,
      "budget_used_percent": 3.1931189157850843
    },
    {
      "type": "training",
      "description": "Training step 537",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:48",
      "total_flops_so_far": 3199059973837180.0,
      "budget_used_percent": 3.1990599738371803
    },
    {
      "type": "training",
      "description": "Training step 538",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:48",
      "total_flops_so_far": 3205001031889276.0,
      "budget_used_percent": 3.2050010318892763
    },
    {
      "type": "training",
      "description": "Training step 539",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:48",
      "total_flops_so_far": 3210942089941372.0,
      "budget_used_percent": 3.210942089941372
    },
    {
      "type": "training",
      "description": "Training step 540",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:49",
      "total_flops_so_far": 3216883147993468.0,
      "budget_used_percent": 3.216883147993468
    },
    {
      "type": "training",
      "description": "Training step 541",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:49",
      "total_flops_so_far": 3222824206045564.0,
      "budget_used_percent": 3.222824206045564
    },
    {
      "type": "training",
      "description": "Training step 542",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:49",
      "total_flops_so_far": 3228765264097660.0,
      "budget_used_percent": 3.22876526409766
    },
    {
      "type": "training",
      "description": "Training step 543",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:50",
      "total_flops_so_far": 3234706322149756.0,
      "budget_used_percent": 3.234706322149756
    },
    {
      "type": "training",
      "description": "Training step 544",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:50",
      "total_flops_so_far": 3240647380201852.0,
      "budget_used_percent": 3.240647380201852
    },
    {
      "type": "training",
      "description": "Training step 545",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:50",
      "total_flops_so_far": 3246588438253948.0,
      "budget_used_percent": 3.246588438253948
    },
    {
      "type": "training",
      "description": "Training step 546",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:51",
      "total_flops_so_far": 3252529496306044.0,
      "budget_used_percent": 3.252529496306044
    },
    {
      "type": "training",
      "description": "Training step 547",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:51",
      "total_flops_so_far": 3258470554358140.0,
      "budget_used_percent": 3.2584705543581403
    },
    {
      "type": "training",
      "description": "Training step 548",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:51",
      "total_flops_so_far": 3264411612410236.0,
      "budget_used_percent": 3.2644116124102363
    },
    {
      "type": "training",
      "description": "Training step 549",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:52",
      "total_flops_so_far": 3270352670462332.0,
      "budget_used_percent": 3.2703526704623322
    },
    {
      "type": "training",
      "description": "Training step 550",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:52",
      "total_flops_so_far": 3276293728514428.0,
      "budget_used_percent": 3.2762937285144282
    },
    {
      "type": "training",
      "description": "Training step 551",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:52",
      "total_flops_so_far": 3282234786566524.0,
      "budget_used_percent": 3.2822347865665242
    },
    {
      "type": "training",
      "description": "Training step 552",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:52",
      "total_flops_so_far": 3288175844618620.0,
      "budget_used_percent": 3.2881758446186202
    },
    {
      "type": "training",
      "description": "Training step 553",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:53",
      "total_flops_so_far": 3294116902670716.0,
      "budget_used_percent": 3.2941169026707158
    },
    {
      "type": "training",
      "description": "Training step 554",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:53",
      "total_flops_so_far": 3300057960722812.0,
      "budget_used_percent": 3.3000579607228118
    },
    {
      "type": "training",
      "description": "Training step 555",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:53",
      "total_flops_so_far": 3305999018774908.0,
      "budget_used_percent": 3.3059990187749078
    },
    {
      "type": "training",
      "description": "Training step 556",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:54",
      "total_flops_so_far": 3311940076827004.0,
      "budget_used_percent": 3.3119400768270038
    },
    {
      "type": "training",
      "description": "Training step 557",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:54",
      "total_flops_so_far": 3317881134879100.0,
      "budget_used_percent": 3.3178811348790997
    },
    {
      "type": "training",
      "description": "Training step 558",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:54",
      "total_flops_so_far": 3323822192931196.0,
      "budget_used_percent": 3.3238221929311957
    },
    {
      "type": "training",
      "description": "Training step 559",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:55",
      "total_flops_so_far": 3329763250983292.0,
      "budget_used_percent": 3.3297632509832917
    },
    {
      "type": "training",
      "description": "Training step 560",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:55",
      "total_flops_so_far": 3335704309035388.0,
      "budget_used_percent": 3.335704309035388
    },
    {
      "type": "training",
      "description": "Training step 561",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:55",
      "total_flops_so_far": 3341645367087484.0,
      "budget_used_percent": 3.341645367087484
    },
    {
      "type": "training",
      "description": "Training step 562",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:56",
      "total_flops_so_far": 3347586425139580.0,
      "budget_used_percent": 3.34758642513958
    },
    {
      "type": "training",
      "description": "Training step 563",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:56",
      "total_flops_so_far": 3353527483191676.0,
      "budget_used_percent": 3.353527483191676
    },
    {
      "type": "training",
      "description": "Training step 564",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:56",
      "total_flops_so_far": 3359468541243772.0,
      "budget_used_percent": 3.359468541243772
    },
    {
      "type": "training",
      "description": "Training step 565",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:57",
      "total_flops_so_far": 3365409599295868.0,
      "budget_used_percent": 3.365409599295868
    },
    {
      "type": "training",
      "description": "Training step 566",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:57",
      "total_flops_so_far": 3371350657347964.0,
      "budget_used_percent": 3.371350657347964
    },
    {
      "type": "training",
      "description": "Training step 567",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:57",
      "total_flops_so_far": 3377291715400060.0,
      "budget_used_percent": 3.3772917154000606
    },
    {
      "type": "training",
      "description": "Training step 568",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:58",
      "total_flops_so_far": 3383232773452156.0,
      "budget_used_percent": 3.3832327734521557
    },
    {
      "type": "training",
      "description": "Training step 569",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:58",
      "total_flops_so_far": 3389173831504252.0,
      "budget_used_percent": 3.3891738315042517
    },
    {
      "type": "training",
      "description": "Training step 570",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:58",
      "total_flops_so_far": 3395114889556348.0,
      "budget_used_percent": 3.3951148895563477
    },
    {
      "type": "training",
      "description": "Training step 571",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:58",
      "total_flops_so_far": 3401055947608444.0,
      "budget_used_percent": 3.4010559476084437
    },
    {
      "type": "training",
      "description": "Training step 572",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:59",
      "total_flops_so_far": 3406997005660540.0,
      "budget_used_percent": 3.40699700566054
    },
    {
      "type": "training",
      "description": "Training step 573",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:59",
      "total_flops_so_far": 3412938063712636.0,
      "budget_used_percent": 3.412938063712636
    },
    {
      "type": "training",
      "description": "Training step 574",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:41:59",
      "total_flops_so_far": 3418879121764732.0,
      "budget_used_percent": 3.418879121764732
    },
    {
      "type": "training",
      "description": "Training step 575",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:00",
      "total_flops_so_far": 3424820179816828.0,
      "budget_used_percent": 3.424820179816828
    },
    {
      "type": "training",
      "description": "Training step 576",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:00",
      "total_flops_so_far": 3430761237868924.0,
      "budget_used_percent": 3.430761237868924
    },
    {
      "type": "training",
      "description": "Training step 577",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:00",
      "total_flops_so_far": 3436702295921020.0,
      "budget_used_percent": 3.43670229592102
    },
    {
      "type": "training",
      "description": "Training step 578",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:01",
      "total_flops_so_far": 3442643353973116.0,
      "budget_used_percent": 3.442643353973116
    },
    {
      "type": "training",
      "description": "Training step 579",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:01",
      "total_flops_so_far": 3448584412025212.0,
      "budget_used_percent": 3.448584412025212
    },
    {
      "type": "training",
      "description": "Training step 580",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:01",
      "total_flops_so_far": 3454525470077308.0,
      "budget_used_percent": 3.4545254700773085
    },
    {
      "type": "training",
      "description": "Training step 581",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:02",
      "total_flops_so_far": 3460466528129404.0,
      "budget_used_percent": 3.4604665281294045
    },
    {
      "type": "training",
      "description": "Training step 582",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:02",
      "total_flops_so_far": 3466407586181500.0,
      "budget_used_percent": 3.4664075861814996
    },
    {
      "type": "training",
      "description": "Training step 583",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:02",
      "total_flops_so_far": 3472348644233596.0,
      "budget_used_percent": 3.4723486442335956
    },
    {
      "type": "training",
      "description": "Training step 584",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:03",
      "total_flops_so_far": 3478289702285692.0,
      "budget_used_percent": 3.478289702285692
    },
    {
      "type": "training",
      "description": "Training step 585",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:03",
      "total_flops_so_far": 3484230760337788.0,
      "budget_used_percent": 3.484230760337788
    },
    {
      "type": "training",
      "description": "Training step 586",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:03",
      "total_flops_so_far": 3490171818389884.0,
      "budget_used_percent": 3.490171818389884
    },
    {
      "type": "training",
      "description": "Training step 587",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:04",
      "total_flops_so_far": 3496112876441980.0,
      "budget_used_percent": 3.49611287644198
    },
    {
      "type": "training",
      "description": "Training step 588",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:04",
      "total_flops_so_far": 3502053934494076.0,
      "budget_used_percent": 3.502053934494076
    },
    {
      "type": "training",
      "description": "Training step 589",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:04",
      "total_flops_so_far": 3507994992546172.0,
      "budget_used_percent": 3.507994992546172
    },
    {
      "type": "training",
      "description": "Training step 590",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:04",
      "total_flops_so_far": 3513936050598268.0,
      "budget_used_percent": 3.513936050598268
    },
    {
      "type": "training",
      "description": "Training step 591",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:05",
      "total_flops_so_far": 3519877108650364.0,
      "budget_used_percent": 3.519877108650364
    },
    {
      "type": "training",
      "description": "Training step 592",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:05",
      "total_flops_so_far": 3525818166702460.0,
      "budget_used_percent": 3.52581816670246
    },
    {
      "type": "training",
      "description": "Training step 593",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:05",
      "total_flops_so_far": 3531759224754556.0,
      "budget_used_percent": 3.5317592247545564
    },
    {
      "type": "training",
      "description": "Training step 594",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:06",
      "total_flops_so_far": 3537700282806652.0,
      "budget_used_percent": 3.5377002828066524
    },
    {
      "type": "training",
      "description": "Training step 595",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:06",
      "total_flops_so_far": 3543641340858748.0,
      "budget_used_percent": 3.5436413408587484
    },
    {
      "type": "training",
      "description": "Training step 596",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:06",
      "total_flops_so_far": 3549582398910844.0,
      "budget_used_percent": 3.5495823989108435
    },
    {
      "type": "training",
      "description": "Training step 597",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:07",
      "total_flops_so_far": 3555523456962940.0,
      "budget_used_percent": 3.55552345696294
    },
    {
      "type": "training",
      "description": "Training step 598",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:07",
      "total_flops_so_far": 3561464515015036.0,
      "budget_used_percent": 3.561464515015036
    },
    {
      "type": "training",
      "description": "Training step 599",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:07",
      "total_flops_so_far": 3567405573067132.0,
      "budget_used_percent": 3.567405573067132
    },
    {
      "type": "training",
      "description": "Training step 600",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:08",
      "total_flops_so_far": 3573346631119228.0,
      "budget_used_percent": 3.573346631119228
    },
    {
      "type": "training",
      "description": "Training step 601",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:08",
      "total_flops_so_far": 3579287689171324.0,
      "budget_used_percent": 3.579287689171324
    },
    {
      "type": "training",
      "description": "Training step 602",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:08",
      "total_flops_so_far": 3585228747223420.0,
      "budget_used_percent": 3.58522874722342
    },
    {
      "type": "training",
      "description": "Training step 603",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:09",
      "total_flops_so_far": 3591169805275516.0,
      "budget_used_percent": 3.591169805275516
    },
    {
      "type": "training",
      "description": "Training step 604",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:09",
      "total_flops_so_far": 3597110863327612.0,
      "budget_used_percent": 3.597110863327612
    },
    {
      "type": "training",
      "description": "Training step 605",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:09",
      "total_flops_so_far": 3603051921379708.0,
      "budget_used_percent": 3.6030519213797083
    },
    {
      "type": "training",
      "description": "Training step 606",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:10",
      "total_flops_so_far": 3608992979431804.0,
      "budget_used_percent": 3.6089929794318043
    },
    {
      "type": "training",
      "description": "Training step 607",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:10",
      "total_flops_so_far": 3614934037483900.0,
      "budget_used_percent": 3.6149340374839003
    },
    {
      "type": "training",
      "description": "Training step 608",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:10",
      "total_flops_so_far": 3620875095535996.0,
      "budget_used_percent": 3.6208750955359963
    },
    {
      "type": "training",
      "description": "Training step 609",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:11",
      "total_flops_so_far": 3626816153588092.0,
      "budget_used_percent": 3.6268161535880923
    },
    {
      "type": "training",
      "description": "Training step 610",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:11",
      "total_flops_so_far": 3632757211640188.0,
      "budget_used_percent": 3.632757211640188
    },
    {
      "type": "training",
      "description": "Training step 611",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:11",
      "total_flops_so_far": 3638698269692284.0,
      "budget_used_percent": 3.638698269692284
    },
    {
      "type": "training",
      "description": "Training step 612",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:11",
      "total_flops_so_far": 3644639327744380.0,
      "budget_used_percent": 3.64463932774438
    },
    {
      "type": "training",
      "description": "Training step 613",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:12",
      "total_flops_so_far": 3650580385796476.0,
      "budget_used_percent": 3.650580385796476
    },
    {
      "type": "training",
      "description": "Training step 614",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:12",
      "total_flops_so_far": 3656521443848572.0,
      "budget_used_percent": 3.656521443848572
    },
    {
      "type": "training",
      "description": "Training step 615",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:12",
      "total_flops_so_far": 3662462501900668.0,
      "budget_used_percent": 3.662462501900668
    },
    {
      "type": "training",
      "description": "Training step 616",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:13",
      "total_flops_so_far": 3668403559952764.0,
      "budget_used_percent": 3.668403559952764
    },
    {
      "type": "training",
      "description": "Training step 617",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:13",
      "total_flops_so_far": 3674344618004860.0,
      "budget_used_percent": 3.6743446180048602
    },
    {
      "type": "training",
      "description": "Training step 618",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:13",
      "total_flops_so_far": 3680285676056956.0,
      "budget_used_percent": 3.6802856760569562
    },
    {
      "type": "training",
      "description": "Training step 619",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:14",
      "total_flops_so_far": 3686226734109052.0,
      "budget_used_percent": 3.686226734109052
    },
    {
      "type": "training",
      "description": "Training step 620",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:14",
      "total_flops_so_far": 3692167792161148.0,
      "budget_used_percent": 3.692167792161148
    },
    {
      "type": "training",
      "description": "Training step 621",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:14",
      "total_flops_so_far": 3698108850213244.0,
      "budget_used_percent": 3.698108850213244
    },
    {
      "type": "training",
      "description": "Training step 622",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:15",
      "total_flops_so_far": 3704049908265340.0,
      "budget_used_percent": 3.70404990826534
    },
    {
      "type": "training",
      "description": "Training step 623",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:15",
      "total_flops_so_far": 3709990966317436.0,
      "budget_used_percent": 3.709990966317436
    },
    {
      "type": "training",
      "description": "Training step 624",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:15",
      "total_flops_so_far": 3715932024369532.0,
      "budget_used_percent": 3.7159320243695317
    },
    {
      "type": "training",
      "description": "Training step 625",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:16",
      "total_flops_so_far": 3721873082421628.0,
      "budget_used_percent": 3.7218730824216277
    },
    {
      "type": "training",
      "description": "Training step 626",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:16",
      "total_flops_so_far": 3727814140473724.0,
      "budget_used_percent": 3.7278141404737237
    },
    {
      "type": "training",
      "description": "Training step 627",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:16",
      "total_flops_so_far": 3733755198525820.0,
      "budget_used_percent": 3.7337551985258197
    },
    {
      "type": "training",
      "description": "Training step 628",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:17",
      "total_flops_so_far": 3739696256577916.0,
      "budget_used_percent": 3.7396962565779157
    },
    {
      "type": "training",
      "description": "Training step 629",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:17",
      "total_flops_so_far": 3745637314630012.0,
      "budget_used_percent": 3.7456373146300117
    },
    {
      "type": "training",
      "description": "Training step 630",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:17",
      "total_flops_so_far": 3751578372682108.0,
      "budget_used_percent": 3.751578372682108
    },
    {
      "type": "training",
      "description": "Training step 631",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:17",
      "total_flops_so_far": 3757519430734204.0,
      "budget_used_percent": 3.757519430734204
    },
    {
      "type": "training",
      "description": "Training step 632",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:18",
      "total_flops_so_far": 3763460488786300.0,
      "budget_used_percent": 3.7634604887863
    },
    {
      "type": "training",
      "description": "Training step 633",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:18",
      "total_flops_so_far": 3769401546838396.0,
      "budget_used_percent": 3.769401546838396
    },
    {
      "type": "training",
      "description": "Training step 634",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:18",
      "total_flops_so_far": 3775342604890492.0,
      "budget_used_percent": 3.775342604890492
    },
    {
      "type": "training",
      "description": "Training step 635",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:19",
      "total_flops_so_far": 3781283662942588.0,
      "budget_used_percent": 3.781283662942588
    },
    {
      "type": "training",
      "description": "Training step 636",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:19",
      "total_flops_so_far": 3787224720994684.0,
      "budget_used_percent": 3.787224720994684
    },
    {
      "type": "training",
      "description": "Training step 637",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:19",
      "total_flops_so_far": 3793165779046780.0,
      "budget_used_percent": 3.79316577904678
    },
    {
      "type": "training",
      "description": "Training step 638",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:20",
      "total_flops_so_far": 3799106837098876.0,
      "budget_used_percent": 3.7991068370988765
    },
    {
      "type": "training",
      "description": "Training step 639",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:20",
      "total_flops_so_far": 3805047895150972.0,
      "budget_used_percent": 3.8050478951509716
    },
    {
      "type": "training",
      "description": "Training step 640",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:20",
      "total_flops_so_far": 3810988953203068.0,
      "budget_used_percent": 3.8109889532030676
    },
    {
      "type": "training",
      "description": "Training step 641",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:21",
      "total_flops_so_far": 3816930011255164.0,
      "budget_used_percent": 3.8169300112551636
    },
    {
      "type": "training",
      "description": "Training step 642",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:21",
      "total_flops_so_far": 3822871069307260.0,
      "budget_used_percent": 3.8228710693072596
    },
    {
      "type": "training",
      "description": "Training step 643",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:21",
      "total_flops_so_far": 3828812127359356.0,
      "budget_used_percent": 3.828812127359356
    },
    {
      "type": "training",
      "description": "Training step 644",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:22",
      "total_flops_so_far": 3834753185411452.0,
      "budget_used_percent": 3.834753185411452
    },
    {
      "type": "training",
      "description": "Training step 645",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:22",
      "total_flops_so_far": 3840694243463548.0,
      "budget_used_percent": 3.840694243463548
    },
    {
      "type": "training",
      "description": "Training step 646",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:22",
      "total_flops_so_far": 3846635301515644.0,
      "budget_used_percent": 3.846635301515644
    },
    {
      "type": "training",
      "description": "Training step 647",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:23",
      "total_flops_so_far": 3852576359567740.0,
      "budget_used_percent": 3.85257635956774
    },
    {
      "type": "training",
      "description": "Training step 648",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:23",
      "total_flops_so_far": 3858517417619836.0,
      "budget_used_percent": 3.858517417619836
    },
    {
      "type": "training",
      "description": "Training step 649",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:23",
      "total_flops_so_far": 3864458475671932.0,
      "budget_used_percent": 3.864458475671932
    },
    {
      "type": "training",
      "description": "Training step 650",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:24",
      "total_flops_so_far": 3870399533724028.0,
      "budget_used_percent": 3.8703995337240285
    },
    {
      "type": "training",
      "description": "Training step 651",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:24",
      "total_flops_so_far": 3876340591776124.0,
      "budget_used_percent": 3.8763405917761244
    },
    {
      "type": "training",
      "description": "Training step 652",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:24",
      "total_flops_so_far": 3882281649828220.0,
      "budget_used_percent": 3.8822816498282204
    },
    {
      "type": "training",
      "description": "Training step 653",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:25",
      "total_flops_so_far": 3888222707880316.0,
      "budget_used_percent": 3.8882227078803155
    },
    {
      "type": "training",
      "description": "Training step 654",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:25",
      "total_flops_so_far": 3894163765932412.0,
      "budget_used_percent": 3.8941637659324115
    },
    {
      "type": "training",
      "description": "Training step 655",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:25",
      "total_flops_so_far": 3900104823984508.0,
      "budget_used_percent": 3.900104823984508
    },
    {
      "type": "training",
      "description": "Training step 656",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:25",
      "total_flops_so_far": 3906045882036604.0,
      "budget_used_percent": 3.906045882036604
    },
    {
      "type": "training",
      "description": "Training step 657",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:26",
      "total_flops_so_far": 3911986940088700.0,
      "budget_used_percent": 3.9119869400887
    },
    {
      "type": "training",
      "description": "Training step 658",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:26",
      "total_flops_so_far": 3917927998140796.0,
      "budget_used_percent": 3.917927998140796
    },
    {
      "type": "training",
      "description": "Training step 659",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:26",
      "total_flops_so_far": 3923869056192892.0,
      "budget_used_percent": 3.923869056192892
    },
    {
      "type": "training",
      "description": "Training step 660",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:27",
      "total_flops_so_far": 3929810114244988.0,
      "budget_used_percent": 3.929810114244988
    },
    {
      "type": "training",
      "description": "Training step 661",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:27",
      "total_flops_so_far": 3935751172297084.0,
      "budget_used_percent": 3.935751172297084
    },
    {
      "type": "training",
      "description": "Training step 662",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:27",
      "total_flops_so_far": 3941692230349180.0,
      "budget_used_percent": 3.94169223034918
    },
    {
      "type": "training",
      "description": "Training step 663",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:28",
      "total_flops_so_far": 3947633288401276.0,
      "budget_used_percent": 3.9476332884012764
    },
    {
      "type": "training",
      "description": "Training step 664",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:28",
      "total_flops_so_far": 3953574346453372.0,
      "budget_used_percent": 3.9535743464533724
    },
    {
      "type": "training",
      "description": "Training step 665",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:28",
      "total_flops_so_far": 3959515404505468.0,
      "budget_used_percent": 3.9595154045054684
    },
    {
      "type": "training",
      "description": "Training step 666",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:29",
      "total_flops_so_far": 3965456462557564.0,
      "budget_used_percent": 3.9654564625575643
    },
    {
      "type": "training",
      "description": "Training step 667",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:29",
      "total_flops_so_far": 3971397520609660.0,
      "budget_used_percent": 3.97139752060966
    },
    {
      "type": "training",
      "description": "Training step 668",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:29",
      "total_flops_so_far": 3977338578661756.0,
      "budget_used_percent": 3.977338578661756
    },
    {
      "type": "training",
      "description": "Training step 669",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:30",
      "total_flops_so_far": 3983279636713852.0,
      "budget_used_percent": 3.983279636713852
    },
    {
      "type": "training",
      "description": "Training step 670",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:30",
      "total_flops_so_far": 3989220694765948.0,
      "budget_used_percent": 3.989220694765948
    },
    {
      "type": "training",
      "description": "Training step 671",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:30",
      "total_flops_so_far": 3995161752818044.0,
      "budget_used_percent": 3.995161752818044
    },
    {
      "type": "training",
      "description": "Training step 672",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:31",
      "total_flops_so_far": 4001102810870140.0,
      "budget_used_percent": 4.00110281087014
    },
    {
      "type": "training",
      "description": "Training step 673",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:31",
      "total_flops_so_far": 4007043868922236.0,
      "budget_used_percent": 4.007043868922236
    },
    {
      "type": "training",
      "description": "Training step 674",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:31",
      "total_flops_so_far": 4012984926974332.0,
      "budget_used_percent": 4.012984926974332
    },
    {
      "type": "training",
      "description": "Training step 675",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:32",
      "total_flops_so_far": 4018925985026428.0,
      "budget_used_percent": 4.018925985026428
    },
    {
      "type": "training",
      "description": "Training step 676",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:32",
      "total_flops_so_far": 4024867043078524.0,
      "budget_used_percent": 4.024867043078524
    },
    {
      "type": "training",
      "description": "Training step 677",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:32",
      "total_flops_so_far": 4030808101130620.0,
      "budget_used_percent": 4.03080810113062
    },
    {
      "type": "training",
      "description": "Training step 678",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:32",
      "total_flops_so_far": 4036749159182716.0,
      "budget_used_percent": 4.036749159182716
    },
    {
      "type": "training",
      "description": "Training step 679",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:33",
      "total_flops_so_far": 4042690217234812.0,
      "budget_used_percent": 4.042690217234812
    },
    {
      "type": "training",
      "description": "Training step 680",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:33",
      "total_flops_so_far": 4048631275286908.0,
      "budget_used_percent": 4.048631275286908
    },
    {
      "type": "training",
      "description": "Training step 681",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:33",
      "total_flops_so_far": 4054572333339004.0,
      "budget_used_percent": 4.054572333339004
    },
    {
      "type": "training",
      "description": "Training step 682",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:34",
      "total_flops_so_far": 4060513391391100.0,
      "budget_used_percent": 4.060513391391099
    },
    {
      "type": "training",
      "description": "Training step 683",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:34",
      "total_flops_so_far": 4066454449443196.0,
      "budget_used_percent": 4.066454449443196
    },
    {
      "type": "training",
      "description": "Training step 684",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:34",
      "total_flops_so_far": 4072395507495292.0,
      "budget_used_percent": 4.072395507495292
    },
    {
      "type": "training",
      "description": "Training step 685",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:35",
      "total_flops_so_far": 4078336565547388.0,
      "budget_used_percent": 4.078336565547388
    },
    {
      "type": "training",
      "description": "Training step 686",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:35",
      "total_flops_so_far": 4084277623599484.0,
      "budget_used_percent": 4.084277623599484
    },
    {
      "type": "training",
      "description": "Training step 687",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:35",
      "total_flops_so_far": 4090218681651580.0,
      "budget_used_percent": 4.09021868165158
    },
    {
      "type": "training",
      "description": "Training step 688",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:36",
      "total_flops_so_far": 4096159739703676.0,
      "budget_used_percent": 4.096159739703676
    },
    {
      "type": "training",
      "description": "Training step 689",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:36",
      "total_flops_so_far": 4102100797755772.0,
      "budget_used_percent": 4.102100797755772
    },
    {
      "type": "training",
      "description": "Training step 690",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:36",
      "total_flops_so_far": 4108041855807868.0,
      "budget_used_percent": 4.108041855807868
    },
    {
      "type": "training",
      "description": "Training step 691",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:37",
      "total_flops_so_far": 4113982913859964.0,
      "budget_used_percent": 4.113982913859964
    },
    {
      "type": "training",
      "description": "Training step 692",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:37",
      "total_flops_so_far": 4119923971912060.0,
      "budget_used_percent": 4.11992397191206
    },
    {
      "type": "training",
      "description": "Training step 693",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:37",
      "total_flops_so_far": 4125865029964156.0,
      "budget_used_percent": 4.125865029964157
    },
    {
      "type": "training",
      "description": "Training step 694",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:38",
      "total_flops_so_far": 4131806088016252.0,
      "budget_used_percent": 4.131806088016252
    },
    {
      "type": "training",
      "description": "Training step 695",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:38",
      "total_flops_so_far": 4137747146068348.0,
      "budget_used_percent": 4.137747146068349
    },
    {
      "type": "training",
      "description": "Training step 696",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:38",
      "total_flops_so_far": 4143688204120444.0,
      "budget_used_percent": 4.143688204120444
    },
    {
      "type": "training",
      "description": "Training step 697",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:39",
      "total_flops_so_far": 4149629262172540.0,
      "budget_used_percent": 4.14962926217254
    },
    {
      "type": "training",
      "description": "Training step 698",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:39",
      "total_flops_so_far": 4155570320224636.0,
      "budget_used_percent": 4.155570320224636
    },
    {
      "type": "training",
      "description": "Training step 699",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:39",
      "total_flops_so_far": 4161511378276732.0,
      "budget_used_percent": 4.161511378276732
    },
    {
      "type": "training",
      "description": "Training step 700",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:40",
      "total_flops_so_far": 4167452436328828.0,
      "budget_used_percent": 4.167452436328828
    },
    {
      "type": "training",
      "description": "Training step 701",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:40",
      "total_flops_so_far": 4173393494380924.0,
      "budget_used_percent": 4.173393494380924
    },
    {
      "type": "training",
      "description": "Training step 702",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:40",
      "total_flops_so_far": 4179334552433020.0,
      "budget_used_percent": 4.17933455243302
    },
    {
      "type": "training",
      "description": "Training step 703",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:40",
      "total_flops_so_far": 4185275610485116.0,
      "budget_used_percent": 4.185275610485116
    },
    {
      "type": "training",
      "description": "Training step 704",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:41",
      "total_flops_so_far": 4191216668537212.0,
      "budget_used_percent": 4.191216668537212
    },
    {
      "type": "training",
      "description": "Training step 705",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:41",
      "total_flops_so_far": 4197157726589308.0,
      "budget_used_percent": 4.1971577265893085
    },
    {
      "type": "training",
      "description": "Training step 706",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:41",
      "total_flops_so_far": 4203098784641404.0,
      "budget_used_percent": 4.203098784641404
    },
    {
      "type": "training",
      "description": "Training step 707",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:42",
      "total_flops_so_far": 4209039842693500.0,
      "budget_used_percent": 4.2090398426935005
    },
    {
      "type": "training",
      "description": "Training step 708",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:42",
      "total_flops_so_far": 4214980900745596.0,
      "budget_used_percent": 4.214980900745596
    },
    {
      "type": "training",
      "description": "Training step 709",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:42",
      "total_flops_so_far": 4220921958797692.0,
      "budget_used_percent": 4.2209219587976925
    },
    {
      "type": "training",
      "description": "Training step 710",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:43",
      "total_flops_so_far": 4226863016849788.0,
      "budget_used_percent": 4.226863016849788
    },
    {
      "type": "training",
      "description": "Training step 711",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:43",
      "total_flops_so_far": 4232804074901884.0,
      "budget_used_percent": 4.232804074901884
    },
    {
      "type": "training",
      "description": "Training step 712",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:43",
      "total_flops_so_far": 4238745132953980.0,
      "budget_used_percent": 4.23874513295398
    },
    {
      "type": "training",
      "description": "Training step 713",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:44",
      "total_flops_so_far": 4244686191006076.0,
      "budget_used_percent": 4.244686191006076
    },
    {
      "type": "training",
      "description": "Training step 714",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:44",
      "total_flops_so_far": 4250627249058172.0,
      "budget_used_percent": 4.250627249058172
    },
    {
      "type": "training",
      "description": "Training step 715",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:44",
      "total_flops_so_far": 4256568307110268.0,
      "budget_used_percent": 4.256568307110268
    },
    {
      "type": "training",
      "description": "Training step 716",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:45",
      "total_flops_so_far": 4262509365162364.0,
      "budget_used_percent": 4.262509365162364
    },
    {
      "type": "training",
      "description": "Training step 717",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:45",
      "total_flops_so_far": 4268450423214460.0,
      "budget_used_percent": 4.26845042321446
    },
    {
      "type": "training",
      "description": "Training step 718",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:45",
      "total_flops_so_far": 4274391481266556.0,
      "budget_used_percent": 4.274391481266556
    },
    {
      "type": "training",
      "description": "Training step 719",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:46",
      "total_flops_so_far": 4280332539318652.0,
      "budget_used_percent": 4.280332539318652
    },
    {
      "type": "training",
      "description": "Training step 720",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:46",
      "total_flops_so_far": 4286273597370748.0,
      "budget_used_percent": 4.286273597370748
    },
    {
      "type": "training",
      "description": "Training step 721",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:46",
      "total_flops_so_far": 4292214655422844.0,
      "budget_used_percent": 4.292214655422844
    },
    {
      "type": "training",
      "description": "Training step 722",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:47",
      "total_flops_so_far": 4298155713474940.0,
      "budget_used_percent": 4.29815571347494
    },
    {
      "type": "training",
      "description": "Training step 723",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:47",
      "total_flops_so_far": 4304096771527036.0,
      "budget_used_percent": 4.304096771527036
    },
    {
      "type": "training",
      "description": "Training step 724",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:47",
      "total_flops_so_far": 4310037829579132.0,
      "budget_used_percent": 4.310037829579132
    },
    {
      "type": "training",
      "description": "Training step 725",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:48",
      "total_flops_so_far": 4315978887631228.0,
      "budget_used_percent": 4.3159788876312275
    },
    {
      "type": "training",
      "description": "Training step 726",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:48",
      "total_flops_so_far": 4321919945683324.0,
      "budget_used_percent": 4.321919945683324
    },
    {
      "type": "training",
      "description": "Training step 727",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:48",
      "total_flops_so_far": 4327861003735420.0,
      "budget_used_percent": 4.3278610037354195
    },
    {
      "type": "training",
      "description": "Training step 728",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:48",
      "total_flops_so_far": 4333802061787516.0,
      "budget_used_percent": 4.333802061787516
    },
    {
      "type": "training",
      "description": "Training step 729",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:49",
      "total_flops_so_far": 4339743119839612.0,
      "budget_used_percent": 4.339743119839612
    },
    {
      "type": "training",
      "description": "Training step 730",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:49",
      "total_flops_so_far": 4345684177891708.0,
      "budget_used_percent": 4.345684177891708
    },
    {
      "type": "training",
      "description": "Training step 731",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:49",
      "total_flops_so_far": 4351625235943804.0,
      "budget_used_percent": 4.351625235943804
    },
    {
      "type": "training",
      "description": "Training step 732",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:50",
      "total_flops_so_far": 4357566293995900.0,
      "budget_used_percent": 4.3575662939959
    },
    {
      "type": "training",
      "description": "Training step 733",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:50",
      "total_flops_so_far": 4363507352047996.0,
      "budget_used_percent": 4.363507352047996
    },
    {
      "type": "training",
      "description": "Training step 734",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:50",
      "total_flops_so_far": 4369448410100092.0,
      "budget_used_percent": 4.369448410100092
    },
    {
      "type": "training",
      "description": "Training step 735",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:51",
      "total_flops_so_far": 4375389468152188.0,
      "budget_used_percent": 4.375389468152188
    },
    {
      "type": "training",
      "description": "Training step 736",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:51",
      "total_flops_so_far": 4381330526204284.0,
      "budget_used_percent": 4.381330526204284
    },
    {
      "type": "training",
      "description": "Training step 737",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:51",
      "total_flops_so_far": 4387271584256380.0,
      "budget_used_percent": 4.38727158425638
    },
    {
      "type": "training",
      "description": "Training step 738",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:52",
      "total_flops_so_far": 4393212642308476.0,
      "budget_used_percent": 4.393212642308476
    },
    {
      "type": "training",
      "description": "Training step 739",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:52",
      "total_flops_so_far": 4399153700360572.0,
      "budget_used_percent": 4.399153700360571
    },
    {
      "type": "training",
      "description": "Training step 740",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:52",
      "total_flops_so_far": 4405094758412668.0,
      "budget_used_percent": 4.405094758412668
    },
    {
      "type": "training",
      "description": "Training step 741",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:53",
      "total_flops_so_far": 4411035816464764.0,
      "budget_used_percent": 4.411035816464763
    },
    {
      "type": "training",
      "description": "Training step 742",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:53",
      "total_flops_so_far": 4416976874516860.0,
      "budget_used_percent": 4.41697687451686
    },
    {
      "type": "training",
      "description": "Training step 743",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:53",
      "total_flops_so_far": 4422917932568956.0,
      "budget_used_percent": 4.422917932568956
    },
    {
      "type": "training",
      "description": "Training step 744",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:54",
      "total_flops_so_far": 4428858990621052.0,
      "budget_used_percent": 4.428858990621052
    },
    {
      "type": "training",
      "description": "Training step 745",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:54",
      "total_flops_so_far": 4434800048673148.0,
      "budget_used_percent": 4.434800048673148
    },
    {
      "type": "training",
      "description": "Training step 746",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:54",
      "total_flops_so_far": 4440741106725244.0,
      "budget_used_percent": 4.440741106725244
    },
    {
      "type": "training",
      "description": "Training step 747",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:55",
      "total_flops_so_far": 4446682164777340.0,
      "budget_used_percent": 4.44668216477734
    },
    {
      "type": "training",
      "description": "Training step 748",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:55",
      "total_flops_so_far": 4452623222829436.0,
      "budget_used_percent": 4.452623222829436
    },
    {
      "type": "training",
      "description": "Training step 749",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:55",
      "total_flops_so_far": 4458564280881532.0,
      "budget_used_percent": 4.458564280881532
    },
    {
      "type": "training",
      "description": "Training step 750",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:56",
      "total_flops_so_far": 4464505338933628.0,
      "budget_used_percent": 4.464505338933629
    },
    {
      "type": "training",
      "description": "Training step 751",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:56",
      "total_flops_so_far": 4470446396985724.0,
      "budget_used_percent": 4.470446396985724
    },
    {
      "type": "training",
      "description": "Training step 752",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:56",
      "total_flops_so_far": 4476387455037820.0,
      "budget_used_percent": 4.476387455037821
    },
    {
      "type": "training",
      "description": "Training step 753",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:56",
      "total_flops_so_far": 4482328513089916.0,
      "budget_used_percent": 4.482328513089915
    },
    {
      "type": "training",
      "description": "Training step 754",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:57",
      "total_flops_so_far": 4488269571142012.0,
      "budget_used_percent": 4.488269571142012
    },
    {
      "type": "training",
      "description": "Training step 755",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:57",
      "total_flops_so_far": 4494210629194108.0,
      "budget_used_percent": 4.494210629194108
    },
    {
      "type": "training",
      "description": "Training step 756",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:57",
      "total_flops_so_far": 4500151687246204.0,
      "budget_used_percent": 4.500151687246204
    },
    {
      "type": "training",
      "description": "Training step 757",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:58",
      "total_flops_so_far": 4506092745298300.0,
      "budget_used_percent": 4.5060927452983
    },
    {
      "type": "training",
      "description": "Training step 758",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:58",
      "total_flops_so_far": 4512033803350396.0,
      "budget_used_percent": 4.512033803350396
    },
    {
      "type": "training",
      "description": "Training step 759",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:58",
      "total_flops_so_far": 4517974861402492.0,
      "budget_used_percent": 4.517974861402492
    },
    {
      "type": "training",
      "description": "Training step 760",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:59",
      "total_flops_so_far": 4523915919454588.0,
      "budget_used_percent": 4.523915919454588
    },
    {
      "type": "training",
      "description": "Training step 761",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:59",
      "total_flops_so_far": 4529856977506684.0,
      "budget_used_percent": 4.529856977506684
    },
    {
      "type": "training",
      "description": "Training step 762",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:42:59",
      "total_flops_so_far": 4535798035558780.0,
      "budget_used_percent": 4.535798035558781
    },
    {
      "type": "training",
      "description": "Training step 763",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:00",
      "total_flops_so_far": 4541739093610876.0,
      "budget_used_percent": 4.541739093610876
    },
    {
      "type": "training",
      "description": "Training step 764",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:00",
      "total_flops_so_far": 4547680151662972.0,
      "budget_used_percent": 4.547680151662973
    },
    {
      "type": "training",
      "description": "Training step 765",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:00",
      "total_flops_so_far": 4553621209715068.0,
      "budget_used_percent": 4.553621209715068
    },
    {
      "type": "training",
      "description": "Training step 766",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:01",
      "total_flops_so_far": 4559562267767164.0,
      "budget_used_percent": 4.559562267767165
    },
    {
      "type": "training",
      "description": "Training step 767",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:01",
      "total_flops_so_far": 4565503325819260.0,
      "budget_used_percent": 4.56550332581926
    },
    {
      "type": "training",
      "description": "Training step 768",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:01",
      "total_flops_so_far": 4571444383871356.0,
      "budget_used_percent": 4.571444383871356
    },
    {
      "type": "training",
      "description": "Training step 769",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:02",
      "total_flops_so_far": 4577385441923452.0,
      "budget_used_percent": 4.577385441923452
    },
    {
      "type": "training",
      "description": "Training step 770",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:02",
      "total_flops_so_far": 4583326499975548.0,
      "budget_used_percent": 4.583326499975548
    },
    {
      "type": "training",
      "description": "Training step 771",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:02",
      "total_flops_so_far": 4589267558027644.0,
      "budget_used_percent": 4.589267558027644
    },
    {
      "type": "training",
      "description": "Training step 772",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:03",
      "total_flops_so_far": 4595208616079740.0,
      "budget_used_percent": 4.59520861607974
    },
    {
      "type": "training",
      "description": "Training step 773",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:03",
      "total_flops_so_far": 4601149674131836.0,
      "budget_used_percent": 4.601149674131836
    },
    {
      "type": "training",
      "description": "Training step 774",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:03",
      "total_flops_so_far": 4607090732183932.0,
      "budget_used_percent": 4.607090732183932
    },
    {
      "type": "training",
      "description": "Training step 775",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:04",
      "total_flops_so_far": 4613031790236028.0,
      "budget_used_percent": 4.613031790236028
    },
    {
      "type": "training",
      "description": "Training step 776",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:04",
      "total_flops_so_far": 4618972848288124.0,
      "budget_used_percent": 4.6189728482881245
    },
    {
      "type": "training",
      "description": "Training step 777",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:04",
      "total_flops_so_far": 4624913906340220.0,
      "budget_used_percent": 4.62491390634022
    },
    {
      "type": "training",
      "description": "Training step 778",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:05",
      "total_flops_so_far": 4630854964392316.0,
      "budget_used_percent": 4.6308549643923165
    },
    {
      "type": "training",
      "description": "Training step 779",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:05",
      "total_flops_so_far": 4636796022444412.0,
      "budget_used_percent": 4.636796022444412
    },
    {
      "type": "training",
      "description": "Training step 780",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:05",
      "total_flops_so_far": 4642737080496508.0,
      "budget_used_percent": 4.6427370804965085
    },
    {
      "type": "training",
      "description": "Training step 781",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:05",
      "total_flops_so_far": 4648678138548604.0,
      "budget_used_percent": 4.648678138548604
    },
    {
      "type": "training",
      "description": "Training step 782",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:06",
      "total_flops_so_far": 4654619196600700.0,
      "budget_used_percent": 4.6546191966007
    },
    {
      "type": "training",
      "description": "Training step 783",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:06",
      "total_flops_so_far": 4660560254652796.0,
      "budget_used_percent": 4.660560254652796
    },
    {
      "type": "training",
      "description": "Training step 784",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:06",
      "total_flops_so_far": 4666501312704892.0,
      "budget_used_percent": 4.6665013127048915
    },
    {
      "type": "training",
      "description": "Training step 785",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:07",
      "total_flops_so_far": 4672442370756988.0,
      "budget_used_percent": 4.672442370756988
    },
    {
      "type": "training",
      "description": "Training step 786",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:07",
      "total_flops_so_far": 4678383428809084.0,
      "budget_used_percent": 4.6783834288090835
    },
    {
      "type": "training",
      "description": "Training step 787",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:07",
      "total_flops_so_far": 4684324486861180.0,
      "budget_used_percent": 4.68432448686118
    },
    {
      "type": "training",
      "description": "Training step 788",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:08",
      "total_flops_so_far": 4690265544913276.0,
      "budget_used_percent": 4.690265544913276
    },
    {
      "type": "training",
      "description": "Training step 789",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:08",
      "total_flops_so_far": 4696206602965372.0,
      "budget_used_percent": 4.696206602965372
    },
    {
      "type": "training",
      "description": "Training step 790",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:08",
      "total_flops_so_far": 4702147661017468.0,
      "budget_used_percent": 4.702147661017468
    },
    {
      "type": "training",
      "description": "Training step 791",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:09",
      "total_flops_so_far": 4708088719069564.0,
      "budget_used_percent": 4.708088719069564
    },
    {
      "type": "training",
      "description": "Training step 792",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:09",
      "total_flops_so_far": 4714029777121660.0,
      "budget_used_percent": 4.71402977712166
    },
    {
      "type": "training",
      "description": "Training step 793",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:09",
      "total_flops_so_far": 4719970835173756.0,
      "budget_used_percent": 4.719970835173756
    },
    {
      "type": "training",
      "description": "Training step 794",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:10",
      "total_flops_so_far": 4725911893225852.0,
      "budget_used_percent": 4.725911893225852
    },
    {
      "type": "training",
      "description": "Training step 795",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:10",
      "total_flops_so_far": 4731852951277948.0,
      "budget_used_percent": 4.731852951277948
    },
    {
      "type": "training",
      "description": "Training step 796",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:10",
      "total_flops_so_far": 4737794009330044.0,
      "budget_used_percent": 4.7377940093300435
    },
    {
      "type": "training",
      "description": "Training step 797",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:11",
      "total_flops_so_far": 4743735067382140.0,
      "budget_used_percent": 4.74373506738214
    },
    {
      "type": "training",
      "description": "Training step 798",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:11",
      "total_flops_so_far": 4749676125434236.0,
      "budget_used_percent": 4.7496761254342355
    },
    {
      "type": "training",
      "description": "Training step 799",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:11",
      "total_flops_so_far": 4755617183486332.0,
      "budget_used_percent": 4.755617183486332
    },
    {
      "type": "training",
      "description": "Training step 800",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:12",
      "total_flops_so_far": 4761558241538428.0,
      "budget_used_percent": 4.761558241538428
    },
    {
      "type": "training",
      "description": "Training step 801",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:12",
      "total_flops_so_far": 4767499299590524.0,
      "budget_used_percent": 4.767499299590524
    },
    {
      "type": "training",
      "description": "Training step 802",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:12",
      "total_flops_so_far": 4773440357642620.0,
      "budget_used_percent": 4.77344035764262
    },
    {
      "type": "training",
      "description": "Training step 803",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:13",
      "total_flops_so_far": 4779381415694716.0,
      "budget_used_percent": 4.779381415694716
    },
    {
      "type": "training",
      "description": "Training step 804",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:13",
      "total_flops_so_far": 4785322473746812.0,
      "budget_used_percent": 4.785322473746812
    },
    {
      "type": "training",
      "description": "Training step 805",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:13",
      "total_flops_so_far": 4791263531798908.0,
      "budget_used_percent": 4.791263531798908
    },
    {
      "type": "training",
      "description": "Training step 806",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:14",
      "total_flops_so_far": 4797204589851004.0,
      "budget_used_percent": 4.797204589851004
    },
    {
      "type": "training",
      "description": "Training step 807",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:14",
      "total_flops_so_far": 4803145647903100.0,
      "budget_used_percent": 4.8031456479031
    },
    {
      "type": "training",
      "description": "Training step 808",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:14",
      "total_flops_so_far": 4809086705955196.0,
      "budget_used_percent": 4.809086705955196
    },
    {
      "type": "training",
      "description": "Training step 809",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:14",
      "total_flops_so_far": 4815027764007292.0,
      "budget_used_percent": 4.815027764007292
    },
    {
      "type": "training",
      "description": "Training step 810",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:15",
      "total_flops_so_far": 4820968822059388.0,
      "budget_used_percent": 4.820968822059387
    },
    {
      "type": "training",
      "description": "Training step 811",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:15",
      "total_flops_so_far": 4826909880111484.0,
      "budget_used_percent": 4.826909880111484
    },
    {
      "type": "training",
      "description": "Training step 812",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:15",
      "total_flops_so_far": 4832850938163580.0,
      "budget_used_percent": 4.832850938163579
    },
    {
      "type": "training",
      "description": "Training step 813",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:16",
      "total_flops_so_far": 4838791996215676.0,
      "budget_used_percent": 4.838791996215676
    },
    {
      "type": "training",
      "description": "Training step 814",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:16",
      "total_flops_so_far": 4844733054267772.0,
      "budget_used_percent": 4.844733054267772
    },
    {
      "type": "training",
      "description": "Training step 815",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:16",
      "total_flops_so_far": 4850674112319868.0,
      "budget_used_percent": 4.850674112319868
    },
    {
      "type": "training",
      "description": "Training step 816",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:17",
      "total_flops_so_far": 4856615170371964.0,
      "budget_used_percent": 4.856615170371964
    },
    {
      "type": "training",
      "description": "Training step 817",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:17",
      "total_flops_so_far": 4862556228424060.0,
      "budget_used_percent": 4.86255622842406
    },
    {
      "type": "training",
      "description": "Training step 818",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:17",
      "total_flops_so_far": 4868497286476156.0,
      "budget_used_percent": 4.868497286476156
    },
    {
      "type": "training",
      "description": "Training step 819",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:18",
      "total_flops_so_far": 4874438344528252.0,
      "budget_used_percent": 4.874438344528252
    },
    {
      "type": "training",
      "description": "Training step 820",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:18",
      "total_flops_so_far": 4880379402580348.0,
      "budget_used_percent": 4.880379402580348
    },
    {
      "type": "training",
      "description": "Training step 821",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:18",
      "total_flops_so_far": 4886320460632444.0,
      "budget_used_percent": 4.886320460632445
    },
    {
      "type": "training",
      "description": "Training step 822",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:19",
      "total_flops_so_far": 4892261518684540.0,
      "budget_used_percent": 4.89226151868454
    },
    {
      "type": "training",
      "description": "Training step 823",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:19",
      "total_flops_so_far": 4898202576736636.0,
      "budget_used_percent": 4.898202576736637
    },
    {
      "type": "training",
      "description": "Training step 824",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:19",
      "total_flops_so_far": 4904143634788732.0,
      "budget_used_percent": 4.904143634788731
    },
    {
      "type": "training",
      "description": "Training step 825",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:20",
      "total_flops_so_far": 4910084692840828.0,
      "budget_used_percent": 4.910084692840828
    },
    {
      "type": "training",
      "description": "Training step 826",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:20",
      "total_flops_so_far": 4916025750892924.0,
      "budget_used_percent": 4.916025750892924
    },
    {
      "type": "training",
      "description": "Training step 827",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:20",
      "total_flops_so_far": 4921966808945020.0,
      "budget_used_percent": 4.92196680894502
    },
    {
      "type": "training",
      "description": "Training step 828",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:21",
      "total_flops_so_far": 4927907866997116.0,
      "budget_used_percent": 4.927907866997116
    },
    {
      "type": "training",
      "description": "Training step 829",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:21",
      "total_flops_so_far": 4933848925049212.0,
      "budget_used_percent": 4.933848925049212
    },
    {
      "type": "training",
      "description": "Training step 830",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:21",
      "total_flops_so_far": 4939789983101308.0,
      "budget_used_percent": 4.939789983101308
    },
    {
      "type": "training",
      "description": "Training step 831",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:22",
      "total_flops_so_far": 4945731041153404.0,
      "budget_used_percent": 4.945731041153404
    },
    {
      "type": "training",
      "description": "Training step 832",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:22",
      "total_flops_so_far": 4951672099205500.0,
      "budget_used_percent": 4.9516720992055
    },
    {
      "type": "training",
      "description": "Training step 833",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:22",
      "total_flops_so_far": 4957613157257596.0,
      "budget_used_percent": 4.9576131572575965
    },
    {
      "type": "training",
      "description": "Training step 834",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:23",
      "total_flops_so_far": 4963554215309692.0,
      "budget_used_percent": 4.963554215309692
    },
    {
      "type": "training",
      "description": "Training step 835",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:23",
      "total_flops_so_far": 4969495273361788.0,
      "budget_used_percent": 4.9694952733617885
    },
    {
      "type": "training",
      "description": "Training step 836",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:23",
      "total_flops_so_far": 4975436331413884.0,
      "budget_used_percent": 4.975436331413884
    },
    {
      "type": "training",
      "description": "Training step 837",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:24",
      "total_flops_so_far": 4981377389465980.0,
      "budget_used_percent": 4.9813773894659805
    },
    {
      "type": "training",
      "description": "Training step 838",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:24",
      "total_flops_so_far": 4987318447518076.0,
      "budget_used_percent": 4.987318447518076
    },
    {
      "type": "training",
      "description": "Training step 839",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:24",
      "total_flops_so_far": 4993259505570172.0,
      "budget_used_percent": 4.993259505570172
    },
    {
      "type": "training",
      "description": "Training step 840",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:24",
      "total_flops_so_far": 4999200563622268.0,
      "budget_used_percent": 4.999200563622268
    },
    {
      "type": "training",
      "description": "Training step 841",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:25",
      "total_flops_so_far": 5005141621674364.0,
      "budget_used_percent": 5.005141621674364
    },
    {
      "type": "training",
      "description": "Training step 842",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:25",
      "total_flops_so_far": 5011082679726460.0,
      "budget_used_percent": 5.01108267972646
    },
    {
      "type": "training",
      "description": "Training step 843",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:25",
      "total_flops_so_far": 5017023737778556.0,
      "budget_used_percent": 5.017023737778556
    },
    {
      "type": "training",
      "description": "Training step 844",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:26",
      "total_flops_so_far": 5022964795830652.0,
      "budget_used_percent": 5.022964795830652
    },
    {
      "type": "training",
      "description": "Training step 845",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:26",
      "total_flops_so_far": 5028905853882748.0,
      "budget_used_percent": 5.028905853882748
    },
    {
      "type": "training",
      "description": "Training step 846",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:26",
      "total_flops_so_far": 5034846911934844.0,
      "budget_used_percent": 5.034846911934844
    },
    {
      "type": "training",
      "description": "Training step 847",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:27",
      "total_flops_so_far": 5040787969986940.0,
      "budget_used_percent": 5.0407879699869405
    },
    {
      "type": "training",
      "description": "Training step 848",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:27",
      "total_flops_so_far": 5046729028039036.0,
      "budget_used_percent": 5.046729028039036
    },
    {
      "type": "training",
      "description": "Training step 849",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:27",
      "total_flops_so_far": 5052670086091132.0,
      "budget_used_percent": 5.052670086091132
    },
    {
      "type": "training",
      "description": "Training step 850",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:28",
      "total_flops_so_far": 5058611144143228.0,
      "budget_used_percent": 5.058611144143228
    },
    {
      "type": "training",
      "description": "Training step 851",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:28",
      "total_flops_so_far": 5064552202195324.0,
      "budget_used_percent": 5.064552202195324
    },
    {
      "type": "training",
      "description": "Training step 852",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:28",
      "total_flops_so_far": 5070493260247420.0,
      "budget_used_percent": 5.07049326024742
    },
    {
      "type": "training",
      "description": "Training step 853",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:29",
      "total_flops_so_far": 5076434318299516.0,
      "budget_used_percent": 5.0764343182995155
    },
    {
      "type": "training",
      "description": "Training step 854",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:29",
      "total_flops_so_far": 5082375376351612.0,
      "budget_used_percent": 5.082375376351612
    },
    {
      "type": "training",
      "description": "Training step 855",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:29",
      "total_flops_so_far": 5088316434403708.0,
      "budget_used_percent": 5.0883164344037075
    },
    {
      "type": "training",
      "description": "Training step 856",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:30",
      "total_flops_so_far": 5094257492455804.0,
      "budget_used_percent": 5.094257492455804
    },
    {
      "type": "training",
      "description": "Training step 857",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:30",
      "total_flops_so_far": 5100198550507900.0,
      "budget_used_percent": 5.1001985505078995
    },
    {
      "type": "training",
      "description": "Training step 858",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:30",
      "total_flops_so_far": 5106139608559996.0,
      "budget_used_percent": 5.106139608559996
    },
    {
      "type": "training",
      "description": "Training step 859",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:31",
      "total_flops_so_far": 5112080666612092.0,
      "budget_used_percent": 5.112080666612092
    },
    {
      "type": "training",
      "description": "Training step 860",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:31",
      "total_flops_so_far": 5118021724664188.0,
      "budget_used_percent": 5.118021724664188
    },
    {
      "type": "training",
      "description": "Training step 861",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:31",
      "total_flops_so_far": 5123962782716284.0,
      "budget_used_percent": 5.123962782716284
    },
    {
      "type": "training",
      "description": "Training step 862",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:32",
      "total_flops_so_far": 5129903840768380.0,
      "budget_used_percent": 5.12990384076838
    },
    {
      "type": "training",
      "description": "Training step 863",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:32",
      "total_flops_so_far": 5135844898820476.0,
      "budget_used_percent": 5.135844898820476
    },
    {
      "type": "training",
      "description": "Training step 864",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:32",
      "total_flops_so_far": 5141785956872572.0,
      "budget_used_percent": 5.141785956872572
    },
    {
      "type": "training",
      "description": "Training step 865",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:33",
      "total_flops_so_far": 5147727014924668.0,
      "budget_used_percent": 5.147727014924668
    },
    {
      "type": "training",
      "description": "Training step 866",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:33",
      "total_flops_so_far": 5153668072976764.0,
      "budget_used_percent": 5.153668072976764
    },
    {
      "type": "training",
      "description": "Training step 867",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:33",
      "total_flops_so_far": 5159609131028860.0,
      "budget_used_percent": 5.159609131028859
    },
    {
      "type": "training",
      "description": "Training step 868",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:34",
      "total_flops_so_far": 5165550189080956.0,
      "budget_used_percent": 5.165550189080956
    },
    {
      "type": "training",
      "description": "Training step 869",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:34",
      "total_flops_so_far": 5171491247133052.0,
      "budget_used_percent": 5.171491247133051
    },
    {
      "type": "training",
      "description": "Training step 870",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:34",
      "total_flops_so_far": 5177432305185148.0,
      "budget_used_percent": 5.177432305185148
    },
    {
      "type": "training",
      "description": "Training step 871",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:35",
      "total_flops_so_far": 5183373363237244.0,
      "budget_used_percent": 5.183373363237244
    },
    {
      "type": "training",
      "description": "Training step 872",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:35",
      "total_flops_so_far": 5189314421289340.0,
      "budget_used_percent": 5.18931442128934
    },
    {
      "type": "training",
      "description": "Training step 873",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:35",
      "total_flops_so_far": 5195255479341436.0,
      "budget_used_percent": 5.195255479341436
    },
    {
      "type": "training",
      "description": "Training step 874",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:35",
      "total_flops_so_far": 5201196537393532.0,
      "budget_used_percent": 5.201196537393532
    },
    {
      "type": "training",
      "description": "Training step 875",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:36",
      "total_flops_so_far": 5207137595445628.0,
      "budget_used_percent": 5.207137595445628
    },
    {
      "type": "training",
      "description": "Training step 876",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:36",
      "total_flops_so_far": 5213078653497724.0,
      "budget_used_percent": 5.213078653497724
    },
    {
      "type": "training",
      "description": "Training step 877",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:36",
      "total_flops_so_far": 5219019711549820.0,
      "budget_used_percent": 5.21901971154982
    },
    {
      "type": "training",
      "description": "Training step 878",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:37",
      "total_flops_so_far": 5224960769601916.0,
      "budget_used_percent": 5.224960769601916
    },
    {
      "type": "training",
      "description": "Training step 879",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:37",
      "total_flops_so_far": 5230901827654012.0,
      "budget_used_percent": 5.230901827654012
    },
    {
      "type": "training",
      "description": "Training step 880",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:37",
      "total_flops_so_far": 5236842885706108.0,
      "budget_used_percent": 5.236842885706109
    },
    {
      "type": "training",
      "description": "Training step 881",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:38",
      "total_flops_so_far": 5242783943758204.0,
      "budget_used_percent": 5.242783943758203
    },
    {
      "type": "training",
      "description": "Training step 882",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:38",
      "total_flops_so_far": 5248725001810300.0,
      "budget_used_percent": 5.2487250018103
    },
    {
      "type": "training",
      "description": "Training step 883",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:38",
      "total_flops_so_far": 5254666059862396.0,
      "budget_used_percent": 5.254666059862396
    },
    {
      "type": "training",
      "description": "Training step 884",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:39",
      "total_flops_so_far": 5260607117914492.0,
      "budget_used_percent": 5.260607117914492
    },
    {
      "type": "training",
      "description": "Training step 885",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:39",
      "total_flops_so_far": 5266548175966588.0,
      "budget_used_percent": 5.266548175966588
    },
    {
      "type": "training",
      "description": "Training step 886",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:39",
      "total_flops_so_far": 5272489234018684.0,
      "budget_used_percent": 5.272489234018684
    },
    {
      "type": "training",
      "description": "Training step 887",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:40",
      "total_flops_so_far": 5278430292070780.0,
      "budget_used_percent": 5.27843029207078
    },
    {
      "type": "training",
      "description": "Training step 888",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:40",
      "total_flops_so_far": 5284371350122876.0,
      "budget_used_percent": 5.284371350122876
    },
    {
      "type": "training",
      "description": "Training step 889",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:40",
      "total_flops_so_far": 5290312408174972.0,
      "budget_used_percent": 5.290312408174972
    },
    {
      "type": "training",
      "description": "Training step 890",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:41",
      "total_flops_so_far": 5296253466227068.0,
      "budget_used_percent": 5.296253466227068
    },
    {
      "type": "training",
      "description": "Training step 891",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:41",
      "total_flops_so_far": 5302194524279164.0,
      "budget_used_percent": 5.302194524279164
    },
    {
      "type": "training",
      "description": "Training step 892",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:41",
      "total_flops_so_far": 5308135582331260.0,
      "budget_used_percent": 5.308135582331261
    },
    {
      "type": "training",
      "description": "Training step 893",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:42",
      "total_flops_so_far": 5314076640383356.0,
      "budget_used_percent": 5.314076640383356
    },
    {
      "type": "training",
      "description": "Training step 894",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:42",
      "total_flops_so_far": 5320017698435452.0,
      "budget_used_percent": 5.320017698435453
    },
    {
      "type": "training",
      "description": "Training step 895",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:42",
      "total_flops_so_far": 5325958756487548.0,
      "budget_used_percent": 5.325958756487548
    },
    {
      "type": "training",
      "description": "Training step 896",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:43",
      "total_flops_so_far": 5331899814539644.0,
      "budget_used_percent": 5.331899814539644
    },
    {
      "type": "training",
      "description": "Training step 897",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:43",
      "total_flops_so_far": 5337840872591740.0,
      "budget_used_percent": 5.33784087259174
    },
    {
      "type": "training",
      "description": "Training step 898",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:43",
      "total_flops_so_far": 5343781930643836.0,
      "budget_used_percent": 5.343781930643836
    },
    {
      "type": "training",
      "description": "Training step 899",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:44",
      "total_flops_so_far": 5349722988695932.0,
      "budget_used_percent": 5.349722988695932
    },
    {
      "type": "training",
      "description": "Training step 900",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:44",
      "total_flops_so_far": 5355664046748028.0,
      "budget_used_percent": 5.355664046748028
    },
    {
      "type": "training",
      "description": "Training step 901",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:44",
      "total_flops_so_far": 5361605104800124.0,
      "budget_used_percent": 5.361605104800124
    },
    {
      "type": "training",
      "description": "Training step 902",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:45",
      "total_flops_so_far": 5367546162852220.0,
      "budget_used_percent": 5.36754616285222
    },
    {
      "type": "training",
      "description": "Training step 903",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:45",
      "total_flops_so_far": 5373487220904316.0,
      "budget_used_percent": 5.373487220904316
    },
    {
      "type": "training",
      "description": "Training step 904",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:45",
      "total_flops_so_far": 5379428278956412.0,
      "budget_used_percent": 5.3794282789564125
    },
    {
      "type": "training",
      "description": "Training step 905",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:46",
      "total_flops_so_far": 5385369337008508.0,
      "budget_used_percent": 5.385369337008508
    },
    {
      "type": "training",
      "description": "Training step 906",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:46",
      "total_flops_so_far": 5391310395060604.0,
      "budget_used_percent": 5.3913103950606045
    },
    {
      "type": "training",
      "description": "Training step 907",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:46",
      "total_flops_so_far": 5397251453112700.0,
      "budget_used_percent": 5.3972514531127
    },
    {
      "type": "training",
      "description": "Training step 908",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:46",
      "total_flops_so_far": 5403192511164796.0,
      "budget_used_percent": 5.4031925111647965
    },
    {
      "type": "training",
      "description": "Training step 909",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:47",
      "total_flops_so_far": 5409133569216892.0,
      "budget_used_percent": 5.409133569216892
    },
    {
      "type": "training",
      "description": "Training step 910",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:47",
      "total_flops_so_far": 5415074627268988.0,
      "budget_used_percent": 5.415074627268988
    },
    {
      "type": "training",
      "description": "Training step 911",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:47",
      "total_flops_so_far": 5421015685321084.0,
      "budget_used_percent": 5.421015685321084
    },
    {
      "type": "training",
      "description": "Training step 912",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:48",
      "total_flops_so_far": 5426956743373180.0,
      "budget_used_percent": 5.42695674337318
    },
    {
      "type": "training",
      "description": "Training step 913",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:48",
      "total_flops_so_far": 5432897801425276.0,
      "budget_used_percent": 5.432897801425276
    },
    {
      "type": "training",
      "description": "Training step 914",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:48",
      "total_flops_so_far": 5438838859477372.0,
      "budget_used_percent": 5.438838859477372
    },
    {
      "type": "training",
      "description": "Training step 915",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:49",
      "total_flops_so_far": 5444779917529468.0,
      "budget_used_percent": 5.444779917529468
    },
    {
      "type": "training",
      "description": "Training step 916",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:49",
      "total_flops_so_far": 5450720975581564.0,
      "budget_used_percent": 5.450720975581564
    },
    {
      "type": "training",
      "description": "Training step 917",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:49",
      "total_flops_so_far": 5456662033633660.0,
      "budget_used_percent": 5.45666203363366
    },
    {
      "type": "training",
      "description": "Training step 918",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:50",
      "total_flops_so_far": 5462603091685756.0,
      "budget_used_percent": 5.462603091685756
    },
    {
      "type": "training",
      "description": "Training step 919",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:50",
      "total_flops_so_far": 5468544149737852.0,
      "budget_used_percent": 5.468544149737852
    },
    {
      "type": "training",
      "description": "Training step 920",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:50",
      "total_flops_so_far": 5474485207789948.0,
      "budget_used_percent": 5.474485207789948
    },
    {
      "type": "training",
      "description": "Training step 921",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:51",
      "total_flops_so_far": 5480426265842044.0,
      "budget_used_percent": 5.480426265842044
    },
    {
      "type": "training",
      "description": "Training step 922",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:51",
      "total_flops_so_far": 5486367323894140.0,
      "budget_used_percent": 5.48636732389414
    },
    {
      "type": "training",
      "description": "Training step 923",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:51",
      "total_flops_so_far": 5492308381946236.0,
      "budget_used_percent": 5.492308381946236
    },
    {
      "type": "training",
      "description": "Training step 924",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:52",
      "total_flops_so_far": 5498249439998332.0,
      "budget_used_percent": 5.4982494399983315
    },
    {
      "type": "training",
      "description": "Training step 925",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:52",
      "total_flops_so_far": 5504190498050428.0,
      "budget_used_percent": 5.504190498050428
    },
    {
      "type": "training",
      "description": "Training step 926",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:52",
      "total_flops_so_far": 5510131556102524.0,
      "budget_used_percent": 5.5101315561025235
    },
    {
      "type": "training",
      "description": "Training step 927",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:53",
      "total_flops_so_far": 5516072614154620.0,
      "budget_used_percent": 5.51607261415462
    },
    {
      "type": "training",
      "description": "Training step 928",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:53",
      "total_flops_so_far": 5522013672206716.0,
      "budget_used_percent": 5.522013672206716
    },
    {
      "type": "training",
      "description": "Training step 929",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:53",
      "total_flops_so_far": 5527954730258812.0,
      "budget_used_percent": 5.527954730258812
    },
    {
      "type": "training",
      "description": "Training step 930",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:54",
      "total_flops_so_far": 5533895788310908.0,
      "budget_used_percent": 5.533895788310908
    },
    {
      "type": "training",
      "description": "Training step 931",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:54",
      "total_flops_so_far": 5539836846363004.0,
      "budget_used_percent": 5.539836846363004
    },
    {
      "type": "training",
      "description": "Training step 932",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:54",
      "total_flops_so_far": 5545777904415100.0,
      "budget_used_percent": 5.5457779044151
    },
    {
      "type": "training",
      "description": "Training step 933",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:55",
      "total_flops_so_far": 5551718962467196.0,
      "budget_used_percent": 5.551718962467196
    },
    {
      "type": "training",
      "description": "Training step 934",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:55",
      "total_flops_so_far": 5557660020519292.0,
      "budget_used_percent": 5.557660020519292
    },
    {
      "type": "training",
      "description": "Training step 935",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:55",
      "total_flops_so_far": 5563601078571388.0,
      "budget_used_percent": 5.563601078571388
    },
    {
      "type": "training",
      "description": "Training step 936",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:56",
      "total_flops_so_far": 5569542136623484.0,
      "budget_used_percent": 5.569542136623484
    },
    {
      "type": "training",
      "description": "Training step 937",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:56",
      "total_flops_so_far": 5575483194675580.0,
      "budget_used_percent": 5.575483194675581
    },
    {
      "type": "training",
      "description": "Training step 938",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:56",
      "total_flops_so_far": 5581424252727676.0,
      "budget_used_percent": 5.581424252727675
    },
    {
      "type": "training",
      "description": "Training step 939",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:57",
      "total_flops_so_far": 5587365310779772.0,
      "budget_used_percent": 5.587365310779772
    },
    {
      "type": "training",
      "description": "Training step 940",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:57",
      "total_flops_so_far": 5593306368831868.0,
      "budget_used_percent": 5.593306368831867
    },
    {
      "type": "training",
      "description": "Training step 941",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:57",
      "total_flops_so_far": 5599247426883964.0,
      "budget_used_percent": 5.599247426883964
    },
    {
      "type": "training",
      "description": "Training step 942",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:58",
      "total_flops_so_far": 5605188484936060.0,
      "budget_used_percent": 5.60518848493606
    },
    {
      "type": "training",
      "description": "Training step 943",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:58",
      "total_flops_so_far": 5611129542988156.0,
      "budget_used_percent": 5.611129542988156
    },
    {
      "type": "training",
      "description": "Training step 944",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:58",
      "total_flops_so_far": 5617070601040252.0,
      "budget_used_percent": 5.617070601040252
    },
    {
      "type": "training",
      "description": "Training step 945",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:59",
      "total_flops_so_far": 5623011659092348.0,
      "budget_used_percent": 5.623011659092348
    },
    {
      "type": "training",
      "description": "Training step 946",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:59",
      "total_flops_so_far": 5628952717144444.0,
      "budget_used_percent": 5.628952717144444
    },
    {
      "type": "training",
      "description": "Training step 947",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:59",
      "total_flops_so_far": 5634893775196540.0,
      "budget_used_percent": 5.63489377519654
    },
    {
      "type": "training",
      "description": "Training step 948",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:43:59",
      "total_flops_so_far": 5640834833248636.0,
      "budget_used_percent": 5.640834833248636
    },
    {
      "type": "training",
      "description": "Training step 949",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:00",
      "total_flops_so_far": 5646775891300732.0,
      "budget_used_percent": 5.646775891300733
    },
    {
      "type": "training",
      "description": "Training step 950",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:00",
      "total_flops_so_far": 5652716949352828.0,
      "budget_used_percent": 5.652716949352828
    },
    {
      "type": "training",
      "description": "Training step 951",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:00",
      "total_flops_so_far": 5658658007404924.0,
      "budget_used_percent": 5.658658007404925
    },
    {
      "type": "training",
      "description": "Training step 952",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:01",
      "total_flops_so_far": 5664599065457020.0,
      "budget_used_percent": 5.664599065457019
    },
    {
      "type": "training",
      "description": "Training step 953",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:01",
      "total_flops_so_far": 5670540123509116.0,
      "budget_used_percent": 5.670540123509116
    },
    {
      "type": "training",
      "description": "Training step 954",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:01",
      "total_flops_so_far": 5676481181561212.0,
      "budget_used_percent": 5.676481181561212
    },
    {
      "type": "training",
      "description": "Training step 955",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:02",
      "total_flops_so_far": 5682422239613308.0,
      "budget_used_percent": 5.682422239613308
    },
    {
      "type": "training",
      "description": "Training step 956",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:02",
      "total_flops_so_far": 5688363297665404.0,
      "budget_used_percent": 5.688363297665404
    },
    {
      "type": "training",
      "description": "Training step 957",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:02",
      "total_flops_so_far": 5694304355717500.0,
      "budget_used_percent": 5.6943043557175
    },
    {
      "type": "training",
      "description": "Training step 958",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:03",
      "total_flops_so_far": 5700245413769596.0,
      "budget_used_percent": 5.700245413769596
    },
    {
      "type": "training",
      "description": "Training step 959",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:03",
      "total_flops_so_far": 5706186471821692.0,
      "budget_used_percent": 5.706186471821692
    },
    {
      "type": "training",
      "description": "Training step 960",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:03",
      "total_flops_so_far": 5712127529873788.0,
      "budget_used_percent": 5.712127529873788
    },
    {
      "type": "training",
      "description": "Training step 961",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:04",
      "total_flops_so_far": 5718068587925884.0,
      "budget_used_percent": 5.718068587925885
    },
    {
      "type": "training",
      "description": "Training step 962",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:04",
      "total_flops_so_far": 5724009645977980.0,
      "budget_used_percent": 5.72400964597798
    },
    {
      "type": "training",
      "description": "Training step 963",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:04",
      "total_flops_so_far": 5729950704030076.0,
      "budget_used_percent": 5.729950704030077
    },
    {
      "type": "training",
      "description": "Training step 964",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:05",
      "total_flops_so_far": 5735891762082172.0,
      "budget_used_percent": 5.735891762082172
    },
    {
      "type": "training",
      "description": "Training step 965",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:05",
      "total_flops_so_far": 5741832820134268.0,
      "budget_used_percent": 5.7418328201342685
    },
    {
      "type": "training",
      "description": "Training step 966",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:05",
      "total_flops_so_far": 5747773878186364.0,
      "budget_used_percent": 5.747773878186364
    },
    {
      "type": "training",
      "description": "Training step 967",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:06",
      "total_flops_so_far": 5753714936238460.0,
      "budget_used_percent": 5.75371493623846
    },
    {
      "type": "training",
      "description": "Training step 968",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:06",
      "total_flops_so_far": 5759655994290556.0,
      "budget_used_percent": 5.759655994290556
    },
    {
      "type": "training",
      "description": "Training step 969",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:06",
      "total_flops_so_far": 5765597052342652.0,
      "budget_used_percent": 5.765597052342652
    },
    {
      "type": "training",
      "description": "Training step 970",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:07",
      "total_flops_so_far": 5771538110394748.0,
      "budget_used_percent": 5.771538110394748
    },
    {
      "type": "training",
      "description": "Training step 971",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:07",
      "total_flops_so_far": 5777479168446844.0,
      "budget_used_percent": 5.777479168446844
    },
    {
      "type": "training",
      "description": "Training step 972",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:07",
      "total_flops_so_far": 5783420226498940.0,
      "budget_used_percent": 5.78342022649894
    },
    {
      "type": "training",
      "description": "Training step 973",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:08",
      "total_flops_so_far": 5789361284551036.0,
      "budget_used_percent": 5.789361284551036
    },
    {
      "type": "training",
      "description": "Training step 974",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:08",
      "total_flops_so_far": 5795302342603132.0,
      "budget_used_percent": 5.795302342603132
    },
    {
      "type": "training",
      "description": "Training step 975",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:08",
      "total_flops_so_far": 5801243400655228.0,
      "budget_used_percent": 5.8012434006552285
    },
    {
      "type": "training",
      "description": "Training step 976",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:09",
      "total_flops_so_far": 5807184458707324.0,
      "budget_used_percent": 5.807184458707324
    },
    {
      "type": "training",
      "description": "Training step 977",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:09",
      "total_flops_so_far": 5813125516759420.0,
      "budget_used_percent": 5.8131255167594205
    },
    {
      "type": "training",
      "description": "Training step 978",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:09",
      "total_flops_so_far": 5819066574811516.0,
      "budget_used_percent": 5.819066574811516
    },
    {
      "type": "training",
      "description": "Training step 979",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:10",
      "total_flops_so_far": 5825007632863612.0,
      "budget_used_percent": 5.8250076328636125
    },
    {
      "type": "training",
      "description": "Training step 980",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:10",
      "total_flops_so_far": 5830948690915708.0,
      "budget_used_percent": 5.830948690915708
    },
    {
      "type": "training",
      "description": "Training step 981",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:10",
      "total_flops_so_far": 5836889748967804.0,
      "budget_used_percent": 5.8368897489678035
    },
    {
      "type": "training",
      "description": "Training step 982",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:11",
      "total_flops_so_far": 5842830807019900.0,
      "budget_used_percent": 5.8428308070199
    },
    {
      "type": "training",
      "description": "Training step 983",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:11",
      "total_flops_so_far": 5848771865071996.0,
      "budget_used_percent": 5.8487718650719955
    },
    {
      "type": "training",
      "description": "Training step 984",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:11",
      "total_flops_so_far": 5854712923124092.0,
      "budget_used_percent": 5.854712923124092
    },
    {
      "type": "training",
      "description": "Training step 985",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:12",
      "total_flops_so_far": 5860653981176188.0,
      "budget_used_percent": 5.8606539811761875
    },
    {
      "type": "training",
      "description": "Training step 986",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:12",
      "total_flops_so_far": 5866595039228284.0,
      "budget_used_percent": 5.866595039228284
    },
    {
      "type": "training",
      "description": "Training step 987",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:12",
      "total_flops_so_far": 5872536097280380.0,
      "budget_used_percent": 5.87253609728038
    },
    {
      "type": "training",
      "description": "Training step 988",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:13",
      "total_flops_so_far": 5878477155332476.0,
      "budget_used_percent": 5.878477155332476
    },
    {
      "type": "training",
      "description": "Training step 989",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:13",
      "total_flops_so_far": 5884418213384572.0,
      "budget_used_percent": 5.884418213384572
    },
    {
      "type": "training",
      "description": "Training step 990",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:13",
      "total_flops_so_far": 5890359271436668.0,
      "budget_used_percent": 5.890359271436668
    },
    {
      "type": "training",
      "description": "Training step 991",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:14",
      "total_flops_so_far": 5896300329488764.0,
      "budget_used_percent": 5.896300329488764
    },
    {
      "type": "training",
      "description": "Training step 992",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:14",
      "total_flops_so_far": 5902241387540860.0,
      "budget_used_percent": 5.90224138754086
    },
    {
      "type": "training",
      "description": "Training step 993",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:14",
      "total_flops_so_far": 5908182445592956.0,
      "budget_used_percent": 5.908182445592956
    },
    {
      "type": "training",
      "description": "Training step 994",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:14",
      "total_flops_so_far": 5914123503645052.0,
      "budget_used_percent": 5.914123503645052
    },
    {
      "type": "training",
      "description": "Training step 995",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:15",
      "total_flops_so_far": 5920064561697148.0,
      "budget_used_percent": 5.9200645616971475
    },
    {
      "type": "training",
      "description": "Training step 996",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:15",
      "total_flops_so_far": 5926005619749244.0,
      "budget_used_percent": 5.926005619749244
    },
    {
      "type": "training",
      "description": "Training step 997",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:15",
      "total_flops_so_far": 5931946677801340.0,
      "budget_used_percent": 5.931946677801339
    },
    {
      "type": "training",
      "description": "Training step 998",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:16",
      "total_flops_so_far": 5937887735853436.0,
      "budget_used_percent": 5.937887735853436
    },
    {
      "type": "training",
      "description": "Training step 999",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:16",
      "total_flops_so_far": 5943828793905532.0,
      "budget_used_percent": 5.943828793905532
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 552526568288.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:18",
      "total_flops_so_far": 5944381320473820.0,
      "budget_used_percent": 5.94438132047382
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 556130834256.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:20",
      "total_flops_so_far": 5944937451308076.0,
      "budget_used_percent": 5.944937451308076
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 554328341080.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:22",
      "total_flops_so_far": 5945491779649156.0,
      "budget_used_percent": 5.945491779649156
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 552526568288.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:23",
      "total_flops_so_far": 5946044306217444.0,
      "budget_used_percent": 5.946044306217444
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 555229497620.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:25",
      "total_flops_so_far": 5946599535715064.0,
      "budget_used_percent": 5.946599535715064
    },
    {
      "type": "training",
      "description": "Training step 1000",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:25",
      "total_flops_so_far": 5952540593767160.0,
      "budget_used_percent": 5.95254059376716
    },
    {
      "type": "training",
      "description": "Training step 1001",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:25",
      "total_flops_so_far": 5958481651819256.0,
      "budget_used_percent": 5.958481651819255
    },
    {
      "type": "training",
      "description": "Training step 1002",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:26",
      "total_flops_so_far": 5964422709871352.0,
      "budget_used_percent": 5.964422709871352
    },
    {
      "type": "training",
      "description": "Training step 1003",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:26",
      "total_flops_so_far": 5970363767923448.0,
      "budget_used_percent": 5.970363767923448
    },
    {
      "type": "training",
      "description": "Training step 1004",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:26",
      "total_flops_so_far": 5976304825975544.0,
      "budget_used_percent": 5.976304825975544
    },
    {
      "type": "training",
      "description": "Training step 1005",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:27",
      "total_flops_so_far": 5982245884027640.0,
      "budget_used_percent": 5.98224588402764
    },
    {
      "type": "training",
      "description": "Training step 1006",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:27",
      "total_flops_so_far": 5988186942079736.0,
      "budget_used_percent": 5.988186942079736
    },
    {
      "type": "training",
      "description": "Training step 1007",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:27",
      "total_flops_so_far": 5994128000131832.0,
      "budget_used_percent": 5.994128000131832
    },
    {
      "type": "training",
      "description": "Training step 1008",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:28",
      "total_flops_so_far": 6000069058183928.0,
      "budget_used_percent": 6.000069058183928
    },
    {
      "type": "training",
      "description": "Training step 1009",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:28",
      "total_flops_so_far": 6006010116236024.0,
      "budget_used_percent": 6.006010116236024
    },
    {
      "type": "training",
      "description": "Training step 1010",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:28",
      "total_flops_so_far": 6011951174288120.0,
      "budget_used_percent": 6.01195117428812
    },
    {
      "type": "training",
      "description": "Training step 1011",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:29",
      "total_flops_so_far": 6017892232340216.0,
      "budget_used_percent": 6.017892232340216
    },
    {
      "type": "training",
      "description": "Training step 1012",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:29",
      "total_flops_so_far": 6023833290392312.0,
      "budget_used_percent": 6.023833290392313
    },
    {
      "type": "training",
      "description": "Training step 1013",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:29",
      "total_flops_so_far": 6029774348444408.0,
      "budget_used_percent": 6.029774348444408
    },
    {
      "type": "training",
      "description": "Training step 1014",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:30",
      "total_flops_so_far": 6035715406496504.0,
      "budget_used_percent": 6.035715406496505
    },
    {
      "type": "training",
      "description": "Training step 1015",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:30",
      "total_flops_so_far": 6041656464548600.0,
      "budget_used_percent": 6.041656464548599
    },
    {
      "type": "training",
      "description": "Training step 1016",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:30",
      "total_flops_so_far": 6047597522600696.0,
      "budget_used_percent": 6.047597522600696
    },
    {
      "type": "training",
      "description": "Training step 1017",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:31",
      "total_flops_so_far": 6053538580652792.0,
      "budget_used_percent": 6.053538580652792
    },
    {
      "type": "training",
      "description": "Training step 1018",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:31",
      "total_flops_so_far": 6059479638704888.0,
      "budget_used_percent": 6.059479638704888
    },
    {
      "type": "training",
      "description": "Training step 1019",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:31",
      "total_flops_so_far": 6065420696756984.0,
      "budget_used_percent": 6.065420696756984
    },
    {
      "type": "training",
      "description": "Training step 1020",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:32",
      "total_flops_so_far": 6071361754809080.0,
      "budget_used_percent": 6.07136175480908
    },
    {
      "type": "training",
      "description": "Training step 1021",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:32",
      "total_flops_so_far": 6077302812861176.0,
      "budget_used_percent": 6.077302812861176
    },
    {
      "type": "training",
      "description": "Training step 1022",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:32",
      "total_flops_so_far": 6083243870913272.0,
      "budget_used_percent": 6.083243870913272
    },
    {
      "type": "training",
      "description": "Training step 1023",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:33",
      "total_flops_so_far": 6089184928965368.0,
      "budget_used_percent": 6.089184928965368
    },
    {
      "type": "training",
      "description": "Training step 1024",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:33",
      "total_flops_so_far": 6095125987017464.0,
      "budget_used_percent": 6.0951259870174646
    },
    {
      "type": "training",
      "description": "Training step 1025",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:33",
      "total_flops_so_far": 6101067045069560.0,
      "budget_used_percent": 6.10106704506956
    },
    {
      "type": "training",
      "description": "Training step 1026",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:34",
      "total_flops_so_far": 6107008103121656.0,
      "budget_used_percent": 6.1070081031216565
    },
    {
      "type": "training",
      "description": "Training step 1027",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:34",
      "total_flops_so_far": 6112949161173752.0,
      "budget_used_percent": 6.112949161173752
    },
    {
      "type": "training",
      "description": "Training step 1028",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:34",
      "total_flops_so_far": 6118890219225848.0,
      "budget_used_percent": 6.1188902192258485
    },
    {
      "type": "training",
      "description": "Training step 1029",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:35",
      "total_flops_so_far": 6124831277277944.0,
      "budget_used_percent": 6.124831277277944
    },
    {
      "type": "training",
      "description": "Training step 1030",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:35",
      "total_flops_so_far": 6130772335330040.0,
      "budget_used_percent": 6.13077233533004
    },
    {
      "type": "training",
      "description": "Training step 1031",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:35",
      "total_flops_so_far": 6136713393382136.0,
      "budget_used_percent": 6.136713393382136
    },
    {
      "type": "training",
      "description": "Training step 1032",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:36",
      "total_flops_so_far": 6142654451434232.0,
      "budget_used_percent": 6.142654451434232
    },
    {
      "type": "training",
      "description": "Training step 1033",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:36",
      "total_flops_so_far": 6148595509486328.0,
      "budget_used_percent": 6.148595509486328
    },
    {
      "type": "training",
      "description": "Training step 1034",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:36",
      "total_flops_so_far": 6154536567538424.0,
      "budget_used_percent": 6.154536567538424
    },
    {
      "type": "training",
      "description": "Training step 1035",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:37",
      "total_flops_so_far": 6160477625590520.0,
      "budget_used_percent": 6.16047762559052
    },
    {
      "type": "training",
      "description": "Training step 1036",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:37",
      "total_flops_so_far": 6166418683642616.0,
      "budget_used_percent": 6.1664186836426165
    },
    {
      "type": "training",
      "description": "Training step 1037",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:37",
      "total_flops_so_far": 6172359741694712.0,
      "budget_used_percent": 6.172359741694712
    },
    {
      "type": "training",
      "description": "Training step 1038",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:38",
      "total_flops_so_far": 6178300799746808.0,
      "budget_used_percent": 6.1783007997468085
    },
    {
      "type": "training",
      "description": "Training step 1039",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:38",
      "total_flops_so_far": 6184241857798904.0,
      "budget_used_percent": 6.184241857798904
    },
    {
      "type": "training",
      "description": "Training step 1040",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:38",
      "total_flops_so_far": 6190182915851000.0,
      "budget_used_percent": 6.1901829158510004
    },
    {
      "type": "training",
      "description": "Training step 1041",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:39",
      "total_flops_so_far": 6196123973903096.0,
      "budget_used_percent": 6.196123973903096
    },
    {
      "type": "training",
      "description": "Training step 1042",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:39",
      "total_flops_so_far": 6202065031955192.0,
      "budget_used_percent": 6.202065031955192
    },
    {
      "type": "training",
      "description": "Training step 1043",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:39",
      "total_flops_so_far": 6208006090007288.0,
      "budget_used_percent": 6.208006090007288
    },
    {
      "type": "training",
      "description": "Training step 1044",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:40",
      "total_flops_so_far": 6213947148059384.0,
      "budget_used_percent": 6.2139471480593835
    },
    {
      "type": "training",
      "description": "Training step 1045",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:40",
      "total_flops_so_far": 6219888206111480.0,
      "budget_used_percent": 6.21988820611148
    },
    {
      "type": "training",
      "description": "Training step 1046",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:40",
      "total_flops_so_far": 6225829264163576.0,
      "budget_used_percent": 6.2258292641635755
    },
    {
      "type": "training",
      "description": "Training step 1047",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:41",
      "total_flops_so_far": 6231770322215672.0,
      "budget_used_percent": 6.231770322215672
    },
    {
      "type": "training",
      "description": "Training step 1048",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:41",
      "total_flops_so_far": 6237711380267768.0,
      "budget_used_percent": 6.2377113802677675
    },
    {
      "type": "training",
      "description": "Training step 1049",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:41",
      "total_flops_so_far": 6243652438319864.0,
      "budget_used_percent": 6.243652438319864
    },
    {
      "type": "training",
      "description": "Training step 1050",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:42",
      "total_flops_so_far": 6249593496371960.0,
      "budget_used_percent": 6.24959349637196
    },
    {
      "type": "training",
      "description": "Training step 1051",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:42",
      "total_flops_so_far": 6255534554424056.0,
      "budget_used_percent": 6.255534554424055
    },
    {
      "type": "training",
      "description": "Training step 1052",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:42",
      "total_flops_so_far": 6261475612476152.0,
      "budget_used_percent": 6.2614756124761515
    },
    {
      "type": "training",
      "description": "Training step 1053",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:42",
      "total_flops_so_far": 6267416670528248.0,
      "budget_used_percent": 6.267416670528247
    },
    {
      "type": "training",
      "description": "Training step 1054",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:43",
      "total_flops_so_far": 6273357728580344.0,
      "budget_used_percent": 6.2733577285803435
    },
    {
      "type": "training",
      "description": "Training step 1055",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:43",
      "total_flops_so_far": 6279298786632440.0,
      "budget_used_percent": 6.27929878663244
    },
    {
      "type": "training",
      "description": "Training step 1056",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:43",
      "total_flops_so_far": 6285239844684536.0,
      "budget_used_percent": 6.2852398446845354
    },
    {
      "type": "training",
      "description": "Training step 1057",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:44",
      "total_flops_so_far": 6291180902736632.0,
      "budget_used_percent": 6.291180902736632
    },
    {
      "type": "training",
      "description": "Training step 1058",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:44",
      "total_flops_so_far": 6297121960788728.0,
      "budget_used_percent": 6.297121960788727
    },
    {
      "type": "training",
      "description": "Training step 1059",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:44",
      "total_flops_so_far": 6303063018840824.0,
      "budget_used_percent": 6.303063018840824
    },
    {
      "type": "training",
      "description": "Training step 1060",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:45",
      "total_flops_so_far": 6309004076892920.0,
      "budget_used_percent": 6.309004076892919
    },
    {
      "type": "training",
      "description": "Training step 1061",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:45",
      "total_flops_so_far": 6314945134945016.0,
      "budget_used_percent": 6.314945134945016
    },
    {
      "type": "training",
      "description": "Training step 1062",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:45",
      "total_flops_so_far": 6320886192997112.0,
      "budget_used_percent": 6.320886192997112
    },
    {
      "type": "training",
      "description": "Training step 1063",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:46",
      "total_flops_so_far": 6326827251049208.0,
      "budget_used_percent": 6.326827251049208
    },
    {
      "type": "training",
      "description": "Training step 1064",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:46",
      "total_flops_so_far": 6332768309101304.0,
      "budget_used_percent": 6.332768309101304
    },
    {
      "type": "training",
      "description": "Training step 1065",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:46",
      "total_flops_so_far": 6338709367153400.0,
      "budget_used_percent": 6.3387093671534
    },
    {
      "type": "training",
      "description": "Training step 1066",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:47",
      "total_flops_so_far": 6344650425205496.0,
      "budget_used_percent": 6.344650425205496
    },
    {
      "type": "training",
      "description": "Training step 1067",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:47",
      "total_flops_so_far": 6350591483257592.0,
      "budget_used_percent": 6.350591483257592
    },
    {
      "type": "training",
      "description": "Training step 1068",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:47",
      "total_flops_so_far": 6356532541309688.0,
      "budget_used_percent": 6.356532541309688
    },
    {
      "type": "training",
      "description": "Training step 1069",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:48",
      "total_flops_so_far": 6362473599361784.0,
      "budget_used_percent": 6.362473599361785
    },
    {
      "type": "training",
      "description": "Training step 1070",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:48",
      "total_flops_so_far": 6368414657413880.0,
      "budget_used_percent": 6.36841465741388
    },
    {
      "type": "training",
      "description": "Training step 1071",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:48",
      "total_flops_so_far": 6374355715465976.0,
      "budget_used_percent": 6.374355715465977
    },
    {
      "type": "training",
      "description": "Training step 1072",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:49",
      "total_flops_so_far": 6380296773518072.0,
      "budget_used_percent": 6.380296773518072
    },
    {
      "type": "training",
      "description": "Training step 1073",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:49",
      "total_flops_so_far": 6386237831570168.0,
      "budget_used_percent": 6.386237831570169
    },
    {
      "type": "training",
      "description": "Training step 1074",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:49",
      "total_flops_so_far": 6392178889622264.0,
      "budget_used_percent": 6.392178889622264
    },
    {
      "type": "training",
      "description": "Training step 1075",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:50",
      "total_flops_so_far": 6398119947674360.0,
      "budget_used_percent": 6.398119947674361
    },
    {
      "type": "training",
      "description": "Training step 1076",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:50",
      "total_flops_so_far": 6404061005726456.0,
      "budget_used_percent": 6.404061005726456
    },
    {
      "type": "training",
      "description": "Training step 1077",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:50",
      "total_flops_so_far": 6410002063778552.0,
      "budget_used_percent": 6.410002063778553
    },
    {
      "type": "training",
      "description": "Training step 1078",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:51",
      "total_flops_so_far": 6415943121830648.0,
      "budget_used_percent": 6.415943121830649
    },
    {
      "type": "training",
      "description": "Training step 1079",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:51",
      "total_flops_so_far": 6421884179882744.0,
      "budget_used_percent": 6.421884179882744
    },
    {
      "type": "training",
      "description": "Training step 1080",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:51",
      "total_flops_so_far": 6427825237934840.0,
      "budget_used_percent": 6.427825237934839
    },
    {
      "type": "training",
      "description": "Training step 1081",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:52",
      "total_flops_so_far": 6433766295986936.0,
      "budget_used_percent": 6.433766295986936
    },
    {
      "type": "training",
      "description": "Training step 1082",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:52",
      "total_flops_so_far": 6439707354039032.0,
      "budget_used_percent": 6.439707354039031
    },
    {
      "type": "training",
      "description": "Training step 1083",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:52",
      "total_flops_so_far": 6445648412091128.0,
      "budget_used_percent": 6.445648412091128
    },
    {
      "type": "training",
      "description": "Training step 1084",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:53",
      "total_flops_so_far": 6451589470143224.0,
      "budget_used_percent": 6.451589470143223
    },
    {
      "type": "training",
      "description": "Training step 1085",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:53",
      "total_flops_so_far": 6457530528195320.0,
      "budget_used_percent": 6.45753052819532
    },
    {
      "type": "training",
      "description": "Training step 1086",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:53",
      "total_flops_so_far": 6463471586247416.0,
      "budget_used_percent": 6.463471586247415
    },
    {
      "type": "training",
      "description": "Training step 1087",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:54",
      "total_flops_so_far": 6469412644299512.0,
      "budget_used_percent": 6.469412644299512
    },
    {
      "type": "training",
      "description": "Training step 1088",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:54",
      "total_flops_so_far": 6475353702351608.0,
      "budget_used_percent": 6.475353702351608
    },
    {
      "type": "training",
      "description": "Training step 1089",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:54",
      "total_flops_so_far": 6481294760403704.0,
      "budget_used_percent": 6.481294760403704
    },
    {
      "type": "training",
      "description": "Training step 1090",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:55",
      "total_flops_so_far": 6487235818455800.0,
      "budget_used_percent": 6.4872358184558
    },
    {
      "type": "training",
      "description": "Training step 1091",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:55",
      "total_flops_so_far": 6493176876507896.0,
      "budget_used_percent": 6.493176876507896
    },
    {
      "type": "training",
      "description": "Training step 1092",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:55",
      "total_flops_so_far": 6499117934559992.0,
      "budget_used_percent": 6.499117934559992
    },
    {
      "type": "training",
      "description": "Training step 1093",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:56",
      "total_flops_so_far": 6505058992612088.0,
      "budget_used_percent": 6.505058992612088
    },
    {
      "type": "training",
      "description": "Training step 1094",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:56",
      "total_flops_so_far": 6511000050664184.0,
      "budget_used_percent": 6.511000050664184
    },
    {
      "type": "training",
      "description": "Training step 1095",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:56",
      "total_flops_so_far": 6516941108716280.0,
      "budget_used_percent": 6.5169411087162805
    },
    {
      "type": "training",
      "description": "Training step 1096",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:57",
      "total_flops_so_far": 6522882166768376.0,
      "budget_used_percent": 6.522882166768376
    },
    {
      "type": "training",
      "description": "Training step 1097",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:57",
      "total_flops_so_far": 6528823224820472.0,
      "budget_used_percent": 6.5288232248204725
    },
    {
      "type": "training",
      "description": "Training step 1098",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:57",
      "total_flops_so_far": 6534764282872568.0,
      "budget_used_percent": 6.534764282872568
    },
    {
      "type": "training",
      "description": "Training step 1099",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:58",
      "total_flops_so_far": 6540705340924664.0,
      "budget_used_percent": 6.5407053409246645
    },
    {
      "type": "training",
      "description": "Training step 1100",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:58",
      "total_flops_so_far": 6546646398976760.0,
      "budget_used_percent": 6.54664639897676
    },
    {
      "type": "training",
      "description": "Training step 1101",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:58",
      "total_flops_so_far": 6552587457028856.0,
      "budget_used_percent": 6.5525874570288565
    },
    {
      "type": "training",
      "description": "Training step 1102",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:59",
      "total_flops_so_far": 6558528515080952.0,
      "budget_used_percent": 6.558528515080953
    },
    {
      "type": "training",
      "description": "Training step 1103",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:59",
      "total_flops_so_far": 6564469573133048.0,
      "budget_used_percent": 6.5644695731330485
    },
    {
      "type": "training",
      "description": "Training step 1104",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:44:59",
      "total_flops_so_far": 6570410631185144.0,
      "budget_used_percent": 6.570410631185145
    },
    {
      "type": "training",
      "description": "Training step 1105",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:00",
      "total_flops_so_far": 6576351689237240.0,
      "budget_used_percent": 6.5763516892372404
    },
    {
      "type": "training",
      "description": "Training step 1106",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:00",
      "total_flops_so_far": 6582292747289336.0,
      "budget_used_percent": 6.582292747289337
    },
    {
      "type": "training",
      "description": "Training step 1107",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:00",
      "total_flops_so_far": 6588233805341432.0,
      "budget_used_percent": 6.5882338053414315
    },
    {
      "type": "training",
      "description": "Training step 1108",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:01",
      "total_flops_so_far": 6594174863393528.0,
      "budget_used_percent": 6.594174863393527
    },
    {
      "type": "training",
      "description": "Training step 1109",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:01",
      "total_flops_so_far": 6600115921445624.0,
      "budget_used_percent": 6.6001159214456235
    },
    {
      "type": "training",
      "description": "Training step 1110",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:01",
      "total_flops_so_far": 6606056979497720.0,
      "budget_used_percent": 6.606056979497719
    },
    {
      "type": "training",
      "description": "Training step 1111",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:02",
      "total_flops_so_far": 6611998037549816.0,
      "budget_used_percent": 6.6119980375498155
    },
    {
      "type": "training",
      "description": "Training step 1112",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:02",
      "total_flops_so_far": 6617939095601912.0,
      "budget_used_percent": 6.617939095601912
    },
    {
      "type": "training",
      "description": "Training step 1113",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:02",
      "total_flops_so_far": 6623880153654008.0,
      "budget_used_percent": 6.6238801536540075
    },
    {
      "type": "training",
      "description": "Training step 1114",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:03",
      "total_flops_so_far": 6629821211706104.0,
      "budget_used_percent": 6.629821211706104
    },
    {
      "type": "training",
      "description": "Training step 1115",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:03",
      "total_flops_so_far": 6635762269758200.0,
      "budget_used_percent": 6.6357622697581995
    },
    {
      "type": "training",
      "description": "Training step 1116",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:03",
      "total_flops_so_far": 6641703327810296.0,
      "budget_used_percent": 6.641703327810296
    },
    {
      "type": "training",
      "description": "Training step 1117",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:04",
      "total_flops_so_far": 6647644385862392.0,
      "budget_used_percent": 6.6476443858623915
    },
    {
      "type": "training",
      "description": "Training step 1118",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:04",
      "total_flops_so_far": 6653585443914488.0,
      "budget_used_percent": 6.653585443914488
    },
    {
      "type": "training",
      "description": "Training step 1119",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:04",
      "total_flops_so_far": 6659526501966584.0,
      "budget_used_percent": 6.6595265019665835
    },
    {
      "type": "training",
      "description": "Training step 1120",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:05",
      "total_flops_so_far": 6665467560018680.0,
      "budget_used_percent": 6.66546756001868
    },
    {
      "type": "training",
      "description": "Training step 1121",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:05",
      "total_flops_so_far": 6671408618070776.0,
      "budget_used_percent": 6.671408618070776
    },
    {
      "type": "training",
      "description": "Training step 1122",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:05",
      "total_flops_so_far": 6677349676122872.0,
      "budget_used_percent": 6.677349676122872
    },
    {
      "type": "training",
      "description": "Training step 1123",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:06",
      "total_flops_so_far": 6683290734174968.0,
      "budget_used_percent": 6.683290734174968
    },
    {
      "type": "training",
      "description": "Training step 1124",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:06",
      "total_flops_so_far": 6689231792227064.0,
      "budget_used_percent": 6.689231792227064
    },
    {
      "type": "training",
      "description": "Training step 1125",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:06",
      "total_flops_so_far": 6695172850279160.0,
      "budget_used_percent": 6.69517285027916
    },
    {
      "type": "training",
      "description": "Training step 1126",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:07",
      "total_flops_so_far": 6701113908331256.0,
      "budget_used_percent": 6.701113908331256
    },
    {
      "type": "training",
      "description": "Training step 1127",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:07",
      "total_flops_so_far": 6707054966383352.0,
      "budget_used_percent": 6.707054966383352
    },
    {
      "type": "training",
      "description": "Training step 1128",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:07",
      "total_flops_so_far": 6712996024435448.0,
      "budget_used_percent": 6.712996024435449
    },
    {
      "type": "training",
      "description": "Training step 1129",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:08",
      "total_flops_so_far": 6718937082487544.0,
      "budget_used_percent": 6.718937082487544
    },
    {
      "type": "training",
      "description": "Training step 1130",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:08",
      "total_flops_so_far": 6724878140539640.0,
      "budget_used_percent": 6.724878140539641
    },
    {
      "type": "training",
      "description": "Training step 1131",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:08",
      "total_flops_so_far": 6730819198591736.0,
      "budget_used_percent": 6.730819198591736
    },
    {
      "type": "training",
      "description": "Training step 1132",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:09",
      "total_flops_so_far": 6736760256643832.0,
      "budget_used_percent": 6.736760256643833
    },
    {
      "type": "training",
      "description": "Training step 1133",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:09",
      "total_flops_so_far": 6742701314695928.0,
      "budget_used_percent": 6.742701314695928
    },
    {
      "type": "training",
      "description": "Training step 1134",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:09",
      "total_flops_so_far": 6748642372748024.0,
      "budget_used_percent": 6.748642372748025
    },
    {
      "type": "training",
      "description": "Training step 1135",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:10",
      "total_flops_so_far": 6754583430800120.0,
      "budget_used_percent": 6.754583430800121
    },
    {
      "type": "training",
      "description": "Training step 1136",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:10",
      "total_flops_so_far": 6760524488852216.0,
      "budget_used_percent": 6.760524488852216
    },
    {
      "type": "training",
      "description": "Training step 1137",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:10",
      "total_flops_so_far": 6766465546904312.0,
      "budget_used_percent": 6.766465546904311
    },
    {
      "type": "training",
      "description": "Training step 1138",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:11",
      "total_flops_so_far": 6772406604956408.0,
      "budget_used_percent": 6.772406604956408
    },
    {
      "type": "training",
      "description": "Training step 1139",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:11",
      "total_flops_so_far": 6778347663008504.0,
      "budget_used_percent": 6.778347663008503
    },
    {
      "type": "training",
      "description": "Training step 1140",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:11",
      "total_flops_so_far": 6784288721060600.0,
      "budget_used_percent": 6.7842887210606
    },
    {
      "type": "training",
      "description": "Training step 1141",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:12",
      "total_flops_so_far": 6790229779112696.0,
      "budget_used_percent": 6.790229779112695
    },
    {
      "type": "training",
      "description": "Training step 1142",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:12",
      "total_flops_so_far": 6796170837164792.0,
      "budget_used_percent": 6.796170837164792
    },
    {
      "type": "training",
      "description": "Training step 1143",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:12",
      "total_flops_so_far": 6802111895216888.0,
      "budget_used_percent": 6.802111895216887
    },
    {
      "type": "training",
      "description": "Training step 1144",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:13",
      "total_flops_so_far": 6808052953268984.0,
      "budget_used_percent": 6.808052953268984
    },
    {
      "type": "training",
      "description": "Training step 1145",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:13",
      "total_flops_so_far": 6813994011321080.0,
      "budget_used_percent": 6.81399401132108
    },
    {
      "type": "training",
      "description": "Training step 1146",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:13",
      "total_flops_so_far": 6819935069373176.0,
      "budget_used_percent": 6.819935069373176
    },
    {
      "type": "training",
      "description": "Training step 1147",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:14",
      "total_flops_so_far": 6825876127425272.0,
      "budget_used_percent": 6.825876127425272
    },
    {
      "type": "training",
      "description": "Training step 1148",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:14",
      "total_flops_so_far": 6831817185477368.0,
      "budget_used_percent": 6.831817185477368
    },
    {
      "type": "training",
      "description": "Training step 1149",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:14",
      "total_flops_so_far": 6837758243529464.0,
      "budget_used_percent": 6.837758243529464
    },
    {
      "type": "training",
      "description": "Training step 1150",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:15",
      "total_flops_so_far": 6843699301581560.0,
      "budget_used_percent": 6.84369930158156
    },
    {
      "type": "training",
      "description": "Training step 1151",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:15",
      "total_flops_so_far": 6849640359633656.0,
      "budget_used_percent": 6.849640359633656
    },
    {
      "type": "training",
      "description": "Training step 1152",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:15",
      "total_flops_so_far": 6855581417685752.0,
      "budget_used_percent": 6.855581417685752
    },
    {
      "type": "training",
      "description": "Training step 1153",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:16",
      "total_flops_so_far": 6861522475737848.0,
      "budget_used_percent": 6.861522475737848
    },
    {
      "type": "training",
      "description": "Training step 1154",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:16",
      "total_flops_so_far": 6867463533789944.0,
      "budget_used_percent": 6.867463533789945
    },
    {
      "type": "training",
      "description": "Training step 1155",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:16",
      "total_flops_so_far": 6873404591842040.0,
      "budget_used_percent": 6.87340459184204
    },
    {
      "type": "training",
      "description": "Training step 1156",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:17",
      "total_flops_so_far": 6879345649894136.0,
      "budget_used_percent": 6.8793456498941365
    },
    {
      "type": "training",
      "description": "Training step 1157",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:17",
      "total_flops_so_far": 6885286707946232.0,
      "budget_used_percent": 6.885286707946232
    },
    {
      "type": "training",
      "description": "Training step 1158",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:17",
      "total_flops_so_far": 6891227765998328.0,
      "budget_used_percent": 6.8912277659983285
    },
    {
      "type": "training",
      "description": "Training step 1159",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:18",
      "total_flops_so_far": 6897168824050424.0,
      "budget_used_percent": 6.897168824050424
    },
    {
      "type": "training",
      "description": "Training step 1160",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:18",
      "total_flops_so_far": 6903109882102520.0,
      "budget_used_percent": 6.9031098821025205
    },
    {
      "type": "training",
      "description": "Training step 1161",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:18",
      "total_flops_so_far": 6909050940154616.0,
      "budget_used_percent": 6.909050940154617
    },
    {
      "type": "training",
      "description": "Training step 1162",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:19",
      "total_flops_so_far": 6914991998206712.0,
      "budget_used_percent": 6.9149919982067125
    },
    {
      "type": "training",
      "description": "Training step 1163",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:19",
      "total_flops_so_far": 6920933056258808.0,
      "budget_used_percent": 6.920933056258809
    },
    {
      "type": "training",
      "description": "Training step 1164",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:19",
      "total_flops_so_far": 6926874114310904.0,
      "budget_used_percent": 6.926874114310904
    },
    {
      "type": "training",
      "description": "Training step 1165",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:20",
      "total_flops_so_far": 6932815172363000.0,
      "budget_used_percent": 6.932815172362999
    },
    {
      "type": "training",
      "description": "Training step 1166",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:20",
      "total_flops_so_far": 6938756230415096.0,
      "budget_used_percent": 6.938756230415096
    },
    {
      "type": "training",
      "description": "Training step 1167",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:20",
      "total_flops_so_far": 6944697288467192.0,
      "budget_used_percent": 6.944697288467191
    },
    {
      "type": "training",
      "description": "Training step 1168",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:21",
      "total_flops_so_far": 6950638346519288.0,
      "budget_used_percent": 6.950638346519288
    },
    {
      "type": "training",
      "description": "Training step 1169",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:21",
      "total_flops_so_far": 6956579404571384.0,
      "budget_used_percent": 6.956579404571384
    },
    {
      "type": "training",
      "description": "Training step 1170",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:21",
      "total_flops_so_far": 6962520462623480.0,
      "budget_used_percent": 6.96252046262348
    },
    {
      "type": "training",
      "description": "Training step 1171",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:22",
      "total_flops_so_far": 6968461520675576.0,
      "budget_used_percent": 6.968461520675576
    },
    {
      "type": "training",
      "description": "Training step 1172",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:22",
      "total_flops_so_far": 6974402578727672.0,
      "budget_used_percent": 6.9744025787276716
    },
    {
      "type": "training",
      "description": "Training step 1173",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:22",
      "total_flops_so_far": 6980343636779768.0,
      "budget_used_percent": 6.980343636779768
    },
    {
      "type": "training",
      "description": "Training step 1174",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:23",
      "total_flops_so_far": 6986284694831864.0,
      "budget_used_percent": 6.9862846948318635
    },
    {
      "type": "training",
      "description": "Training step 1175",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:23",
      "total_flops_so_far": 6992225752883960.0,
      "budget_used_percent": 6.99222575288396
    },
    {
      "type": "training",
      "description": "Training step 1176",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:23",
      "total_flops_so_far": 6998166810936056.0,
      "budget_used_percent": 6.9981668109360555
    },
    {
      "type": "training",
      "description": "Training step 1177",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:24",
      "total_flops_so_far": 7004107868988152.0,
      "budget_used_percent": 7.004107868988152
    },
    {
      "type": "training",
      "description": "Training step 1178",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:24",
      "total_flops_so_far": 7010048927040248.0,
      "budget_used_percent": 7.010048927040248
    },
    {
      "type": "training",
      "description": "Training step 1179",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:24",
      "total_flops_so_far": 7015989985092344.0,
      "budget_used_percent": 7.015989985092344
    },
    {
      "type": "training",
      "description": "Training step 1180",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:25",
      "total_flops_so_far": 7021931043144440.0,
      "budget_used_percent": 7.02193104314444
    },
    {
      "type": "training",
      "description": "Training step 1181",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:25",
      "total_flops_so_far": 7027872101196536.0,
      "budget_used_percent": 7.027872101196536
    },
    {
      "type": "training",
      "description": "Training step 1182",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:25",
      "total_flops_so_far": 7033813159248632.0,
      "budget_used_percent": 7.033813159248632
    },
    {
      "type": "training",
      "description": "Training step 1183",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:26",
      "total_flops_so_far": 7039754217300728.0,
      "budget_used_percent": 7.039754217300728
    },
    {
      "type": "training",
      "description": "Training step 1184",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:26",
      "total_flops_so_far": 7045695275352824.0,
      "budget_used_percent": 7.045695275352824
    },
    {
      "type": "training",
      "description": "Training step 1185",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:26",
      "total_flops_so_far": 7051636333404920.0,
      "budget_used_percent": 7.05163633340492
    },
    {
      "type": "training",
      "description": "Training step 1186",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:27",
      "total_flops_so_far": 7057577391457016.0,
      "budget_used_percent": 7.057577391457016
    },
    {
      "type": "training",
      "description": "Training step 1187",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:27",
      "total_flops_so_far": 7063518449509112.0,
      "budget_used_percent": 7.063518449509113
    },
    {
      "type": "training",
      "description": "Training step 1188",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:27",
      "total_flops_so_far": 7069459507561208.0,
      "budget_used_percent": 7.069459507561208
    },
    {
      "type": "training",
      "description": "Training step 1189",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:28",
      "total_flops_so_far": 7075400565613304.0,
      "budget_used_percent": 7.075400565613305
    },
    {
      "type": "training",
      "description": "Training step 1190",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:28",
      "total_flops_so_far": 7081341623665400.0,
      "budget_used_percent": 7.0813416236654
    },
    {
      "type": "training",
      "description": "Training step 1191",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:28",
      "total_flops_so_far": 7087282681717496.0,
      "budget_used_percent": 7.087282681717497
    },
    {
      "type": "training",
      "description": "Training step 1192",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:29",
      "total_flops_so_far": 7093223739769592.0,
      "budget_used_percent": 7.093223739769591
    },
    {
      "type": "training",
      "description": "Training step 1193",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:29",
      "total_flops_so_far": 7099164797821688.0,
      "budget_used_percent": 7.099164797821687
    },
    {
      "type": "training",
      "description": "Training step 1194",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:29",
      "total_flops_so_far": 7105105855873784.0,
      "budget_used_percent": 7.105105855873783
    },
    {
      "type": "training",
      "description": "Training step 1195",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:30",
      "total_flops_so_far": 7111046913925880.0,
      "budget_used_percent": 7.11104691392588
    },
    {
      "type": "training",
      "description": "Training step 1196",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:30",
      "total_flops_so_far": 7116987971977976.0,
      "budget_used_percent": 7.116987971977975
    },
    {
      "type": "training",
      "description": "Training step 1197",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:30",
      "total_flops_so_far": 7122929030030072.0,
      "budget_used_percent": 7.122929030030072
    },
    {
      "type": "training",
      "description": "Training step 1198",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:31",
      "total_flops_so_far": 7128870088082168.0,
      "budget_used_percent": 7.128870088082167
    },
    {
      "type": "training",
      "description": "Training step 1199",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:31",
      "total_flops_so_far": 7134811146134264.0,
      "budget_used_percent": 7.134811146134264
    },
    {
      "type": "training",
      "description": "Training step 1200",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:31",
      "total_flops_so_far": 7140752204186360.0,
      "budget_used_percent": 7.140752204186359
    },
    {
      "type": "training",
      "description": "Training step 1201",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:32",
      "total_flops_so_far": 7146693262238456.0,
      "budget_used_percent": 7.146693262238456
    },
    {
      "type": "training",
      "description": "Training step 1202",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:32",
      "total_flops_so_far": 7152634320290552.0,
      "budget_used_percent": 7.152634320290552
    },
    {
      "type": "training",
      "description": "Training step 1203",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:32",
      "total_flops_so_far": 7158575378342648.0,
      "budget_used_percent": 7.158575378342648
    },
    {
      "type": "training",
      "description": "Training step 1204",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:33",
      "total_flops_so_far": 7164516436394744.0,
      "budget_used_percent": 7.164516436394744
    },
    {
      "type": "training",
      "description": "Training step 1205",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:33",
      "total_flops_so_far": 7170457494446840.0,
      "budget_used_percent": 7.17045749444684
    },
    {
      "type": "training",
      "description": "Training step 1206",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:33",
      "total_flops_so_far": 7176398552498936.0,
      "budget_used_percent": 7.176398552498936
    },
    {
      "type": "training",
      "description": "Training step 1207",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:34",
      "total_flops_so_far": 7182339610551032.0,
      "budget_used_percent": 7.182339610551032
    },
    {
      "type": "training",
      "description": "Training step 1208",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:34",
      "total_flops_so_far": 7188280668603128.0,
      "budget_used_percent": 7.188280668603128
    },
    {
      "type": "training",
      "description": "Training step 1209",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:34",
      "total_flops_so_far": 7194221726655224.0,
      "budget_used_percent": 7.194221726655224
    },
    {
      "type": "training",
      "description": "Training step 1210",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:35",
      "total_flops_so_far": 7200162784707320.0,
      "budget_used_percent": 7.20016278470732
    },
    {
      "type": "training",
      "description": "Training step 1211",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:35",
      "total_flops_so_far": 7206103842759416.0,
      "budget_used_percent": 7.206103842759417
    },
    {
      "type": "training",
      "description": "Training step 1212",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:35",
      "total_flops_so_far": 7212044900811512.0,
      "budget_used_percent": 7.212044900811512
    },
    {
      "type": "training",
      "description": "Training step 1213",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:36",
      "total_flops_so_far": 7217985958863608.0,
      "budget_used_percent": 7.217985958863609
    },
    {
      "type": "training",
      "description": "Training step 1214",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:36",
      "total_flops_so_far": 7223927016915704.0,
      "budget_used_percent": 7.223927016915704
    },
    {
      "type": "training",
      "description": "Training step 1215",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:36",
      "total_flops_so_far": 7229868074967800.0,
      "budget_used_percent": 7.229868074967801
    },
    {
      "type": "training",
      "description": "Training step 1216",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:37",
      "total_flops_so_far": 7235809133019896.0,
      "budget_used_percent": 7.235809133019896
    },
    {
      "type": "training",
      "description": "Training step 1217",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:37",
      "total_flops_so_far": 7241750191071992.0,
      "budget_used_percent": 7.241750191071993
    },
    {
      "type": "training",
      "description": "Training step 1218",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:37",
      "total_flops_so_far": 7247691249124088.0,
      "budget_used_percent": 7.247691249124088
    },
    {
      "type": "training",
      "description": "Training step 1219",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:38",
      "total_flops_so_far": 7253632307176184.0,
      "budget_used_percent": 7.253632307176185
    },
    {
      "type": "training",
      "description": "Training step 1220",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:38",
      "total_flops_so_far": 7259573365228280.0,
      "budget_used_percent": 7.259573365228281
    },
    {
      "type": "training",
      "description": "Training step 1221",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:38",
      "total_flops_so_far": 7265514423280376.0,
      "budget_used_percent": 7.265514423280376
    },
    {
      "type": "training",
      "description": "Training step 1222",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:39",
      "total_flops_so_far": 7271455481332472.0,
      "budget_used_percent": 7.271455481332471
    },
    {
      "type": "training",
      "description": "Training step 1223",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:39",
      "total_flops_so_far": 7277396539384568.0,
      "budget_used_percent": 7.277396539384568
    },
    {
      "type": "training",
      "description": "Training step 1224",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:39",
      "total_flops_so_far": 7283337597436664.0,
      "budget_used_percent": 7.283337597436663
    },
    {
      "type": "training",
      "description": "Training step 1225",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:40",
      "total_flops_so_far": 7289278655488760.0,
      "budget_used_percent": 7.28927865548876
    },
    {
      "type": "training",
      "description": "Training step 1226",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:40",
      "total_flops_so_far": 7295219713540856.0,
      "budget_used_percent": 7.295219713540855
    },
    {
      "type": "training",
      "description": "Training step 1227",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:40",
      "total_flops_so_far": 7301160771592952.0,
      "budget_used_percent": 7.301160771592952
    },
    {
      "type": "training",
      "description": "Training step 1228",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:41",
      "total_flops_so_far": 7307101829645048.0,
      "budget_used_percent": 7.307101829645048
    },
    {
      "type": "training",
      "description": "Training step 1229",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:41",
      "total_flops_so_far": 7313042887697144.0,
      "budget_used_percent": 7.313042887697144
    },
    {
      "type": "training",
      "description": "Training step 1230",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:41",
      "total_flops_so_far": 7318983945749240.0,
      "budget_used_percent": 7.31898394574924
    },
    {
      "type": "training",
      "description": "Training step 1231",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:42",
      "total_flops_so_far": 7324925003801336.0,
      "budget_used_percent": 7.324925003801336
    },
    {
      "type": "training",
      "description": "Training step 1232",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:42",
      "total_flops_so_far": 7330866061853432.0,
      "budget_used_percent": 7.330866061853432
    },
    {
      "type": "training",
      "description": "Training step 1233",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:42",
      "total_flops_so_far": 7336807119905528.0,
      "budget_used_percent": 7.336807119905528
    },
    {
      "type": "training",
      "description": "Training step 1234",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:43",
      "total_flops_so_far": 7342748177957624.0,
      "budget_used_percent": 7.342748177957624
    },
    {
      "type": "training",
      "description": "Training step 1235",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:43",
      "total_flops_so_far": 7348689236009720.0,
      "budget_used_percent": 7.3486892360097205
    },
    {
      "type": "training",
      "description": "Training step 1236",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:43",
      "total_flops_so_far": 7354630294061816.0,
      "budget_used_percent": 7.354630294061816
    },
    {
      "type": "training",
      "description": "Training step 1237",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:44",
      "total_flops_so_far": 7360571352113912.0,
      "budget_used_percent": 7.3605713521139124
    },
    {
      "type": "training",
      "description": "Training step 1238",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:44",
      "total_flops_so_far": 7366512410166008.0,
      "budget_used_percent": 7.366512410166008
    },
    {
      "type": "training",
      "description": "Training step 1239",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:44",
      "total_flops_so_far": 7372453468218104.0,
      "budget_used_percent": 7.372453468218104
    },
    {
      "type": "training",
      "description": "Training step 1240",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:45",
      "total_flops_so_far": 7378394526270200.0,
      "budget_used_percent": 7.3783945262702
    },
    {
      "type": "training",
      "description": "Training step 1241",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:45",
      "total_flops_so_far": 7384335584322296.0,
      "budget_used_percent": 7.384335584322296
    },
    {
      "type": "training",
      "description": "Training step 1242",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:45",
      "total_flops_so_far": 7390276642374392.0,
      "budget_used_percent": 7.390276642374392
    },
    {
      "type": "training",
      "description": "Training step 1243",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:46",
      "total_flops_so_far": 7396217700426488.0,
      "budget_used_percent": 7.396217700426488
    },
    {
      "type": "training",
      "description": "Training step 1244",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:46",
      "total_flops_so_far": 7402158758478584.0,
      "budget_used_percent": 7.402158758478585
    },
    {
      "type": "training",
      "description": "Training step 1245",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:46",
      "total_flops_so_far": 7408099816530680.0,
      "budget_used_percent": 7.40809981653068
    },
    {
      "type": "training",
      "description": "Training step 1246",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:47",
      "total_flops_so_far": 7414040874582776.0,
      "budget_used_percent": 7.414040874582777
    },
    {
      "type": "training",
      "description": "Training step 1247",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:47",
      "total_flops_so_far": 7419981932634872.0,
      "budget_used_percent": 7.419981932634872
    },
    {
      "type": "training",
      "description": "Training step 1248",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:47",
      "total_flops_so_far": 7425922990686968.0,
      "budget_used_percent": 7.425922990686969
    },
    {
      "type": "training",
      "description": "Training step 1249",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:48",
      "total_flops_so_far": 7431864048739064.0,
      "budget_used_percent": 7.4318640487390635
    },
    {
      "type": "training",
      "description": "Training step 1250",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:48",
      "total_flops_so_far": 7437805106791160.0,
      "budget_used_percent": 7.437805106791159
    },
    {
      "type": "training",
      "description": "Training step 1251",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:48",
      "total_flops_so_far": 7443746164843256.0,
      "budget_used_percent": 7.4437461648432555
    },
    {
      "type": "training",
      "description": "Training step 1252",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:49",
      "total_flops_so_far": 7449687222895352.0,
      "budget_used_percent": 7.449687222895351
    },
    {
      "type": "training",
      "description": "Training step 1253",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:49",
      "total_flops_so_far": 7455628280947448.0,
      "budget_used_percent": 7.4556282809474475
    },
    {
      "type": "training",
      "description": "Training step 1254",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:49",
      "total_flops_so_far": 7461569338999544.0,
      "budget_used_percent": 7.461569338999544
    },
    {
      "type": "training",
      "description": "Training step 1255",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:50",
      "total_flops_so_far": 7467510397051640.0,
      "budget_used_percent": 7.467510397051639
    },
    {
      "type": "training",
      "description": "Training step 1256",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:50",
      "total_flops_so_far": 7473451455103736.0,
      "budget_used_percent": 7.473451455103736
    },
    {
      "type": "training",
      "description": "Training step 1257",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:50",
      "total_flops_so_far": 7479392513155832.0,
      "budget_used_percent": 7.479392513155831
    },
    {
      "type": "training",
      "description": "Training step 1258",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:51",
      "total_flops_so_far": 7485333571207928.0,
      "budget_used_percent": 7.485333571207928
    },
    {
      "type": "training",
      "description": "Training step 1259",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:51",
      "total_flops_so_far": 7491274629260024.0,
      "budget_used_percent": 7.491274629260023
    },
    {
      "type": "training",
      "description": "Training step 1260",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:51",
      "total_flops_so_far": 7497215687312120.0,
      "budget_used_percent": 7.49721568731212
    },
    {
      "type": "training",
      "description": "Training step 1261",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:52",
      "total_flops_so_far": 7503156745364216.0,
      "budget_used_percent": 7.503156745364216
    },
    {
      "type": "training",
      "description": "Training step 1262",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:52",
      "total_flops_so_far": 7509097803416312.0,
      "budget_used_percent": 7.509097803416312
    },
    {
      "type": "training",
      "description": "Training step 1263",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:52",
      "total_flops_so_far": 7515038861468408.0,
      "budget_used_percent": 7.515038861468408
    },
    {
      "type": "training",
      "description": "Training step 1264",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:53",
      "total_flops_so_far": 7520979919520504.0,
      "budget_used_percent": 7.520979919520504
    },
    {
      "type": "training",
      "description": "Training step 1265",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:53",
      "total_flops_so_far": 7526920977572600.0,
      "budget_used_percent": 7.5269209775726
    },
    {
      "type": "training",
      "description": "Training step 1266",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:53",
      "total_flops_so_far": 7532862035624696.0,
      "budget_used_percent": 7.532862035624696
    },
    {
      "type": "training",
      "description": "Training step 1267",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:54",
      "total_flops_so_far": 7538803093676792.0,
      "budget_used_percent": 7.538803093676792
    },
    {
      "type": "training",
      "description": "Training step 1268",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:54",
      "total_flops_so_far": 7544744151728888.0,
      "budget_used_percent": 7.544744151728889
    },
    {
      "type": "training",
      "description": "Training step 1269",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:54",
      "total_flops_so_far": 7550685209780984.0,
      "budget_used_percent": 7.550685209780984
    },
    {
      "type": "training",
      "description": "Training step 1270",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:55",
      "total_flops_so_far": 7556626267833080.0,
      "budget_used_percent": 7.556626267833081
    },
    {
      "type": "training",
      "description": "Training step 1271",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:55",
      "total_flops_so_far": 7562567325885176.0,
      "budget_used_percent": 7.562567325885176
    },
    {
      "type": "training",
      "description": "Training step 1272",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:55",
      "total_flops_so_far": 7568508383937272.0,
      "budget_used_percent": 7.568508383937273
    },
    {
      "type": "training",
      "description": "Training step 1273",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:56",
      "total_flops_so_far": 7574449441989368.0,
      "budget_used_percent": 7.574449441989368
    },
    {
      "type": "training",
      "description": "Training step 1274",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:56",
      "total_flops_so_far": 7580390500041464.0,
      "budget_used_percent": 7.580390500041465
    },
    {
      "type": "training",
      "description": "Training step 1275",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:56",
      "total_flops_so_far": 7586331558093560.0,
      "budget_used_percent": 7.58633155809356
    },
    {
      "type": "training",
      "description": "Training step 1276",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:57",
      "total_flops_so_far": 7592272616145656.0,
      "budget_used_percent": 7.592272616145657
    },
    {
      "type": "training",
      "description": "Training step 1277",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:57",
      "total_flops_so_far": 7598213674197752.0,
      "budget_used_percent": 7.598213674197753
    },
    {
      "type": "training",
      "description": "Training step 1278",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:57",
      "total_flops_so_far": 7604154732249848.0,
      "budget_used_percent": 7.604154732249848
    },
    {
      "type": "training",
      "description": "Training step 1279",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:58",
      "total_flops_so_far": 7610095790301944.0,
      "budget_used_percent": 7.610095790301943
    },
    {
      "type": "training",
      "description": "Training step 1280",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:58",
      "total_flops_so_far": 7616036848354040.0,
      "budget_used_percent": 7.61603684835404
    },
    {
      "type": "training",
      "description": "Training step 1281",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:58",
      "total_flops_so_far": 7621977906406136.0,
      "budget_used_percent": 7.621977906406135
    },
    {
      "type": "training",
      "description": "Training step 1282",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:59",
      "total_flops_so_far": 7627918964458232.0,
      "budget_used_percent": 7.627918964458232
    },
    {
      "type": "training",
      "description": "Training step 1283",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:59",
      "total_flops_so_far": 7633860022510328.0,
      "budget_used_percent": 7.633860022510327
    },
    {
      "type": "training",
      "description": "Training step 1284",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:45:59",
      "total_flops_so_far": 7639801080562424.0,
      "budget_used_percent": 7.639801080562424
    },
    {
      "type": "training",
      "description": "Training step 1285",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:00",
      "total_flops_so_far": 7645742138614520.0,
      "budget_used_percent": 7.645742138614519
    },
    {
      "type": "training",
      "description": "Training step 1286",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:00",
      "total_flops_so_far": 7651683196666616.0,
      "budget_used_percent": 7.651683196666616
    },
    {
      "type": "training",
      "description": "Training step 1287",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:00",
      "total_flops_so_far": 7657624254718712.0,
      "budget_used_percent": 7.657624254718712
    },
    {
      "type": "training",
      "description": "Training step 1288",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:01",
      "total_flops_so_far": 7663565312770808.0,
      "budget_used_percent": 7.663565312770808
    },
    {
      "type": "training",
      "description": "Training step 1289",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:01",
      "total_flops_so_far": 7669506370822904.0,
      "budget_used_percent": 7.669506370822904
    },
    {
      "type": "training",
      "description": "Training step 1290",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:01",
      "total_flops_so_far": 7675447428875000.0,
      "budget_used_percent": 7.675447428875
    },
    {
      "type": "training",
      "description": "Training step 1291",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:02",
      "total_flops_so_far": 7681388486927096.0,
      "budget_used_percent": 7.681388486927096
    },
    {
      "type": "training",
      "description": "Training step 1292",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:02",
      "total_flops_so_far": 7687329544979192.0,
      "budget_used_percent": 7.687329544979192
    },
    {
      "type": "training",
      "description": "Training step 1293",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:02",
      "total_flops_so_far": 7693270603031288.0,
      "budget_used_percent": 7.693270603031288
    },
    {
      "type": "training",
      "description": "Training step 1294",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:03",
      "total_flops_so_far": 7699211661083384.0,
      "budget_used_percent": 7.6992116610833845
    },
    {
      "type": "training",
      "description": "Training step 1295",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:03",
      "total_flops_so_far": 7705152719135480.0,
      "budget_used_percent": 7.70515271913548
    },
    {
      "type": "training",
      "description": "Training step 1296",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:03",
      "total_flops_so_far": 7711093777187576.0,
      "budget_used_percent": 7.7110937771875765
    },
    {
      "type": "training",
      "description": "Training step 1297",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:04",
      "total_flops_so_far": 7717034835239672.0,
      "budget_used_percent": 7.717034835239672
    },
    {
      "type": "training",
      "description": "Training step 1298",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:04",
      "total_flops_so_far": 7722975893291768.0,
      "budget_used_percent": 7.7229758932917685
    },
    {
      "type": "training",
      "description": "Training step 1299",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:04",
      "total_flops_so_far": 7728916951343864.0,
      "budget_used_percent": 7.728916951343864
    },
    {
      "type": "training",
      "description": "Training step 1300",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:05",
      "total_flops_so_far": 7734858009395960.0,
      "budget_used_percent": 7.7348580093959605
    },
    {
      "type": "training",
      "description": "Training step 1301",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:05",
      "total_flops_so_far": 7740799067448056.0,
      "budget_used_percent": 7.740799067448057
    },
    {
      "type": "training",
      "description": "Training step 1302",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:05",
      "total_flops_so_far": 7746740125500152.0,
      "budget_used_percent": 7.7467401255001525
    },
    {
      "type": "training",
      "description": "Training step 1303",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:06",
      "total_flops_so_far": 7752681183552248.0,
      "budget_used_percent": 7.752681183552249
    },
    {
      "type": "training",
      "description": "Training step 1304",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:06",
      "total_flops_so_far": 7758622241604344.0,
      "budget_used_percent": 7.758622241604344
    },
    {
      "type": "training",
      "description": "Training step 1305",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:06",
      "total_flops_so_far": 7764563299656440.0,
      "budget_used_percent": 7.764563299656441
    },
    {
      "type": "training",
      "description": "Training step 1306",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:07",
      "total_flops_so_far": 7770504357708536.0,
      "budget_used_percent": 7.7705043577085355
    },
    {
      "type": "training",
      "description": "Training step 1307",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:07",
      "total_flops_so_far": 7776445415760632.0,
      "budget_used_percent": 7.776445415760631
    },
    {
      "type": "training",
      "description": "Training step 1308",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:07",
      "total_flops_so_far": 7782386473812728.0,
      "budget_used_percent": 7.7823864738127275
    },
    {
      "type": "training",
      "description": "Training step 1309",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:08",
      "total_flops_so_far": 7788327531864824.0,
      "budget_used_percent": 7.788327531864823
    },
    {
      "type": "training",
      "description": "Training step 1310",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:08",
      "total_flops_so_far": 7794268589916920.0,
      "budget_used_percent": 7.7942685899169195
    },
    {
      "type": "training",
      "description": "Training step 1311",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:08",
      "total_flops_so_far": 7800209647969016.0,
      "budget_used_percent": 7.800209647969016
    },
    {
      "type": "training",
      "description": "Training step 1312",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:09",
      "total_flops_so_far": 7806150706021112.0,
      "budget_used_percent": 7.8061507060211115
    },
    {
      "type": "training",
      "description": "Training step 1313",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:09",
      "total_flops_so_far": 7812091764073208.0,
      "budget_used_percent": 7.812091764073208
    },
    {
      "type": "training",
      "description": "Training step 1314",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:09",
      "total_flops_so_far": 7818032822125304.0,
      "budget_used_percent": 7.8180328221253035
    },
    {
      "type": "training",
      "description": "Training step 1315",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:10",
      "total_flops_so_far": 7823973880177400.0,
      "budget_used_percent": 7.8239738801774
    },
    {
      "type": "training",
      "description": "Training step 1316",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:10",
      "total_flops_so_far": 7829914938229496.0,
      "budget_used_percent": 7.8299149382294955
    },
    {
      "type": "training",
      "description": "Training step 1317",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:10",
      "total_flops_so_far": 7835855996281592.0,
      "budget_used_percent": 7.835855996281592
    },
    {
      "type": "training",
      "description": "Training step 1318",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:11",
      "total_flops_so_far": 7841797054333688.0,
      "budget_used_percent": 7.8417970543336875
    },
    {
      "type": "training",
      "description": "Training step 1319",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:11",
      "total_flops_so_far": 7847738112385784.0,
      "budget_used_percent": 7.847738112385784
    },
    {
      "type": "training",
      "description": "Training step 1320",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:11",
      "total_flops_so_far": 7853679170437880.0,
      "budget_used_percent": 7.85367917043788
    },
    {
      "type": "training",
      "description": "Training step 1321",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:12",
      "total_flops_so_far": 7859620228489976.0,
      "budget_used_percent": 7.859620228489976
    },
    {
      "type": "training",
      "description": "Training step 1322",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:12",
      "total_flops_so_far": 7865561286542072.0,
      "budget_used_percent": 7.865561286542072
    },
    {
      "type": "training",
      "description": "Training step 1323",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:12",
      "total_flops_so_far": 7871502344594168.0,
      "budget_used_percent": 7.871502344594168
    },
    {
      "type": "training",
      "description": "Training step 1324",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:13",
      "total_flops_so_far": 7877443402646264.0,
      "budget_used_percent": 7.877443402646264
    },
    {
      "type": "training",
      "description": "Training step 1325",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:13",
      "total_flops_so_far": 7883384460698360.0,
      "budget_used_percent": 7.88338446069836
    },
    {
      "type": "training",
      "description": "Training step 1326",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:13",
      "total_flops_so_far": 7889325518750456.0,
      "budget_used_percent": 7.889325518750456
    },
    {
      "type": "training",
      "description": "Training step 1327",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:14",
      "total_flops_so_far": 7895266576802552.0,
      "budget_used_percent": 7.895266576802553
    },
    {
      "type": "training",
      "description": "Training step 1328",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:14",
      "total_flops_so_far": 7901207634854648.0,
      "budget_used_percent": 7.901207634854648
    },
    {
      "type": "training",
      "description": "Training step 1329",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:14",
      "total_flops_so_far": 7907148692906744.0,
      "budget_used_percent": 7.907148692906745
    },
    {
      "type": "training",
      "description": "Training step 1330",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:15",
      "total_flops_so_far": 7913089750958840.0,
      "budget_used_percent": 7.91308975095884
    },
    {
      "type": "training",
      "description": "Training step 1331",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:15",
      "total_flops_so_far": 7919030809010936.0,
      "budget_used_percent": 7.919030809010937
    },
    {
      "type": "training",
      "description": "Training step 1332",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:15",
      "total_flops_so_far": 7924971867063032.0,
      "budget_used_percent": 7.924971867063032
    },
    {
      "type": "training",
      "description": "Training step 1333",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:16",
      "total_flops_so_far": 7930912925115128.0,
      "budget_used_percent": 7.930912925115129
    },
    {
      "type": "training",
      "description": "Training step 1334",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:16",
      "total_flops_so_far": 7936853983167224.0,
      "budget_used_percent": 7.936853983167225
    },
    {
      "type": "training",
      "description": "Training step 1335",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:16",
      "total_flops_so_far": 7942795041219320.0,
      "budget_used_percent": 7.94279504121932
    },
    {
      "type": "training",
      "description": "Training step 1336",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:17",
      "total_flops_so_far": 7948736099271416.0,
      "budget_used_percent": 7.948736099271415
    },
    {
      "type": "training",
      "description": "Training step 1337",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:17",
      "total_flops_so_far": 7954677157323512.0,
      "budget_used_percent": 7.954677157323512
    },
    {
      "type": "training",
      "description": "Training step 1338",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:17",
      "total_flops_so_far": 7960618215375608.0,
      "budget_used_percent": 7.960618215375607
    },
    {
      "type": "training",
      "description": "Training step 1339",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:18",
      "total_flops_so_far": 7966559273427704.0,
      "budget_used_percent": 7.966559273427704
    },
    {
      "type": "training",
      "description": "Training step 1340",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:18",
      "total_flops_so_far": 7972500331479800.0,
      "budget_used_percent": 7.972500331479799
    },
    {
      "type": "training",
      "description": "Training step 1341",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:18",
      "total_flops_so_far": 7978441389531896.0,
      "budget_used_percent": 7.978441389531896
    },
    {
      "type": "training",
      "description": "Training step 1342",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:19",
      "total_flops_so_far": 7984382447583992.0,
      "budget_used_percent": 7.984382447583991
    },
    {
      "type": "training",
      "description": "Training step 1343",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:19",
      "total_flops_so_far": 7990323505636088.0,
      "budget_used_percent": 7.990323505636088
    },
    {
      "type": "training",
      "description": "Training step 1344",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:19",
      "total_flops_so_far": 7996264563688184.0,
      "budget_used_percent": 7.996264563688184
    },
    {
      "type": "training",
      "description": "Training step 1345",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:20",
      "total_flops_so_far": 8002205621740280.0,
      "budget_used_percent": 8.00220562174028
    },
    {
      "type": "training",
      "description": "Training step 1346",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:20",
      "total_flops_so_far": 8008146679792376.0,
      "budget_used_percent": 8.008146679792375
    },
    {
      "type": "training",
      "description": "Training step 1347",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:20",
      "total_flops_so_far": 8014087737844472.0,
      "budget_used_percent": 8.014087737844472
    },
    {
      "type": "training",
      "description": "Training step 1348",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:21",
      "total_flops_so_far": 8020028795896568.0,
      "budget_used_percent": 8.020028795896568
    },
    {
      "type": "training",
      "description": "Training step 1349",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:21",
      "total_flops_so_far": 8025969853948664.0,
      "budget_used_percent": 8.025969853948665
    },
    {
      "type": "training",
      "description": "Training step 1350",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:21",
      "total_flops_so_far": 8031910912000760.0,
      "budget_used_percent": 8.03191091200076
    },
    {
      "type": "training",
      "description": "Training step 1351",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:22",
      "total_flops_so_far": 8037851970052856.0,
      "budget_used_percent": 8.037851970052856
    },
    {
      "type": "training",
      "description": "Training step 1352",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:22",
      "total_flops_so_far": 8043793028104952.0,
      "budget_used_percent": 8.043793028104952
    },
    {
      "type": "training",
      "description": "Training step 1353",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:22",
      "total_flops_so_far": 8049734086157048.0,
      "budget_used_percent": 8.049734086157049
    },
    {
      "type": "training",
      "description": "Training step 1354",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:23",
      "total_flops_so_far": 8055675144209144.0,
      "budget_used_percent": 8.055675144209145
    },
    {
      "type": "training",
      "description": "Training step 1355",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:23",
      "total_flops_so_far": 8061616202261240.0,
      "budget_used_percent": 8.06161620226124
    },
    {
      "type": "training",
      "description": "Training step 1356",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:23",
      "total_flops_so_far": 8067557260313336.0,
      "budget_used_percent": 8.067557260313336
    },
    {
      "type": "training",
      "description": "Training step 1357",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:24",
      "total_flops_so_far": 8073498318365432.0,
      "budget_used_percent": 8.073498318365433
    },
    {
      "type": "training",
      "description": "Training step 1358",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:24",
      "total_flops_so_far": 8079439376417528.0,
      "budget_used_percent": 8.079439376417529
    },
    {
      "type": "training",
      "description": "Training step 1359",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:24",
      "total_flops_so_far": 8085380434469624.0,
      "budget_used_percent": 8.085380434469624
    },
    {
      "type": "training",
      "description": "Training step 1360",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:25",
      "total_flops_so_far": 8091321492521720.0,
      "budget_used_percent": 8.09132149252172
    },
    {
      "type": "training",
      "description": "Training step 1361",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:25",
      "total_flops_so_far": 8097262550573816.0,
      "budget_used_percent": 8.097262550573816
    },
    {
      "type": "training",
      "description": "Training step 1362",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:25",
      "total_flops_so_far": 8103203608625912.0,
      "budget_used_percent": 8.103203608625913
    },
    {
      "type": "training",
      "description": "Training step 1363",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:26",
      "total_flops_so_far": 8109144666678008.0,
      "budget_used_percent": 8.109144666678008
    },
    {
      "type": "training",
      "description": "Training step 1364",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:26",
      "total_flops_so_far": 8115085724730104.0,
      "budget_used_percent": 8.115085724730104
    },
    {
      "type": "training",
      "description": "Training step 1365",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:26",
      "total_flops_so_far": 8121026782782200.0,
      "budget_used_percent": 8.121026782782199
    },
    {
      "type": "training",
      "description": "Training step 1366",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:27",
      "total_flops_so_far": 8126967840834296.0,
      "budget_used_percent": 8.126967840834295
    },
    {
      "type": "training",
      "description": "Training step 1367",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:27",
      "total_flops_so_far": 8132908898886392.0,
      "budget_used_percent": 8.132908898886392
    },
    {
      "type": "training",
      "description": "Training step 1368",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:27",
      "total_flops_so_far": 8138849956938488.0,
      "budget_used_percent": 8.138849956938488
    },
    {
      "type": "training",
      "description": "Training step 1369",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:28",
      "total_flops_so_far": 8144791014990584.0,
      "budget_used_percent": 8.144791014990584
    },
    {
      "type": "training",
      "description": "Training step 1370",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:28",
      "total_flops_so_far": 8150732073042680.0,
      "budget_used_percent": 8.150732073042679
    },
    {
      "type": "training",
      "description": "Training step 1371",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:28",
      "total_flops_so_far": 8156673131094776.0,
      "budget_used_percent": 8.156673131094776
    },
    {
      "type": "training",
      "description": "Training step 1372",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:29",
      "total_flops_so_far": 8162614189146872.0,
      "budget_used_percent": 8.162614189146872
    },
    {
      "type": "training",
      "description": "Training step 1373",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:29",
      "total_flops_so_far": 8168555247198968.0,
      "budget_used_percent": 8.168555247198968
    },
    {
      "type": "training",
      "description": "Training step 1374",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:29",
      "total_flops_so_far": 8174496305251064.0,
      "budget_used_percent": 8.174496305251063
    },
    {
      "type": "training",
      "description": "Training step 1375",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:30",
      "total_flops_so_far": 8180437363303160.0,
      "budget_used_percent": 8.18043736330316
    },
    {
      "type": "training",
      "description": "Training step 1376",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:30",
      "total_flops_so_far": 8186378421355256.0,
      "budget_used_percent": 8.186378421355256
    },
    {
      "type": "training",
      "description": "Training step 1377",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:30",
      "total_flops_so_far": 8192319479407352.0,
      "budget_used_percent": 8.192319479407352
    },
    {
      "type": "training",
      "description": "Training step 1378",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:31",
      "total_flops_so_far": 8198260537459448.0,
      "budget_used_percent": 8.198260537459449
    },
    {
      "type": "training",
      "description": "Training step 1379",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:31",
      "total_flops_so_far": 8204201595511544.0,
      "budget_used_percent": 8.204201595511543
    },
    {
      "type": "training",
      "description": "Training step 1380",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:31",
      "total_flops_so_far": 8210142653563640.0,
      "budget_used_percent": 8.21014265356364
    },
    {
      "type": "training",
      "description": "Training step 1381",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:32",
      "total_flops_so_far": 8216083711615736.0,
      "budget_used_percent": 8.216083711615736
    },
    {
      "type": "training",
      "description": "Training step 1382",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:32",
      "total_flops_so_far": 8222024769667832.0,
      "budget_used_percent": 8.222024769667833
    },
    {
      "type": "training",
      "description": "Training step 1383",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:32",
      "total_flops_so_far": 8227965827719928.0,
      "budget_used_percent": 8.227965827719927
    },
    {
      "type": "training",
      "description": "Training step 1384",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:33",
      "total_flops_so_far": 8233906885772024.0,
      "budget_used_percent": 8.233906885772024
    },
    {
      "type": "training",
      "description": "Training step 1385",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:33",
      "total_flops_so_far": 8239847943824120.0,
      "budget_used_percent": 8.23984794382412
    },
    {
      "type": "training",
      "description": "Training step 1386",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:33",
      "total_flops_so_far": 8245789001876216.0,
      "budget_used_percent": 8.245789001876217
    },
    {
      "type": "training",
      "description": "Training step 1387",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:34",
      "total_flops_so_far": 8251730059928312.0,
      "budget_used_percent": 8.251730059928313
    },
    {
      "type": "training",
      "description": "Training step 1388",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:34",
      "total_flops_so_far": 8257671117980408.0,
      "budget_used_percent": 8.257671117980408
    },
    {
      "type": "training",
      "description": "Training step 1389",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:34",
      "total_flops_so_far": 8263612176032504.0,
      "budget_used_percent": 8.263612176032504
    },
    {
      "type": "training",
      "description": "Training step 1390",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:35",
      "total_flops_so_far": 8269553234084600.0,
      "budget_used_percent": 8.2695532340846
    },
    {
      "type": "training",
      "description": "Training step 1391",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:35",
      "total_flops_so_far": 8275494292136696.0,
      "budget_used_percent": 8.275494292136697
    },
    {
      "type": "training",
      "description": "Training step 1392",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:35",
      "total_flops_so_far": 8281435350188792.0,
      "budget_used_percent": 8.281435350188792
    },
    {
      "type": "training",
      "description": "Training step 1393",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:36",
      "total_flops_so_far": 8287376408240888.0,
      "budget_used_percent": 8.287376408240888
    },
    {
      "type": "training",
      "description": "Training step 1394",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:36",
      "total_flops_so_far": 8293317466292984.0,
      "budget_used_percent": 8.293317466292983
    },
    {
      "type": "training",
      "description": "Training step 1395",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:36",
      "total_flops_so_far": 8299258524345080.0,
      "budget_used_percent": 8.29925852434508
    },
    {
      "type": "training",
      "description": "Training step 1396",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:37",
      "total_flops_so_far": 8305199582397176.0,
      "budget_used_percent": 8.305199582397176
    },
    {
      "type": "training",
      "description": "Training step 1397",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:37",
      "total_flops_so_far": 8311140640449272.0,
      "budget_used_percent": 8.311140640449272
    },
    {
      "type": "training",
      "description": "Training step 1398",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:37",
      "total_flops_so_far": 8317081698501368.0,
      "budget_used_percent": 8.317081698501367
    },
    {
      "type": "training",
      "description": "Training step 1399",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:38",
      "total_flops_so_far": 8323022756553464.0,
      "budget_used_percent": 8.323022756553463
    },
    {
      "type": "training",
      "description": "Training step 1400",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:38",
      "total_flops_so_far": 8328963814605560.0,
      "budget_used_percent": 8.32896381460556
    },
    {
      "type": "training",
      "description": "Training step 1401",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:38",
      "total_flops_so_far": 8334904872657656.0,
      "budget_used_percent": 8.334904872657656
    },
    {
      "type": "training",
      "description": "Training step 1402",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:39",
      "total_flops_so_far": 8340845930709752.0,
      "budget_used_percent": 8.340845930709753
    },
    {
      "type": "training",
      "description": "Training step 1403",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:39",
      "total_flops_so_far": 8346786988761848.0,
      "budget_used_percent": 8.346786988761847
    },
    {
      "type": "training",
      "description": "Training step 1404",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:39",
      "total_flops_so_far": 8352728046813944.0,
      "budget_used_percent": 8.352728046813944
    },
    {
      "type": "training",
      "description": "Training step 1405",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:40",
      "total_flops_so_far": 8358669104866040.0,
      "budget_used_percent": 8.35866910486604
    },
    {
      "type": "training",
      "description": "Training step 1406",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:40",
      "total_flops_so_far": 8364610162918136.0,
      "budget_used_percent": 8.364610162918137
    },
    {
      "type": "training",
      "description": "Training step 1407",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:40",
      "total_flops_so_far": 8370551220970232.0,
      "budget_used_percent": 8.370551220970231
    },
    {
      "type": "training",
      "description": "Training step 1408",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:41",
      "total_flops_so_far": 8376492279022328.0,
      "budget_used_percent": 8.376492279022328
    },
    {
      "type": "training",
      "description": "Training step 1409",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:41",
      "total_flops_so_far": 8382433337074424.0,
      "budget_used_percent": 8.382433337074424
    },
    {
      "type": "training",
      "description": "Training step 1410",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:41",
      "total_flops_so_far": 8388374395126520.0,
      "budget_used_percent": 8.38837439512652
    },
    {
      "type": "training",
      "description": "Training step 1411",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:42",
      "total_flops_so_far": 8394315453178616.0,
      "budget_used_percent": 8.394315453178617
    },
    {
      "type": "training",
      "description": "Training step 1412",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:42",
      "total_flops_so_far": 8400256511230712.0,
      "budget_used_percent": 8.400256511230712
    },
    {
      "type": "training",
      "description": "Training step 1413",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:42",
      "total_flops_so_far": 8406197569282808.0,
      "budget_used_percent": 8.406197569282808
    },
    {
      "type": "training",
      "description": "Training step 1414",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:43",
      "total_flops_so_far": 8412138627334904.0,
      "budget_used_percent": 8.412138627334905
    },
    {
      "type": "training",
      "description": "Training step 1415",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:43",
      "total_flops_so_far": 8418079685387000.0,
      "budget_used_percent": 8.418079685387001
    },
    {
      "type": "training",
      "description": "Training step 1416",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:43",
      "total_flops_so_far": 8424020743439096.0,
      "budget_used_percent": 8.424020743439096
    },
    {
      "type": "training",
      "description": "Training step 1417",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:44",
      "total_flops_so_far": 8429961801491192.0,
      "budget_used_percent": 8.429961801491192
    },
    {
      "type": "training",
      "description": "Training step 1418",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:44",
      "total_flops_so_far": 8435902859543288.0,
      "budget_used_percent": 8.435902859543289
    },
    {
      "type": "training",
      "description": "Training step 1419",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:44",
      "total_flops_so_far": 8441843917595384.0,
      "budget_used_percent": 8.441843917595385
    },
    {
      "type": "training",
      "description": "Training step 1420",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:45",
      "total_flops_so_far": 8447784975647480.0,
      "budget_used_percent": 8.44778497564748
    },
    {
      "type": "training",
      "description": "Training step 1421",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:45",
      "total_flops_so_far": 8453726033699576.0,
      "budget_used_percent": 8.453726033699576
    },
    {
      "type": "training",
      "description": "Training step 1422",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:45",
      "total_flops_so_far": 8459667091751672.0,
      "budget_used_percent": 8.45966709175167
    },
    {
      "type": "training",
      "description": "Training step 1423",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:46",
      "total_flops_so_far": 8465608149803768.0,
      "budget_used_percent": 8.465608149803767
    },
    {
      "type": "training",
      "description": "Training step 1424",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:46",
      "total_flops_so_far": 8471549207855864.0,
      "budget_used_percent": 8.471549207855864
    },
    {
      "type": "training",
      "description": "Training step 1425",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:46",
      "total_flops_so_far": 8477490265907960.0,
      "budget_used_percent": 8.47749026590796
    },
    {
      "type": "training",
      "description": "Training step 1426",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:47",
      "total_flops_so_far": 8483431323960056.0,
      "budget_used_percent": 8.483431323960055
    },
    {
      "type": "training",
      "description": "Training step 1427",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:47",
      "total_flops_so_far": 8489372382012152.0,
      "budget_used_percent": 8.489372382012151
    },
    {
      "type": "training",
      "description": "Training step 1428",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:47",
      "total_flops_so_far": 8495313440064248.0,
      "budget_used_percent": 8.495313440064248
    },
    {
      "type": "training",
      "description": "Training step 1429",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:48",
      "total_flops_so_far": 8501254498116344.0,
      "budget_used_percent": 8.501254498116344
    },
    {
      "type": "training",
      "description": "Training step 1430",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:48",
      "total_flops_so_far": 8507195556168440.0,
      "budget_used_percent": 8.50719555616844
    },
    {
      "type": "training",
      "description": "Training step 1431",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:48",
      "total_flops_so_far": 8513136614220536.0,
      "budget_used_percent": 8.513136614220535
    },
    {
      "type": "training",
      "description": "Training step 1432",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:49",
      "total_flops_so_far": 8519077672272632.0,
      "budget_used_percent": 8.519077672272632
    },
    {
      "type": "training",
      "description": "Training step 1433",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:49",
      "total_flops_so_far": 8525018730324728.0,
      "budget_used_percent": 8.525018730324728
    },
    {
      "type": "training",
      "description": "Training step 1434",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:49",
      "total_flops_so_far": 8530959788376824.0,
      "budget_used_percent": 8.530959788376824
    },
    {
      "type": "training",
      "description": "Training step 1435",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:50",
      "total_flops_so_far": 8536900846428920.0,
      "budget_used_percent": 8.53690084642892
    },
    {
      "type": "training",
      "description": "Training step 1436",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:50",
      "total_flops_so_far": 8542841904481016.0,
      "budget_used_percent": 8.542841904481016
    },
    {
      "type": "training",
      "description": "Training step 1437",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:50",
      "total_flops_so_far": 8548782962533112.0,
      "budget_used_percent": 8.548782962533112
    },
    {
      "type": "training",
      "description": "Training step 1438",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:51",
      "total_flops_so_far": 8554724020585208.0,
      "budget_used_percent": 8.554724020585208
    },
    {
      "type": "training",
      "description": "Training step 1439",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:51",
      "total_flops_so_far": 8560665078637304.0,
      "budget_used_percent": 8.560665078637305
    },
    {
      "type": "training",
      "description": "Training step 1440",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:52",
      "total_flops_so_far": 8566606136689400.0,
      "budget_used_percent": 8.5666061366894
    },
    {
      "type": "training",
      "description": "Training step 1441",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:52",
      "total_flops_so_far": 8572547194741496.0,
      "budget_used_percent": 8.572547194741496
    },
    {
      "type": "training",
      "description": "Training step 1442",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:52",
      "total_flops_so_far": 8578488252793592.0,
      "budget_used_percent": 8.578488252793592
    },
    {
      "type": "training",
      "description": "Training step 1443",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:53",
      "total_flops_so_far": 8584429310845688.0,
      "budget_used_percent": 8.584429310845689
    },
    {
      "type": "training",
      "description": "Training step 1444",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:53",
      "total_flops_so_far": 8590370368897784.0,
      "budget_used_percent": 8.590370368897785
    },
    {
      "type": "training",
      "description": "Training step 1445",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:53",
      "total_flops_so_far": 8596311426949880.0,
      "budget_used_percent": 8.59631142694988
    },
    {
      "type": "training",
      "description": "Training step 1446",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:54",
      "total_flops_so_far": 8602252485001976.0,
      "budget_used_percent": 8.602252485001976
    },
    {
      "type": "training",
      "description": "Training step 1447",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:54",
      "total_flops_so_far": 8608193543054072.0,
      "budget_used_percent": 8.608193543054073
    },
    {
      "type": "training",
      "description": "Training step 1448",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:54",
      "total_flops_so_far": 8614134601106168.0,
      "budget_used_percent": 8.61413460110617
    },
    {
      "type": "training",
      "description": "Training step 1449",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:55",
      "total_flops_so_far": 8620075659158264.0,
      "budget_used_percent": 8.620075659158264
    },
    {
      "type": "training",
      "description": "Training step 1450",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:55",
      "total_flops_so_far": 8626016717210360.0,
      "budget_used_percent": 8.626016717210359
    },
    {
      "type": "training",
      "description": "Training step 1451",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:56",
      "total_flops_so_far": 8631957775262456.0,
      "budget_used_percent": 8.631957775262455
    },
    {
      "type": "training",
      "description": "Training step 1452",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:56",
      "total_flops_so_far": 8637898833314552.0,
      "budget_used_percent": 8.637898833314551
    },
    {
      "type": "training",
      "description": "Training step 1453",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:56",
      "total_flops_so_far": 8643839891366648.0,
      "budget_used_percent": 8.643839891366648
    },
    {
      "type": "training",
      "description": "Training step 1454",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:57",
      "total_flops_so_far": 8649780949418744.0,
      "budget_used_percent": 8.649780949418744
    },
    {
      "type": "training",
      "description": "Training step 1455",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:57",
      "total_flops_so_far": 8655722007470840.0,
      "budget_used_percent": 8.655722007470839
    },
    {
      "type": "training",
      "description": "Training step 1456",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:57",
      "total_flops_so_far": 8661663065522936.0,
      "budget_used_percent": 8.661663065522935
    },
    {
      "type": "training",
      "description": "Training step 1457",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:58",
      "total_flops_so_far": 8667604123575032.0,
      "budget_used_percent": 8.667604123575032
    },
    {
      "type": "training",
      "description": "Training step 1458",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:58",
      "total_flops_so_far": 8673545181627128.0,
      "budget_used_percent": 8.673545181627128
    },
    {
      "type": "training",
      "description": "Training step 1459",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:58",
      "total_flops_so_far": 8679486239679224.0,
      "budget_used_percent": 8.679486239679225
    },
    {
      "type": "training",
      "description": "Training step 1460",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:59",
      "total_flops_so_far": 8685427297731320.0,
      "budget_used_percent": 8.68542729773132
    },
    {
      "type": "training",
      "description": "Training step 1461",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:59",
      "total_flops_so_far": 8691368355783416.0,
      "budget_used_percent": 8.691368355783416
    },
    {
      "type": "training",
      "description": "Training step 1462",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:46:59",
      "total_flops_so_far": 8697309413835512.0,
      "budget_used_percent": 8.697309413835512
    },
    {
      "type": "training",
      "description": "Training step 1463",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:00",
      "total_flops_so_far": 8703250471887608.0,
      "budget_used_percent": 8.703250471887609
    },
    {
      "type": "training",
      "description": "Training step 1464",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:00",
      "total_flops_so_far": 8709191529939704.0,
      "budget_used_percent": 8.709191529939703
    },
    {
      "type": "training",
      "description": "Training step 1465",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:00",
      "total_flops_so_far": 8715132587991800.0,
      "budget_used_percent": 8.7151325879918
    },
    {
      "type": "training",
      "description": "Training step 1466",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:01",
      "total_flops_so_far": 8721073646043896.0,
      "budget_used_percent": 8.721073646043896
    },
    {
      "type": "training",
      "description": "Training step 1467",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:01",
      "total_flops_so_far": 8727014704095992.0,
      "budget_used_percent": 8.727014704095993
    },
    {
      "type": "training",
      "description": "Training step 1468",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:01",
      "total_flops_so_far": 8732955762148088.0,
      "budget_used_percent": 8.732955762148089
    },
    {
      "type": "training",
      "description": "Training step 1469",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:02",
      "total_flops_so_far": 8738896820200184.0,
      "budget_used_percent": 8.738896820200184
    },
    {
      "type": "training",
      "description": "Training step 1470",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:02",
      "total_flops_so_far": 8744837878252280.0,
      "budget_used_percent": 8.74483787825228
    },
    {
      "type": "training",
      "description": "Training step 1471",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:02",
      "total_flops_so_far": 8750778936304376.0,
      "budget_used_percent": 8.750778936304377
    },
    {
      "type": "training",
      "description": "Training step 1472",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:03",
      "total_flops_so_far": 8756719994356472.0,
      "budget_used_percent": 8.756719994356473
    },
    {
      "type": "training",
      "description": "Training step 1473",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:03",
      "total_flops_so_far": 8762661052408568.0,
      "budget_used_percent": 8.762661052408568
    },
    {
      "type": "training",
      "description": "Training step 1474",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:03",
      "total_flops_so_far": 8768602110460664.0,
      "budget_used_percent": 8.768602110460664
    },
    {
      "type": "training",
      "description": "Training step 1475",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:04",
      "total_flops_so_far": 8774543168512760.0,
      "budget_used_percent": 8.77454316851276
    },
    {
      "type": "training",
      "description": "Training step 1476",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:04",
      "total_flops_so_far": 8780484226564856.0,
      "budget_used_percent": 8.780484226564857
    },
    {
      "type": "training",
      "description": "Training step 1477",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:04",
      "total_flops_so_far": 8786425284616952.0,
      "budget_used_percent": 8.786425284616952
    },
    {
      "type": "training",
      "description": "Training step 1478",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:05",
      "total_flops_so_far": 8792366342669048.0,
      "budget_used_percent": 8.792366342669048
    },
    {
      "type": "training",
      "description": "Training step 1479",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:05",
      "total_flops_so_far": 8798307400721144.0,
      "budget_used_percent": 8.798307400721143
    },
    {
      "type": "training",
      "description": "Training step 1480",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:05",
      "total_flops_so_far": 8804248458773240.0,
      "budget_used_percent": 8.80424845877324
    },
    {
      "type": "training",
      "description": "Training step 1481",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:06",
      "total_flops_so_far": 8810189516825336.0,
      "budget_used_percent": 8.810189516825336
    },
    {
      "type": "training",
      "description": "Training step 1482",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:06",
      "total_flops_so_far": 8816130574877432.0,
      "budget_used_percent": 8.816130574877432
    },
    {
      "type": "training",
      "description": "Training step 1483",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:06",
      "total_flops_so_far": 8822071632929528.0,
      "budget_used_percent": 8.822071632929527
    },
    {
      "type": "training",
      "description": "Training step 1484",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:07",
      "total_flops_so_far": 8828012690981624.0,
      "budget_used_percent": 8.828012690981623
    },
    {
      "type": "training",
      "description": "Training step 1485",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:07",
      "total_flops_so_far": 8833953749033720.0,
      "budget_used_percent": 8.83395374903372
    },
    {
      "type": "training",
      "description": "Training step 1486",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:07",
      "total_flops_so_far": 8839894807085816.0,
      "budget_used_percent": 8.839894807085816
    },
    {
      "type": "training",
      "description": "Training step 1487",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:08",
      "total_flops_so_far": 8845835865137912.0,
      "budget_used_percent": 8.845835865137913
    },
    {
      "type": "training",
      "description": "Training step 1488",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:08",
      "total_flops_so_far": 8851776923190008.0,
      "budget_used_percent": 8.851776923190007
    },
    {
      "type": "training",
      "description": "Training step 1489",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:08",
      "total_flops_so_far": 8857717981242104.0,
      "budget_used_percent": 8.857717981242104
    },
    {
      "type": "training",
      "description": "Training step 1490",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:09",
      "total_flops_so_far": 8863659039294200.0,
      "budget_used_percent": 8.8636590392942
    },
    {
      "type": "training",
      "description": "Training step 1491",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:09",
      "total_flops_so_far": 8869600097346296.0,
      "budget_used_percent": 8.869600097346297
    },
    {
      "type": "training",
      "description": "Training step 1492",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:09",
      "total_flops_so_far": 8875541155398392.0,
      "budget_used_percent": 8.875541155398391
    },
    {
      "type": "training",
      "description": "Training step 1493",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:10",
      "total_flops_so_far": 8881482213450488.0,
      "budget_used_percent": 8.881482213450488
    },
    {
      "type": "training",
      "description": "Training step 1494",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:10",
      "total_flops_so_far": 8887423271502584.0,
      "budget_used_percent": 8.887423271502584
    },
    {
      "type": "training",
      "description": "Training step 1495",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:10",
      "total_flops_so_far": 8893364329554680.0,
      "budget_used_percent": 8.89336432955468
    },
    {
      "type": "training",
      "description": "Training step 1496",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:11",
      "total_flops_so_far": 8899305387606776.0,
      "budget_used_percent": 8.899305387606777
    },
    {
      "type": "training",
      "description": "Training step 1497",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:11",
      "total_flops_so_far": 8905246445658872.0,
      "budget_used_percent": 8.905246445658872
    },
    {
      "type": "training",
      "description": "Training step 1498",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:11",
      "total_flops_so_far": 8911187503710968.0,
      "budget_used_percent": 8.911187503710968
    },
    {
      "type": "training",
      "description": "Training step 1499",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:12",
      "total_flops_so_far": 8917128561763064.0,
      "budget_used_percent": 8.917128561763064
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 552526568288.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:14",
      "total_flops_so_far": 8917681088331352.0,
      "budget_used_percent": 8.917681088331351
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 556130834256.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:16",
      "total_flops_so_far": 8918237219165608.0,
      "budget_used_percent": 8.918237219165608
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 554328341080.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:17",
      "total_flops_so_far": 8918791547506688.0,
      "budget_used_percent": 8.918791547506688
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 552526568288.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:19",
      "total_flops_so_far": 8919344074074976.0,
      "budget_used_percent": 8.919344074074976
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 555229497620.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:21",
      "total_flops_so_far": 8919899303572596.0,
      "budget_used_percent": 8.919899303572596
    },
    {
      "type": "training",
      "description": "Training step 1500",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:21",
      "total_flops_so_far": 8925840361624692.0,
      "budget_used_percent": 8.925840361624692
    },
    {
      "type": "training",
      "description": "Training step 1501",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:21",
      "total_flops_so_far": 8931781419676788.0,
      "budget_used_percent": 8.931781419676788
    },
    {
      "type": "training",
      "description": "Training step 1502",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:22",
      "total_flops_so_far": 8937722477728884.0,
      "budget_used_percent": 8.937722477728883
    },
    {
      "type": "training",
      "description": "Training step 1503",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:22",
      "total_flops_so_far": 8943663535780980.0,
      "budget_used_percent": 8.94366353578098
    },
    {
      "type": "training",
      "description": "Training step 1504",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:22",
      "total_flops_so_far": 8949604593833076.0,
      "budget_used_percent": 8.949604593833076
    },
    {
      "type": "training",
      "description": "Training step 1505",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:23",
      "total_flops_so_far": 8955545651885172.0,
      "budget_used_percent": 8.955545651885172
    },
    {
      "type": "training",
      "description": "Training step 1506",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:23",
      "total_flops_so_far": 8961486709937268.0,
      "budget_used_percent": 8.961486709937267
    },
    {
      "type": "training",
      "description": "Training step 1507",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:23",
      "total_flops_so_far": 8967427767989364.0,
      "budget_used_percent": 8.967427767989363
    },
    {
      "type": "training",
      "description": "Training step 1508",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:24",
      "total_flops_so_far": 8973368826041460.0,
      "budget_used_percent": 8.97336882604146
    },
    {
      "type": "training",
      "description": "Training step 1509",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:24",
      "total_flops_so_far": 8979309884093556.0,
      "budget_used_percent": 8.979309884093556
    },
    {
      "type": "training",
      "description": "Training step 1510",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:24",
      "total_flops_so_far": 8985250942145652.0,
      "budget_used_percent": 8.985250942145653
    },
    {
      "type": "training",
      "description": "Training step 1511",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:25",
      "total_flops_so_far": 8991192000197748.0,
      "budget_used_percent": 8.991192000197747
    },
    {
      "type": "training",
      "description": "Training step 1512",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:25",
      "total_flops_so_far": 8997133058249844.0,
      "budget_used_percent": 8.997133058249844
    },
    {
      "type": "training",
      "description": "Training step 1513",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:25",
      "total_flops_so_far": 9003074116301940.0,
      "budget_used_percent": 9.00307411630194
    },
    {
      "type": "training",
      "description": "Training step 1514",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:26",
      "total_flops_so_far": 9009015174354036.0,
      "budget_used_percent": 9.009015174354037
    },
    {
      "type": "training",
      "description": "Training step 1515",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:26",
      "total_flops_so_far": 9014956232406132.0,
      "budget_used_percent": 9.014956232406131
    },
    {
      "type": "training",
      "description": "Training step 1516",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:26",
      "total_flops_so_far": 9020897290458228.0,
      "budget_used_percent": 9.020897290458228
    },
    {
      "type": "training",
      "description": "Training step 1517",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:27",
      "total_flops_so_far": 9026838348510324.0,
      "budget_used_percent": 9.026838348510324
    },
    {
      "type": "training",
      "description": "Training step 1518",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:27",
      "total_flops_so_far": 9032779406562420.0,
      "budget_used_percent": 9.03277940656242
    },
    {
      "type": "training",
      "description": "Training step 1519",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:27",
      "total_flops_so_far": 9038720464614516.0,
      "budget_used_percent": 9.038720464614517
    },
    {
      "type": "training",
      "description": "Training step 1520",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:28",
      "total_flops_so_far": 9044661522666612.0,
      "budget_used_percent": 9.044661522666612
    },
    {
      "type": "training",
      "description": "Training step 1521",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:28",
      "total_flops_so_far": 9050602580718708.0,
      "budget_used_percent": 9.050602580718708
    },
    {
      "type": "training",
      "description": "Training step 1522",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:28",
      "total_flops_so_far": 9056543638770804.0,
      "budget_used_percent": 9.056543638770805
    },
    {
      "type": "training",
      "description": "Training step 1523",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:29",
      "total_flops_so_far": 9062484696822900.0,
      "budget_used_percent": 9.062484696822901
    },
    {
      "type": "training",
      "description": "Training step 1524",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:29",
      "total_flops_so_far": 9068425754874996.0,
      "budget_used_percent": 9.068425754874996
    },
    {
      "type": "training",
      "description": "Training step 1525",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:29",
      "total_flops_so_far": 9074366812927092.0,
      "budget_used_percent": 9.074366812927092
    },
    {
      "type": "training",
      "description": "Training step 1526",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:30",
      "total_flops_so_far": 9080307870979188.0,
      "budget_used_percent": 9.080307870979187
    },
    {
      "type": "training",
      "description": "Training step 1527",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:30",
      "total_flops_so_far": 9086248929031284.0,
      "budget_used_percent": 9.086248929031283
    },
    {
      "type": "training",
      "description": "Training step 1528",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:30",
      "total_flops_so_far": 9092189987083380.0,
      "budget_used_percent": 9.09218998708338
    },
    {
      "type": "training",
      "description": "Training step 1529",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:31",
      "total_flops_so_far": 9098131045135476.0,
      "budget_used_percent": 9.098131045135476
    },
    {
      "type": "training",
      "description": "Training step 1530",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:31",
      "total_flops_so_far": 9104072103187572.0,
      "budget_used_percent": 9.10407210318757
    },
    {
      "type": "training",
      "description": "Training step 1531",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:31",
      "total_flops_so_far": 9110013161239668.0,
      "budget_used_percent": 9.110013161239667
    },
    {
      "type": "training",
      "description": "Training step 1532",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:32",
      "total_flops_so_far": 9115954219291764.0,
      "budget_used_percent": 9.115954219291764
    },
    {
      "type": "training",
      "description": "Training step 1533",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:32",
      "total_flops_so_far": 9121895277343860.0,
      "budget_used_percent": 9.12189527734386
    },
    {
      "type": "training",
      "description": "Training step 1534",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:32",
      "total_flops_so_far": 9127836335395956.0,
      "budget_used_percent": 9.127836335395955
    },
    {
      "type": "training",
      "description": "Training step 1535",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:33",
      "total_flops_so_far": 9133777393448052.0,
      "budget_used_percent": 9.133777393448051
    },
    {
      "type": "training",
      "description": "Training step 1536",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:33",
      "total_flops_so_far": 9139718451500148.0,
      "budget_used_percent": 9.139718451500148
    },
    {
      "type": "training",
      "description": "Training step 1537",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:33",
      "total_flops_so_far": 9145659509552244.0,
      "budget_used_percent": 9.145659509552244
    },
    {
      "type": "training",
      "description": "Training step 1538",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:34",
      "total_flops_so_far": 9151600567604340.0,
      "budget_used_percent": 9.15160056760434
    },
    {
      "type": "training",
      "description": "Training step 1539",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:34",
      "total_flops_so_far": 9157541625656436.0,
      "budget_used_percent": 9.157541625656435
    },
    {
      "type": "training",
      "description": "Training step 1540",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:34",
      "total_flops_so_far": 9163482683708532.0,
      "budget_used_percent": 9.163482683708532
    },
    {
      "type": "training",
      "description": "Training step 1541",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:35",
      "total_flops_so_far": 9169423741760628.0,
      "budget_used_percent": 9.169423741760628
    },
    {
      "type": "training",
      "description": "Training step 1542",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:35",
      "total_flops_so_far": 9175364799812724.0,
      "budget_used_percent": 9.175364799812725
    },
    {
      "type": "training",
      "description": "Training step 1543",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:35",
      "total_flops_so_far": 9181305857864820.0,
      "budget_used_percent": 9.181305857864821
    },
    {
      "type": "training",
      "description": "Training step 1544",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:36",
      "total_flops_so_far": 9187246915916916.0,
      "budget_used_percent": 9.187246915916916
    },
    {
      "type": "training",
      "description": "Training step 1545",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:36",
      "total_flops_so_far": 9193187973969012.0,
      "budget_used_percent": 9.193187973969012
    },
    {
      "type": "training",
      "description": "Training step 1546",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:36",
      "total_flops_so_far": 9199129032021108.0,
      "budget_used_percent": 9.199129032021109
    },
    {
      "type": "training",
      "description": "Training step 1547",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:37",
      "total_flops_so_far": 9205070090073204.0,
      "budget_used_percent": 9.205070090073205
    },
    {
      "type": "training",
      "description": "Training step 1548",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:37",
      "total_flops_so_far": 9211011148125300.0,
      "budget_used_percent": 9.2110111481253
    },
    {
      "type": "training",
      "description": "Training step 1549",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:37",
      "total_flops_so_far": 9216952206177396.0,
      "budget_used_percent": 9.216952206177396
    },
    {
      "type": "training",
      "description": "Training step 1550",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:38",
      "total_flops_so_far": 9222893264229492.0,
      "budget_used_percent": 9.222893264229493
    },
    {
      "type": "training",
      "description": "Training step 1551",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:38",
      "total_flops_so_far": 9228834322281588.0,
      "budget_used_percent": 9.228834322281589
    },
    {
      "type": "training",
      "description": "Training step 1552",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:39",
      "total_flops_so_far": 9234775380333684.0,
      "budget_used_percent": 9.234775380333685
    },
    {
      "type": "training",
      "description": "Training step 1553",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:39",
      "total_flops_so_far": 9240716438385780.0,
      "budget_used_percent": 9.24071643838578
    },
    {
      "type": "training",
      "description": "Training step 1554",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:39",
      "total_flops_so_far": 9246657496437876.0,
      "budget_used_percent": 9.246657496437875
    },
    {
      "type": "training",
      "description": "Training step 1555",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:40",
      "total_flops_so_far": 9252598554489972.0,
      "budget_used_percent": 9.252598554489971
    },
    {
      "type": "training",
      "description": "Training step 1556",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:40",
      "total_flops_so_far": 9258539612542068.0,
      "budget_used_percent": 9.258539612542068
    },
    {
      "type": "training",
      "description": "Training step 1557",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:40",
      "total_flops_so_far": 9264480670594164.0,
      "budget_used_percent": 9.264480670594164
    },
    {
      "type": "training",
      "description": "Training step 1558",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:41",
      "total_flops_so_far": 9270421728646260.0,
      "budget_used_percent": 9.270421728646259
    },
    {
      "type": "training",
      "description": "Training step 1559",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:41",
      "total_flops_so_far": 9276362786698356.0,
      "budget_used_percent": 9.276362786698355
    },
    {
      "type": "training",
      "description": "Training step 1560",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:41",
      "total_flops_so_far": 9282303844750452.0,
      "budget_used_percent": 9.282303844750452
    },
    {
      "type": "training",
      "description": "Training step 1561",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:42",
      "total_flops_so_far": 9288244902802548.0,
      "budget_used_percent": 9.288244902802548
    },
    {
      "type": "training",
      "description": "Training step 1562",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:42",
      "total_flops_so_far": 9294185960854644.0,
      "budget_used_percent": 9.294185960854644
    },
    {
      "type": "training",
      "description": "Training step 1563",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:42",
      "total_flops_so_far": 9300127018906740.0,
      "budget_used_percent": 9.300127018906739
    },
    {
      "type": "training",
      "description": "Training step 1564",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:43",
      "total_flops_so_far": 9306068076958836.0,
      "budget_used_percent": 9.306068076958836
    },
    {
      "type": "training",
      "description": "Training step 1565",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:43",
      "total_flops_so_far": 9312009135010932.0,
      "budget_used_percent": 9.312009135010932
    },
    {
      "type": "training",
      "description": "Training step 1566",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:43",
      "total_flops_so_far": 9317950193063028.0,
      "budget_used_percent": 9.317950193063028
    },
    {
      "type": "training",
      "description": "Training step 1567",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:44",
      "total_flops_so_far": 9323891251115124.0,
      "budget_used_percent": 9.323891251115125
    },
    {
      "type": "training",
      "description": "Training step 1568",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:44",
      "total_flops_so_far": 9329832309167220.0,
      "budget_used_percent": 9.32983230916722
    },
    {
      "type": "training",
      "description": "Training step 1569",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:44",
      "total_flops_so_far": 9335773367219316.0,
      "budget_used_percent": 9.335773367219316
    },
    {
      "type": "training",
      "description": "Training step 1570",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:45",
      "total_flops_so_far": 9341714425271412.0,
      "budget_used_percent": 9.341714425271412
    },
    {
      "type": "training",
      "description": "Training step 1571",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:45",
      "total_flops_so_far": 9347655483323508.0,
      "budget_used_percent": 9.347655483323509
    },
    {
      "type": "training",
      "description": "Training step 1572",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:45",
      "total_flops_so_far": 9353596541375604.0,
      "budget_used_percent": 9.353596541375603
    },
    {
      "type": "training",
      "description": "Training step 1573",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:46",
      "total_flops_so_far": 9359537599427700.0,
      "budget_used_percent": 9.3595375994277
    },
    {
      "type": "training",
      "description": "Training step 1574",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:46",
      "total_flops_so_far": 9365478657479796.0,
      "budget_used_percent": 9.365478657479796
    },
    {
      "type": "training",
      "description": "Training step 1575",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:46",
      "total_flops_so_far": 9371419715531892.0,
      "budget_used_percent": 9.371419715531893
    },
    {
      "type": "training",
      "description": "Training step 1576",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:47",
      "total_flops_so_far": 9377360773583988.0,
      "budget_used_percent": 9.37736077358399
    },
    {
      "type": "training",
      "description": "Training step 1577",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:47",
      "total_flops_so_far": 9383301831636084.0,
      "budget_used_percent": 9.383301831636084
    },
    {
      "type": "training",
      "description": "Training step 1578",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:47",
      "total_flops_so_far": 9389242889688180.0,
      "budget_used_percent": 9.38924288968818
    },
    {
      "type": "training",
      "description": "Training step 1579",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:48",
      "total_flops_so_far": 9395183947740276.0,
      "budget_used_percent": 9.395183947740277
    },
    {
      "type": "training",
      "description": "Training step 1580",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:48",
      "total_flops_so_far": 9401125005792372.0,
      "budget_used_percent": 9.401125005792373
    },
    {
      "type": "training",
      "description": "Training step 1581",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:48",
      "total_flops_so_far": 9407066063844468.0,
      "budget_used_percent": 9.407066063844468
    },
    {
      "type": "training",
      "description": "Training step 1582",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:49",
      "total_flops_so_far": 9413007121896564.0,
      "budget_used_percent": 9.413007121896564
    },
    {
      "type": "training",
      "description": "Training step 1583",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:49",
      "total_flops_so_far": 9418948179948660.0,
      "budget_used_percent": 9.418948179948659
    },
    {
      "type": "training",
      "description": "Training step 1584",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:49",
      "total_flops_so_far": 9424889238000756.0,
      "budget_used_percent": 9.424889238000755
    },
    {
      "type": "training",
      "description": "Training step 1585",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:50",
      "total_flops_so_far": 9430830296052852.0,
      "budget_used_percent": 9.430830296052852
    },
    {
      "type": "training",
      "description": "Training step 1586",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:50",
      "total_flops_so_far": 9436771354104948.0,
      "budget_used_percent": 9.436771354104948
    },
    {
      "type": "training",
      "description": "Training step 1587",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:50",
      "total_flops_so_far": 9442712412157044.0,
      "budget_used_percent": 9.442712412157043
    },
    {
      "type": "training",
      "description": "Training step 1588",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:51",
      "total_flops_so_far": 9448653470209140.0,
      "budget_used_percent": 9.44865347020914
    },
    {
      "type": "training",
      "description": "Training step 1589",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:51",
      "total_flops_so_far": 9454594528261236.0,
      "budget_used_percent": 9.454594528261236
    },
    {
      "type": "training",
      "description": "Training step 1590",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:51",
      "total_flops_so_far": 9460535586313332.0,
      "budget_used_percent": 9.460535586313332
    },
    {
      "type": "training",
      "description": "Training step 1591",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:52",
      "total_flops_so_far": 9466476644365428.0,
      "budget_used_percent": 9.466476644365427
    },
    {
      "type": "training",
      "description": "Training step 1592",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:52",
      "total_flops_so_far": 9472417702417524.0,
      "budget_used_percent": 9.472417702417523
    },
    {
      "type": "training",
      "description": "Training step 1593",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:52",
      "total_flops_so_far": 9478358760469620.0,
      "budget_used_percent": 9.47835876046962
    },
    {
      "type": "training",
      "description": "Training step 1594",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:53",
      "total_flops_so_far": 9484299818521716.0,
      "budget_used_percent": 9.484299818521716
    },
    {
      "type": "training",
      "description": "Training step 1595",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:53",
      "total_flops_so_far": 9490240876573812.0,
      "budget_used_percent": 9.490240876573813
    },
    {
      "type": "training",
      "description": "Training step 1596",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:53",
      "total_flops_so_far": 9496181934625908.0,
      "budget_used_percent": 9.496181934625907
    },
    {
      "type": "training",
      "description": "Training step 1597",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:54",
      "total_flops_so_far": 9502122992678004.0,
      "budget_used_percent": 9.502122992678004
    },
    {
      "type": "training",
      "description": "Training step 1598",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:54",
      "total_flops_so_far": 9508064050730100.0,
      "budget_used_percent": 9.5080640507301
    },
    {
      "type": "training",
      "description": "Training step 1599",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:55",
      "total_flops_so_far": 9514005108782196.0,
      "budget_used_percent": 9.514005108782197
    },
    {
      "type": "training",
      "description": "Training step 1600",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:55",
      "total_flops_so_far": 9519946166834292.0,
      "budget_used_percent": 9.519946166834291
    },
    {
      "type": "training",
      "description": "Training step 1601",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:55",
      "total_flops_so_far": 9525887224886388.0,
      "budget_used_percent": 9.525887224886388
    },
    {
      "type": "training",
      "description": "Training step 1602",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:56",
      "total_flops_so_far": 9531828282938484.0,
      "budget_used_percent": 9.531828282938484
    },
    {
      "type": "training",
      "description": "Training step 1603",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:56",
      "total_flops_so_far": 9537769340990580.0,
      "budget_used_percent": 9.53776934099058
    },
    {
      "type": "training",
      "description": "Training step 1604",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:56",
      "total_flops_so_far": 9543710399042676.0,
      "budget_used_percent": 9.543710399042677
    },
    {
      "type": "training",
      "description": "Training step 1605",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:57",
      "total_flops_so_far": 9549651457094772.0,
      "budget_used_percent": 9.549651457094772
    },
    {
      "type": "training",
      "description": "Training step 1606",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:57",
      "total_flops_so_far": 9555592515146868.0,
      "budget_used_percent": 9.555592515146868
    },
    {
      "type": "training",
      "description": "Training step 1607",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:57",
      "total_flops_so_far": 9561533573198964.0,
      "budget_used_percent": 9.561533573198965
    },
    {
      "type": "training",
      "description": "Training step 1608",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:58",
      "total_flops_so_far": 9567474631251060.0,
      "budget_used_percent": 9.567474631251061
    },
    {
      "type": "training",
      "description": "Training step 1609",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:58",
      "total_flops_so_far": 9573415689303156.0,
      "budget_used_percent": 9.573415689303157
    },
    {
      "type": "training",
      "description": "Training step 1610",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:58",
      "total_flops_so_far": 9579356747355252.0,
      "budget_used_percent": 9.579356747355252
    },
    {
      "type": "training",
      "description": "Training step 1611",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:59",
      "total_flops_so_far": 9585297805407348.0,
      "budget_used_percent": 9.585297805407347
    },
    {
      "type": "training",
      "description": "Training step 1612",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:59",
      "total_flops_so_far": 9591238863459444.0,
      "budget_used_percent": 9.591238863459443
    },
    {
      "type": "training",
      "description": "Training step 1613",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:47:59",
      "total_flops_so_far": 9597179921511540.0,
      "budget_used_percent": 9.59717992151154
    },
    {
      "type": "training",
      "description": "Training step 1614",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:00",
      "total_flops_so_far": 9603120979563636.0,
      "budget_used_percent": 9.603120979563636
    },
    {
      "type": "training",
      "description": "Training step 1615",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:00",
      "total_flops_so_far": 9609062037615732.0,
      "budget_used_percent": 9.60906203761573
    },
    {
      "type": "training",
      "description": "Training step 1616",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:00",
      "total_flops_so_far": 9615003095667828.0,
      "budget_used_percent": 9.615003095667827
    },
    {
      "type": "training",
      "description": "Training step 1617",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:01",
      "total_flops_so_far": 9620944153719924.0,
      "budget_used_percent": 9.620944153719924
    },
    {
      "type": "training",
      "description": "Training step 1618",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:01",
      "total_flops_so_far": 9626885211772020.0,
      "budget_used_percent": 9.62688521177202
    },
    {
      "type": "training",
      "description": "Training step 1619",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:01",
      "total_flops_so_far": 9632826269824116.0,
      "budget_used_percent": 9.632826269824116
    },
    {
      "type": "training",
      "description": "Training step 1620",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:02",
      "total_flops_so_far": 9638767327876212.0,
      "budget_used_percent": 9.638767327876211
    },
    {
      "type": "training",
      "description": "Training step 1621",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:02",
      "total_flops_so_far": 9644708385928308.0,
      "budget_used_percent": 9.644708385928308
    },
    {
      "type": "training",
      "description": "Training step 1622",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:02",
      "total_flops_so_far": 9650649443980404.0,
      "budget_used_percent": 9.650649443980404
    },
    {
      "type": "training",
      "description": "Training step 1623",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:03",
      "total_flops_so_far": 9656590502032500.0,
      "budget_used_percent": 9.6565905020325
    },
    {
      "type": "training",
      "description": "Training step 1624",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:03",
      "total_flops_so_far": 9662531560084596.0,
      "budget_used_percent": 9.662531560084595
    },
    {
      "type": "training",
      "description": "Training step 1625",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:03",
      "total_flops_so_far": 9668472618136692.0,
      "budget_used_percent": 9.668472618136692
    },
    {
      "type": "training",
      "description": "Training step 1626",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:04",
      "total_flops_so_far": 9674413676188788.0,
      "budget_used_percent": 9.674413676188788
    },
    {
      "type": "training",
      "description": "Training step 1627",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:04",
      "total_flops_so_far": 9680354734240884.0,
      "budget_used_percent": 9.680354734240884
    },
    {
      "type": "training",
      "description": "Training step 1628",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:04",
      "total_flops_so_far": 9686295792292980.0,
      "budget_used_percent": 9.68629579229298
    },
    {
      "type": "training",
      "description": "Training step 1629",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:05",
      "total_flops_so_far": 9692236850345076.0,
      "budget_used_percent": 9.692236850345076
    },
    {
      "type": "training",
      "description": "Training step 1630",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:05",
      "total_flops_so_far": 9698177908397172.0,
      "budget_used_percent": 9.698177908397172
    },
    {
      "type": "training",
      "description": "Training step 1631",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:05",
      "total_flops_so_far": 9704118966449268.0,
      "budget_used_percent": 9.704118966449268
    },
    {
      "type": "training",
      "description": "Training step 1632",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:06",
      "total_flops_so_far": 9710060024501364.0,
      "budget_used_percent": 9.710060024501365
    },
    {
      "type": "training",
      "description": "Training step 1633",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:06",
      "total_flops_so_far": 9716001082553460.0,
      "budget_used_percent": 9.716001082553461
    },
    {
      "type": "training",
      "description": "Training step 1634",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:07",
      "total_flops_so_far": 9721942140605556.0,
      "budget_used_percent": 9.721942140605556
    },
    {
      "type": "training",
      "description": "Training step 1635",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:07",
      "total_flops_so_far": 9727883198657652.0,
      "budget_used_percent": 9.727883198657652
    },
    {
      "type": "training",
      "description": "Training step 1636",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:07",
      "total_flops_so_far": 9733824256709748.0,
      "budget_used_percent": 9.733824256709749
    },
    {
      "type": "training",
      "description": "Training step 1637",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:08",
      "total_flops_so_far": 9739765314761844.0,
      "budget_used_percent": 9.739765314761845
    },
    {
      "type": "training",
      "description": "Training step 1638",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:08",
      "total_flops_so_far": 9745706372813940.0,
      "budget_used_percent": 9.74570637281394
    },
    {
      "type": "training",
      "description": "Training step 1639",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:08",
      "total_flops_so_far": 9751647430866036.0,
      "budget_used_percent": 9.751647430866036
    },
    {
      "type": "training",
      "description": "Training step 1640",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:09",
      "total_flops_so_far": 9757588488918132.0,
      "budget_used_percent": 9.757588488918131
    },
    {
      "type": "training",
      "description": "Training step 1641",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:09",
      "total_flops_so_far": 9763529546970228.0,
      "budget_used_percent": 9.763529546970227
    },
    {
      "type": "training",
      "description": "Training step 1642",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:10",
      "total_flops_so_far": 9769470605022324.0,
      "budget_used_percent": 9.769470605022324
    },
    {
      "type": "training",
      "description": "Training step 1643",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:10",
      "total_flops_so_far": 9775411663074420.0,
      "budget_used_percent": 9.77541166307442
    },
    {
      "type": "training",
      "description": "Training step 1644",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:10",
      "total_flops_so_far": 9781352721126516.0,
      "budget_used_percent": 9.781352721126515
    },
    {
      "type": "training",
      "description": "Training step 1645",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:11",
      "total_flops_so_far": 9787293779178612.0,
      "budget_used_percent": 9.787293779178611
    },
    {
      "type": "training",
      "description": "Training step 1646",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:11",
      "total_flops_so_far": 9793234837230708.0,
      "budget_used_percent": 9.793234837230708
    },
    {
      "type": "training",
      "description": "Training step 1647",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:11",
      "total_flops_so_far": 9799175895282804.0,
      "budget_used_percent": 9.799175895282804
    },
    {
      "type": "training",
      "description": "Training step 1648",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:12",
      "total_flops_so_far": 9805116953334900.0,
      "budget_used_percent": 9.805116953334899
    },
    {
      "type": "training",
      "description": "Training step 1649",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:12",
      "total_flops_so_far": 9811058011386996.0,
      "budget_used_percent": 9.811058011386995
    },
    {
      "type": "training",
      "description": "Training step 1650",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:12",
      "total_flops_so_far": 9816999069439092.0,
      "budget_used_percent": 9.816999069439092
    },
    {
      "type": "training",
      "description": "Training step 1651",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:13",
      "total_flops_so_far": 9822940127491188.0,
      "budget_used_percent": 9.822940127491188
    },
    {
      "type": "training",
      "description": "Training step 1652",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:13",
      "total_flops_so_far": 9828881185543284.0,
      "budget_used_percent": 9.828881185543285
    },
    {
      "type": "training",
      "description": "Training step 1653",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:13",
      "total_flops_so_far": 9834822243595380.0,
      "budget_used_percent": 9.83482224359538
    },
    {
      "type": "training",
      "description": "Training step 1654",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:14",
      "total_flops_so_far": 9840763301647476.0,
      "budget_used_percent": 9.840763301647476
    },
    {
      "type": "training",
      "description": "Training step 1655",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:14",
      "total_flops_so_far": 9846704359699572.0,
      "budget_used_percent": 9.846704359699572
    },
    {
      "type": "training",
      "description": "Training step 1656",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:14",
      "total_flops_so_far": 9852645417751668.0,
      "budget_used_percent": 9.852645417751669
    },
    {
      "type": "training",
      "description": "Training step 1657",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:15",
      "total_flops_so_far": 9858586475803764.0,
      "budget_used_percent": 9.858586475803763
    },
    {
      "type": "training",
      "description": "Training step 1658",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:15",
      "total_flops_so_far": 9864527533855860.0,
      "budget_used_percent": 9.86452753385586
    },
    {
      "type": "training",
      "description": "Training step 1659",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:15",
      "total_flops_so_far": 9870468591907956.0,
      "budget_used_percent": 9.870468591907956
    },
    {
      "type": "training",
      "description": "Training step 1660",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:16",
      "total_flops_so_far": 9876409649960052.0,
      "budget_used_percent": 9.876409649960053
    },
    {
      "type": "training",
      "description": "Training step 1661",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:16",
      "total_flops_so_far": 9882350708012148.0,
      "budget_used_percent": 9.882350708012149
    },
    {
      "type": "training",
      "description": "Training step 1662",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:16",
      "total_flops_so_far": 9888291766064244.0,
      "budget_used_percent": 9.888291766064244
    },
    {
      "type": "training",
      "description": "Training step 1663",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:17",
      "total_flops_so_far": 9894232824116340.0,
      "budget_used_percent": 9.89423282411634
    },
    {
      "type": "training",
      "description": "Training step 1664",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:17",
      "total_flops_so_far": 9900173882168436.0,
      "budget_used_percent": 9.900173882168437
    },
    {
      "type": "training",
      "description": "Training step 1665",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:17",
      "total_flops_so_far": 9906114940220532.0,
      "budget_used_percent": 9.906114940220533
    },
    {
      "type": "training",
      "description": "Training step 1666",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:18",
      "total_flops_so_far": 9912055998272628.0,
      "budget_used_percent": 9.912055998272628
    },
    {
      "type": "training",
      "description": "Training step 1667",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:18",
      "total_flops_so_far": 9917997056324724.0,
      "budget_used_percent": 9.917997056324724
    },
    {
      "type": "training",
      "description": "Training step 1668",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:18",
      "total_flops_so_far": 9923938114376820.0,
      "budget_used_percent": 9.923938114376819
    },
    {
      "type": "training",
      "description": "Training step 1669",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:19",
      "total_flops_so_far": 9929879172428916.0,
      "budget_used_percent": 9.929879172428915
    },
    {
      "type": "training",
      "description": "Training step 1670",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:19",
      "total_flops_so_far": 9935820230481012.0,
      "budget_used_percent": 9.935820230481012
    },
    {
      "type": "training",
      "description": "Training step 1671",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:19",
      "total_flops_so_far": 9941761288533108.0,
      "budget_used_percent": 9.941761288533108
    },
    {
      "type": "training",
      "description": "Training step 1672",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:20",
      "total_flops_so_far": 9947702346585204.0,
      "budget_used_percent": 9.947702346585203
    },
    {
      "type": "training",
      "description": "Training step 1673",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:20",
      "total_flops_so_far": 9953643404637300.0,
      "budget_used_percent": 9.9536434046373
    },
    {
      "type": "training",
      "description": "Training step 1674",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:20",
      "total_flops_so_far": 9959584462689396.0,
      "budget_used_percent": 9.959584462689396
    },
    {
      "type": "training",
      "description": "Training step 1675",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:21",
      "total_flops_so_far": 9965525520741492.0,
      "budget_used_percent": 9.965525520741492
    },
    {
      "type": "training",
      "description": "Training step 1676",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:21",
      "total_flops_so_far": 9971466578793588.0,
      "budget_used_percent": 9.971466578793589
    },
    {
      "type": "training",
      "description": "Training step 1677",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:21",
      "total_flops_so_far": 9977407636845684.0,
      "budget_used_percent": 9.977407636845683
    },
    {
      "type": "training",
      "description": "Training step 1678",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:22",
      "total_flops_so_far": 9983348694897780.0,
      "budget_used_percent": 9.98334869489778
    },
    {
      "type": "training",
      "description": "Training step 1679",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:22",
      "total_flops_so_far": 9989289752949876.0,
      "budget_used_percent": 9.989289752949876
    },
    {
      "type": "training",
      "description": "Training step 1680",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:23",
      "total_flops_so_far": 9995230811001972.0,
      "budget_used_percent": 9.995230811001973
    },
    {
      "type": "training",
      "description": "Training step 1681",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:23",
      "total_flops_so_far": 1.0001171869054068e+16,
      "budget_used_percent": 10.001171869054067
    },
    {
      "type": "training",
      "description": "Training step 1682",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:23",
      "total_flops_so_far": 1.0007112927106164e+16,
      "budget_used_percent": 10.007112927106164
    },
    {
      "type": "training",
      "description": "Training step 1683",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:24",
      "total_flops_so_far": 1.001305398515826e+16,
      "budget_used_percent": 10.01305398515826
    },
    {
      "type": "training",
      "description": "Training step 1684",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:24",
      "total_flops_so_far": 1.0018995043210356e+16,
      "budget_used_percent": 10.018995043210356
    },
    {
      "type": "training",
      "description": "Training step 1685",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:24",
      "total_flops_so_far": 1.0024936101262452e+16,
      "budget_used_percent": 10.024936101262453
    },
    {
      "type": "training",
      "description": "Training step 1686",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:25",
      "total_flops_so_far": 1.0030877159314548e+16,
      "budget_used_percent": 10.030877159314548
    },
    {
      "type": "training",
      "description": "Training step 1687",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:25",
      "total_flops_so_far": 1.0036818217366644e+16,
      "budget_used_percent": 10.036818217366644
    },
    {
      "type": "training",
      "description": "Training step 1688",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:25",
      "total_flops_so_far": 1.004275927541874e+16,
      "budget_used_percent": 10.04275927541874
    },
    {
      "type": "training",
      "description": "Training step 1689",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:26",
      "total_flops_so_far": 1.0048700333470836e+16,
      "budget_used_percent": 10.048700333470837
    },
    {
      "type": "training",
      "description": "Training step 1690",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:26",
      "total_flops_so_far": 1.0054641391522932e+16,
      "budget_used_percent": 10.054641391522932
    },
    {
      "type": "training",
      "description": "Training step 1691",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:26",
      "total_flops_so_far": 1.0060582449575028e+16,
      "budget_used_percent": 10.060582449575028
    },
    {
      "type": "training",
      "description": "Training step 1692",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:27",
      "total_flops_so_far": 1.0066523507627124e+16,
      "budget_used_percent": 10.066523507627124
    },
    {
      "type": "training",
      "description": "Training step 1693",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:27",
      "total_flops_so_far": 1.007246456567922e+16,
      "budget_used_percent": 10.07246456567922
    },
    {
      "type": "training",
      "description": "Training step 1694",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:27",
      "total_flops_so_far": 1.0078405623731316e+16,
      "budget_used_percent": 10.078405623731317
    },
    {
      "type": "training",
      "description": "Training step 1695",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:28",
      "total_flops_so_far": 1.0084346681783412e+16,
      "budget_used_percent": 10.084346681783412
    },
    {
      "type": "training",
      "description": "Training step 1696",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:28",
      "total_flops_so_far": 1.0090287739835508e+16,
      "budget_used_percent": 10.090287739835507
    },
    {
      "type": "training",
      "description": "Training step 1697",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:28",
      "total_flops_so_far": 1.0096228797887604e+16,
      "budget_used_percent": 10.096228797887603
    },
    {
      "type": "training",
      "description": "Training step 1698",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:29",
      "total_flops_so_far": 1.01021698559397e+16,
      "budget_used_percent": 10.1021698559397
    },
    {
      "type": "training",
      "description": "Training step 1699",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:29",
      "total_flops_so_far": 1.0108110913991796e+16,
      "budget_used_percent": 10.108110913991796
    },
    {
      "type": "training",
      "description": "Training step 1700",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:29",
      "total_flops_so_far": 1.0114051972043892e+16,
      "budget_used_percent": 10.114051972043892
    },
    {
      "type": "training",
      "description": "Training step 1701",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:30",
      "total_flops_so_far": 1.0119993030095988e+16,
      "budget_used_percent": 10.119993030095987
    },
    {
      "type": "training",
      "description": "Training step 1702",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:30",
      "total_flops_so_far": 1.0125934088148084e+16,
      "budget_used_percent": 10.125934088148083
    },
    {
      "type": "training",
      "description": "Training step 1703",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:30",
      "total_flops_so_far": 1.013187514620018e+16,
      "budget_used_percent": 10.13187514620018
    },
    {
      "type": "training",
      "description": "Training step 1704",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:31",
      "total_flops_so_far": 1.0137816204252276e+16,
      "budget_used_percent": 10.137816204252276
    },
    {
      "type": "training",
      "description": "Training step 1705",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:31",
      "total_flops_so_far": 1.0143757262304372e+16,
      "budget_used_percent": 10.143757262304371
    },
    {
      "type": "training",
      "description": "Training step 1706",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:31",
      "total_flops_so_far": 1.0149698320356468e+16,
      "budget_used_percent": 10.149698320356467
    },
    {
      "type": "training",
      "description": "Training step 1707",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:32",
      "total_flops_so_far": 1.0155639378408564e+16,
      "budget_used_percent": 10.155639378408564
    },
    {
      "type": "training",
      "description": "Training step 1708",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:32",
      "total_flops_so_far": 1.016158043646066e+16,
      "budget_used_percent": 10.16158043646066
    },
    {
      "type": "training",
      "description": "Training step 1709",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:32",
      "total_flops_so_far": 1.0167521494512756e+16,
      "budget_used_percent": 10.167521494512757
    },
    {
      "type": "training",
      "description": "Training step 1710",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:33",
      "total_flops_so_far": 1.0173462552564852e+16,
      "budget_used_percent": 10.173462552564851
    },
    {
      "type": "training",
      "description": "Training step 1711",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:33",
      "total_flops_so_far": 1.0179403610616948e+16,
      "budget_used_percent": 10.179403610616948
    },
    {
      "type": "training",
      "description": "Training step 1712",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:34",
      "total_flops_so_far": 1.0185344668669044e+16,
      "budget_used_percent": 10.185344668669044
    },
    {
      "type": "training",
      "description": "Training step 1713",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:34",
      "total_flops_so_far": 1.019128572672114e+16,
      "budget_used_percent": 10.19128572672114
    },
    {
      "type": "training",
      "description": "Training step 1714",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:34",
      "total_flops_so_far": 1.0197226784773236e+16,
      "budget_used_percent": 10.197226784773235
    },
    {
      "type": "training",
      "description": "Training step 1715",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:35",
      "total_flops_so_far": 1.0203167842825332e+16,
      "budget_used_percent": 10.203167842825332
    },
    {
      "type": "training",
      "description": "Training step 1716",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:35",
      "total_flops_so_far": 1.0209108900877428e+16,
      "budget_used_percent": 10.209108900877428
    },
    {
      "type": "training",
      "description": "Training step 1717",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:35",
      "total_flops_so_far": 1.0215049958929524e+16,
      "budget_used_percent": 10.215049958929525
    },
    {
      "type": "training",
      "description": "Training step 1718",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:36",
      "total_flops_so_far": 1.022099101698162e+16,
      "budget_used_percent": 10.220991016981621
    },
    {
      "type": "training",
      "description": "Training step 1719",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:36",
      "total_flops_so_far": 1.0226932075033716e+16,
      "budget_used_percent": 10.226932075033716
    },
    {
      "type": "training",
      "description": "Training step 1720",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:36",
      "total_flops_so_far": 1.0232873133085812e+16,
      "budget_used_percent": 10.232873133085812
    },
    {
      "type": "training",
      "description": "Training step 1721",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:37",
      "total_flops_so_far": 1.0238814191137908e+16,
      "budget_used_percent": 10.238814191137909
    },
    {
      "type": "training",
      "description": "Training step 1722",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:37",
      "total_flops_so_far": 1.0244755249190004e+16,
      "budget_used_percent": 10.244755249190005
    },
    {
      "type": "training",
      "description": "Training step 1723",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:37",
      "total_flops_so_far": 1.02506963072421e+16,
      "budget_used_percent": 10.2506963072421
    },
    {
      "type": "training",
      "description": "Training step 1724",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:38",
      "total_flops_so_far": 1.0256637365294196e+16,
      "budget_used_percent": 10.256637365294196
    },
    {
      "type": "training",
      "description": "Training step 1725",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:38",
      "total_flops_so_far": 1.0262578423346292e+16,
      "budget_used_percent": 10.262578423346291
    },
    {
      "type": "training",
      "description": "Training step 1726",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:38",
      "total_flops_so_far": 1.0268519481398388e+16,
      "budget_used_percent": 10.268519481398387
    },
    {
      "type": "training",
      "description": "Training step 1727",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:39",
      "total_flops_so_far": 1.0274460539450484e+16,
      "budget_used_percent": 10.274460539450484
    },
    {
      "type": "training",
      "description": "Training step 1728",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:39",
      "total_flops_so_far": 1.028040159750258e+16,
      "budget_used_percent": 10.28040159750258
    },
    {
      "type": "training",
      "description": "Training step 1729",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:39",
      "total_flops_so_far": 1.0286342655554676e+16,
      "budget_used_percent": 10.286342655554675
    },
    {
      "type": "training",
      "description": "Training step 1730",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:40",
      "total_flops_so_far": 1.0292283713606772e+16,
      "budget_used_percent": 10.292283713606771
    },
    {
      "type": "training",
      "description": "Training step 1731",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:40",
      "total_flops_so_far": 1.0298224771658868e+16,
      "budget_used_percent": 10.298224771658868
    },
    {
      "type": "training",
      "description": "Training step 1732",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:40",
      "total_flops_so_far": 1.0304165829710964e+16,
      "budget_used_percent": 10.304165829710964
    },
    {
      "type": "training",
      "description": "Training step 1733",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:41",
      "total_flops_so_far": 1.031010688776306e+16,
      "budget_used_percent": 10.310106887763059
    },
    {
      "type": "training",
      "description": "Training step 1734",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:41",
      "total_flops_so_far": 1.0316047945815156e+16,
      "budget_used_percent": 10.316047945815155
    },
    {
      "type": "training",
      "description": "Training step 1735",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:41",
      "total_flops_so_far": 1.0321989003867252e+16,
      "budget_used_percent": 10.321989003867252
    },
    {
      "type": "training",
      "description": "Training step 1736",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:42",
      "total_flops_so_far": 1.0327930061919348e+16,
      "budget_used_percent": 10.327930061919348
    },
    {
      "type": "training",
      "description": "Training step 1737",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:42",
      "total_flops_so_far": 1.0333871119971444e+16,
      "budget_used_percent": 10.333871119971445
    },
    {
      "type": "training",
      "description": "Training step 1738",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:42",
      "total_flops_so_far": 1.033981217802354e+16,
      "budget_used_percent": 10.33981217802354
    },
    {
      "type": "training",
      "description": "Training step 1739",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:43",
      "total_flops_so_far": 1.0345753236075636e+16,
      "budget_used_percent": 10.345753236075636
    },
    {
      "type": "training",
      "description": "Training step 1740",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:43",
      "total_flops_so_far": 1.0351694294127732e+16,
      "budget_used_percent": 10.351694294127732
    },
    {
      "type": "training",
      "description": "Training step 1741",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:43",
      "total_flops_so_far": 1.0357635352179828e+16,
      "budget_used_percent": 10.357635352179829
    },
    {
      "type": "training",
      "description": "Training step 1742",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:44",
      "total_flops_so_far": 1.0363576410231924e+16,
      "budget_used_percent": 10.363576410231925
    },
    {
      "type": "training",
      "description": "Training step 1743",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:44",
      "total_flops_so_far": 1.036951746828402e+16,
      "budget_used_percent": 10.36951746828402
    },
    {
      "type": "training",
      "description": "Training step 1744",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:45",
      "total_flops_so_far": 1.0375458526336116e+16,
      "budget_used_percent": 10.375458526336116
    },
    {
      "type": "training",
      "description": "Training step 1745",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:45",
      "total_flops_so_far": 1.0381399584388212e+16,
      "budget_used_percent": 10.381399584388213
    },
    {
      "type": "training",
      "description": "Training step 1746",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:45",
      "total_flops_so_far": 1.0387340642440308e+16,
      "budget_used_percent": 10.387340642440309
    },
    {
      "type": "training",
      "description": "Training step 1747",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:46",
      "total_flops_so_far": 1.0393281700492404e+16,
      "budget_used_percent": 10.393281700492404
    },
    {
      "type": "training",
      "description": "Training step 1748",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:46",
      "total_flops_so_far": 1.03992227585445e+16,
      "budget_used_percent": 10.3992227585445
    },
    {
      "type": "training",
      "description": "Training step 1749",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:46",
      "total_flops_so_far": 1.0405163816596596e+16,
      "budget_used_percent": 10.405163816596597
    },
    {
      "type": "training",
      "description": "Training step 1750",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:47",
      "total_flops_so_far": 1.0411104874648692e+16,
      "budget_used_percent": 10.411104874648693
    },
    {
      "type": "training",
      "description": "Training step 1751",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:47",
      "total_flops_so_far": 1.0417045932700788e+16,
      "budget_used_percent": 10.41704593270079
    },
    {
      "type": "training",
      "description": "Training step 1752",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:47",
      "total_flops_so_far": 1.0422986990752884e+16,
      "budget_used_percent": 10.422986990752884
    },
    {
      "type": "training",
      "description": "Training step 1753",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:48",
      "total_flops_so_far": 1.042892804880498e+16,
      "budget_used_percent": 10.428928048804979
    },
    {
      "type": "training",
      "description": "Training step 1754",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:48",
      "total_flops_so_far": 1.0434869106857076e+16,
      "budget_used_percent": 10.434869106857075
    },
    {
      "type": "training",
      "description": "Training step 1755",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:48",
      "total_flops_so_far": 1.0440810164909172e+16,
      "budget_used_percent": 10.440810164909172
    },
    {
      "type": "training",
      "description": "Training step 1756",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:49",
      "total_flops_so_far": 1.0446751222961268e+16,
      "budget_used_percent": 10.446751222961268
    },
    {
      "type": "training",
      "description": "Training step 1757",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:49",
      "total_flops_so_far": 1.0452692281013364e+16,
      "budget_used_percent": 10.452692281013363
    },
    {
      "type": "training",
      "description": "Training step 1758",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:49",
      "total_flops_so_far": 1.045863333906546e+16,
      "budget_used_percent": 10.458633339065459
    },
    {
      "type": "training",
      "description": "Training step 1759",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:50",
      "total_flops_so_far": 1.0464574397117556e+16,
      "budget_used_percent": 10.464574397117556
    },
    {
      "type": "training",
      "description": "Training step 1760",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:50",
      "total_flops_so_far": 1.0470515455169652e+16,
      "budget_used_percent": 10.470515455169652
    },
    {
      "type": "training",
      "description": "Training step 1761",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:50",
      "total_flops_so_far": 1.0476456513221748e+16,
      "budget_used_percent": 10.476456513221748
    },
    {
      "type": "training",
      "description": "Training step 1762",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:51",
      "total_flops_so_far": 1.0482397571273844e+16,
      "budget_used_percent": 10.482397571273843
    },
    {
      "type": "training",
      "description": "Training step 1763",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:51",
      "total_flops_so_far": 1.048833862932594e+16,
      "budget_used_percent": 10.48833862932594
    },
    {
      "type": "training",
      "description": "Training step 1764",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:51",
      "total_flops_so_far": 1.0494279687378036e+16,
      "budget_used_percent": 10.494279687378036
    },
    {
      "type": "training",
      "description": "Training step 1765",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:52",
      "total_flops_so_far": 1.0500220745430132e+16,
      "budget_used_percent": 10.500220745430132
    },
    {
      "type": "training",
      "description": "Training step 1766",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:52",
      "total_flops_so_far": 1.0506161803482228e+16,
      "budget_used_percent": 10.506161803482229
    },
    {
      "type": "training",
      "description": "Training step 1767",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:52",
      "total_flops_so_far": 1.0512102861534324e+16,
      "budget_used_percent": 10.512102861534323
    },
    {
      "type": "training",
      "description": "Training step 1768",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:53",
      "total_flops_so_far": 1.051804391958642e+16,
      "budget_used_percent": 10.51804391958642
    },
    {
      "type": "training",
      "description": "Training step 1769",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:53",
      "total_flops_so_far": 1.0523984977638516e+16,
      "budget_used_percent": 10.523984977638516
    },
    {
      "type": "training",
      "description": "Training step 1770",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:53",
      "total_flops_so_far": 1.0529926035690612e+16,
      "budget_used_percent": 10.529926035690613
    },
    {
      "type": "training",
      "description": "Training step 1771",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:54",
      "total_flops_so_far": 1.0535867093742708e+16,
      "budget_used_percent": 10.535867093742707
    },
    {
      "type": "training",
      "description": "Training step 1772",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:54",
      "total_flops_so_far": 1.0541808151794804e+16,
      "budget_used_percent": 10.541808151794804
    },
    {
      "type": "training",
      "description": "Training step 1773",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:55",
      "total_flops_so_far": 1.05477492098469e+16,
      "budget_used_percent": 10.5477492098469
    },
    {
      "type": "training",
      "description": "Training step 1774",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:55",
      "total_flops_so_far": 1.0553690267898996e+16,
      "budget_used_percent": 10.553690267898997
    },
    {
      "type": "training",
      "description": "Training step 1775",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:55",
      "total_flops_so_far": 1.0559631325951092e+16,
      "budget_used_percent": 10.559631325951093
    },
    {
      "type": "training",
      "description": "Training step 1776",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:56",
      "total_flops_so_far": 1.0565572384003188e+16,
      "budget_used_percent": 10.565572384003188
    },
    {
      "type": "training",
      "description": "Training step 1777",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:56",
      "total_flops_so_far": 1.0571513442055284e+16,
      "budget_used_percent": 10.571513442055284
    },
    {
      "type": "training",
      "description": "Training step 1778",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:56",
      "total_flops_so_far": 1.057745450010738e+16,
      "budget_used_percent": 10.57745450010738
    },
    {
      "type": "training",
      "description": "Training step 1779",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:57",
      "total_flops_so_far": 1.0583395558159476e+16,
      "budget_used_percent": 10.583395558159477
    },
    {
      "type": "training",
      "description": "Training step 1780",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:57",
      "total_flops_so_far": 1.0589336616211572e+16,
      "budget_used_percent": 10.589336616211572
    },
    {
      "type": "training",
      "description": "Training step 1781",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:57",
      "total_flops_so_far": 1.0595277674263668e+16,
      "budget_used_percent": 10.595277674263668
    },
    {
      "type": "training",
      "description": "Training step 1782",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:58",
      "total_flops_so_far": 1.0601218732315764e+16,
      "budget_used_percent": 10.601218732315763
    },
    {
      "type": "training",
      "description": "Training step 1783",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:58",
      "total_flops_so_far": 1.060715979036786e+16,
      "budget_used_percent": 10.60715979036786
    },
    {
      "type": "training",
      "description": "Training step 1784",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:58",
      "total_flops_so_far": 1.0613100848419956e+16,
      "budget_used_percent": 10.613100848419956
    },
    {
      "type": "training",
      "description": "Training step 1785",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:59",
      "total_flops_so_far": 1.0619041906472052e+16,
      "budget_used_percent": 10.619041906472052
    },
    {
      "type": "training",
      "description": "Training step 1786",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:59",
      "total_flops_so_far": 1.0624982964524148e+16,
      "budget_used_percent": 10.624982964524147
    },
    {
      "type": "training",
      "description": "Training step 1787",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:48:59",
      "total_flops_so_far": 1.0630924022576244e+16,
      "budget_used_percent": 10.630924022576243
    },
    {
      "type": "training",
      "description": "Training step 1788",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:00",
      "total_flops_so_far": 1.063686508062834e+16,
      "budget_used_percent": 10.63686508062834
    },
    {
      "type": "training",
      "description": "Training step 1789",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:00",
      "total_flops_so_far": 1.0642806138680436e+16,
      "budget_used_percent": 10.642806138680436
    },
    {
      "type": "training",
      "description": "Training step 1790",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:00",
      "total_flops_so_far": 1.0648747196732532e+16,
      "budget_used_percent": 10.648747196732531
    },
    {
      "type": "training",
      "description": "Training step 1791",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:01",
      "total_flops_so_far": 1.0654688254784628e+16,
      "budget_used_percent": 10.654688254784627
    },
    {
      "type": "training",
      "description": "Training step 1792",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:01",
      "total_flops_so_far": 1.0660629312836724e+16,
      "budget_used_percent": 10.660629312836724
    },
    {
      "type": "training",
      "description": "Training step 1793",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:01",
      "total_flops_so_far": 1.066657037088882e+16,
      "budget_used_percent": 10.66657037088882
    },
    {
      "type": "training",
      "description": "Training step 1794",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:02",
      "total_flops_so_far": 1.0672511428940916e+16,
      "budget_used_percent": 10.672511428940917
    },
    {
      "type": "training",
      "description": "Training step 1795",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:02",
      "total_flops_so_far": 1.0678452486993012e+16,
      "budget_used_percent": 10.678452486993011
    },
    {
      "type": "training",
      "description": "Training step 1796",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:02",
      "total_flops_so_far": 1.0684393545045108e+16,
      "budget_used_percent": 10.684393545045108
    },
    {
      "type": "training",
      "description": "Training step 1797",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:03",
      "total_flops_so_far": 1.0690334603097204e+16,
      "budget_used_percent": 10.690334603097204
    },
    {
      "type": "training",
      "description": "Training step 1798",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:03",
      "total_flops_so_far": 1.06962756611493e+16,
      "budget_used_percent": 10.6962756611493
    },
    {
      "type": "training",
      "description": "Training step 1799",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:04",
      "total_flops_so_far": 1.0702216719201396e+16,
      "budget_used_percent": 10.702216719201395
    },
    {
      "type": "training",
      "description": "Training step 1800",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:04",
      "total_flops_so_far": 1.0708157777253492e+16,
      "budget_used_percent": 10.708157777253492
    },
    {
      "type": "training",
      "description": "Training step 1801",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:04",
      "total_flops_so_far": 1.0714098835305588e+16,
      "budget_used_percent": 10.714098835305588
    },
    {
      "type": "training",
      "description": "Training step 1802",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:05",
      "total_flops_so_far": 1.0720039893357684e+16,
      "budget_used_percent": 10.720039893357685
    },
    {
      "type": "training",
      "description": "Training step 1803",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:05",
      "total_flops_so_far": 1.072598095140978e+16,
      "budget_used_percent": 10.725980951409781
    },
    {
      "type": "training",
      "description": "Training step 1804",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:05",
      "total_flops_so_far": 1.0731922009461876e+16,
      "budget_used_percent": 10.731922009461876
    },
    {
      "type": "training",
      "description": "Training step 1805",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:06",
      "total_flops_so_far": 1.0737863067513972e+16,
      "budget_used_percent": 10.737863067513972
    },
    {
      "type": "training",
      "description": "Training step 1806",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:06",
      "total_flops_so_far": 1.0743804125566068e+16,
      "budget_used_percent": 10.743804125566069
    },
    {
      "type": "training",
      "description": "Training step 1807",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:06",
      "total_flops_so_far": 1.0749745183618164e+16,
      "budget_used_percent": 10.749745183618165
    },
    {
      "type": "training",
      "description": "Training step 1808",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:07",
      "total_flops_so_far": 1.075568624167026e+16,
      "budget_used_percent": 10.755686241670261
    },
    {
      "type": "training",
      "description": "Training step 1809",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:07",
      "total_flops_so_far": 1.0761627299722356e+16,
      "budget_used_percent": 10.761627299722356
    },
    {
      "type": "training",
      "description": "Training step 1810",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:07",
      "total_flops_so_far": 1.0767568357774452e+16,
      "budget_used_percent": 10.76756835777445
    },
    {
      "type": "training",
      "description": "Training step 1811",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:08",
      "total_flops_so_far": 1.0773509415826548e+16,
      "budget_used_percent": 10.773509415826547
    },
    {
      "type": "training",
      "description": "Training step 1812",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:08",
      "total_flops_so_far": 1.0779450473878644e+16,
      "budget_used_percent": 10.779450473878644
    },
    {
      "type": "training",
      "description": "Training step 1813",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:08",
      "total_flops_so_far": 1.078539153193074e+16,
      "budget_used_percent": 10.78539153193074
    },
    {
      "type": "training",
      "description": "Training step 1814",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:09",
      "total_flops_so_far": 1.0791332589982836e+16,
      "budget_used_percent": 10.791332589982835
    },
    {
      "type": "training",
      "description": "Training step 1815",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:09",
      "total_flops_so_far": 1.0797273648034932e+16,
      "budget_used_percent": 10.797273648034931
    },
    {
      "type": "training",
      "description": "Training step 1816",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:09",
      "total_flops_so_far": 1.0803214706087028e+16,
      "budget_used_percent": 10.803214706087028
    },
    {
      "type": "training",
      "description": "Training step 1817",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:10",
      "total_flops_so_far": 1.0809155764139124e+16,
      "budget_used_percent": 10.809155764139124
    },
    {
      "type": "training",
      "description": "Training step 1818",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:10",
      "total_flops_so_far": 1.081509682219122e+16,
      "budget_used_percent": 10.81509682219122
    },
    {
      "type": "training",
      "description": "Training step 1819",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:10",
      "total_flops_so_far": 1.0821037880243316e+16,
      "budget_used_percent": 10.821037880243315
    },
    {
      "type": "training",
      "description": "Training step 1820",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:11",
      "total_flops_so_far": 1.0826978938295412e+16,
      "budget_used_percent": 10.826978938295412
    },
    {
      "type": "training",
      "description": "Training step 1821",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:11",
      "total_flops_so_far": 1.0832919996347508e+16,
      "budget_used_percent": 10.832919996347508
    },
    {
      "type": "training",
      "description": "Training step 1822",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:11",
      "total_flops_so_far": 1.0838861054399604e+16,
      "budget_used_percent": 10.838861054399604
    },
    {
      "type": "training",
      "description": "Training step 1823",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:12",
      "total_flops_so_far": 1.08448021124517e+16,
      "budget_used_percent": 10.8448021124517
    },
    {
      "type": "training",
      "description": "Training step 1824",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:12",
      "total_flops_so_far": 1.0850743170503796e+16,
      "budget_used_percent": 10.850743170503796
    },
    {
      "type": "training",
      "description": "Training step 1825",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:12",
      "total_flops_so_far": 1.0856684228555892e+16,
      "budget_used_percent": 10.856684228555892
    },
    {
      "type": "training",
      "description": "Training step 1826",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:13",
      "total_flops_so_far": 1.0862625286607988e+16,
      "budget_used_percent": 10.862625286607988
    },
    {
      "type": "training",
      "description": "Training step 1827",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:13",
      "total_flops_so_far": 1.0868566344660084e+16,
      "budget_used_percent": 10.868566344660085
    },
    {
      "type": "training",
      "description": "Training step 1828",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:14",
      "total_flops_so_far": 1.087450740271218e+16,
      "budget_used_percent": 10.87450740271218
    },
    {
      "type": "training",
      "description": "Training step 1829",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:14",
      "total_flops_so_far": 1.0880448460764276e+16,
      "budget_used_percent": 10.880448460764276
    },
    {
      "type": "training",
      "description": "Training step 1830",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:14",
      "total_flops_so_far": 1.0886389518816372e+16,
      "budget_used_percent": 10.886389518816372
    },
    {
      "type": "training",
      "description": "Training step 1831",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:15",
      "total_flops_so_far": 1.0892330576868468e+16,
      "budget_used_percent": 10.892330576868469
    },
    {
      "type": "training",
      "description": "Training step 1832",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:15",
      "total_flops_so_far": 1.0898271634920564e+16,
      "budget_used_percent": 10.898271634920565
    },
    {
      "type": "training",
      "description": "Training step 1833",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:15",
      "total_flops_so_far": 1.090421269297266e+16,
      "budget_used_percent": 10.90421269297266
    },
    {
      "type": "training",
      "description": "Training step 1834",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:16",
      "total_flops_so_far": 1.0910153751024756e+16,
      "budget_used_percent": 10.910153751024756
    },
    {
      "type": "training",
      "description": "Training step 1835",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:16",
      "total_flops_so_far": 1.0916094809076852e+16,
      "budget_used_percent": 10.916094809076853
    },
    {
      "type": "training",
      "description": "Training step 1836",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:16",
      "total_flops_so_far": 1.0922035867128948e+16,
      "budget_used_percent": 10.92203586712895
    },
    {
      "type": "training",
      "description": "Training step 1837",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:17",
      "total_flops_so_far": 1.0927976925181044e+16,
      "budget_used_percent": 10.927976925181044
    },
    {
      "type": "training",
      "description": "Training step 1838",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:17",
      "total_flops_so_far": 1.093391798323314e+16,
      "budget_used_percent": 10.93391798323314
    },
    {
      "type": "training",
      "description": "Training step 1839",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:17",
      "total_flops_so_far": 1.0939859041285236e+16,
      "budget_used_percent": 10.939859041285235
    },
    {
      "type": "training",
      "description": "Training step 1840",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:18",
      "total_flops_so_far": 1.0945800099337332e+16,
      "budget_used_percent": 10.945800099337331
    },
    {
      "type": "training",
      "description": "Training step 1841",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:18",
      "total_flops_so_far": 1.0951741157389428e+16,
      "budget_used_percent": 10.951741157389428
    },
    {
      "type": "training",
      "description": "Training step 1842",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:18",
      "total_flops_so_far": 1.0957682215441524e+16,
      "budget_used_percent": 10.957682215441524
    },
    {
      "type": "training",
      "description": "Training step 1843",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:19",
      "total_flops_so_far": 1.096362327349362e+16,
      "budget_used_percent": 10.963623273493619
    },
    {
      "type": "training",
      "description": "Training step 1844",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:19",
      "total_flops_so_far": 1.0969564331545716e+16,
      "budget_used_percent": 10.969564331545715
    },
    {
      "type": "training",
      "description": "Training step 1845",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:19",
      "total_flops_so_far": 1.0975505389597812e+16,
      "budget_used_percent": 10.975505389597812
    },
    {
      "type": "training",
      "description": "Training step 1846",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:20",
      "total_flops_so_far": 1.0981446447649908e+16,
      "budget_used_percent": 10.981446447649908
    },
    {
      "type": "training",
      "description": "Training step 1847",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:20",
      "total_flops_so_far": 1.0987387505702004e+16,
      "budget_used_percent": 10.987387505702003
    },
    {
      "type": "training",
      "description": "Training step 1848",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:20",
      "total_flops_so_far": 1.09933285637541e+16,
      "budget_used_percent": 10.9933285637541
    },
    {
      "type": "training",
      "description": "Training step 1849",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:21",
      "total_flops_so_far": 1.0999269621806196e+16,
      "budget_used_percent": 10.999269621806196
    },
    {
      "type": "training",
      "description": "Training step 1850",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:21",
      "total_flops_so_far": 1.1005210679858292e+16,
      "budget_used_percent": 11.005210679858292
    },
    {
      "type": "training",
      "description": "Training step 1851",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:22",
      "total_flops_so_far": 1.1011151737910388e+16,
      "budget_used_percent": 11.011151737910389
    },
    {
      "type": "training",
      "description": "Training step 1852",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:22",
      "total_flops_so_far": 1.1017092795962484e+16,
      "budget_used_percent": 11.017092795962483
    },
    {
      "type": "training",
      "description": "Training step 1853",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:22",
      "total_flops_so_far": 1.102303385401458e+16,
      "budget_used_percent": 11.02303385401458
    },
    {
      "type": "training",
      "description": "Training step 1854",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:23",
      "total_flops_so_far": 1.1028974912066676e+16,
      "budget_used_percent": 11.028974912066676
    },
    {
      "type": "training",
      "description": "Training step 1855",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:23",
      "total_flops_so_far": 1.1034915970118772e+16,
      "budget_used_percent": 11.034915970118773
    },
    {
      "type": "training",
      "description": "Training step 1856",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:23",
      "total_flops_so_far": 1.1040857028170868e+16,
      "budget_used_percent": 11.040857028170867
    },
    {
      "type": "training",
      "description": "Training step 1857",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:24",
      "total_flops_so_far": 1.1046798086222964e+16,
      "budget_used_percent": 11.046798086222964
    },
    {
      "type": "training",
      "description": "Training step 1858",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:24",
      "total_flops_so_far": 1.105273914427506e+16,
      "budget_used_percent": 11.05273914427506
    },
    {
      "type": "training",
      "description": "Training step 1859",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:24",
      "total_flops_so_far": 1.1058680202327156e+16,
      "budget_used_percent": 11.058680202327157
    },
    {
      "type": "training",
      "description": "Training step 1860",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:25",
      "total_flops_so_far": 1.1064621260379252e+16,
      "budget_used_percent": 11.064621260379253
    },
    {
      "type": "training",
      "description": "Training step 1861",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:25",
      "total_flops_so_far": 1.1070562318431348e+16,
      "budget_used_percent": 11.070562318431348
    },
    {
      "type": "training",
      "description": "Training step 1862",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:25",
      "total_flops_so_far": 1.1076503376483444e+16,
      "budget_used_percent": 11.076503376483444
    },
    {
      "type": "training",
      "description": "Training step 1863",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:26",
      "total_flops_so_far": 1.108244443453554e+16,
      "budget_used_percent": 11.08244443453554
    },
    {
      "type": "training",
      "description": "Training step 1864",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:26",
      "total_flops_so_far": 1.1088385492587636e+16,
      "budget_used_percent": 11.088385492587637
    },
    {
      "type": "training",
      "description": "Training step 1865",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:26",
      "total_flops_so_far": 1.1094326550639732e+16,
      "budget_used_percent": 11.094326550639732
    },
    {
      "type": "training",
      "description": "Training step 1866",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:27",
      "total_flops_so_far": 1.1100267608691828e+16,
      "budget_used_percent": 11.100267608691828
    },
    {
      "type": "training",
      "description": "Training step 1867",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:27",
      "total_flops_so_far": 1.1106208666743924e+16,
      "budget_used_percent": 11.106208666743923
    },
    {
      "type": "training",
      "description": "Training step 1868",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:27",
      "total_flops_so_far": 1.111214972479602e+16,
      "budget_used_percent": 11.11214972479602
    },
    {
      "type": "training",
      "description": "Training step 1869",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:28",
      "total_flops_so_far": 1.1118090782848116e+16,
      "budget_used_percent": 11.118090782848116
    },
    {
      "type": "training",
      "description": "Training step 1870",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:28",
      "total_flops_so_far": 1.1124031840900212e+16,
      "budget_used_percent": 11.124031840900212
    },
    {
      "type": "training",
      "description": "Training step 1871",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:28",
      "total_flops_so_far": 1.1129972898952308e+16,
      "budget_used_percent": 11.129972898952307
    },
    {
      "type": "training",
      "description": "Training step 1872",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:29",
      "total_flops_so_far": 1.1135913957004404e+16,
      "budget_used_percent": 11.135913957004403
    },
    {
      "type": "training",
      "description": "Training step 1873",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:29",
      "total_flops_so_far": 1.11418550150565e+16,
      "budget_used_percent": 11.1418550150565
    },
    {
      "type": "training",
      "description": "Training step 1874",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:30",
      "total_flops_so_far": 1.1147796073108596e+16,
      "budget_used_percent": 11.147796073108596
    },
    {
      "type": "training",
      "description": "Training step 1875",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:30",
      "total_flops_so_far": 1.1153737131160692e+16,
      "budget_used_percent": 11.153737131160693
    },
    {
      "type": "training",
      "description": "Training step 1876",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:30",
      "total_flops_so_far": 1.1159678189212788e+16,
      "budget_used_percent": 11.159678189212787
    },
    {
      "type": "training",
      "description": "Training step 1877",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:31",
      "total_flops_so_far": 1.1165619247264884e+16,
      "budget_used_percent": 11.165619247264884
    },
    {
      "type": "training",
      "description": "Training step 1878",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:31",
      "total_flops_so_far": 1.117156030531698e+16,
      "budget_used_percent": 11.17156030531698
    },
    {
      "type": "training",
      "description": "Training step 1879",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:31",
      "total_flops_so_far": 1.1177501363369076e+16,
      "budget_used_percent": 11.177501363369077
    },
    {
      "type": "training",
      "description": "Training step 1880",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:32",
      "total_flops_so_far": 1.1183442421421172e+16,
      "budget_used_percent": 11.183442421421171
    },
    {
      "type": "training",
      "description": "Training step 1881",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:32",
      "total_flops_so_far": 1.1189383479473268e+16,
      "budget_used_percent": 11.189383479473268
    },
    {
      "type": "training",
      "description": "Training step 1882",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:32",
      "total_flops_so_far": 1.1195324537525364e+16,
      "budget_used_percent": 11.195324537525364
    },
    {
      "type": "training",
      "description": "Training step 1883",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:33",
      "total_flops_so_far": 1.120126559557746e+16,
      "budget_used_percent": 11.20126559557746
    },
    {
      "type": "training",
      "description": "Training step 1884",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:33",
      "total_flops_so_far": 1.1207206653629556e+16,
      "budget_used_percent": 11.207206653629557
    },
    {
      "type": "training",
      "description": "Training step 1885",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:33",
      "total_flops_so_far": 1.1213147711681652e+16,
      "budget_used_percent": 11.213147711681652
    },
    {
      "type": "training",
      "description": "Training step 1886",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:34",
      "total_flops_so_far": 1.1219088769733748e+16,
      "budget_used_percent": 11.219088769733748
    },
    {
      "type": "training",
      "description": "Training step 1887",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:34",
      "total_flops_so_far": 1.1225029827785844e+16,
      "budget_used_percent": 11.225029827785844
    },
    {
      "type": "training",
      "description": "Training step 1888",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:34",
      "total_flops_so_far": 1.123097088583794e+16,
      "budget_used_percent": 11.230970885837941
    },
    {
      "type": "training",
      "description": "Training step 1889",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:35",
      "total_flops_so_far": 1.1236911943890036e+16,
      "budget_used_percent": 11.236911943890036
    },
    {
      "type": "training",
      "description": "Training step 1890",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:35",
      "total_flops_so_far": 1.1242853001942132e+16,
      "budget_used_percent": 11.242853001942132
    },
    {
      "type": "training",
      "description": "Training step 1891",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:35",
      "total_flops_so_far": 1.1248794059994228e+16,
      "budget_used_percent": 11.248794059994228
    },
    {
      "type": "training",
      "description": "Training step 1892",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:36",
      "total_flops_so_far": 1.1254735118046324e+16,
      "budget_used_percent": 11.254735118046325
    },
    {
      "type": "training",
      "description": "Training step 1893",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:36",
      "total_flops_so_far": 1.126067617609842e+16,
      "budget_used_percent": 11.260676176098421
    },
    {
      "type": "training",
      "description": "Training step 1894",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:36",
      "total_flops_so_far": 1.1266617234150516e+16,
      "budget_used_percent": 11.266617234150516
    },
    {
      "type": "training",
      "description": "Training step 1895",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:37",
      "total_flops_so_far": 1.1272558292202612e+16,
      "budget_used_percent": 11.272558292202612
    },
    {
      "type": "training",
      "description": "Training step 1896",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:37",
      "total_flops_so_far": 1.1278499350254708e+16,
      "budget_used_percent": 11.278499350254707
    },
    {
      "type": "training",
      "description": "Training step 1897",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:38",
      "total_flops_so_far": 1.1284440408306804e+16,
      "budget_used_percent": 11.284440408306804
    },
    {
      "type": "training",
      "description": "Training step 1898",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:38",
      "total_flops_so_far": 1.12903814663589e+16,
      "budget_used_percent": 11.2903814663589
    },
    {
      "type": "training",
      "description": "Training step 1899",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:38",
      "total_flops_so_far": 1.1296322524410996e+16,
      "budget_used_percent": 11.296322524410996
    },
    {
      "type": "training",
      "description": "Training step 1900",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:39",
      "total_flops_so_far": 1.1302263582463092e+16,
      "budget_used_percent": 11.302263582463091
    },
    {
      "type": "training",
      "description": "Training step 1901",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:39",
      "total_flops_so_far": 1.1308204640515188e+16,
      "budget_used_percent": 11.308204640515187
    },
    {
      "type": "training",
      "description": "Training step 1902",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:39",
      "total_flops_so_far": 1.1314145698567284e+16,
      "budget_used_percent": 11.314145698567284
    },
    {
      "type": "training",
      "description": "Training step 1903",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:40",
      "total_flops_so_far": 1.132008675661938e+16,
      "budget_used_percent": 11.32008675661938
    },
    {
      "type": "training",
      "description": "Training step 1904",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:40",
      "total_flops_so_far": 1.1326027814671476e+16,
      "budget_used_percent": 11.326027814671475
    },
    {
      "type": "training",
      "description": "Training step 1905",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:40",
      "total_flops_so_far": 1.1331968872723572e+16,
      "budget_used_percent": 11.331968872723571
    },
    {
      "type": "training",
      "description": "Training step 1906",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:41",
      "total_flops_so_far": 1.1337909930775668e+16,
      "budget_used_percent": 11.337909930775668
    },
    {
      "type": "training",
      "description": "Training step 1907",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:41",
      "total_flops_so_far": 1.1343850988827764e+16,
      "budget_used_percent": 11.343850988827764
    },
    {
      "type": "training",
      "description": "Training step 1908",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:41",
      "total_flops_so_far": 1.134979204687986e+16,
      "budget_used_percent": 11.34979204687986
    },
    {
      "type": "training",
      "description": "Training step 1909",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:42",
      "total_flops_so_far": 1.1355733104931956e+16,
      "budget_used_percent": 11.355733104931955
    },
    {
      "type": "training",
      "description": "Training step 1910",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:42",
      "total_flops_so_far": 1.1361674162984052e+16,
      "budget_used_percent": 11.361674162984052
    },
    {
      "type": "training",
      "description": "Training step 1911",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:43",
      "total_flops_so_far": 1.1367615221036148e+16,
      "budget_used_percent": 11.367615221036148
    },
    {
      "type": "training",
      "description": "Training step 1912",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:43",
      "total_flops_so_far": 1.1373556279088244e+16,
      "budget_used_percent": 11.373556279088245
    },
    {
      "type": "training",
      "description": "Training step 1913",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:43",
      "total_flops_so_far": 1.137949733714034e+16,
      "budget_used_percent": 11.37949733714034
    },
    {
      "type": "training",
      "description": "Training step 1914",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:44",
      "total_flops_so_far": 1.1385438395192436e+16,
      "budget_used_percent": 11.385438395192436
    },
    {
      "type": "training",
      "description": "Training step 1915",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:44",
      "total_flops_so_far": 1.1391379453244532e+16,
      "budget_used_percent": 11.391379453244532
    },
    {
      "type": "training",
      "description": "Training step 1916",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:44",
      "total_flops_so_far": 1.1397320511296628e+16,
      "budget_used_percent": 11.397320511296629
    },
    {
      "type": "training",
      "description": "Training step 1917",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:45",
      "total_flops_so_far": 1.1403261569348724e+16,
      "budget_used_percent": 11.403261569348725
    },
    {
      "type": "training",
      "description": "Training step 1918",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:45",
      "total_flops_so_far": 1.140920262740082e+16,
      "budget_used_percent": 11.40920262740082
    },
    {
      "type": "training",
      "description": "Training step 1919",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:46",
      "total_flops_so_far": 1.1415143685452916e+16,
      "budget_used_percent": 11.415143685452916
    },
    {
      "type": "training",
      "description": "Training step 1920",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:46",
      "total_flops_so_far": 1.1421084743505012e+16,
      "budget_used_percent": 11.421084743505013
    },
    {
      "type": "training",
      "description": "Training step 1921",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:46",
      "total_flops_so_far": 1.1427025801557108e+16,
      "budget_used_percent": 11.42702580155711
    },
    {
      "type": "training",
      "description": "Training step 1922",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:47",
      "total_flops_so_far": 1.1432966859609204e+16,
      "budget_used_percent": 11.432966859609204
    },
    {
      "type": "training",
      "description": "Training step 1923",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:47",
      "total_flops_so_far": 1.14389079176613e+16,
      "budget_used_percent": 11.4389079176613
    },
    {
      "type": "training",
      "description": "Training step 1924",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:47",
      "total_flops_so_far": 1.1444848975713396e+16,
      "budget_used_percent": 11.444848975713395
    },
    {
      "type": "training",
      "description": "Training step 1925",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:48",
      "total_flops_so_far": 1.1450790033765492e+16,
      "budget_used_percent": 11.450790033765491
    },
    {
      "type": "training",
      "description": "Training step 1926",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:48",
      "total_flops_so_far": 1.1456731091817588e+16,
      "budget_used_percent": 11.456731091817588
    },
    {
      "type": "training",
      "description": "Training step 1927",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:48",
      "total_flops_so_far": 1.1462672149869684e+16,
      "budget_used_percent": 11.462672149869684
    },
    {
      "type": "training",
      "description": "Training step 1928",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:49",
      "total_flops_so_far": 1.146861320792178e+16,
      "budget_used_percent": 11.468613207921779
    },
    {
      "type": "training",
      "description": "Training step 1929",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:49",
      "total_flops_so_far": 1.1474554265973876e+16,
      "budget_used_percent": 11.474554265973875
    },
    {
      "type": "training",
      "description": "Training step 1930",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:49",
      "total_flops_so_far": 1.1480495324025972e+16,
      "budget_used_percent": 11.480495324025972
    },
    {
      "type": "training",
      "description": "Training step 1931",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:50",
      "total_flops_so_far": 1.1486436382078068e+16,
      "budget_used_percent": 11.486436382078068
    },
    {
      "type": "training",
      "description": "Training step 1932",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:50",
      "total_flops_so_far": 1.1492377440130164e+16,
      "budget_used_percent": 11.492377440130163
    },
    {
      "type": "training",
      "description": "Training step 1933",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:50",
      "total_flops_so_far": 1.149831849818226e+16,
      "budget_used_percent": 11.49831849818226
    },
    {
      "type": "training",
      "description": "Training step 1934",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:51",
      "total_flops_so_far": 1.1504259556234356e+16,
      "budget_used_percent": 11.504259556234356
    },
    {
      "type": "training",
      "description": "Training step 1935",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:51",
      "total_flops_so_far": 1.1510200614286452e+16,
      "budget_used_percent": 11.510200614286452
    },
    {
      "type": "training",
      "description": "Training step 1936",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:51",
      "total_flops_so_far": 1.1516141672338548e+16,
      "budget_used_percent": 11.516141672338549
    },
    {
      "type": "training",
      "description": "Training step 1937",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:52",
      "total_flops_so_far": 1.1522082730390644e+16,
      "budget_used_percent": 11.522082730390643
    },
    {
      "type": "training",
      "description": "Training step 1938",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:52",
      "total_flops_so_far": 1.152802378844274e+16,
      "budget_used_percent": 11.52802378844274
    },
    {
      "type": "training",
      "description": "Training step 1939",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:52",
      "total_flops_so_far": 1.1533964846494836e+16,
      "budget_used_percent": 11.533964846494836
    },
    {
      "type": "training",
      "description": "Training step 1940",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:53",
      "total_flops_so_far": 1.1539905904546932e+16,
      "budget_used_percent": 11.539905904546933
    },
    {
      "type": "training",
      "description": "Training step 1941",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:53",
      "total_flops_so_far": 1.1545846962599028e+16,
      "budget_used_percent": 11.545846962599029
    },
    {
      "type": "training",
      "description": "Training step 1942",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:54",
      "total_flops_so_far": 1.1551788020651124e+16,
      "budget_used_percent": 11.551788020651124
    },
    {
      "type": "training",
      "description": "Training step 1943",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:54",
      "total_flops_so_far": 1.155772907870322e+16,
      "budget_used_percent": 11.55772907870322
    },
    {
      "type": "training",
      "description": "Training step 1944",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:54",
      "total_flops_so_far": 1.1563670136755316e+16,
      "budget_used_percent": 11.563670136755317
    },
    {
      "type": "training",
      "description": "Training step 1945",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:55",
      "total_flops_so_far": 1.1569611194807412e+16,
      "budget_used_percent": 11.569611194807413
    },
    {
      "type": "training",
      "description": "Training step 1946",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:55",
      "total_flops_so_far": 1.1575552252859508e+16,
      "budget_used_percent": 11.575552252859508
    },
    {
      "type": "training",
      "description": "Training step 1947",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:55",
      "total_flops_so_far": 1.1581493310911604e+16,
      "budget_used_percent": 11.581493310911604
    },
    {
      "type": "training",
      "description": "Training step 1948",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:56",
      "total_flops_so_far": 1.15874343689637e+16,
      "budget_used_percent": 11.5874343689637
    },
    {
      "type": "training",
      "description": "Training step 1949",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:56",
      "total_flops_so_far": 1.1593375427015796e+16,
      "budget_used_percent": 11.593375427015797
    },
    {
      "type": "training",
      "description": "Training step 1950",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:56",
      "total_flops_so_far": 1.1599316485067892e+16,
      "budget_used_percent": 11.599316485067893
    },
    {
      "type": "training",
      "description": "Training step 1951",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:57",
      "total_flops_so_far": 1.1605257543119988e+16,
      "budget_used_percent": 11.605257543119988
    },
    {
      "type": "training",
      "description": "Training step 1952",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:57",
      "total_flops_so_far": 1.1611198601172084e+16,
      "budget_used_percent": 11.611198601172084
    },
    {
      "type": "training",
      "description": "Training step 1953",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:57",
      "total_flops_so_far": 1.161713965922418e+16,
      "budget_used_percent": 11.61713965922418
    },
    {
      "type": "training",
      "description": "Training step 1954",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:58",
      "total_flops_so_far": 1.1623080717276276e+16,
      "budget_used_percent": 11.623080717276276
    },
    {
      "type": "training",
      "description": "Training step 1955",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:58",
      "total_flops_so_far": 1.1629021775328372e+16,
      "budget_used_percent": 11.629021775328372
    },
    {
      "type": "training",
      "description": "Training step 1956",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:58",
      "total_flops_so_far": 1.1634962833380468e+16,
      "budget_used_percent": 11.634962833380467
    },
    {
      "type": "training",
      "description": "Training step 1957",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:59",
      "total_flops_so_far": 1.1640903891432564e+16,
      "budget_used_percent": 11.640903891432563
    },
    {
      "type": "training",
      "description": "Training step 1958",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:59",
      "total_flops_so_far": 1.164684494948466e+16,
      "budget_used_percent": 11.64684494948466
    },
    {
      "type": "training",
      "description": "Training step 1959",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:49:59",
      "total_flops_so_far": 1.1652786007536756e+16,
      "budget_used_percent": 11.652786007536756
    },
    {
      "type": "training",
      "description": "Training step 1960",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:00",
      "total_flops_so_far": 1.1658727065588852e+16,
      "budget_used_percent": 11.658727065588852
    },
    {
      "type": "training",
      "description": "Training step 1961",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:00",
      "total_flops_so_far": 1.1664668123640948e+16,
      "budget_used_percent": 11.664668123640947
    },
    {
      "type": "training",
      "description": "Training step 1962",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:01",
      "total_flops_so_far": 1.1670609181693044e+16,
      "budget_used_percent": 11.670609181693044
    },
    {
      "type": "training",
      "description": "Training step 1963",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:01",
      "total_flops_so_far": 1.167655023974514e+16,
      "budget_used_percent": 11.67655023974514
    },
    {
      "type": "training",
      "description": "Training step 1964",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:01",
      "total_flops_so_far": 1.1682491297797236e+16,
      "budget_used_percent": 11.682491297797236
    },
    {
      "type": "training",
      "description": "Training step 1965",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:02",
      "total_flops_so_far": 1.1688432355849332e+16,
      "budget_used_percent": 11.688432355849333
    },
    {
      "type": "training",
      "description": "Training step 1966",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:02",
      "total_flops_so_far": 1.1694373413901428e+16,
      "budget_used_percent": 11.694373413901427
    },
    {
      "type": "training",
      "description": "Training step 1967",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:02",
      "total_flops_so_far": 1.1700314471953524e+16,
      "budget_used_percent": 11.700314471953524
    },
    {
      "type": "training",
      "description": "Training step 1968",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:03",
      "total_flops_so_far": 1.170625553000562e+16,
      "budget_used_percent": 11.70625553000562
    },
    {
      "type": "training",
      "description": "Training step 1969",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:03",
      "total_flops_so_far": 1.1712196588057716e+16,
      "budget_used_percent": 11.712196588057717
    },
    {
      "type": "training",
      "description": "Training step 1970",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:03",
      "total_flops_so_far": 1.1718137646109812e+16,
      "budget_used_percent": 11.718137646109811
    },
    {
      "type": "training",
      "description": "Training step 1971",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:04",
      "total_flops_so_far": 1.1724078704161908e+16,
      "budget_used_percent": 11.724078704161908
    },
    {
      "type": "training",
      "description": "Training step 1972",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:04",
      "total_flops_so_far": 1.1730019762214004e+16,
      "budget_used_percent": 11.730019762214004
    },
    {
      "type": "training",
      "description": "Training step 1973",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:04",
      "total_flops_so_far": 1.17359608202661e+16,
      "budget_used_percent": 11.7359608202661
    },
    {
      "type": "training",
      "description": "Training step 1974",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:05",
      "total_flops_so_far": 1.1741901878318196e+16,
      "budget_used_percent": 11.741901878318197
    },
    {
      "type": "training",
      "description": "Training step 1975",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:05",
      "total_flops_so_far": 1.1747842936370292e+16,
      "budget_used_percent": 11.747842936370292
    },
    {
      "type": "training",
      "description": "Training step 1976",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:05",
      "total_flops_so_far": 1.1753783994422388e+16,
      "budget_used_percent": 11.753783994422388
    },
    {
      "type": "training",
      "description": "Training step 1977",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:06",
      "total_flops_so_far": 1.1759725052474484e+16,
      "budget_used_percent": 11.759725052474485
    },
    {
      "type": "training",
      "description": "Training step 1978",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:06",
      "total_flops_so_far": 1.176566611052658e+16,
      "budget_used_percent": 11.765666110526581
    },
    {
      "type": "training",
      "description": "Training step 1979",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:06",
      "total_flops_so_far": 1.1771607168578676e+16,
      "budget_used_percent": 11.771607168578676
    },
    {
      "type": "training",
      "description": "Training step 1980",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:07",
      "total_flops_so_far": 1.1777548226630772e+16,
      "budget_used_percent": 11.777548226630772
    },
    {
      "type": "training",
      "description": "Training step 1981",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:07",
      "total_flops_so_far": 1.1783489284682868e+16,
      "budget_used_percent": 11.783489284682867
    },
    {
      "type": "training",
      "description": "Training step 1982",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:07",
      "total_flops_so_far": 1.1789430342734964e+16,
      "budget_used_percent": 11.789430342734963
    },
    {
      "type": "training",
      "description": "Training step 1983",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:08",
      "total_flops_so_far": 1.179537140078706e+16,
      "budget_used_percent": 11.79537140078706
    },
    {
      "type": "training",
      "description": "Training step 1984",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:08",
      "total_flops_so_far": 1.1801312458839156e+16,
      "budget_used_percent": 11.801312458839156
    },
    {
      "type": "training",
      "description": "Training step 1985",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:09",
      "total_flops_so_far": 1.1807253516891252e+16,
      "budget_used_percent": 11.807253516891251
    },
    {
      "type": "training",
      "description": "Training step 1986",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:09",
      "total_flops_so_far": 1.1813194574943348e+16,
      "budget_used_percent": 11.813194574943347
    },
    {
      "type": "training",
      "description": "Training step 1987",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:09",
      "total_flops_so_far": 1.1819135632995444e+16,
      "budget_used_percent": 11.819135632995444
    },
    {
      "type": "training",
      "description": "Training step 1988",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:10",
      "total_flops_so_far": 1.182507669104754e+16,
      "budget_used_percent": 11.82507669104754
    },
    {
      "type": "training",
      "description": "Training step 1989",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:10",
      "total_flops_so_far": 1.1831017749099636e+16,
      "budget_used_percent": 11.831017749099635
    },
    {
      "type": "training",
      "description": "Training step 1990",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:10",
      "total_flops_so_far": 1.1836958807151732e+16,
      "budget_used_percent": 11.836958807151731
    },
    {
      "type": "training",
      "description": "Training step 1991",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:11",
      "total_flops_so_far": 1.1842899865203828e+16,
      "budget_used_percent": 11.842899865203828
    },
    {
      "type": "training",
      "description": "Training step 1992",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:11",
      "total_flops_so_far": 1.1848840923255924e+16,
      "budget_used_percent": 11.848840923255924
    },
    {
      "type": "training",
      "description": "Training step 1993",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:11",
      "total_flops_so_far": 1.185478198130802e+16,
      "budget_used_percent": 11.85478198130802
    },
    {
      "type": "training",
      "description": "Training step 1994",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:12",
      "total_flops_so_far": 1.1860723039360116e+16,
      "budget_used_percent": 11.860723039360115
    },
    {
      "type": "training",
      "description": "Training step 1995",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:12",
      "total_flops_so_far": 1.1866664097412212e+16,
      "budget_used_percent": 11.866664097412212
    },
    {
      "type": "training",
      "description": "Training step 1996",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:12",
      "total_flops_so_far": 1.1872605155464308e+16,
      "budget_used_percent": 11.872605155464308
    },
    {
      "type": "training",
      "description": "Training step 1997",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:13",
      "total_flops_so_far": 1.1878546213516404e+16,
      "budget_used_percent": 11.878546213516405
    },
    {
      "type": "training",
      "description": "Training step 1998",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:13",
      "total_flops_so_far": 1.18844872715685e+16,
      "budget_used_percent": 11.8844872715685
    },
    {
      "type": "training",
      "description": "Training step 1999",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:13",
      "total_flops_so_far": 1.1890428329620596e+16,
      "budget_used_percent": 11.890428329620596
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 552526568288.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:15",
      "total_flops_so_far": 1.1890980856188884e+16,
      "budget_used_percent": 11.890980856188884
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 556130834256.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:17",
      "total_flops_so_far": 1.189153698702314e+16,
      "budget_used_percent": 11.891536987023139
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 554328341080.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:19",
      "total_flops_so_far": 1.189209131536422e+16,
      "budget_used_percent": 11.89209131536422
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 552526568288.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:21",
      "total_flops_so_far": 1.1892643841932508e+16,
      "budget_used_percent": 11.89264384193251
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 555229497620.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:22",
      "total_flops_so_far": 1.1893199071430128e+16,
      "budget_used_percent": 11.893199071430129
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 0",
      "context_len": 600,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 552526568288.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:24",
      "total_flops_so_far": 1.1893751597998416e+16,
      "budget_used_percent": 11.893751597998415
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 1",
      "context_len": 604,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 556130834256.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:26",
      "total_flops_so_far": 1.1894307728832672e+16,
      "budget_used_percent": 11.894307728832672
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 2",
      "context_len": 602,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 554328341080.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:28",
      "total_flops_so_far": 1.1894862057173752e+16,
      "budget_used_percent": 11.894862057173752
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 3",
      "context_len": 600,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 552526568288.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:29",
      "total_flops_so_far": 1.189541458374204e+16,
      "budget_used_percent": 11.89541458374204
    },
    {
      "type": "inference",
      "description": "Validation inference on sequence 4",
      "context_len": 603,
      "gen_len": 48,
      "batch_size": 1,
      "flops": 555229497620.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-23 16:50:31",
      "total_flops_so_far": 1.189596981323966e+16,
      "budget_used_percent": 11.89596981323966
    }
  ],
  "total_flops": 1.189596981323966e+16,
  "budget_used_percent": 11.89596981323966
}